<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Gobbledygook</title>
        <link>https://blog.martinfenner.org/</link>
        <description>Martin Fenner writes about how the internet is changing scholarly communication.</description>
        <lastBuildDate>Sun, 14 Feb 2021 12:32:25 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <copyright>Copyright © 2007-2021 Martin Fenner. Distributed under the terms of the Creative Commons Attribution 4.0 License.</copyright>
        <item>
            <title><![CDATA[Thank you PLOS]]></title>
            <link>https://blog.martinfenner.org/posts/thank-you-plos</link>
            <guid>ae704746-6ec6-4d90-b126-d88206036d2f</guid>
            <pubDate>Wed, 29 Jul 2015 10:52:00 GMT</pubDate>
            <description><![CDATA[Starting next week
[https://www.datacite.org/news/martin-fenner-and-laura-rueda-join-datacite-team.html] 
I will work as the DataCite Technical Director, and I am excited about this new
opportunity. But this is material for another post, here I want to reflect on
the last three years working as Technical Lead for the PLOS Article-Level
Metrics [http://lagotto.io/plos/] project.

It feels much longer than three years, but until May 2012 I worked as medical
oncologist at Hannover Medical School, t]]></description>
            <content:encoded><![CDATA[<p><a href="https://www.datacite.org/news/martin-fenner-and-laura-rueda-join-datacite-team.html">Starting next week</a> I will work as the DataCite Technical Director, and I am excited about this new opportunity. But this is material for another post, here I want to reflect on the last three years working as Technical Lead for the <a href="http://lagotto.io/plos/">PLOS Article-Level Metrics</a> project.</p><p>It feels much longer than three years, but until May 2012 I worked as medical oncologist at Hannover Medical School, treating patient with cancer, attending interdisciplinary tumor boards and helping with clinical trials. It was a very brave move by PLOS to hire me at this point, especially since I <a href="https://blog.martinfenner.org/posts/why-work-where-we-live/">worked remotely</a> from Germany rather than in the San Francisco office. I will be forever thankful to PLOS for giving me this opportunity.</p><p>Two factors probably played a role in this decision: I have been blogging about how the internet is changing scholarly communication since 2007, and since September 2010 I had <a href="http://blogs.plos.org/mfenner/">my blog on the PLOS Blogs Network</a>. I had also visited the PLOS offices in San Francisco, and had met several PLOS people at conferences, including Pete Binfield, Rich Cave, Mark Patterson, Brian Mossop, Jennifer Lin and Liz Allen. I had <a href="http://blogs.plos.org/mfenner/2009/08/15/plos_one_interview_with_peter_binfield/">interviewed</a> Pete Binfield about PLOS ONE and the PLOS Article-Level Metrics project in August 2009, shortly after the project was launched.</p><p>The other factor was the hackathon at the <a href="http://www.nature.com/spoton/event/science-online-london-2011/">2011 Science Online London</a> conference. We were a really small group of people (I remember Jason Hoyt, Victor Henning, Kristi Holmes and Cameron Neylon, Mendeley was hosting the event), but I had the idea to hack the open source PLOS Article-Level Metrics application. This hack turned into <a href="https://blog.martinfenner.org/posts/announcing-sciencecard/">ScienceCard</a>, a version of the PLOS Article-Level Metrics application focussing on people rather than articles, and the application was a finalist for the <a href="http://blog.mendeley.com/highlighting-research/the-top-101-apps-in-the-mendeley-plos-binary-battle/">Mendeley/PLOS API Binary Battle</a>. ScienceCard doesn’t exist anymore, but the concept of organizing metrics around a person lives on in ImpactStory (see my profile <a href="https://impactstory.org/mfenner">here</a>), facilitated by the launch of ORCID in October 2012. More importantly - without me knowing it - ScienceCard demonstrated that I could work with and extend the PLOS Article-Level Metrics code, and I think I was the first person outside of PLOS doing this. Which must have helped when PLOS was looking for a technical lead for the project a few months later.</p><p>In other words, blogging and hacking code can lead to great job opportunities.</p><p>While at PLOS I not only learned a ton of things about article-level metrics and all its challenges and opportunities, but also many basic skills needed in software development. Which is important, as my formal training is in clinical medicine and molecular biology, and doing software development in your free time (which I had done since the 1990s) only gets you so far. Some of the unexpected things I learned:</p><ul><li><strong><strong>Visualizations</strong></strong>: while it was clear that I was expected to generate visualizations for the PLOS Article-Level Metrics data, I didn’t expect this to go so deep, first with R and later with <a href="http://d3js.org/">d3.js</a>. Najko Jahn introduced me to using R to analyze the PLOS data, and I later worked closely with Scott Chamberlain from the <a href="https://ropensci.org/">rOpenSci</a> project to help improve their <a href="https://ropensci.org/tutorials/alm_tutorial.html">alm package</a>. The Javascript work with d3.js started with <a href="http://article-level-metrics.plos.org/alm-workshop-2012/hackathon/#altviz">AlmViz</a> at the 2012 ALM hackathon and later was done in close collaboration with Juan Alperin from the <a href="https://pkp.sfu.ca/">Public Knowledge Project</a>.</li><li><strong><strong>DevOps</strong></strong>: the intersection of software development and system administration. I became a big fan and have spent endless hours learning how to automate the configuration and deployment of servers and other infrastructure.</li><li><strong>O<strong>pen source community building</strong></strong>: again something I was expected to do around the PLOS article-level metrics open source application, but I never expected this to be so challenging and time-consuming, but also rewarding.</li></ul><p>I thank everyone at PLOS who I had the pleasure to work with over the years, in particular Kristen Ratan, Cameron Neylon, Donna Okubo, Mei Yan Leung, Liz Allen, Catriona MacCallum, Matt Hodgkinson, Theo Bloom, Damian Pattinson, Ginny Barbour, Emma Ganley, Roli Roberts, Eric Martens, Susan Au, Matt Willman, Edgar Munoz, Rachel Drysdale, CJ Rayhill, Lisa Siegel, Jennifer Song, Polina Grinbaum, John Bertrand, Mike Baehr, Clark Hartsock, Adam Hyde, and Holly Allen. A very special thanks goes to Jennifer Lin, Rich Cave and John Chodacki who worked with me on a daily basis.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Component DOIs Revisited]]></title>
            <link>https://blog.martinfenner.org/posts/component-dois-revisited</link>
            <guid>0983610b-64df-4dff-96dc-5de43c56a237</guid>
            <pubDate>Thu, 09 Jul 2015 10:19:00 GMT</pubDate>
            <description><![CDATA[Four years ago I wrote a blog post
[https://blog.martinfenner.org/posts/direct-links-to-figures-and-tables-using-component-dois/] 
about component DOIs. It is time to revisit the topic, in particular since our
approach to citing data associated with a publication has changed since 2011.

Component DOIs are explained in the CrossRef Help System
[http://help.crossref.org/components]:

> DOIs may be assigned to items that are part of a journal article, book chapter,
or any other content item. A com]]></description>
            <content:encoded><![CDATA[<p>Four years ago I wrote a <a href="https://blog.martinfenner.org/posts/direct-links-to-figures-and-tables-using-component-dois/">blog post</a> about component DOIs. It is time to revisit the topic, in particular since our approach to citing data associated with a publication has changed since 2011.</p><p>Component DOIs are explained in the <a href="http://help.crossref.org/components">CrossRef Help System</a>:</p><blockquote>DOIs may be assigned to items that are part of a journal article, book chapter, or any other content item. A component would typically be a figure, table, or image which is part of or referred to by the parent item. Assigning a DOI to a component allows direct linking to the component item.</blockquote><p>Component DOIs are DOIs, i.e. persistent identifiers that link directly to the resource in question, e.g. a figure in a publication. The component DOI for a figure in a PLOS paper used in the 2011 post still <a href="http://doi.org/10.1371/journal.pone.0006022.g002">works as expected</a>, despite changes to the URL of the journal landing page.</p><p>The problem with component DOIs is the problem with DOIs in general: there is basic functionality common to all DOIs, and there are additional services specific to subgroups of DOIs. This confuses users - in particular since there is no easy way to immediately see what kind of DOI they have in front of them - and in the case of component DOIs there is one important feature missing.</p><p>DOis are assigned by registration agencies (CrossRef and DataCite are the most relevant ones for scholarly content), and these RAs have built different services around DOIs, e.g. different ways to describe and search the metadata (title, authors, etc.) associated with a DOI. Component DOIs are again different, the most important difference is that in the CrossRef implementation they they are not discoverable by querying the CrossRef system (Feeney, 2010). Component DOIs are also always associated with a parent DOI (for the article, book, etc.). Although this is the expected behaviour, we shouldn’t expect component DOIs to always look like an extension of the parent DOI, as in <code>10.1371/journal.pone.0006022.g002</code> used in the example above.</p><p>In essence, a component DOI is a <strong><strong>DOI light</strong></strong>. We can use them for persistent linking, but we can’t use them for discovery via the CrossRef Metadata Search (and by extension other indexing services). A common use case for component DOIs is supplementary information in a journal article. Content in supplementary information files is already much harder to find than content in the body of an article, using component DOIs instead of regular DOIs makes the content again harder to find.</p><p>All of this might not have been much of an issue when I wrote the 2011 post, but making the data underlying a publication publicly available and discoverable is increasingly becoming something that funders, publishers and institutions expect. Most of these data are not deposited in dedicated data repositories, but in supplementary information files (for PLOS articles published since March 2014 this is true for more than 50% of papers). Using regular DOIs for supplementary information files with proper metadata and proper inclusion in indexing services will make it easier to find, access and reuse these data.</p><p>Unfortunately that still leaves us with the problem that the supplementary information files then will have CrossRef DOIs, whereas data repositories typically use DataCite DOIs, so that we need to search for these datasets in two different places. But that is material for another post.</p><h2 id="references">References</h2><p>Feeney, P. (2010). DOIs for Journals: Linking and Beyond. <em>Information Standards Quarterly</em>, <em>22</em>(3), 27. <a href="https://doi.org/10.3789/isqv22n3.2010.06">https://doi.org/10.3789/isqv22n3.2010.06</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Why should we work where we live?]]></title>
            <link>https://blog.martinfenner.org/posts/why-should-we-work-where-we-live</link>
            <guid>66d00cb0-23c6-4e9d-9561-7dab13ffaa63</guid>
            <pubDate>Sun, 28 Jun 2015 10:23:00 GMT</pubDate>
            <description><![CDATA[At the SciFoo Camp [http://www.digital-science.com/events/scifoo-camp-2015/] 
this weekend Erin McKiernan [https://emckiernan.wordpress.com/] and I moderated
an unconference session on the topic Why should we work where we live? This was
a spontaneous idea after we had talked about this topic on Friday (Erin lives in
Mexico with a job in Canada, I live in Germany and work for an organization in
San Francisco).

We quickly realized that this situation is far from uncommon in the space we
work in ]]></description>
            <content:encoded><![CDATA[<p>At the <a href="http://www.digital-science.com/events/scifoo-camp-2015/">SciFoo Camp</a> this weekend <a href="https://emckiernan.wordpress.com/">Erin McKiernan</a> and I moderated an unconference session on the topic <strong>Why should we work where we live?</strong> This was a spontaneous idea after we had talked about this topic on Friday (Erin lives in Mexico with a job in Canada, I live in Germany and work for an organization in San Francisco).</p><p>We quickly realized that this situation is far from uncommon in the space we work in (science and science communication). Most commonly the reason is compromises we have to make when both partners have to find an adequate job. It can be a big challenge for a couple to find senior jobs in academia in the same city or region, especially outside of academic clusters such as Boston, New York or London.</p><p>The other big reason for work remote is that some research can only happen in special places, for example in high-energy physics, astronomy or the geosciences. And of course there are other flavors of the same situation, e.g. when a principal investigator moves to a new institution and PhD students or postdocs can’t or don’t want to move with him/her. And most academics have to do at least some remote work, since they will spend a good amount of time travelling to conferences or collaboration partners.</p><p>The discussion in the session centered on the social and technical challenges of working remotely. We didn’t have time to go into the legal aspects (e.g. taxes when you work in a different country), or the challenges organizing your personal life, particular difficult when you have children.</p><p>We shared our experience with online collaboration tools, and video conferencing with Skype, Google Hangouts or similar was central to this. Videoconferencing can be a challenge with slow internet connectivity, a situation that luckily is constantly improving.</p><p>Private group chat tool such as <a href="https://www.hipchat.com/">HipChat</a> or <a href="https://slack.com/">Slack</a> are becoming increasingly popular outside the Tech sector and are a great alternative to email. They not only provide a platform for quick messages between two people, but also serve as a backchannel for informal “water cooler” discussions in an organization.</p><p>Another essential category is tools that track your work so that your remote colleagues not only can collaborate with you, but also see the work you are doing. As a supervisor you quickly see the work that was done the past week, a much more reasonable approach than looking at physical presence at work (where people might be doing all kinds of other things and personal productivity varies). Tracking your work is easy if you are a software developer like me and can look at code commited to version control, tickets closed, etc. For research this is more challenging, in particular if the workflow is not digital yet and for example all experiments are documented in a paper notebook. It seems that one requirement for remote work in science is digitalization of your work, but that is a direction we are heading anyway and which has other advantages (e.g. improving reproducibility). If there are no specialized tools for documenting your work available, then a note-taking tool such as <a href="https://www.onenote.com/">OneNote</a> or <a href="https://evernote.com/">Evernote</a> can be helpful. The digitization and automation of work is obviously limited in wet labs that require direct interactions with samples and instruments.</p><p>The social aspects of remote work might be the bigger challenge. There is still a big reluctance in supervisors and administrators to this, assuming that people will only be productive if someone is watching them. This assumption is very short sighted, as what drives PhDs and postdocs to work hard is not supervision, but the intrinsic motivation to accomplish something, in particular in light of the very competitive situation for permanent jobs in academia. The book <a href="http://37signals.com/remote/">Remote</a> by Jason Fried talks about this in great detail in the context of software development, but the same principles apply to work in science. What supervisors and administrators loose in direct oversight they can in attracting talent they would otherwise not get. Remote work only works if supported by the host institution, for example by adapting internal workflows and communications to make remote work the default rather than an exception.</p><p>Remote work is usually more successful and satisfying if combined with physical presence at the workplace. Reasons for this are not only the part of the work that can’t be done remotely, but more importantly the social aspect. How extensive this physical presence is depends on the circumstances. Some level of remote work has become part of almost everyone’s job in science, as it includes working at home in the evenings or on weekends, or work while traveling.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Persistent Identifiers and URLs]]></title>
            <link>https://blog.martinfenner.org/posts/persistent-identifiers-and-urls</link>
            <guid>f89e2997-a9e4-4924-bd70-16231baec3e6</guid>
            <pubDate>Wed, 03 Jun 2015 10:43:00 GMT</pubDate>
            <description><![CDATA[Just like the rest of the internet, much of our scholarly infrastructure is
built around the Hypertext Transfer Protocol (HTTP), increasingly HTTPS for
security, and soon HTTP/2 [https://http2.github.io/] for better performance. In
this infrastructure Universal Resource Locators (URLs) are essential to locate
resources (sic) such as scholarly articles, datasets, researchers,
organizations, or grants. Read this
[http://site.thomsonreuters.com/site/data-identifiers/] recent Thomson Reuters
report ]]></description>
            <content:encoded><![CDATA[<p>Just like the rest of the internet, much of our scholarly infrastructure is built around the Hypertext Transfer Protocol (HTTP), increasingly HTTPS for security, and soon <a href="https://http2.github.io/">HTTP/2</a> for better performance. In this infrastructure Universal Resource Locators (URLs) are essential to locate resources (sic) such as scholarly articles, datasets, researchers, organizations, or grants. Read <a href="http://site.thomsonreuters.com/site/data-identifiers/">this</a> recent Thomson Reuters report for a good recent perspective on this topic. While this works for the most part, there are some issues with URLs - not specific to scholarly content, but particularly import here:</p><ol><li>multiple URLs can point to the same resource</li><li>URLs can be long and look ugly</li><li>URLs can change or break, making it hard or impossible to locate the resource</li><li>we are used to central indexes (or databases) describing these resources, allowing us to do sophisticated queries not possible in a generic web search, e.g. find all publications by author John Doe, published since 2012.</li></ol><p>No. 1 is a problem relevant to all URLs, e.g. web searches or liking/commenting a particular web page. Originally suggested by Google, <a href="https://support.google.com/webmasters/answer/139066?hl=en">Canonical URLs</a> are essential for services such as Facebook or <a href="https://hypothes.is/blog/cross-format-annotation/">Hypothes.is</a>. They have been formalized in <a href="http://tools.ietf.org/html/rfc6596">rfc6596</a> and are commonly used.</p><p>No. 2 can be a problem, in particular if we are not careful in designing appropriate URLs for landing pages (see next paragraph), but rather use something long and unreadable that also includes query parameters, etc. If we have no control over how the URL looks like, we can use URL shortener services such as <a href="https://bitly.com/">bit.ly</a>, which of course have become a common sight on the web. <a href="http://shortdoi.org/">ShortDOIs</a> are an URL shortener for DOIs, but they don’t seem to have gained much traction.</p><p>No. 3 is a particularly important issue, commonly referred to as <strong><strong>link rot</strong></strong> and described extensively for the scholarly literature, e.g. by (Klein et al., 2014). There are several technical solutions to this problem, a common approach is to use a landing page for the resource that will never change (and follows the recommendations by Tim Berners-Lee for <a href="http://www.w3.org/Provider/Style/URI.html">Cool URIs</a>, and then use redirection to point to the current location of the resource. This is easily for changes of the URL path using web server <a href="http://httpd.apache.org/docs/2.4/rewrite/remapping.html">redirect rules</a>. It gets more complicated if the server name also changes, in particular if it is the server holding the landing page. Thinking this through you realize that the only way this can be done on a larger scale is via one or more centralized services that not only provide the technical infrastructure for a central redirection (or resolver) service, but also come with a social contract of rules that everyone submitting URLs to the service has to follow - a major difference to URL shorteners, which don’t solve the link rot problem.</p><p>The above is of course a description of the DOI service provided by CrossRef, DataCite, and others, as well as similar persistent identifier services. Unfortunately some persistent identifier services don’t do the above: they create and use persistent identifiers, but there is no central resolver service that maps these identifiers back to URLs. This breaks the integration with the bigger scholarly infrastructure based on URLs. One common example are nucleotide sequences such as U65091 (Shioda, Fenner, &amp; Isselbacher, 1997), there is no single corresponding URL because the sequence can be found in all three main nucleotide databases: <a href="http://www.ncbi.nlm.nih.gov/nuccore/U65091">http://www.ncbi.nlm.nih.gov/nuccore/U65091</a>, <a href="http://www.ebi.ac.uk/ena/data/view/U65091">http://www.ebi.ac.uk/ena/data/view/U65091</a>, and <a href="http://getentry.ddbj.nig.ac.jp/getentry/na/U65091">http://getentry.ddbj.nig.ac.jp/getentry/na/U65091</a>. It would help to have a central resolver, e.g. <a href="http://nucleotide.org/U65091">http://nucleotide.org/U65091</a> that then redirects to one of the three databases based on geographical location or user preference.</p><p>There are also problems with DOIs. They use the <a href="http://www.handle.net/">Handle</a> system to resolve the identifier to a location, and this system was built in the 1990s as infrastructure <a href="http://www.handle.net/faq.html">independent of</a> URLs or DNS (Domain Name Service), at a time when it wasn’t clear yet that URLs and associated standards would become ubiquitous. I don’t have numbers, but practically all DOIs are of course now resolved to URLs using the <a href="http://www.doi.org/factsheets/DOIProxy.html">DOI proxy server</a> at <a href="http://doi.org/">http://doi.org</a> (preferred) or <a href="http://dx.doi.org/">http://dx.doi.org</a>. One main consequence of this is that DOIs are frequently not written as URLs - e.g. doi:10.5555/12345678 instead of <a href="http://doi.org/10.5555/12345678">http://doi.org/10.5555/12345678</a> - again breaking the integration with the bigger scholarly infrastructure. The CrossRef <a href="http://www.crossref.org/02publishers/doi_display_guidelines.html">DOI display guidelines</a> clearly state that DOIs should be written as URLs in <em>the online environment</em>, which basically is whenever DOIs are used, as PDFs and even Word documents know how to handle URLs. Unfortunately this guideline is still frequently ignored. The above is of course also true for other persistent identifiers using the Handle system, e.g. <a href="http://www.pidconsortium.eu/">EPIC</a>.</p><p>The other problem with the DOI system is that it doesn’t address issue No. 4, i.e. provide a central metadata index for the resources that use the system. This job is left to the DOI registration agencies such as CrossRef and DataCite, who have implemented a central metadata store (e.g. <a href="http://search.crossref.org/">CrossRef</a>, <a href="http://search.datacite.org/">DataCite</a>) in different ways (e.g. using different metadata schemata), or not at all. This means that we have to look in several places to find all DOis associated with author John Doe, published since 2012. Obviously we are used to looking up information in multiple places, but not being able to look up the metadata for a DOI without some extra work (finding out the registration agency for the DOI and then going to the respective metadata store) is a problem. One way around these problems is to use the <a href="http://www.crosscite.org/cn/">DOI Content Negotiation Service</a>.</p><p>Another problem with the DOI system is more a social than a technical issue. Neither CrossRef nor DataCite seem to enfource that DOIs should alsways resolve to URLs when using a computer program. DOI resolution for humans works fine, but computers, e.g. command line tools such as cURL, can run into issues such as requiring cookies, javascript or user input, or permission problems getting to the journal landing page (see <a href="https://martinfenner.ghost.io/2013/10/13/broken-dois">this earlier blog post</a> for some numbers). People seem to forget that a DOI that is not actionable is not really useful, and that scholarly infrastructure is not only used by people, but of course also by automated tools.</p><p>The persistent identifiers used in our scholarly infrastructure would benefit from a clearer focus on the problems they should solve, startin with No. 1-4 above. One problem is that we probably focus too much on the persistence problem, implied also by the term <strong><strong>persistent identifier</strong></strong> or <strong><strong>PID</strong></strong>. What we have neglected is the resolvable problem, i.e. making as easy as possible to get from the persistent identifier to the resource and/or its metadata. Based on the <a href="http://www.knowledge-exchange.info/Default.aspx?ID=462">Den Haag Manifesto</a> and suggested by Todd Vision, we therefore proposed the term <strong><strong>trusted identifier</strong></strong> with the following characteristics in the conceptual model of interoperability for the <a href="http://odin-project.eu/">ODIN Project</a> (ODIN Project, Fenner, Thorisson, Ruiz, &amp; Brase, 2013):</p><ul><li>are unique on a global scale, allowing large numbers of unique identifiers</li><li>resolve as HTTP URI’s with support for content negotiation, and these HTTP URI’s should be persistent.</li><li>come with metadata that describe their most relevant properties, including a minimum set of common metadata elements. A search of metadata elements across all trusted identifiers of that service should be possible.</li><li>are interoperable with other identifiers through metadata elements that describe their relationship.</li><li>are issued and managed by an organization that focuses on that goal as its primary mission, has a sustainable business model and a critical mass of member organizations that have agreed to common procedures and policies, has a governing body, and is committed to using open technologies.</li></ul><p>While not directly relevant for resolving persistent identifiers as URLs, the last point is really important for any persistent identifier infrastructure, described in detail recently by (Bilder, Lin, &amp; Neylon, 2015).</p><p>If I would design a persistent identifier service today (as if we would need yet another persistent identifier service), I would build the system around an URL shortening service that I control. The URLs could look very similar to what we have with DOIs now, e.g. <a href="http://doi.org/10.5555/12345678">http://doi.org/10.5555/12345678</a>, but it would be clear that persistent identifiers are URLs, not something separate. Plus we could take adavantage of all the lessons learned - and possibly even reuse open source code - with URL shorteners, which are much more widely used than scholarly persistent identifiers.</p><p><em>Update 6/4/15: added link to Thomson Reuters <a href="http://site.thomsonreuters.com/site/data-identifiers/">report</a> on identifiers and open data.</em></p><h2 id="references">References</h2><p>Bilder, G., Lin, J., &amp; Neylon, C. (2015). Principles for open scholarly infrastructures-v1. <a href="https://doi.org/10.6084/m9.figshare.1314859">https://doi.org/10.6084/m9.figshare.1314859</a></p><p>Klein, M., Van de Sompel, H., Sanderson, R., Shankar, H., Balakireva, L., Zhou, K., &amp; Tobin, R. (2014). Scholarly context not found: one in five articles suffers from reference rot. <em>PLoS ONE</em>, <em>9</em>(12), e115253. <a href="https://doi.org/10.1371/journal.pone.0115253">https://doi.org/10.1371/journal.pone.0115253</a></p><p>ODIN Project, Fenner, M., Thorisson, G., Ruiz, S., &amp; Brase, J. (2013). D4.1 conceptual model of interoperability. <a href="https://doi.org/10.6084/m9.figshare.824314">https://doi.org/10.6084/m9.figshare.824314</a></p><p>Shioda, T., Fenner, M. H., &amp; Isselbacher, K. J. (1997). Mus musculus melanocyte-specific gene 1 (msg1) mRNA, complete cds. ENA. Retrieved from <a href="http://www.ebi.ac.uk/ena/data/view/U65091">http://www.ebi.ac.uk/ena/data/view/U65091</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Human-readable and machine-readable Persistent Identifiers]]></title>
            <link>https://blog.martinfenner.org/posts/human-readable-and-machine-readable-persistent-identifiers</link>
            <guid>e92e15e3-4449-4e6e-b901-f36aabc6fb36</guid>
            <pubDate>Wed, 27 May 2015 10:45:00 GMT</pubDate>
            <description><![CDATA[Yesterday Julie McMurry and co-authors published a preprint 10 Simple rules for
design, provision, and reuse of persistent identifiers for life science data 
(McMurry et al., 2015). This is an important paper trying to address a
fundamental problem: how can we make persistent identifiers both human-readable
and machine-readable?

Don’t be fooled by the title (used frequently by PLOS Computational Biology
[http://www.ploscollections.org/article/browse/issue/info%3Adoi%2F10.1371%2Fissue.pcol.v03.i]]></description>
            <content:encoded><![CDATA[<p>Yesterday Julie McMurry and co-authors published a preprint <strong><strong>10 Simple rules for design, provision, and reuse of persistent identifiers for life science data</strong></strong> (McMurry et al., 2015). This is an important paper trying to address a fundamental problem: how can we make persistent identifiers both human-readable and machine-readable?</p><p>Don’t be fooled by the title (used frequently by <a href="http://www.ploscollections.org/article/browse/issue/info%3Adoi%2F10.1371%2Fissue.pcol.v03.i01">PLOS Computational Biology</a>) - the paper doesn’t describe simple rules that help the average life sciences researcher. Rather, the paper deals with rather complex issues, and has 36 authors.</p><p>There is general agreement that we need persistent identifiers for scholarly communication, and that also includes life sciences datasets, the focus of the paper. What is less clear is how to express these persistent identifiers. An identifier such as <strong><strong>AB020317</strong></strong> - for the mouse p53 gene - is ambiguous. It is not clear without additional information that this is an identifier for the GenBank nucleotide database, rather than <a href="https://www.flickr.com/photos/alexcycu/8936663973/">something completely different</a>. One common approach to make this identifier unambiguous is to use URIs (Uniform Resource Identifiers), e.g. <a href="http://www.ncbi.nlm.nih.gov/nuccore/AB020317">http://www.ncbi.nlm.nih.gov/nuccore/AB020317</a> in this case.</p><p>The paper doesn’t like this approach, and even states that “URIs are still among the most commonly used and most problematic identifiers in the bio-data ecosystem”. The text also states that “their length makes them unwieldy for humans working with the data or for referencing in publications or other text”, but doesn’t go into any detail why URIs are “problematic identifiers”, or why length is an issue in an online environment.</p><p>This is an important weakness of the paper, because the authors propose an alternative: CURIEs or <strong><strong>compact URIs</strong></strong>. CURIEs were <a href="http://www.w3.org/TR/curie/">proposed</a> by the W3C a few years ago, as a way to make URIs <a href="http://crosstech.crossref.org/2008/12/curies_a_cure_for_uris.html">more human-readable</a>. The idea is simple, we use a namespace in addition to the local identifier, separated by a colon, e.g. <strong><strong><a href="http://www.ebi.ac.uk/ena/data/view/AB020317">Genbank:AB020317</a></strong></strong>.</p><p>This approach has of course been common practice in the life sciences before CURIEs or even the WWW existed, and is still the most common approach how identifiers for life sciences data are referenced in the scholarly literature. Unfortunately there are important problems with CURIEs, most of them mentioned in the paper:</p><ul><li>Persistent identifiers need to be resolvable, without additional information we don’t know what to do with <strong><strong><a href="http://www.ebi.ac.uk/ena/data/view/AB020317">Genbank:AB020317</a></strong></strong>. Most life sciences researchers understand this CURIE, but that might not necessarily be true for less commonly used namespaces</li><li>Namespaces are not necessarily unique, the paper uses <strong><strong>GEO</strong></strong> (which could mean Gene Expression Omnibus or GeoNames Ontology) as an example</li><li>Rule 3 in the paper goes into great detail what characters and patterns should be avoided in local identifiers that are part of a CURIE. It is not clear whether these recommendations will always be followed or how to check them</li><li>CURIEs should follow a pattern (regular expression) so that they can be extracted from a text. We know (Kafkas, Kim, &amp; McEntyre, 2013) that extracting identifiers from journal articles is possible, but difficult</li></ul><p>URIs don’t have the problems listed above: they resolve, are unique, and there is good understanding (and available tools) of how a valid URI should look like and how to extract URIs from text documents. That is why URIs are good representations of persistent identifiers.</p><p>Another problem I have with CURIEs: the idea doesn’t seem to have caught on from the initial work more than five years ago (background reading <a href="http://manu.sporny.org/2011/case-for-curies/">here</a>). I’m not even sure what percentage of persistent identifier experts know about CURIEs.</p><p>My recommendation for life sciences data: express persistent identifiers as URIs. Now that can go into 10 simple rules for the average life sciences researcher.</p><p><em>P.S. This blog uses a tool <a href="http://blog.martinfenner.org/2013/07/02/auto-generating-links-to-data-and-resources/">I wrote two years ago</a> that automatically turns CURIEs in the text into links.</em></p><h2 id="references">References</h2><p>Kafkas, Ş., Kim, J.-H., &amp; McEntyre, J. R. (2013). Database Citation in Full Text Biomedical Articles. <em>PLoS ONE</em>. <a href="http://doi.org/10.1371/journal.pone.0063184">doi:10.1371/journal.pone.0063184</a></p><p>McMurry, J., Blomberg, N., Burdett, T., Conte, N., Dumontier, M., Fellows, D. K., … Parkinson, H. (2015). 10 Simple rules for design, provision, and reuse of persistent identifiers for life science data. <a href="http://doi.org/10.5281/zenodo.18003">doi:10.5281/zenodo.18003</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Introducing the Scholarly Markdown Bundle]]></title>
            <link>https://blog.martinfenner.org/posts/introducing-the-scholarly-markdown-bundle</link>
            <guid>c53ee3a5-a102-4482-be12-09f53966469a</guid>
            <pubDate>Thu, 23 Apr 2015 11:48:00 GMT</pubDate>
            <description><![CDATA[Using Markdown to author scholarly documents is an attractive alternative to the
standard authoring tools Microsoft Word and LaTeX. The feeling shared by many is
that Scholarly Markdown
[http://blog.martinfenner.org/2013/06/17/what-is-scholarly-markdown/] is 80%
there, and that more effort is needed for the remaining 20% - moving markdown
from a niche into the mainstream. What is mainly needed is building tools that
connect the existing tools and ideas, resulting in one or more services
attracti]]></description>
            <content:encoded><![CDATA[<p>Using Markdown to author scholarly documents is an attractive alternative to the standard authoring tools Microsoft Word and LaTeX. The feeling shared by many is that <a href="http://blog.martinfenner.org/2013/06/17/what-is-scholarly-markdown/">Scholarly Markdown</a> is 80% there, and that more effort is needed for the remaining 20% - moving markdown from a niche into the mainstream. What is mainly needed is building tools that connect the existing tools and ideas, resulting in one or more services attractive to a critical number of users. But maybe we also need to rethink the essential parts of Scholarly Markdown. In this post I propose that we expand the concept and define the <em>Scholarly Markdown Bundle</em>.</p><p>It is becoming increasingly clear that scholarly work can’t be adaequately described in a single text document, most commonly the journal article. Not only are there associated metadata, assets such as figures and supplementary information, but also the research data and software needed to produce the work described in the publication. The obvious next step is to think of scholarly work as a collection of objects, most clearly described by Carol Goble and others as <a href="https://researchobject.github.io/specifications/bundle/">Research Object Bundle</a>.</p><p>There will probably never be a single authoring tool and format that pleases everyone. Markdown has particular inherent strengths and weaknesses, complex math or tables will probably always be easier with other formats. The strength of markdown is the simplicity of the format. Some things are hard or impossible to do, but many other things are much simpler. Creating a useful markdown editor is much easier than a word processor reading/writing <code>docx</code> format. Markdown is also a perfect format to <a href="http://blog.martinfenner.org/2014/08/25/using-microsoft-word-with-git/">work with</a> version control systems such as git.</p><p>This low barrier of entry makes markdown perfect to be integrated into many workflows. And we can go one step further than ePub and Research Object Bundle, which use the related Universal Container Format (<a href="https://wikidocs.adobe.com/wiki/display/PDFNAV/Universal+Container+Format">UCF</a>) and ePub Open Container Format (<a href="http://www.idpf.org/epub/301/spec/epub-ocf.html">OCF</a>), respectively. Instead of using zip to compress a folder into a single file we can use git version control instead: git provides the commands <code>git bundle</code> and <code>git archive</code> to compress a project under version control with or without version history. I feel this format is both more powerful So I propose the <em>Scholarly Markdown Bundle</em>:</p><ul><li>a git repository with one or more markdown files, either as a folder, or compressed into a single file using <code>git bundle</code></li><li>a particular flavor or markdown called Scholarly Markdown, and discussed here and elsewhere before</li><li>a <code>citeproc.json</code> file in the root of the project that contains all metadata relevant to the container, including references</li></ul><p>The <code>citeproc.json</code> file is similar to the minimal metadata schema <a href="https://github.com/mbjones/codemeta">codemeta</a> proposed by Matt Jones and others, but is in the format used by Pandoc today. This is <a href="http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/">important</a> because it adds citation parsing support out of the box. The last two points rely on the <a href="http://pandoc.org/">Pandoc</a> document conversion tool, so Scholarly Markdown bundles are really <strong><strong>markdown</strong></strong> + <strong><strong>Pandoc</strong></strong> + <strong><strong>Citeproc/CSL</strong></strong> + <strong><strong>git</strong></strong>. The format is flexible enough to not only describe scholarly articles, but also other kinds of scholarly works, including scientific software managed with git version control. And it integrates nicely with a number of existing workflows, e.g. an R project using RStudio for both code and text (in Rmarkdown). This format should also work for blogs like this one, but I would have to separate the blog posts from the Jekyll site generator code, a direction I suggested in the <a href="http://blog.martinfenner.org/2015/03/23/blogging-beyond-jekyll/">last</a> post.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Blogging Beyond Jekyll]]></title>
            <link>https://blog.martinfenner.org/posts/blogging-beyond-jekyll</link>
            <guid>4bdd1c2c-0b6f-4035-88a2-5c51e9a32055</guid>
            <pubDate>Mon, 23 Mar 2015 11:50:00 GMT</pubDate>
            <description><![CDATA[This blog has been on four different platforms since starting in 2007: a custom
blogging engine and then Movable Type [https://movabletype.org/] on Nature
Network [http://network.nature.com/] 2007-2010, Wordpress on the PLOS Blogs
Network [http://blogs.plos.org/mfenner/] 2010-2013, and the static blogging
engine Jekyll [http://jekyllrb.com/] hosted on Github Pages since 2013. It might
be time for yet another blogging platform change.

The main reason to switch from Wordpress to Jekyll was the co]]></description>
            <content:encoded><![CDATA[<p>This blog has been on four different platforms since starting in 2007: a custom blogging engine and then <a href="https://movabletype.org/">Movable Type</a> on <a href="http://network.nature.com/">Nature Network</a> 2007-2010, Wordpress on the <a href="http://blogs.plos.org/mfenner/">PLOS Blogs Network</a> 2010-2013, and the static blogging engine <a href="http://jekyllrb.com/">Jekyll</a> hosted on Github Pages since 2013. It might be time for yet another blogging platform change.</p><p>The main reason to switch from Wordpress to Jekyll was the concept of a static site generator: write posts in <a href="http://commonmark.org/">markdown format</a>, store them in a Github repository, and then have Jekyll automatically generate the HTML pages hosted on <a href="https://pages.github.com/">Github Pages</a>. The main attraction was the blog posts in markdown format stored in git version control without the need of a database. Jekyll is the glue to make all this work, and I was able to customize Jekyll to my needs, e.g. by using <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a> for the markdown to html conversion.</p><p>While this workflow still makes sense for this blog, there are a number of shortcomings:</p><ul><li>Jekyll needs to rebuild the entire site every time I publish a new post. While this isn’t much of a problem for the size of this blog, it doesn’t scale well for larger sites. And the process is more complex if you use custom jekyll plugins like this blog, as you can’t use the automatic Jekyll pipeline provided by Github (hint: use a Travis continous integration server <a href="http://blog.martinfenner.org/2014/03/10/continuous-publishing/">to build the site</a>)</li><li>the web is moving to increasingly sophisticated javascript frontends, using frameworks such as <a href="https://angularjs.org/">Angular.js</a>, <a href="http://emberjs.com/">Ember.js</a>, or frontend libraries for scholarly documents such as <a href="http://elifesciences.org/elife-news/lens">Lens</a>. While they can be used together with Jekyll, that is not a typical use case.</li><li>the tight integration between the code to generate the website and the content (Wordpress and other blogging engines have the same approach) is not always the best solution, e.g. when you want to want to generate the pages for something that is not a blog (e.g. a <a href="http://book.openingscience.org/">book</a>).</li></ul><p>What could we do instead?</p><blockquote>Build a Javascript frontend where the content is served via an API built around markdown documents, stored in git version control.</blockquote><h3 id="api">API</h3><p>The blog posts are still written in markdown, stored (and version-controlled in a Github repository), but we would now access the content via API. The easiest solution is to use the <a href="https://developer.github.com/v3/repos/contents/">Github Contents API</a> and either do the markdown to html conversion in javascript yourself, or let the Github API do the conversion to HTML for you. Alternatively we could build our own API, e.g. because we want to control the markdown to html conversion, or need additional functionality such as fulltext search. And of course the two approaches can be combined, e.g. via a Github webhook that triggers the markdown to html coversion every time a document is added or updated, and stores the converted documents in the same repo.</p><h3 id="frontend">Frontend</h3><p>The frontend should be written as a one-page javascript application, not requiring a server backend. In contrast to the Jekyll workflow the frontend code doesn’t need to be updated every time we post a blog post. Since this is a very common scenario, there are probably several solutions out there already. Please mention them in the comments if you have suggestions. One candidate is <a href="https://github.com/elifesciences/lens/">Lens</a> mentioned above - a beautiful frontend for scholarly documents. Lens displays documents in the <a href="http://jats.nlm.nih.gov/publishing/tag-library/1.0/index.html">JATS</a> XML format, so your API would have to provide that format.</p><h3 id="conclusions">Conclusions</h3><p>The separation into API and frontend is of course old news. But for blogs this seems to still be a fairly new concept, in particular when combined with a backend using documents stored in git version control rather than in a database. Wordpress added a <a href="https://wordpress.org/plugins/json-rest-api/">REST API Plugin</a> in 2014, and the Ghost blogging framework (which uses a database backend) also seems to <a href="https://trello.com/b/EceUgtCL/ghost-roadmap">go into that general direction</a>. Please ping me if you like the idea and want to contribute, or have implemented something like this already.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Metadata in Microsoft Word documents]]></title>
            <link>https://blog.martinfenner.org/posts/metadata-in-microsoft-word-documents</link>
            <guid>5398a86a-fd4c-439d-9773-34527b79ac7d</guid>
            <pubDate>Fri, 20 Mar 2015 11:52:00 GMT</pubDate>
            <description><![CDATA[Metadata such as author, title, journal or persistent identifier are essential
for scholarly documents, and some of us are spending a significant part of our
time adding or fixing metadata. Unfortunately we sometimes don’t pay enough
attention to the flow of metadata, i.e. we ignore already existing metadata, or
reinvent the wheel in how we describe or store them.

Storing metadata in text-based formats is usually straightforward. This blog
post is written in markdown with a YAML header [http://]]></description>
            <content:encoded><![CDATA[<p>Metadata such as author, title, journal or persistent identifier are essential for scholarly documents, and some of us are spending a significant part of our time adding or fixing metadata. Unfortunately we sometimes don’t pay enough attention to the flow of metadata, i.e. we ignore already existing metadata, or reinvent the wheel in how we describe or store them.</p><p>Storing metadata in text-based formats is usually straightforward. This blog post is written in markdown with a <a href="http://yaml.org/">YAML header</a> - think of YAML as the more human-readable version of JSON - at the beginning of the document:</p><pre><code>---
title: Metadata in Microsoft Word documents
---</code></pre><p>This is then translated into this HTML when the blog post is published:</p><pre><code>&lt;meta property="dc:title" content="Metadata in Microsoft Word documents" /&gt;</code></pre><p>XML is of course a very natural format for metadata, here for example <a href="http://jats.nlm.nih.gov/publishing/tag-library/1.0/index.html">JATS</a> used for scholarly articles:</p><pre><code>&lt;article-title&gt;Metadata in Microsoft Word documents&lt;/article-title&gt;</code></pre><p>Many scholarly documents start out as Microsoft Word documents. And while the <code>docx</code> format introduced by Microsoft in Microsoft Office 2007 <a href="http://officeopenxml.com/">is XML-based</a>, few users are aware of this fact. And probably even fewer users (including myself) ever go to the <code>Properties…</code> settings of a <code>docx</code> document and add a <code>title</code>, <code>keywords</code> or other metadata (the <code>author</code> is usually set automatically).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/IC164149.gif" class="kg-image" alt><figcaption>Microsoft Word 2007 Properties. Image from <a href="https://msdn.microsoft.com/en-us/library/bb308936(v=office.12).aspx">Microsoft Developer Network</a></figcaption></figure><p>This is very unfortunate, as these metadata are very often required, e.g. in a journal article submission, and then need to be collected again, usually either by asking the author to fill out a web form, and/or by extracting the metadata (e.g. title) from the document.</p><p>The best place for metadata is with the document (not <em>in</em> the document), and if the file format (<code>docx</code> in this case) supports it, we should take advantage of this. The main benefit: metadata stay with the text when the document is sent to co-authors via email, or put on a file server, or into Dropbox.</p><p>In the case of <code>docx</code>, the metadata support is actually pretty good, using the standard <a href="http://dublincore.org/">Dublin Core</a>, and storing the metadata in a separate file called <code>core.xml</code>. You can see this file if you unzip your <code>docx</code> file (e.g. after giving it a <code>zip</code> extension). The <code>core.xml</code> file for this blog post (after converting the markdown file to <code>docx</code> using <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a>) looks like this:</p><pre><code>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;cp:coreProperties xmlns:cp="http://schemas.openxmlformats.org/package/2006/metadata/core-properties" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcterms="http://purl.org/dc/terms/" xmlns:dcmitype="http://purl.org/dc/dcmitype/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"&gt;&lt;dc:title&gt;Metadata in Microsoft Word documents&lt;/dc:title&gt;&lt;dc:creator&gt;&lt;/dc:creator&gt;&lt;/cp:coreProperties&gt;</code></pre><p>Because <code>docx</code> is XML, we can read/write this file not only in Microsoft Word, e.g. using macros, but also outside of Microsoft Word, e.g. in workflows that converts <code>docx</code> documents into other formats, or tools that check <code>docx</code> files for required metadata (e.g. by using <a href="https://martinfenner.ghost.io/2015/03/20/metadata-in-microsoft-word-documents/2014/08/18/introducing-rakali/">rakali</a> that I wrote last year). So please encourage authors to use the Microsoft Word <code>Properties…</code> settings, and update existing tools to take advantage of the Dublin Core metadata stored in every <code>docx</code> file.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[First analysis of software metrics]]></title>
            <link>https://blog.martinfenner.org/posts/first-analysis-of-software-metrics</link>
            <guid>f66851ce-9224-4c1b-b880-4fa675d01b05</guid>
            <pubDate>Sat, 28 Feb 2015 11:55:00 GMT</pubDate>
            <description><![CDATA[Last week I wrote about [/2015/02/19/metrics-for-scientific-software/] 
software.lagotto.io [http://software.lagotto.io/], an instance of the lagotto
[https://github.com/articlemetrics/lagotto] open source software collecting
metrics for the about 1,400 software repositories included in Sciencetoolbox
[http://sciencetoolbox.org/]. In this post I want to report the first results
analyzing the data.

If you want to follow along, please go to 
https://github.com/mfenner/software-analysis, this repo]]></description>
            <content:encoded><![CDATA[<p>Last week <a href="https://martinfenner.ghost.io/2015/02/19/metrics-for-scientific-software/">I wrote about</a> <a href="http://software.lagotto.io/">software.lagotto.io</a>, an instance of the <a href="https://github.com/articlemetrics/lagotto">lagotto</a> open source software collecting metrics for the about 1,400 software repositories included in <a href="http://sciencetoolbox.org/">Sciencetoolbox</a>. In this post I want to report the first results analyzing the data.</p><p>If you want to follow along, please go to <a href="https://github.com/mfenner/software-analysis">https://github.com/mfenner/software-analysis</a>, this repository holds all the data, as well as the R code used for analysis. A special thanks goes to <a href="http://scottchamberlain.info/">Scott Chamberlain</a> who greatly helped me by tweaking the <a href="https://github.com/ropensci/alm">alm</a> R package to support URLs instead of DOIs as identifiers.</p><p>The first step in the analysis is to get an overview of the external sources citing or discussing the software package:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/software.lagotto.io_2.png" class="kg-image" alt srcset="https://martinfenner.ghost.io/content/images/size/w600/2021/01/software.lagotto.io_2.png 600w, https://martinfenner.ghost.io/content/images/size/w1000/2021/01/software.lagotto.io_2.png 1000w, https://martinfenner.ghost.io/content/images/2021/01/software.lagotto.io_2.png 1444w" sizes="(min-width: 720px) 720px"><figcaption>Number of software repositories (out of 1,404) with at least one event. Data from <a href="http://software.lagotto.io">software.lagotto.io</a></figcaption></figure><p>This is basically the same figure as in the <a href="https://martinfenner.ghost.io/2015/02/19/metrics-for-scientific-software/">previous post</a>, but with two differences: I have added a <a href="http://www.nature.com/opensearch/">Nature.com OpenSearch</a> data source, and I have found an additional 64 repositories cited in scholarly articles via an Europe PMC fulltext Search that also includes the reference lists (thanks to <a href="http://www.ebi.ac.uk/about/people/johanna-mcentyre">Jo McEntyre</a>).</p><p>I am not sure why we are not picking up any Wikipedia citations, and have to take a closer look. The ORCID source also needs tweaking, and there are some issues with the <a href="http://wordpress.com/" rel="nofollow">Wordpress.com</a> data that I have to look into as well. Citations in the scholarly literature are obviously the most interesting data, and we have three Github repos with more than 25 citations, including <a href="https://github.com/najoshi/sickle">https://github.com/najoshi/sickle</a> with 54 citations. As most repositories in our sample are cited only once if at all, a correlation with Github stars and forks is not useful. Sickle is popular on Github (52 stars and 32 forks), but it is not clear that this activity is correlated to citations (e.g. because there are more citations than stars).</p><p>The vast majority of software repos in this analysis are hosted by Github, so we have the numbers of stars and forks for those. It is interesting, although probably not very surprising, that the number of Github stargazers and forks is highly correlated:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/github_likes_readers-1.png" class="kg-image" alt srcset="https://martinfenner.ghost.io/content/images/size/w600/2021/01/github_likes_readers-1.png 600w, https://martinfenner.ghost.io/content/images/2021/01/github_likes_readers-1.png 672w"><figcaption>Correlation between Github stargazers and forks, log-log scale. Data from <a href="http://software.lagotto.io">software.lagotto.io</a></figcaption></figure><p>We can find Facebook activity (likes, comments or shares) for one third of the repositories. There is a reasonably good correlation between Facebook activity and number of Github forks:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/facebook_github_readers-1.png" class="kg-image" alt srcset="https://martinfenner.ghost.io/content/images/size/w600/2021/01/facebook_github_readers-1.png 600w, https://martinfenner.ghost.io/content/images/2021/01/facebook_github_readers-1.png 672w"><figcaption>Correlation between combined Facebook activity and Github forks, log-log scale. Data from <a href="http://software.lagotto.io">software.lagotto.io</a></figcaption></figure><p>One interesting analysis would be to look at the repositories that have been forked much more often relative to their Facebook activity, e.g. <a href="https://github.com/cloudera/impala">Impala</a> with 1,207 Github stars and 458 forks, but only 5 Facebook shares. One limitation of the analysis is that we are not tracking Facebook (or other social media) activity for all forks of a repo.</p><p>We found Reddit discussions mentioning one of the repositories in 7% of cases. Once we have a larger sample size it would be interesting to correlate this activity with Github stars and forks, similar to what we did for Facebook. By far the most popular repository from our sample on Reddit is <a href="https://github.com/Bitcoin/Bitcoin">Bitcoin</a>, followed by <a href="https://github.com/jquery/jquery">JQuery</a>. Twitter activity is notoriously difficult to collect since Twitter doesn’t keep tweets very long, hence probably the low numbers compared to Facebook and Reddit.</p><p>Feel free to play with the data and scripts provided at <a href="https://github.com/mfenner/software-analysis">https://github.com/mfenner/software-analysis</a>, my next step is probably to include a much larger number of software repositories.</p><p>It has not escaped our notice that the kind of analysis described above could be applied to any software repository, not just scientific software.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Why there is no iTunes for science papers]]></title>
            <link>https://blog.martinfenner.org/posts/why-there-is-no-itunes-for-science-papers</link>
            <guid>b898d6d8-3030-455c-b08a-108215ba2b97</guid>
            <pubDate>Mon, 23 Feb 2015 11:58:00 GMT</pubDate>
            <description><![CDATA[The iTunes Store was opened by Apple in 2003 to sell digital music and other
digital assets. Since 2009 music purchased in the iTunes store is free of
Digital Rights Management (DRM). Apple became the largest music vendor worldwide
in 2010, and by 2013 had sold 25 billion songs.

Scholarly articles are distributed almost exclusively in digital form. While
there is an increasing number of journal articles freely available via green or
gold open access, the majority of them still can only be read ]]></description>
            <content:encoded><![CDATA[<p>The iTunes Store was opened by Apple in 2003 to sell digital music and other digital assets. Since 2009 music purchased in the iTunes store is free of Digital Rights Management (DRM). Apple became the largest music vendor worldwide in 2010, and by 2013 had sold 25 billion songs.</p><p>Scholarly articles are distributed almost exclusively in digital form. While there is an increasing number of journal articles freely available via green or gold open access, the majority of them still can only be read if the reader works at an institution with a subscription to the journal. Many journals also allow the reader to buy a single article of interest, for prices between $10 and more than $30:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/pay_per_view_nature-1.png" class="kg-image" alt srcset="https://martinfenner.ghost.io/content/images/size/w600/2021/01/pay_per_view_nature-1.png 600w, https://martinfenner.ghost.io/content/images/size/w1000/2021/01/pay_per_view_nature-1.png 1000w, https://martinfenner.ghost.io/content/images/2021/01/pay_per_view_nature-1.png 1150w" sizes="(min-width: 720px) 720px"><figcaption>For an article in Nature</figcaption></figure><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/pay_per_view_lancet-1.png" class="kg-image" alt srcset="https://martinfenner.ghost.io/content/images/size/w600/2021/01/pay_per_view_lancet-1.png 600w, https://martinfenner.ghost.io/content/images/2021/01/pay_per_view_lancet-1.png 606w"><figcaption>For an article in Lancet</figcaption></figure><p>There is also a document delivery service provided by libraries, but that option varies considerably by country and in Germany for example means a scanned article as printout rather than the original PDF because of a change in German copyright law a few years ago. There are also the services <a href="https://www.deepdyve.com/">DeepDyve</a> and <a href="https://www.readcube.com/">ReadCube</a>, but again you don’t get the PDF (or only for prices similar to those quoted above), but rather limited access for reading and printing.</p><p>In summary, affordable access to scholarly content by subscription publishers is in a dire state: you either have to work at an academic institution subscribing to the desired journal, get only a crippled version of the article (online viewing only), or pay up to $30 for a single article, which clearly doesn’t scale beyond very occasional use.</p><p>With this background it is obvious that several people have discussed the iTunes Store-like model to sell scholarly articles:</p><ul><li><a href="http://crosstech.crossref.org/2009/09/prc_report_and_ipub_revisited.html">PRC Report and “iPub” revisited</a></li><li><a href="http://www.popsci.com/science/article/2009-10/deepdyve-launches-itunes-science-papers">DeepDyve launches iTunes Store-like service for science papers</a></li><li><a href="http://scienceblogs.com/digitalbio/2012/01/10/could-an-itunes-like-model-wor/">Could an iTunes-like model work with scientific publishing?</a></li><li><a href="http://www.bostonglobe.com/business/2012/10/07/start-readcube-program-uses-itunes-payment-model-for-access-scientific-articles/1UopCX1qfEE3uO2UEzuM7L/story.html">A plan to open up science journals</a></li><li><a href="http://www.newyorker.com/tech/elements/when-the-rebel-alliance-sells-out">When the Rebel Alliance Sells Out</a></li></ul><p>The best already existing platforms to build such as service are reference managers, as most of them have learned now to manage PDF files, and have an online component. ReadCube is offering a pay-per-view option already, Papers, Mendeley, Endnote or others could get into this business.</p><p>One of the big advantages of payments for single articles is transparency, as institutions and users only pay for what they actually use. Price transparency is one of the big problems with the <em>big deal</em> contracts that academic institutions have with publishers - read <a href="http://dx.doi.org/10.1073/pnas.1403006111">this article</a> for more info.</p><p>But rather than becoming the predominant way to pay for digital music, services such as DeepDyve and ReadCube are only playing a marginal role. Why is that so?</p><ul><li>whereas digital music is paid for by the consumer, there is usually a middleman in the form of the library for scholarly articles, which makes the payment process more complex.</li><li>subscription publishers have focused all their efforts on selling big deals with increasing numbers of journals to libraries. Prices of $30 per article are clearly intended to discourage payment for single articles (which could jeopardize journal bundles) rather than offering an earnest payment option.</li><li>Apple was in a strong negotiation position with record labels when starting the iTunes store (the extremely popular iPod, record labels scared of file-sharing platforms such as Napster). No organization is in a similar position with scientific publishers, and services such as ReadCube or Mendeley are handicapped because they are associated with a particular publisher</li></ul><p>Unless several large publishers and/or a smart third-party with enough muscle start an initiative in this space, e.g. by bringing the pay-per-view prices to a reasonable level (e.g. $4.99), we will never see an iTunes Store-like service for scholarly articles, and this currently looks like the most likely outcome. We may have reached the point where it is too late, as most publishers seem to already work towards another payment model: gold open access where the authors pay the article costs.</p><p><em>Update 3/2/15: added link to 2009 CrossTech blog post.</em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Metrics for scientific software]]></title>
            <link>https://blog.martinfenner.org/posts/metrics-for-scientific-software</link>
            <guid>31c00475-7abf-4bc8-830c-4b37bafa0359</guid>
            <pubDate>Thu, 19 Feb 2015 12:00:00 GMT</pubDate>
            <description><![CDATA[One of the challenges of collecting metrics for scholarly outputs is persistent
identifiers. For journal articles the Digital Object Identifier (DOI) has become
the de-facto standard, other popular identifiers are the pmid from PubMed, the
identifiers used by Scopus and Web of Science, and the arxiv ID for ArXiV
preprints.

For other research outputs the picture is less clear. DOIs are also used for
datasets, but so are many other identifiers, in particular in the life sciences.

To collect metr]]></description>
            <content:encoded><![CDATA[<p>One of the challenges of collecting metrics for scholarly outputs is persistent identifiers. For journal articles the Digital Object Identifier (DOI) has become the de-facto standard, other popular identifiers are the pmid from PubMed, the identifiers used by Scopus and Web of Science, and the arxiv ID for ArXiV preprints.</p><p>For other research outputs the picture is less clear. DOIs are also used for datasets, but so are many other identifiers, in particular in the life sciences.</p><p>To collect metrics for research outputs, the requirements are slightly different. We need identifiers understood by the services collecting the metrics, not by the data repository or other service that is holding the research output (the only exception is usage stats, which are generated locally). For many services, in particular social media such as Facebook, Twitter or Reddit, the primary identifier for a resource is a URL. This means that we should have one or more URLs for every research output where we want to track the metrics - typically the publisher or data repository landing page. Since URLs can be messy, Google, Facebook and others have come up with the concept of a <a href="http://googlewebmastercentral.blogspot.de/2009/02/specify-your-canonical.html">canonical URL</a>, and some care should go into constructing proper canonical URLs (see <a href="http://blog.martinfenner.org/2013/10/13/broken-dois/">this blog post</a> for examples of what can go wrong).</p><p>The <a href="http://www.knowledge-exchange.info/Default.aspx?ID=462">Den Haag Manifesto</a> is the result of a <strong><strong>Knowledge Exchange</strong></strong> workshop held in June 2011 and tries to bring Persistent Identifiers and Linked Open Data together. The first principle is very much in line with what I said above:</p><blockquote>Make sure PIDs can be referred to as HTTP URI’s, including support for content negotiation.</blockquote><p>Or, to put this differently: URLs are good enough to start collecting metrics for scholarly outputs. Scientific software is a good example where persistent identifiers are not commonly used (despite efforts such as <a href="https://guides.github.com/activities/citable-code/">this one</a>), but we can still collect many meaningful metrics using the repository URL (and the open source software <a href="https://github.com/articlemetrics/lagotto">lagotto</a>):</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/software.lagotto.io.png" class="kg-image" alt="Number of software repositories (out of 1,404) with at least one event. Data from software.lagotto.io"><figcaption>Number of software repositories (out of 1,404) with at least one event. Data from <a href="http://software.lagotto.io/" style="box-sizing: border-box; background: transparent; color: rgb(52, 152, 219); text-decoration: none;">software.lagotto.io</a></figcaption></figure><p>The last three rows are citations in the scholarly literature found via fulltext search of BioMed Central, Europe PMC and PLOS. URLs (in contrast to persistent identifiers represented as strings and/or numbers) are easy to find, the main limitation is not so much using a URL rather than a DOI, but that scientific software typically is mentioned in the text without appearing in the reference list. This makes it hard to impossible to find articles mentioning the software that are not open access, which unfortunately is still the majority of them.</p><p>We are of course also tracking the discussion of the software in social media, and are collecting the number of stars and forks in Github and Bitbucket. Overall there is quite a lot of activity, here are some examples:</p><ul><li><a href="http://software.lagotto.io/works/url/https://github.com/najoshi/sickle">Windowed Adaptive Trimming for fastq files using quality</a></li><li><a href="https://github.com/lh3/wgsim">Reads simulator</a></li><li><a href="http://software.lagotto.io/works/url/https://github.com/lh3/seqtk">Toolkit for processing sequences in FASTA/Q formats</a></li></ul><p>All three software repos have been cited in the scholarly literature at least ten times. What is missing is infrastructure that tracks the citations of scientific software, so that we can give proper scientific credit to the authors of the software, and can discover other research projects using the same tools. <a href="http://software.lagotto.io/">software.lagotto.io</a> uses a list of software repos collected by Jure Triglav for <a href="http://sciencetoolbox.org/">ScienceToolbox</a>, and a scientific software index is indeed one of the important missing pieces.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Manifests and Reference Lists]]></title>
            <link>https://blog.martinfenner.org/posts/manifests-and-reference-lists</link>
            <guid>c19c5ec6-6e50-47fc-b51a-c5c2a97a9860</guid>
            <pubDate>Thu, 05 Feb 2015 12:03:00 GMT</pubDate>
            <description><![CDATA[Last month at the Force15 conference
[https://www.force11.org/meetings/force2015/pre-conference-meeting-list] in
Oxford Ian Mulvany [https://twitter.com/IanMulvany] and I ran a workshop on 
data
citation support in reference managers
[/2015/01/05/data-citation-support-in-reference-managers/]. The report of that
workshop isn’t done yet, but I can say that it was a success - we now have a
pretty good idea what the problems are and what needs to be done to fix them.
The short summary of the worksho]]></description>
            <content:encoded><![CDATA[<p>Last month at the <a href="https://www.force11.org/meetings/force2015/pre-conference-meeting-list">Force15 conference</a> in Oxford <a href="https://twitter.com/IanMulvany">Ian Mulvany</a> and I ran a workshop on <a href="https://martinfenner.ghost.io/2015/01/05/data-citation-support-in-reference-managers/">data citation support in reference managers</a>. The report of that workshop isn’t done yet, but I can say that it was a success - we now have a pretty good idea what the problems are and what needs to be done to fix them. The short summary of the workshop is in <a href="https://speakerdeck.com/mfenner/workshop-summary-reference-managers-and-data-citation">this</a> slidedeck of the presentation that summarized the workshop for the other Force15 attendees.</p><p>The whole idea of the workshop was to treat data citation as similar as possible to the citation of journal articles, i.e. to allow authors to use the same tools (reference managers) and conventions (citation styles). Putting a data citation into a reference list makes it easier to find that data citation because reference lists contain more metadata, are more structured, and more accessible than data citations in the form of identifiers or links within the body text of the article.</p><p>But I have to admit that there is one problem with reference lists: although there is always some self-citation, reference lists usually contain references to articles (and other resources) created by other people and before the article was published. It feels a little bit odd to put a dataset created by the same group of people and published at the same time into the reference list. And although we could use a separate reference list or highlight the data associated with the article in some other way, what we really want is something slightly different, a manifest file.</p><p>The journal article has been a (mainly) textual document for many centuries not because this is the essence of science communication, but rather because there was no practical way to include all the other information (raw data, tools used for experiments, etc.). Very few of these limitations remain with the digital journal article that we have since the 1990s, but we have for the most part failed to change the format other than going from paper to PDF. One of many examples: figures in publications typically still are has limited as they were decades ago with no way to see the data underlying the figure, options for selecting what data points are shown, or animation for time-based information.</p><p>So what we really care about is the sum of artifacts and resources that together make what Carol Goble and others call research object (Bechhofer et al., 2010), the journal article is an important part, but clearly doesn’t include everthing that is needed to understand and reproduce the work. Reference lists can help with linking to some of the resources not included in the article text, but they typically don’t link to supplementary information or other places where the underlying data are made available, or to the figures of the article. Although some publishers provide navigation tools for readers to get to this information, what we really need is a machine-readable list of all the resources used in an article.</p><p>As it happens, this is exactly what the ePub format for electronic books is doing, as every ePub must include a manifest file that lists all the files that are part of the publication, defined in the <a href="http://www.idpf.org/epub/20/spec/OPF_2.0.1_draft.htm">Open Packaging Format (OPF)</a>. I need to do more research to figure out how to do this with <a href="http://jats.nlm.nih.gov/archiving/tag-library/1.0/index.html">JATS</a>, the standard for scholarly articles, and how to generate something similar to the manifest file when using different formats, e.g. html or markdown. This has to be linked to some of the information we are collecting already, e.g. described in JATS (Beck, 2011), or the <code>relatedIdentifier</code> in the DataCite metadata (Starr, 2014).</p><h2 id="references">References</h2><p>Bechhofer, S., Bechhofer, S., De Roure, D., Gamble, M., Goble, C., &amp; Buchan, I. (2010). Research Objects: Towards Exchange and Reuse of Digital Knowledge. <em>Nature Precedings</em>, (713). <a href="https://doi.org/10.1038/npre.2010.4626.1">https://doi.org/10.1038/npre.2010.4626.1</a></p><p>Beck, J. (2011). NISO Z39.96 The Journal Article Tag Suite (JATS): What Happened to the NLM DTDs? <em>The journal of electronic publishing : JEP</em>, <em>14</em>(1). <a href="https://doi.org/10.3998/3336451.0014.106">https://doi.org/10.3998/3336451.0014.106</a></p><p>Starr, J. (2014). DataCite Metadata Schema for the Publication and Citation of Research Data, 1–38. <a href="https://doi.org/10.5438/0010">https://doi.org/10.5438/0010</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Data Citation Support in Reference Managers]]></title>
            <link>https://blog.martinfenner.org/posts/data-citation-support-in-reference-managers</link>
            <guid>b36a3870-6498-4514-9102-2ee4763ef739</guid>
            <pubDate>Mon, 05 Jan 2015 14:55:00 GMT</pubDate>
            <description><![CDATA[This is the title of an upcoming workshop next Sunday organized by Ian Mulvany
and myself. The workshop is a pre-conference event
[https://www.force11.org/meetings/force2015/pre-conference-meeting-list] of the 
Force15 [https://www.force11.org/meetings/force2015] conference in Oxford. This
blog post summarizes some of the issues and work that needs to be done.

Data Citation is one of the big themes of the Force15 conference, and a lot of
progress has been made, including the Joint Declaration o]]></description>
            <content:encoded><![CDATA[<p>This is the title of an upcoming workshop next Sunday organized by Ian Mulvany and myself. The workshop is a <a href="https://www.force11.org/meetings/force2015/pre-conference-meeting-list">pre-conference event</a> of the <a href="https://www.force11.org/meetings/force2015">Force15</a> conference in Oxford. This blog post summarizes some of the issues and work that needs to be done.</p><p>Data Citation is one of the big themes of the Force15 conference, and a lot of progress has been made, including the Joint Declaration of Data Citation Principles (Data Citation Synthesis Group 2014) that start with the following paragraph on <strong><strong>Importance</strong></strong>:</p><blockquote>Data should be considered legitimate, citable products of research. Data citations should be accorded the same importance in the scholarly record as citations of other research objects, such as publications.</blockquote><p>Convincing researchers, funders, university administrators and others that data citation is important is crucial. But for researchers to actually adopt data citation to the same degree as citations of the scholarly literature, more needs to be done:</p><ul><li>incentives (both carrots and sticks) by funders, institutions, and scholarly societies</li><li>training in data management</li><li>data repositories and other tools and services for the public sharing of data</li><li>tools and services that help citing those datasets</li></ul><p>The focus of the workshop is on the last bullet point, and I would argue that more work still needs to be done here compared to the first three bullet points.</p><h2 id="reference-managers">Reference Managers</h2><p>Researchers use reference managers to handle the citations in the manuscripts they write. This is both a common practice that everybody understands, and there are a plethora of tools - both free and paid - available. Most reference managers were originally built to handle citations of journal articles and maybe books or book chapters, and many of them also help with managing the associated PDF files. In the last 15 years we have seen an dramatic increase of non-article citations in reference lists, mainly to web resources (Klein et al., 2014):</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/02/journal.pone.0115253.g002.png" class="kg-image" alt srcset="https://martinfenner.ghost.io/content/images/size/w600/2021/02/journal.pone.0115253.g002.png 600w, https://martinfenner.ghost.io/content/images/size/w1000/2021/02/journal.pone.0115253.g002.png 1000w, https://martinfenner.ghost.io/content/images/size/w1600/2021/02/journal.pone.0115253.g002.png 1600w, https://martinfenner.ghost.io/content/images/size/w2400/2021/02/journal.pone.0115253.g002.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>From Fig. 2: STM articles and URI references per publication year - Elsevier corpus (Klein et al. 2014)</figcaption></figure><p>References managers have started to adapt to these changes in citation patterns. Similarly they have become better in handling non-textual resources such as slide decks, datasets, or movies. Nobody should type in references by hand in 2015, as reference managers have come up with several ways of importing metadata about citations:</p><ul><li>import references stored in a file using a format such as BibTex or RIS</li><li>import references by talking to an external API</li><li>import references via a bookmarklet that grabs information from the current webpage in the browser</li></ul><p>Endnote and Papers typically use the second approach whereas Mendeley, Zotero (and others) work almost exclusively via bookmarklets (and there are of course combinations of both). Bookmarklets in general work better for web resources and other content that is not indexed in a central service such as Web of Science or Scopus. This is also true for research data, as there are currently few central research data indexing services - the Thomson Reuters <a href="http://wokinfo.com/products_tools/multidisciplinary/dci/">Data Citation Index</a> and <a href="https://www.datacite.org/">DataCite</a> are two examples in this category. But there are also thousands of data repositories, many of them listed in re3data (Pampel et al., 2013).</p><p>The reference manager <a href="https://www.zotero.org/">Zotero</a> has built a large open source ecosystem around bookmarklets (what they call <a href="https://github.com/zotero/translators">web translators</a>), making it straightforward to add support for a new resource, as I have done for <a href="https://github.com/zotero/translators/blob/master/NCBI%20Nucleotide.js">GenBank nucleotide sequence datasets</a> in November after learning the basics in a <a href="http://blog.martinfenner.org/2014/10/17/webinar-on-writing-zotero-translators/">webinar</a> given by Sebastian Karcher, a frequent contributor to Zotero web translators.</p><p>There is no technical reason that reference managers can’t support a broad range of objects to cite, including datasets. And integration of data citation into the reference manager workflow is not only the easiest and most natural way for the author of a paper, but also makes it easier to discover these citations - reference lists are simply much better for that than links in the text, in particular if the content is behind subscription walls. There is a long tradition in the life sciences to put identifiers for genetic sequences used in a publication right into the text (usually into the methods section). Links in the body text are worse than references in reference lists, <a href="http://blog.martinfenner.org/2013/07/02/auto-generating-links-to-data-and-resources/">identifiers without a link</a> are even worse, as they are very hard to find in an automated way (Kafkas, Kim, &amp; McEntyre, 2013).</p><p>Please come to our workshop on Sunday afternoon if you are in Oxford and are interested in this topic. <a href="https://www.eventbrite.com/e/data-citation-support-in-reference-managers-tickets-15136593960">Registration</a> is free, and the workshop will include both presentations about the current state of data citation support in the reference managers Endnote, Papers, Mendeley and Zotero, and work in smaller groups on practical implementations.</p><h2 id="references">References</h2><p>Kafkas, Ş., Kim, J.-H., &amp; McEntyre, J. R. (2013). Database Citation in Full Text Biomedical Articles. <em>PLoS ONE</em>. https://doi.org/<a href="http://doi.org/10.1371/journal.pone.0063184">10.1371/journal.pone.0063184</a></p><p>Klein, M., Van de Sompel, H., Sanderson, R., Shankar, H., Balakireva, L., Zhou, K., &amp; Tobin, R. (2014). Scholarly context not found: one in five articles suffers from reference rot. <em>PLoS ONE</em>, <em>9</em>(12), e115253. https://doi.org/<a href="http://doi.org/10.1371/journal.pone.0115253">10.1371/journal.pone.0115253</a></p><p>Pampel, H., Vierkant, P., Scholze, F., Bertelmann, R., Kindling, M., Klump, J., … Dierolf, U. (2013). Making Research Data Repositories Visible: The re3data.org Registry. <em>PLoS ONE</em>, <em>8</em>(11), e78080. https://doi.org/<a href="http://doi.org/10.1371/journal.pone.0078080">10.1371/journal.pone.0078080</a></p><p>Data Citation Synthesis Group. (2014). <em>Joint Declaration of Data Citation Principles</em>. Force11. <a href="https://doi.org/10.25490/A97F-EGYK">https://doi.org/10.25490/A97F-EGYK</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Webinar on Writing Zotero Translators]]></title>
            <link>https://blog.martinfenner.org/posts/webinar-on-writing-zotero-translators</link>
            <guid>a7068104-63a6-4807-b229-9bf24eac6693</guid>
            <pubDate>Fri, 17 Oct 2014 14:59:00 GMT</pubDate>
            <description><![CDATA[In a blog post two weeks ago
[http://blog.martinfenner.org/2014/10/01/please-keep-it-simple-citations-links-and-references/] 
I argued for the need for reference managers to properly support data citation,
if we want data citation to become a standard activity. I am happy to announce
two events working towards that goal.

November 3rd: Webinar on writing Zotero web translators
Sebastian Karcher
[https://www.zotero.org/blog/community-spotlight-sebastian-karcher/], one of the
most prolific authors]]></description>
            <content:encoded><![CDATA[<p>In a <a href="http://blog.martinfenner.org/2014/10/01/please-keep-it-simple-citations-links-and-references/">blog post two weeks ago</a> I argued for the need for reference managers to properly support data citation, if we want data citation to become a standard activity. I am happy to announce two events working towards that goal.</p><h2 id="november-3rd-webinar-on-writing-zotero-web-translators">November 3rd: Webinar on writing Zotero web translators</h2><p><a href="https://www.zotero.org/blog/community-spotlight-sebastian-karcher/">Sebastian Karcher</a>, one of the most prolific authors of Zotero web translators (and citation styles), has kindly offered to hold an introductory webinar on writing Zotero web translators. These web translators allow Zotero to import metadata about a scholarly work from a variety of places, and new web translators for repositories that hold research data (or software) would go a long way towards making data citation easier for authors. <a href="https://www.zotero.org/support/dev/translators">Web translators</a> are written in Javascript and only basic Javascript knowledge is required. The free webinar takes place on November 3rd on 5 PM UK time (12 PM EST) and the registration form is <a href="http://www.eventbrite.com/e/writing-zotero-translators-webinar-tickets-13768797845">here</a>.</p><h2 id="january-11-force11-pre-conference-workshop-on-data-citation-support-in-reference-managers">January 11: Force11 Pre-Conference workshop on Data Citation Support in Reference Managers</h2><p><a href="https://www.force11.org/meetings/force2015/pre-conference-meeting-list">This workshop</a>, coorganized with Ian Mulvany, will extend the Zotero web translator work to other reference managers, including Papers and Mendeley. This will be a hackathon with the goal to get some things working in these reference managers, but it should also be interesting for others, as we will discuss what is missing to make data citation work in reference managers.</p><p>My personal goal is to learn to write a Zotero web translator in the webinar, and then write a working web translator for the three biological databases ENA, PDB and Uniprot before the January workshop. And hopefully these activities generate enough interest that other people write web translators for their favorite research data database or software repository, and that the proprietary reference managers Papers and Mendeley (and hopefully others) also add support for these data sources.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Let's do an unconference]]></title>
            <link>https://blog.martinfenner.org/posts/lets-do-an-unconference</link>
            <guid>03829097-0a63-4baf-b901-ab3d0b84b631</guid>
            <pubDate>Tue, 14 Oct 2014 15:02:00 GMT</pubDate>
            <description><![CDATA[This year’s SpotOn London conference
[http://blogs.nature.com/ofschemesandmemes/2014/10/09/how-to-get-a-ticket-for-this-years-spoton-london] 
takes place November 14-15 and the registration has opened this Monday. I am
helping organize this conference since 2009, and I again look forward to the
sessions, and - more importantly - the discussions with people in and between
sessions this year.

The name (ScienceBlogging London, ScienceOnline London, SpotOn London), the
location (Royal Institution, ]]></description>
            <content:encoded><![CDATA[<p>This year’s <a href="http://blogs.nature.com/ofschemesandmemes/2014/10/09/how-to-get-a-ticket-for-this-years-spoton-london">SpotOn London conference</a> takes place November 14-15 and the registration has opened this Monday. I am helping organize this conference since 2009, and I again look forward to the sessions, and - more importantly - the discussions with people in and between sessions this year.</p><p>The name (ScienceBlogging London, ScienceOnline London, SpotOn London), the location (Royal Institution, British Library, Wellcome Conference Center), the people organizing (too many to mention, but Nature Publishing Group always at the core), and the fringe events (lots of cool things from <a href="http://blog.mendeley.com/academic-life/science-blogging-2008-part-i/">science tours</a> to <a href="http://www.nature.com/spoton/event/spoton-london-2012-fringe-event-the-story-collider-2/">Story Collider</a>) and the format have always changed slightly over the years, and this year again is a bit different. The biggest change is obviously that <a href="https://twitter.com/louwoodley">Lou Woodley</a> is no longer an organizer (as she announced at last year’s conference), but this is also the first SpotOn conference with a theme:</p><blockquote>The challenges of balancing the public and the private in the digital age</blockquote><p>This is obviously a very broad topic, but nicely encompasses many important issues that we are dealing with in scholarly communication today. The draft program is posted <a href="http://blogs.nature.com/ofschemesandmemes/2014/10/13/spoton-london-2014-draft-programme">here</a>, and I’m helping organize the sessions on <strong><strong>sharing sensitive data</strong></strong> and <strong><strong>open peer review</strong></strong>. More details will follow for all these sessions.</p><p>The second day of the conference will be in unconference (or barcamp) format and the program drafted by the delegates in the morning. This format is popular in the science communications community (I first heard about the project that became my current job at <a href="https://blog.martinfenner.org/posts/i_was_at_scibarcamp_palo_alto/">SciBarCamp in 2009</a>), and SpotOn London has used this format in the first conference in 2008 (and again in 2009):</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/2817131778_336979a571_z.jpg" class="kg-image" alt srcset="https://martinfenner.ghost.io/content/images/size/w600/2021/01/2817131778_336979a571_z.jpg 600w, https://martinfenner.ghost.io/content/images/2021/01/2817131778_336979a571_z.jpg 640w"><figcaption><a href="https://www.flickr.com/photos/dullhunk/2817131778/">Flickr photo by Duncan Hull</a></figcaption></figure><p>For people not familiar with this format the idea of a conference (day) without predetermined topics or speakers sounds scary. As it turns out, the problem is usually not the lack of ideas or people wanting to talk, but rather how to coordinate this in a way that everyone who wants to get involved can do so, and it doesn’t become a discussion among those with the loudest voices (and biggest egos). My experience with SpotOn London and other conferences I enjoyed is that the best sessions are usually those that allow for a good discussion, and not those with the most polished PowerPoint slides. Some suggestions for when you attend an unconference for the first time:</p><ul><li>go to sessions with topics you know little about, but want to learn more</li><li>when suggesting a session, do this together with others</li><li>suggest topics that are focussed and unusual, not the obvious ones we always talk about</li><li>don’t even think about doing a PowerPoint presentation</li><li>when moderating a session, be a good moderator, not a good speaker</li></ul><h2 id="further-reading">Further reading</h2><ul><li><a href="http://en.wikipedia.org/wiki/Science_Foo_Camp">Wikipedia: SciFoo</a></li><li><a href="http://blogs.nature.com/nascent/2007/08/barcamb_cambridge.html">Ian Mulvany: BarCamp Cambridge 2007</a></li><li><a href="http://science.easternblot.net/?p=613">Eva Amsen: SciBarCamp Toronto 2008</a></li><li><a href="https://blog.martinfenner.org/posts/action_points/">Me: BibCamp Hannover 2010</a></li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Please keep it simple: citations, links and references]]></title>
            <link>https://blog.martinfenner.org/posts/please-keep-it-simple-citations-links-and-references</link>
            <guid>83c6aac6-270c-4c70-97bc-93f1969127c1</guid>
            <pubDate>Wed, 01 Oct 2014 15:06:00 GMT</pubDate>
            <description><![CDATA[In my last post [/2014/09/16/please-keep-it-simple/] I wrote about the
importance of keeping things simple in scholarly publishing, today I want to go
into more detail with one example: citations in scholarly documents.

LEGO scientists discuss how they can cite their dataCitations are an essential
part of scholarly documents, and they are summarized in the references section
at the end of the article or book chapter. The problem is that not everything
that is cited in a scholarly document ends ]]></description>
            <content:encoded><![CDATA[<p>In my <a href="https://martinfenner.ghost.io/2014/09/16/please-keep-it-simple/">last post</a> I wrote about the importance of keeping things simple in scholarly publishing, today I want to go into more detail with one example: citations in scholarly documents.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/lego_discussion.jpg" class="kg-image" alt srcset="https://martinfenner.ghost.io/content/images/size/w600/2021/01/lego_discussion.jpg 600w, https://martinfenner.ghost.io/content/images/2021/01/lego_discussion.jpg 700w"><figcaption>LEGO scientists discuss how they can cite their data</figcaption></figure><p>Citations are an essential part of scholarly documents, and they are summarized in the references section at the end of the article or book chapter. The problem is that not everything that is cited in a scholarly document ends up in the references list. Examples of this include:</p><ul><li>web links, e.g. to reagents or other resources</li><li>identifiers for biological databases such as GenBank that are typically included in the text as identifiers or as links</li><li>footnotes with links to external resources</li></ul><p>In other words: we are not consistent in how we cite other content. And this is a problem because we are making it more difficult than necessary for authors, publishers and everyone else to handle these various citation flavors and, more importantly, we are loosing citations along the way. This is a particular problem for data citation, as the seminal 2013 paper by Kafkas et al. (Kafkas, Kim, &amp; McEntyre, 2013) has shown for citations to the three biological databases ENA (European Nucleotide Archive), PDB and Uniprot:</p><ul><li>there is a large numbers of accession numbers in the Open Access subset of PubMed Central (e.g. 160,112 ENA accession numbers for papers published up until June 2012)</li><li>text mining using the <a href="http://www.ebi.ac.uk/webservices/whatizit/">Whatizit</a> tool can retrieve most of these identifiers</li><li>there is only partial overlap between database identifiers annotated by publishers and database identifiers found by text mining</li><li>the overlap is even smaller between papers citing database identifiers, and papers cited in biological databases such as ENA</li><li>the study was limited to Open Access journals, as only for them the fulltext articles could be text mined</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/ena_overlap.png" class="kg-image" alt><figcaption>Comparison between article-to-database and database to citations (Kafkas et al., 2013).</figcaption></figure><p>In other words, even though including identifiers for biological databases has been an accepted community standard that every author and publisher is following for a long time, the proper citation of these identifiers is still often broken. The picture doesn’t seem to be any better for DOIs for datasets: while they are fairly common by now, their use in scholarly articles differs widely from appearance in the references list to links in the materials and methods section to no mention at all.</p><p>There are various ways how this can be fixed (e.g. requiring authors to use biological database identifiers in a consistent way, better text mining tools, opening up subscription content to text mining), but the best solution is the simplest one: every citation in a paper should go into the references list. As an example I have added the ENA mRNA U65091 (Shioda, Fenner, &amp; Isselbacher, 1997) - something I worked on a long time ago - to the references list of this post.</p><h2 id="technology">Technology</h2><p>For this to work, it is essential that reference managers - the software authors use to generate the references list - properly support citations to data, including biological databases. It appears that all major reference managers support datasets as reference type and there is good community agreement what a data citation should look like (<a href="https://www.force11.org/datacitation">Joint Declaration of Data Citation Principles</a>). What is missing is support for easily importing the required metadata for these datasets, and reference managers use two approaches for this:</p><ul><li>query external databases via API and pull in the required metadata (e.g. Papers, Endnote)</li><li>browse to the webpage describing the database entry and import the metadata via bookmarklet/web importer (e.g. Zotero, Mendeley)</li></ul><p>Both approaches require custom code for every database. Whereas many reference managers use Citation Style Language (<a href="http://citationstyles.org/">CSL</a>) as a standard way to format references, no such standard exists for web importers. Which means that every reference manager has to implement this separately, and most of them are not open source software so that the community could help.</p><p>PLOS Labs is holding a <a href="http://www.ploslabs.org/citation-hackathon/">Citation Hackathon</a> on October 18 in their San Francisco office. While I can’t attend in person, I want to contribute to this hackathon in three ways:</p><ul><li>do an evaluation of how the reference managers Papers, Mendeley and Zotero (the three reference managers I use) support citations to the biological databases ENA, PDB and Uniprot and what is missing</li><li>look at existing aggregators of this information (e.g. <a href="http://identifiers.org/">Identifiers.org</a>) to figure out whether the import process can be simplified</li><li>start work on Zotero <a href="https://www.zotero.org/support/dev/translators/coding#web_translators">web translators</a> for these three databases. Zotero is open source software and the web translators are written in Javascript</li></ul><p>Please contact me if you are interested in helping with this, e.g. with a joint virtual hackathon on the 18th (or in person in London or Cambridge on October 15 if that works better).</p><p>Together with <a href="https://twitter.com/IanMulvany">Ian Mulvany</a> from eLife and others from Papers and Mendeley we have also submitted a proposal for a pre-conference workshop/hackathon for the <a href="https://www.force11.org/meetings/force2015">Force2015 Conference</a> in January to work on this for a broader set of databases, which should for example also include software repositories. One question is how we properly handle the citation of large numbers of datasets (1000s to millions), we could for example allow a range of identifiers in a citation. We also need tools to convert identifiers and links in existing documents to proper references, something that we <a href="https://martinfenner.ghost.io/2013/06/24/citations-in-markdown-part-3/">have also discussed on this blog</a>, and we need to discuss how our bibliographic file formats (e.g. bibtex) support these citation types. I <a href="https://martinfenner.ghost.io/2013/07/30/citeproc-yaml-for-bibliographies/">said before</a> that I am a big fan of Citeproc YAML (or JSON, the bibliographic format used by CSL) as bibliographic exchange format, and I know that the PLOS Labs hackathon will also touch on this.</p><h2 id="community">Community</h2><p>While adding reference manager support for a wider range of citations is the first step, the bigger challenge is community support. I don’t think that it is a big mental jump for an author to use the reference manager to cite a biological database rather than typing in the identifier directly in the text (the hard work is registering the identifier in the first place), but this needs support by the community, and in particular journal editors. The important message is that citations should be done in a consistent way and authors don’t have to think about doing this differently for datasets or other relevant resources, or different publishers implementing this differently. I think the paper by Kafkas et al. (2013) clearly shows that our current recommendations for adding identifiers to biological databases is broken, and that we need to do something if we take data citation seriously.</p><p>There are several concerns about adding every citation to the references list. One of them is that we shouldn’t mix citations of scholarly articles with citations of other things, e.g. research data. I would argue that not only are we seeing an increasing number of citations to other resources in reference lists (Yang, Han, Ding, &amp; Song, 2012), but that we can of course group citations by citation type, in addition to the sorting by appearance in the text or last name of first author that is common now.</p><p>Another concern is that citations of datasets are something else that citations to scholarly articles, because the former are typically citations of content created by the same group of people at the time the journal article was also created. I would argue that again we can highlight this by how we display the references, and that I hope that this changes once data citation becomes more widespread.</p><p>What should or should not be cited in a scholarly document is of course a big discussion topic. What I am arguing is that everything that is cited should go into the references list, but that doesn’t change at all what should be cited. Personal communications are an example of something that should probably not be cited and therefore should also not go into the references list.</p><h2 id="references">References</h2><p>Kafkas, Ş., Kim, J.-H., &amp; McEntyre, J. R. (2013). Database Citation in Full Text Biomedical Articles. <em>PLoS ONE</em>. <a href="http://doi.org/10.1371/journal.pone.0063184">https://doi.org/10.1371/journal.pone.0063184</a></p><p>Shioda, T., Fenner, M. H., &amp; Isselbacher, K. J. (1997). Mus musculus melanocyte-specific gene 1 (msg1) mRNA, complete cds. ENA. Retrieved from <a href="http://www.ebi.ac.uk/ena/data/view/U65091">http://www.ebi.ac.uk/ena/data/view/U65091</a></p><p>Yang, S., Han, R., Ding, J., &amp; Song, Y. (2012). The distribution of Web citations. <em>Information Processing &amp; Management</em>, <em>48</em>(4), 779–790. <a href="http://doi.org/10.1016/j.ipm.2011.10.002">https://doi.org/10.1016/j.ipm.2011.10.002</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Please keep it simple]]></title>
            <link>https://blog.martinfenner.org/posts/please-keep-it-simple</link>
            <guid>f0e984e7-01d3-4899-b91f-10839b40d68c</guid>
            <pubDate>Tue, 16 Sep 2014 15:08:00 GMT</pubDate>
            <description><![CDATA[Doing scientific research is becoming increasingly complex, both in terms of the
tools and technologies used, and in the collaboration across disciplines and
locations that is increasingly commonplace. While the way we write up and
publish research is of course also very different from 25 years ago, I would
argue that our tools and services haven’t quite evolved at the same pace.

Of course there are important trends that enable what the Royal Institution 
calls
[https://royalsociety.org/policy/]]></description>
            <content:encoded><![CDATA[<p>Doing scientific research is becoming increasingly complex, both in terms of the tools and technologies used, and in the collaboration across disciplines and locations that is increasingly commonplace. While the way we write up and publish research is of course also very different from 25 years ago, I would argue that our tools and services haven’t quite evolved at the same pace.</p><p>Of course there are important trends that enable what the Royal Institution <a href="https://royalsociety.org/policy/projects/science-public-enterprise/Report/">calls</a> <em>Science as an Open Enterprise</em>, most importantly Open Access, which has broken down many barriers for open collaboration. But very few organizations - commercial or non-profit - see it as their primary mission to make it easier for researchers to collaborate and produce great science, in the sense that everything else is secondary and this focus is really obvious to everyone.</p><p>The following are just some examples that make you laugh hard or cry out loud:</p><ul><li>Finding relevant scholarly content. Why is still so hard?</li><li>Reading a paper. The majority of scholalry content is still not Open Access. It is embarassing how difficult it can be to get the fulltext paper from a subscription journal - too slow, too expensive, and sometimes even crippled in functionality.</li><li>Creating figures for publication. This process is still so painful that it hurts. And publishers often create artificial limitations in file type (TIFF or Postscript) and file size (10 MB??).</li><li>Licenses for scholarly content. We don’t need choice, but a few licenses that everyone understands and that don’t hinder sharing and collaboration</li><li>Secure login. I can use my Facebook or Google login almost everywhere, but as a scholar I have a different username and password at my institution, funder, the various publishers I submit too, and the scholarly services I frequently use?</li><li>Citation styles. Why do we still have at least 3,000 styles?</li></ul><p>Citation styles is a perfect example of a problem that should have been solved as soon as we made the switch to digital publishing. I can travel through half of Europe without showing my passport, and using the same currency, but I need to reformat citations every time I submit to a different journal? And I have to use the same tool for this as my coauthors, as the different reference managers don’t work with each other?</p><p>Too often there are other intentions at work in parallel. While notable, they sometimes stand in conflict with the goal of making a researcher’s life easier. A perfect example is the manuscript submission process. In parallel to the tools getting better and easier to use, the demands on the author seem to be increasing at an even greater rate, both in the data and metadata he or she should provide, and in the work submitting authors are asked to do that traditionally have been done by publishers. Another good example are peer review and evaluation. The proportion of time spent doing research vs. time spent doing administrative work seems to decreasing and not increasing.</p><p>I wish more people and organizations would stand up and state that keeping it simple is their primary goal.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[CommonMark and the Future of Scholarly Markdown]]></title>
            <link>https://blog.martinfenner.org/posts/commonmark-and-the-future-of-scholarly-markdown</link>
            <guid>3acc3fd3-5b94-4486-a9a7-55b578384046</guid>
            <pubDate>Sun, 07 Sep 2014 15:10:00 GMT</pubDate>
            <description><![CDATA[One of the important outcomes of the Markdown for Science
[https://github.com/scholmd/scholmd/wiki] workshop that took place in June 2013
was a decision on a name - Scholarly Markdown - and a brief definition
[https://github.com/scholmd/scholmd/wiki/What-is-Markdown]:

 1. Markdown that supports the requirements of scientific texts
 2. Markdown as format that glues open scientific text resources together
 3. A reference implementation with documentation and tests
 4. A community

In my eyes this]]></description>
            <content:encoded><![CDATA[<p>One of the important outcomes of the <a href="https://github.com/scholmd/scholmd/wiki">Markdown for Science</a> workshop that took place in June 2013 was a decision on a name - <em>Scholarly Markdown</em> - and a brief <a href="https://github.com/scholmd/scholmd/wiki/What-is-Markdown">definition</a>:</p><ol><li>Markdown that supports the requirements of scientific texts</li><li>Markdown as format that glues open scientific text resources together</li><li>A reference implementation with documentation and tests</li><li>A community</li></ol><p>In my eyes this is still a great definition. And this week something important happened that is very relevant for Scholarly Markdown. A small group of people deeply involved in Markdown announced <a href="http://commonmark.org/">Standard Markdown</a>:</p><blockquote>We propose a standard, unambiguous syntax specification for Markdown, along with a suite of comprehensive tests to validate Markdown implementations against this specification. We believe this is necessary, even essential, for the future of Markdown.</blockquote><p>Markdown is in widespread use, but a lack of standard syntax and set of comprehensive tests has hindered the adoption for more complex use cases, the development of cross-platform tools, and the use of markdown as a document interchange format. I am therefore 100% behind this initiative. In particular since this is not just an initiative by large commercial organizations heavily using Markdown such as Stackexchange, Github or Reddit, but that the entire spec and both reference implementations have been written by <a href="http://johnmacfarlane.net/">John MacFarlane</a>, the author of Pandoc, the universal document converter. Not only does Pandoc already support many of the features required by Scholarly Markdown (e.g. math and citations), but John is the Chair of the Department of Philosophy at UC Berkeley.</p><p>Markdown was developed in 2004 by John Gruber, and he <a href="http://daringfireball.net/projects/markdown/license">holds the rights</a> to the name Markdown. He didn’t want this initiative to use the name <strong><strong>Standard Markdown</strong></strong>, so the implementation was <a href="http://blog.codinghorror.com/standard-markdown-is-now-common-markdown/">renamed</a> to <a href="http://commonmark.org/">CommonMark</a>.</p><p>The consequences of all this for Scholarly Markdown?</p><ul><li>CommonMark focusses on the basic features of the language, but once the specification is agreed upon and implemented by a critical mass of tools, it is clear that there needs to be a standardized way to handle extensions of the language. This is both about features used by lots of people such as tables, but also functionality relevant only for scholarly content.</li><li>This brings us one gigantic step closer to a reference implementation and set of tests for Scholarly Markdown, as hopefully Scholarly Markdown can build upon the work by John and the CommonMark team.</li><li>The name Scholarly Markdown might not be a good idea going forward. We should either change the name to align with CommonMark, or we should come up with a totally different name, something that the screenwriters have done with <a href="http://fountain.io/">their</a> implementation of Markdown.</li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Microsoft Word with git]]></title>
            <link>https://blog.martinfenner.org/posts/using-microsoft-word-with-git</link>
            <guid>dcb09e05-e221-4a23-9567-3e72ce02716c</guid>
            <pubDate>Mon, 25 Aug 2014 15:12:00 GMT</pubDate>
            <description><![CDATA[One of the major challenges of writing a journal article is to keep track of
versions - both the different versions you create as the document progresses,
and to merge in the changes made by your collaborators. For most academics
Microsoft Word is the default writing tool, and it is both very good and very
bad in this. Very good because the track changes feature makes it easy to see
what has changed since the last version and who made the changes. Very bad
because this feature is built around ke]]></description>
            <content:encoded><![CDATA[<p>One of the major challenges of writing a journal article is to keep track of versions - both the different versions you create as the document progresses, and to merge in the changes made by your collaborators. For most academics Microsoft Word is the default writing tool, and it is both very good and very bad in this. Very good because the <em>track changes</em> feature makes it easy to see what has changed since the last version and who made the changes. Very bad because this feature is built around keeping everything in a single Word document, so that only one person can work on on a manuscript at a time. This usually means sending manuscripts around by email, and being very careful about not confusing different versions of the document, which requires <a href="http://www.phdcomics.com/comics/archive.php?comicid=1531">creativity</a>.</p><p>Approaches to overcome these challenges are to a) integrate the Word documents into collaboration tools such as Sharepoint and Office 365, or document sharing services such as Dropbox and Google Docs (if you use it just for that), or b) use a different authoring tool altogether. If neither of these approaches works for you, you have a third option: use the version control system <strong><strong>git</strong></strong>.</p><p><a href="http://www.mulvany.net/presentations/WikimaniaOpenScholarshipTalk.slides.html#/3">Git</a> is software that helps with <a href="http://blog.martinfenner.org/2014/08/25/using-microsoft-word-with-git/(http://git-scm.com/book/en/Getting-Started-About-Version-Control)">tracking changes to files</a> so that you can recall specific versions later. Git is typically used to track changes of software source code (and was originally developed by Linus Torvalds for Linux kernel development in 2005), but in fact git can be used for any file where we need to keep track of versions over time. Git is open source software that runs locally on your computer, so please go ahead and start tracking changes to your manuscripts (or other complex documents) with git. Any time you want to store a version, do a <code>git commit</code> with a little description and an optional tag.</p><p>This approach is not ideal, as git was written with source code in text format in mind and for example doesn’t understand what has changed between two revisions of a Word document. Some people will tell you to never store binary files in a version control system, but don’t listen to them. Instead give git a tool to convert Word documents into plain text, and git will then happily tell you what has changed between revisions. Several tools can do this, but since earlier this month Pandoc can read Word documents in <code>docx</code> format. Do the following to have Pandoc convert Word documents into markdown, and to compare the revisions by word and not by line (which makes more sense):</p><pre><code># .gitattributes file in root folder of your git project
*.docx diff=pandoc</code></pre><pre><code># .gitconfig file in your home folder
[diff "pandoc"]
  textconv=pandoc --to=markdown
  prompt = false
[alias]
  wdiff = diff --word-diff=color --unified=1</code></pre><p>You can then use <code>git wdiff important_file.docx</code> to see the changes (with deletions in red and insertions in green), or <code>git log -p --word-diff=color important_file.docx</code> to see all changes over time.</p><p>While you can now track revisions of a Word document and see the changes, you also want to be able to merge different versions of a Word document together so that you and your collaborators can work on the manuscript in parallel. Git can’t merge binary files together, so you need to first convert the Word document into a format that git understands. Just as in the previous example we can use Pandoc for that, with markdown as the textual format. This would also work with HTML or LaTeX, but the simplicity of markdown makes it better suited for version control which doesn’t know about the markup of these formats.</p><p>One of the reasons that git became so popular with software developers is that it is a <strong><strong>distributed version control system</strong></strong> instead of a centralized system such as Subversion. This means that you can track all revisions locally on your computer, but can still synchronize your revisions with another user. <strong><strong>Github</strong></strong> is a popular service that facilitates this synchronization and adds some nice features on top. One way to collaborate with your co-authors is therefore to set up a Github repository (public or private) for your manuscript, and store the master version of the manuscript in markdown format. Instead of working on the master version directly, you would use Pandoc to convert back and forth between this master version in markdown format and your Word document, and would continue to use Word as authoring tool. <a href="http://blog.martinfenner.org/2014/08/18/introducing-rakali/">Rakali</a> is a Pandoc tool that I released last week that can help automate this document conversion. Github has a a number of features to facilitate collaboration that can be used here, e.g. Github issues for discussion and task management.</p><p>There are still a few rough edges in the workflow described above (e.g. only partial support of Word track changes), but it is an interesting approach to collaborate using Microsoft Word and git. And this workflow can of course be enhanced to also include authors that write in LaTeX or one of the other formats that Pandoc supports. One nice side effect of using markdown is that Github will automatically render a webpage for the document (which it will not do for HTML without extra effort).</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Introducing Rakali]]></title>
            <link>https://blog.martinfenner.org/posts/introducing-rakali</link>
            <guid>6d23947a-61d8-40c4-b020-51ba50dac3d1</guid>
            <pubDate>Mon, 18 Aug 2014 15:16:00 GMT</pubDate>
            <description><![CDATA[In July and August I attended the Open Knowledge Festival
[http://2014.okfestival.org/] and Wikimania
[http://wikimania2014.wikimedia.org/wiki/Programme]. At both events I had many
interesting discussions around open source tools for open access scholarly
publishing, and I was part of a panel
[http://wikimania2014.wikimedia.org/wiki/Submissions/The_Full_OA_Stack_-_Open_Access_and_Open_Source] 
on that topic at Wikimania last Sunday. Some of my thoughts were summarized in a
blog post a few weeks ]]></description>
            <content:encoded><![CDATA[<p>In July and August I attended the <a href="http://2014.okfestival.org/">Open Knowledge Festival</a> and <a href="http://wikimania2014.wikimedia.org/wiki/Programme">Wikimania</a>. At both events I had many interesting discussions around open source tools for open access scholarly publishing, and I was part of a <a href="http://wikimania2014.wikimedia.org/wiki/Submissions/The_Full_OA_Stack_-_Open_Access_and_Open_Source">panel</a> on that topic at Wikimania last Sunday. Some of my thoughts were summarized in a blog post a few weeks ago (<a href="http://blog.martinfenner.org/2014/07/18/roads-not-stagecoaches/">Build Roads not Stagecoaches</a>). Today I am happy to announce the first public release of a tool that hopefully contributes to making publishing of open content a bit easier.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/rakali.jpg" class="kg-image" alt="LEGO Researchers are excited that they don’t have to use Microsoft Word for manuscript writing anymore."><figcaption>LEGO Researchers are excited that they don’t have to use Microsoft Word for manuscript writing anymore.</figcaption></figure><p><a href="https://github.com/rakali/rakali.rb">Rakali</a> is a Ruby gem that acts as a wrapper for the <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a> universal document converter. Pandoc is a wonderful tool to convert documents between file formats and supports many file formats and features important for scholarly publishing. Pandoc 1.13 was <a href="http://johnmacfarlane.net/pandoc/releases.html">released</a> last Friday, and one of the most exciting new features is a reader for Microsoft Word (<code>docx</code>) documents. Pandoc has supported the conversion to <code>docx</code> for a while, but now you can use the most popular file format for writing scholarly documents and turn your <code>docx</code> files into HTML, PDF, LateX, markdown, or a number of other formats, making it much easier to collaborate, and to use <code>docx</code> with Pandoc in scholarly publishing workflows. A good example would be arXiv, which <a href="http://arxiv.org/help/submit#text">doesn’t support</a> <code>docx</code> for text submissions. Instead of turning it into PDF the manuscript can now be converted to LaTeX - the preferred file format at arXiv - before submission.</p><p>I built <strong><strong>Rakali</strong></strong> to make it easier to use Pandoc to convert large numbers of documents in an automated way:</p><ul><li>bulk conversion of all files in a folder with a specific extension, e.g. <code>md</code>.</li><li>input via a configuration file in yaml format instead of via the command line</li><li>validation of documents via <a href="http://json-schema.org/">JSON Schema</a>, using the <a href="https://github.com/hoxworth/json-schema">json-schema</a> Ruby gem.</li><li>Logging via <code>stdout</code> and <code>stderr</code>.</li></ul><p>One interesting way to use Rakali and Pandoc is as part of a <a href="http://blog.martinfenner.org/2014/03/10/continuous-publishing/">continuous publishing</a> workflow that involves git and Github, automatically converting all files in a folder when something is pushed to the repository using a continuous integration tool, and exiting the continuous integration run when one of the files doesn’t validate. Look into the Rakali <a href="http://blog.martinfenner.org/2014/08/18/introducing-rakali/%5BRakali%5D(https://github.com/rakali/rakali.rb)">repo</a> for an example.</p><p>The most interesting aspect of Rakali is probably validation via JSON Schema. File conversion with Pandoc is a two-step process, the intermediate format is an internal representation of the document in something called the <a href="http://blog.martinfenner.org/2013/11/17/the-grammar-of-scholarly-communication/">abstract syntax tree</a> or AST. Pandoc makes the AST accessible in JSON format, making it straightforward to manipulate a document before the conversion into the target format with something called <a href="http://johnmacfarlane.net/pandoc/scripting.html">JSON filters</a>.</p><p>Validation of XML documents using <a href="https://en.wikipedia.org/wiki/Document_type_definition">DTDs</a>, <a href="http://relaxng.org/">RELAX NG</a> and other standards has of course been around for a long time, but validation of JSON documents is still relatively new. Since many Pandoc document conversion workflows don’t involve any XML I thought it would make more sense to validate against the AST, and we can use JSON Schema for that. I have started a <a href="https://github.com/rakali/pandoc-schemata">Github repository</a> with schemata for the Pandoc AST, and hope to evolve them over time using Rakali as a tool. An example log output (from the Rakali test suite, stopping file conversion because title and layout metadata are missing) looks like this:</p><pre><code>Validation Error: The property '#/0/unMeta' did not contain a required property of 'title' in schema 9b6d454d-e609-537b-b761-9599b6c01072# for file empty.md
Validation Error: The property '#/0/unMeta' did not contain a required property of 'layout' in schema 9b6d454d-e609-537b-b761-9599b6c01072# for file empty.md
Fatal: Conversion of file empty.md failed.</code></pre><p>As I had argued before, the challenge for building open source tools for science is to <a href="http://blog.martinfenner.org/2014/07/24/dont-reinvent-the-wheel/">not duplicate the work of others</a>, and to integrate well with existing tools by focussing on one aspect and doing that aspect well. It also helps to think about infrastructure (<a href="http://blog.martinfenner.org/2014/07/18/roads-not-stagecoaches/">the roads</a>) instead of only focussing on the user-facing aspects. There are obviously many document conversion tools out there, but Pandoc is certainly one of the oldest and most established ones for scholarly content. Rakali therefore builds on top of Pandoc and tries to play well with other existing tools and services, e.g. by using the UNIX <code>stdout</code> and <code>stderr</code> for reporting, and by using a file-based approach that works well with version control systems such as git. And since Rakali is a Ruby gem it can not only be used as a standalone command line tool, but can also be easily integrated into other Ruby applications.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Visualizing Scholarly Content]]></title>
            <link>https://blog.martinfenner.org/posts/visualizing-scholarly-content</link>
            <guid>cb59dc57-e557-438a-b87d-1d0a26b06fa1</guid>
            <pubDate>Sat, 09 Aug 2014 15:19:00 GMT</pubDate>
            <description><![CDATA[One topic I will cover this Sunday in a presentation on Open Scholarship Tools
[http://wikimania2014.wikimedia.org/wiki/Submissions/Open_Scholarship_Tools_-_a_whirlwind_tour.] 
at Wikimania 2014 together with Ian Mulvany [https://twitter.com/ianmulvany] is
visualization.

Data visualization is all about telling stories with data, something that is of
course not only important for scholarly content, but for example increasingly
common in journalism. This is a big and complex topic, but I hope the]]></description>
            <content:encoded><![CDATA[<p>One topic I will cover this Sunday in a presentation on <a href="http://wikimania2014.wikimedia.org/wiki/Submissions/Open_Scholarship_Tools_-_a_whirlwind_tour.">Open Scholarship Tools</a> at <em>Wikimania 2014</em> together with <a href="https://twitter.com/ianmulvany">Ian Mulvany</a> is visualization.</p><p>Data visualization is all about <em>telling stories with data</em>, something that is of course not only important for scholarly content, but for example increasingly common in journalism. This is a big and complex topic, but I hope the following will get you started.</p><h3 id="learn-the-basics">Learn the Basics</h3><p>Work on visualization of scientific data should start with a good understanding of the best practices and pitfalls of data visualization in general, as well as the specific aspects of visualizing scientific data. The following resources have helped me get started - please suggest more in the comments:</p><ul><li><a href="http://book.flowingdata.com/">Visualize this</a>. A book from Nathan Yau published in 2011. Very helpful in understanding the different ways data can be visualized (e.g. when to use a treemap or what is a <a href="https://en.wikipedia.org/wiki/Choropleth_map">chloropleth map</a>), and an introduction to some tools using practical examples. Nathan’s <a href="http://flowingdata.com/">FlowingData</a> blog is also a great resource.</li><li><a href="https://github.com/mbostock/d3/wiki/Gallery">D3 Gallery</a>. Lots of examples generated using Mike Bostock’s d3.js visualization library. A great inspiration for data visualization on the web, even if you use a different visualization tool.</li><li><a href="http://docs.ggplot2.org/current/index.html">ggplot2</a>. Not only a very popular visualization library for the R language by Hadley Wickham, but also an implementation of Leland Wilkison’s Grammar of Graphics. The <a href="http://www.springer.com/statistics/computational+statistics/book/978-0-387-98140-6">ggplot2 book</a> describes this powerful concept (p. 14):</li></ul><blockquote>In brief, the grammar tells us that a statistical graphic is a mapping from data to aesthetic attributes (colour, shape, size) of geometric objects (points, lines, bars). The plot may also contain statistical transformations of the data and is drawn on a specific coordinate system. Faceting can be used to generate the same plot for different subsets of the dataset. It is the combination of these independent components that make up a graphic.</blockquote><h3 id="learn-to-use-at-least-one-visualization-tool">Learn to use at least one visualization tool</h3><p>There are many great tools available, pick one and learn it well. Some options include:</p><ul><li><strong><strong>Excel</strong></strong>. Probably the most popular tool for data visualization. Commercial, with open source alternatives such as Libre Office.</li><li><strong><strong>R</strong></strong>. Software for statistical computing and analysis. Open source. <a href="http://www.rstudio.com/">RStudio</a> is a powerful user interface for R and a good way to get started.</li><li><a href="http://d3js.org/"><strong>d3.js</strong></a>. A visualization library for Javascript. Open source.</li><li><a href="http://www.graphpad.com/scientific-software/prism/"><strong>Prism</strong></a>. A popular visualization tool among scientists. Commercial.</li><li><a href="https://datawrapper.de/"><strong>Datawrapper</strong></a>. An open source tool and hosted service for data visualization.</li></ul><p>I do most visualizations in either R or d3.js. Both are open source tools with a large community and a rich set of libraries, examples and documentation, and both take a systematic approach to data visualization (see grammar of graphics above).</p><h3 id="learn-data-analysis">Learn data analysis</h3><p>Unless your interest is more in information design - see <a href="http://www.informationisbeautiful.net/">Information is beautiful</a> for some great examples - data visualization is tightly coupled with data analysis. You need to know at least the basics of data analysis to do proper data visualizations, e.g. how to handle wrongly formatted data (e.g. text in a number column), missing values and outliers. The most time-consuming step in my experience is data transformation, i.e. bringing data into the format that you want for the analysis and visualization.</p><p>R, Python and the relatively new <a href="http://julialang.org/">Julia</a> are popular languages for data analysis available as open source. There are many packages for these languages that help with common data analysis problems. One additional advantage of using a proper language over a set of tools cobbled together is that it is easy to automatically recreate a visualization with a new set of data - convenient when you need to analyze and visualize an ongoing experiment that repeatedly produces new data.</p><h3 id="use-a-vector-file-format">Use a vector file format</h3><p>Too many scientific data are still visualized using bitmap graphic formats such as <code>tiff</code>, <code>jpg</code> and <code>png</code>. These formats are not appropriate for charts and only make sense for images. They don’t scale to the screen resolution, and it is <a href="http://blog.f1000research.com/2014/02/20/the-importance-of-providing-data-and-not-just-images-of-data/">very hard to impossible</a> to reuse or even modify them. Use vector graphic formats such as <code>svg</code> or <code>pdf</code> instead. <code>svg</code> is my preferred format because in contrast to <code>pdf</code> it can be embedded into a larger HTML document, and R and d3.js (my preferred visualization tools) can generate this format. <a href="http://www.inkscape.org/">Inkscape</a> is an open source SVG editor, and the commercial <strong><strong>Adobe Illustrator</strong></strong> can be used to manually polish graphics in <code>svg</code> or <code>pdf</code> format, e.g. for journal publication.</p><h3 id="get-inspired-by-great-visualizations">Get inspired by great visualizations</h3><p>At the end of the day data visualization is all about telling a story with data. Unfortunately the current state of affairs for scientific visualizations is very different. In my opinion most graphs and figures used in publications don’t provide the data underlying the visualization (<a href="https://datawrapper.de/">Datawrapper</a> is a great example how this can be done), focus too much on detail rather than the overall message, don’t take advantage of the different chart types available, and are sometimes even misleading. And I’m not even talking about the fact that figures in scholarly papers are <a href="http://dx.doi.org/10.12688/f1000research.4263.1">almost never</a> interactive. It rarely happens that I read a paper and get excited by looking at a figure - if I do it is usually because the underlying data are so compelling that even the simplest visualization will convey the right message.</p><p>We should become more creative with visualizing data in scholalry documents, and one important step towards that goal is publishers accepting more reasonable file formats in manuscript submissions - instead of just <code>tiff</code> and <code>eps</code> (<a href="http://www.plosone.org/static/figureGuidelines#figures">PLOS</a>), or <code>tiff</code>, <code>eps</code> and <code>pdf</code> (<a href="http://www.sciencemag.org/site/feature/contribinfo/prep/prep_revfigs.xhtml#format">Science</a>), and often with a 10 MB file site limit.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What is a DOI?]]></title>
            <link>https://blog.martinfenner.org/posts/what-is-doi</link>
            <guid>2caa27fe-c069-41a0-bed2-931ab552c163</guid>
            <pubDate>Wed, 06 Aug 2014 15:24:00 GMT</pubDate>
            <description><![CDATA[This Sunday Ian Mulvany [https://twitter.com/ianmulvany] and I will do a
presentation on Open Scholarship Tools
[http://wikimania2014.wikimedia.org/wiki/Submissions/Open_Scholarship_Tools_-_a_whirlwind_tour.] 
at Wikimania 2014 in London. From the abstract:

> This presentation will give a broad overview of tools and standards that are
helping with Open Scholarship today.
One of the four broad topics we have picked are digital object identifiers
(DOI)s. We want to introduce them to people new to]]></description>
            <content:encoded><![CDATA[<p>This Sunday <a href="https://twitter.com/ianmulvany">Ian Mulvany</a> and I will do a presentation on <a href="http://wikimania2014.wikimedia.org/wiki/Submissions/Open_Scholarship_Tools_-_a_whirlwind_tour.">Open Scholarship Tools</a> at <em>Wikimania 2014</em> in London. From the abstract:</p><blockquote>This presentation will give a broad overview of tools and standards that are helping with Open Scholarship today.</blockquote><p>One of the four broad topics we have picked are <em>digital object identifiers (DOI)s</em>. We want to introduce them to people new to them, and we want to show some tricks and cool things to people who already now them. Along the way we will also try to debunk some myths about DOIs.</p><h3 id="what-a-doi-looks-like">What a DOI looks like</h3><p>DOIs - or better DOI names - start with a prefix in the format <code>10.x</code> where x is 4-5 digits. The suffix is determined by the organization registering the DOI, and there is no consistent pattern across organizations. The DOI name is typically expressed as a URL (see below). An example DOI would look like: <a href="http://dx.doi.org/10.5555/12345678">http://dx.doi.org/10.5555/12345678</a>. Something in the format <strong><strong>10/hvx</strong></strong> or <a href="http://doi.org/hvx">http://doi.org/hvx</a> is a <a href="http://shortdoi.org/">shortDOI</a>, and <strong><strong>1721.1/26698</strong></strong> or <a href="http://hdl.handle.net/1721.1/26698">http://hdl.handle.net/1721.1/26698</a> is a handle. BTW, all DOIs names are also handles, so <a href="http://hdl.handle.net/10/hvx">http://hdl.handle.net/10/hvx</a> for the shortDOI example above will resolve correctly.</p><h3 id="dois-are-persistent-identifiers">DOIs are persistent identifiers</h3><p>Links to resources can change, particularly over long periods of time. Persistent identifiers are needed so that readers can still find the content we reference in a scholarly work (or anything else where persistent linking is important) 10 or 50 years later. There are many kinds of persistent identifiers, one of the key concepts - and a major difference to URLs - is to separate the identifier for the resource from its location. Persistent identifiers require technical infrastructure to resolve identifiers (DOIs use the <a href="http://www.handle.net/">Handle System</a>) and to allow long-term archiving of resources. DOI registration agencies such as DataCite or CrossRef are required to provide that persistence. Other persistent identifier schemes besides DOIs include <a href="http://en.wikipedia.org/wiki/PURL">persistent uniform resource locators (PURLs)</a> and <a href="http://en.wikipedia.org/wiki/Archival_Resource_Key">Archival Resource Keys (ARKs)</a>.</p><h3 id="dois-have-attached-metadata">DOIs have attached metadata</h3><p>All DOIs have metadata attached to them. The metadata are supplied by the resource provider, e.g. publisher, and exposed in services run by registration agencies, for example metadata search and content negotiation (see below). There is a minimal set of required metadata for every DOI, but beyond that, different registration agencies will use different metadata schemata, and most metadata are optional. Metadata are important to build centralized discovery services, making it easier to describe a resource, e.g. journal article citing another article. Some of the more recent additions to metadata schemata include persistent identifiers for people (<a href="http://orcid.org/">ORCID</a>) and funding agencies (<a href="http://www.crossref.org/fundref/">FundRef</a>), and license information. The following API call will retrieve all publications registered with CrossRef that use a <a href="http://creativecommons.org/licenses/by/3.0/deed.en_US">Creative Commons Attribution license</a> (and where this information has been provided by the publisher):</p><pre><code>http://api.crossref.org/funders/10.13039/100000001/works?filter=license.url:http://creativecommons.org/licenses/by/3.0/deed.en_US</code></pre><h3 id="dois-support-link-tracking">DOIs support link tracking</h3><p>Links to other resources are an important part of the metadata, and describing all citations between a large number scholarly documents is a task that can only really be accomplished by a central resource. To solve this very problem DOIs were invented and the CrossRef organization started around 15 years ago.</p><h3 id="not-every-doi-is-the-same">Not every DOI is the same</h3><p>The DOI system <a href="http://www.doi.org/doi_handbook/1_Introduction.html">originated from an initiative by scholarly publishers</a> (first announced at the Frankfurt Book Fair in 1997), with citation linking of journal articles its first application. This citation linking system is managed by <a href="http://www.crossref.org/">CrossRef</a>, a non-profit member organization of scholarly publishers, and <a href="http://search.crossref.org/help/status">more than half</a> of the about <a href="http://www.doi.org/faq.html">100 million DOIs</a> that have been assigned to date are managed by them.</p><p>But many DOIs are assigned by one of the other 8 <a href="http://www.doi.org/RA_Coverage.html">registration agencies</a>. You probably know <a href="http://www.datacite.org/">DataCite</a>, but did you know that the <a href="http://publications.europa.eu/index_en.htm">Publications Office of the European Union (OP)</a> and the <a href="http://www.eidr.org/">Entertainment Identifier Registry (EIDR)</a> also assign DOIs? The distinction is important, because some of the functionality is a service of the registration agency - metadata search for example is offered by CrossRef (<a href="http://search.crossref.org/">http://search.crossref.org</a>) and DataCite (<a href="http://search.datacite.org/">http://search.datacite.org</a>), but you can’t search for a DataCite DOI in the CrossRef metadata search. There is an API to find out the registration agency behind a DOI so that you know what services to expect:</p><pre><code>http://api.crossref.org/works/10.6084/m9.figshare.821213/agency

{
  "status": "ok",
  "message-type": "work-agency",
  "message-version": "1.0.0",
  "message": {
    "DOI": "10.6084/m9.figshare.821213",
    "agency": {
      "id": "datacite",
      "label": "DataCite"
    }
  }
}</code></pre><h3 id="dois-are-urls">DOIs are URLs</h3><p><a href="http://www.doi.org/faq.html">DOI names may be expressed as URLs (URIs) through a HTTP proxy server</a> - e.g. <a href="http://dx.doi.org/10.5555/12345679">http://dx.doi.org/10.5555/12345679</a>, and this is how DOIs are typically resolved. For this reason the <a href="http://www.crossref.org/02publishers/doi_display_guidelines.htm">CrossRef DOI Display Guidelines</a> recommend that <em>CrossRef DOIs should always be displayed as permanent URLs in the online environment</em>. Because DOIs can be expressed as URLs, they also have their features:</p><h4 id="special-characters">Special characters</h4><p>Because DOIs can be expressed as URLs, DOIs <a href="http://www.crossref.org/02publishers/15doi_guidelines.html">should only include characters allowed in URLs</a>, something that wasn’t always true in the past and can cause problems, e.g. when using SICIs (<a href="https://en.wikipedia.org/wiki/Serial_Item_and_Contribution_Identifier">Serial Item and Contribution Identifier</a>), an extension of the ISSN for journals:</p><pre><code>10.4567/0361-9230(1997)42:&lt;OaEoSR&gt;2.0.TX;2-B</code></pre><h4 id="content-negotiation">Content negotiation</h4><p>The DOI resolver at <em>doi.org</em> (or <em>dx.doi.org</em>) normally resolves to the resource location, e.g. a landing page at a publisher website. Requests that are not for content type <code>text/html</code> are redirected to the registration agency metadata service (currently for CrossRef, DataCite and mEDRA DOIs). Using <a href="http://www.crosscite.org/cn/">content negotiation</a>, we can ask the metadata service to send us the metadata in a format we specify (e.g. Citeproc JSON, bibtex or even a formatted citation in one of thousands of citation styles) instead of getting redirected to the resource. This is a great way to collect bibliographic information, e.g. to format citations for a manuscript. In theory we could also use content negotiation to get a particular representation of a resource, e.g. <code>application/pdf</code> for a PDF of a paper or <code>text/csv</code> for a dataset in CSV format. This is not widely support and I don’t know the details of the implementation in the DOI resolver, but you can try this (content negotation is easier with the command line than with a browser):</p><pre><code>curl -LH "Accept: application/pdf" http://dx.doi.org/10.7717/peerj.500 &gt;peerj.500.pdf</code></pre><p>This will save the PDF of the 500th PeerJ paper published last week.</p><h4 id="fragment-identifiers">Fragment identifiers</h4><p>As discussed in <a href="http://blog.martinfenner.org/2014/08/02/fragment-identifiers-and-dois/">my last blog post</a>, we can use frament identifiers to subsections of a document with DOIs, e.g. <a href="http://dx.doi.org/10.1371/journal.pone.0103437#s2">http://dx.doi.org/10.1371/journal.pone.0103437#s2</a> or <a href="http://doi.org/10.5446/12780#t=00:20,00:27">http://doi.org/10.5446/12780#t=00:20,00:27</a>, just as we can with every other URL. This is a nice way to directly link to a specific document section, e.g. when discussing a paper on Twitter. Fragment identifiers are implemented by the client (typically web browser) and depend on the document type, but for DOIs that resolve to fulltext HTML documents they can add granularity to the DOI without much effort.</p><h4 id="queries">Queries</h4><p>URLs obviously support queries, but that is a feature I haven’t yet seen with DOIs. Queries would allow interesting features, partly overlapping with what is possible with fragment identifiers and content negotiation, e.g. <code><a href="http://dx.doi.org/10.7717/peerj.500?format=pdf" rel="nofollow">http://dx.doi.org/10.7717/peerj.500?format=pdf</a></code>. II hope to find out more until Sunday.</p><h3 id="outlook">Outlook</h3><p>My biggest wish? Make DOIs more machine-readable. They are primarily intended for human users, enabling them to find the content associated with a DOI. But they sometimes don’t work as well as they could with automated tools, one example are the <a href="http://blog.martinfenner.org/2013/10/13/broken-dois/">challenges automatically resolving a DOI</a> that I described in a blog post last year. Thinking about DOIs as URLs - and using them this way - is the right direction.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Fragment Identifiers and DOIs]]></title>
            <link>https://blog.martinfenner.org/posts/fragment-identifiers-and-dois</link>
            <guid>33c7abb9-7959-4fcf-9368-56882a8d5076</guid>
            <pubDate>Sat, 02 Aug 2014 15:26:00 GMT</pubDate>
            <description><![CDATA[Before all our content turned digital, we already used page numbers to describe
a specific section of a book or longer document, with older manuscripts using
the folio [https://en.wikipedia.org/wiki/Folio] before that. Page numbers have
transitioned to electronic books with readers such as the Kindle supporting
them
eventually
[http://pogue.blogs.nytimes.com/2011/02/08/page-numbers-for-kindle-books-an-imperfect-solution/?_php=true&_type=blogs&_r=0]
.

Image by Al Silonov from Wikimedia Commons
[]]></description>
            <content:encoded><![CDATA[<p>Before all our content turned digital, we already used <strong><strong>page numbers</strong></strong> to describe a specific section of a book or longer document, with older manuscripts using the <a href="https://en.wikipedia.org/wiki/Folio">folio</a> before that. Page numbers have transitioned to electronic books with readers such as the Kindle <a href="http://pogue.blogs.nytimes.com/2011/02/08/page-numbers-for-kindle-books-an-imperfect-solution/?_php=true&amp;_type=blogs&amp;_r=0">supporting them eventually</a>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/Folio_-number-.jpg" class="kg-image" alt srcset="https://martinfenner.ghost.io/content/images/size/w600/2021/01/Folio_-number-.jpg 600w, https://martinfenner.ghost.io/content/images/2021/01/Folio_-number-.jpg 746w" sizes="(min-width: 720px) 720px"><figcaption>Image by Al Silonov from <a href="http://commons.wikimedia.org/wiki/File:Folio_(number).jpg">Wikimedia Commons</a>. This file is licensed under the <a href="http://creativecommons.org/licenses/by-sa/3.0/deed.en">Creative Commons Attribution-Share Alike 3.0 Unported</a> license.</figcaption></figure><p>For content on the web we can use the <code>#</code> fragment identifier, e.g. <a href="https://en.wikipedia.org/wiki/Fragment_identifier#Proposals">https://en.wikipedia.org/wiki/Fragment_identifier#Proposals</a> to navigate to a specific section of a web page. How the linking to this fragment is handled, depends on the <strong><strong>MIME</strong></strong> type of the document, and will for example be done differently for a text page than a video - YouTube understands minutes and seconds into a video as fragment identifier, e.g. <a href="https://www.youtube.com/watch?v=0UNRZEsLxKc#t=54m52s">https://www.youtube.com/watch?v=0UNRZEsLxKc#t=54m52s</a>. Fragment identifiers are not only helpful to link to a subsection of a document, but of course also for navigation within a document.</p><p>All this is of course very relevant to scholarly content, which is usually much more structured, with most journal articles following the <a href="https://en.wikipedia.org/wiki/IMRAD">IMRAD</a> - introduction, methods, results, and discussion - format, usually with additional sections such as abstract, references, etc. One approach to link to figures and tables within a scholarly articles is using <a href="https://blog.martinfenner.org/posts/direct-links-to-figures-and-tables-using-component-dois/">component DOIs</a>, e.g. specific DOIs for parts of a larger document. The publisher <strong><strong>PLOS</strong></strong> has been using them for a long time, and the <a href="https://martinfenner.ghost.io/2014/07/24/dont-reinvent-the-wheel/">number of component DOIs is rising</a>, but most scholarly journal articles don’t use component DOIs. And whereas component DOIs are a great concept for content such as figures (allowing us to describe the MIME type and other relevant metadata), they are probably not the best tool to link to a section or paragraph of a scholarly document.</p><p>As it turns out, we already have a tool for that, as the DOI proxy server gracefully forwards fragment identifiers (how did I miss this?). We can therefore use a DOI with a fragment identifier to</p><ul><li>Results section: <a href="http://doi.org/10.1371/journal.pone.0103437#s2">http://doi.org/10.1371/journal.pone.0103437#s2</a></li><li>Specific reference: <a href="http://doi.org/10.12688/f1000research.4263.1#ref-7">http://doi.org/10.12688/f1000research.4263.1#ref-7</a></li><li>Decision letter: <a href="http://doi.org/10.7554/eLife.00471#decision-letter">http://doi.org/10.7554/eLife.00471#decision-letter</a></li></ul><p>Obviously this only works if the DOI is resolved to the full-text of a resource, and not a landing page. And how the fragment identifiers are named and implemented is up to the publisher, and the DOI resolver has no information about them. These specific links are particularly nice for discussions of a paper, whether it is on Twitter or in a discussion forum. It appears that at least the Twitter link shortener keeps the fragment identifier, the link to the eLife decision letter is shortened to <a href="http://t.co/URWaYmGHnY">http://t.co/URWaYmGHnY</a>. This kind of linking works particularly well if the publisher is using a fine-grained system of fragment identifiers, the publisher PeerJ for example allows links to a specific paragraph - e.g. <a href="http://doi.org/10.7717/peerj.500#p-15">http://doi.org/10.7717/peerj.500#p-15</a> - and allows users to <a href="http://blog.peerj.com/post/62886292466/peerj-questions-a-new-way-to-never-publish-forget">ask a question</a> right next to that section.</p><p>The examples above all use MIME type <code>text/html</code>, as this is what the example DOIs resolve to by default. I don’t if and how publishers have implemented fragment identifiers for other formats such as PDF or ePub, and what happens if you combine fragment identifiers with <a href="http://www.crosscite.org/cn/">content negotiation</a>. The shortDOI service works with fragment identifiers as well: <a href="http://doi.org/pxd#decision-letter">http://doi.org/pxd#decision-letter</a>. Another interesting question would be how fragment identifiers are handled for datasets. Typically separate DOIs are assigned for multiple related datasets, but there could also be a place for fragment identifiers as well, e.g. to specify a subset via a date range. The solution depends again on the content type, and the popular <code>text/csv</code> is unfortunately not well suited for this, whereas JSON – using <a href="http://tools.ietf.org/html/rfc6901">JSON Pointer</a> – would work well.</p><p><em>Update 8/2/14: <a href="https://twitter.com/ldodds">Leigh Dodds</a> points out that handling the fragment identifier is up to the client and the fragment identifier is not sent to the server. Acrobat reader for example supports the <code>#page=</code> fragment identifier. He also mentions that there is a <a href="http://tools.ietf.org/html/rfc7111">RFC7111</a> for fragment identifiers for the text/csv media type - browsers in the future might support something like <code><a href="http://example.com/data.csv#row=5-7" rel="nofollow">http://example.com/data.csv#row=5-7</a></code>.</em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[One Ring to Rule them All]]></title>
            <link>https://blog.martinfenner.org/posts/one-ring-to-rule-them-all</link>
            <guid>57359c3c-929d-492b-957f-03e3ca0ef30b</guid>
            <pubDate>Wed, 30 Jul 2014 15:29:00 GMT</pubDate>
            <description><![CDATA[> One Ring to rule them all, One Ring to find them, One Ring to bring them all and
in the darkness bind them.
Yesterday 60 years ago the first volume of the Lord of the Rings trilogy by 
J.R.R. Tolkien was published. The quote above obviously doesn’t quiet apply to
scholarly publishing, but one recurring theme that I have often heard in the
last few years is that of a need for a canonical digital document format for
scholarly content that rules all other formats.

Document formats in scholarly P]]></description>
            <content:encoded><![CDATA[<blockquote>One Ring to rule them all, One Ring to find them, One Ring to bring them all and in the darkness bind them.</blockquote><p>Yesterday 60 years ago the first volume of the <em>Lord of the Rings</em> trilogy by <em>J.R.R. Tolkien</em> was published. The quote above obviously doesn’t quiet apply to scholarly publishing, but one recurring theme that I have often heard in the last few years is that of a need for a canonical digital document format for scholarly content that rules all other formats.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/rings.png" class="kg-image" alt="Document formats in scholarly Publishing"><figcaption>Document formats in scholarly Publishing</figcaption></figure><p>A few years ago almost everyone you would have said that <code>xml</code> is that format, with the NLM Archiving and Interchange Tag Suite - which has evolved into <a href="http://jats.nlm.nih.gov/publishing/">JATS</a> - probably the most commonly used Document Type Definition (DTD). <code>xml</code> does many things really well, but also has important shortcomings, most importantly that it is probably not a good format for authors (and don’t tell me that <code>docx</code> and <code>odt</code> are XML-based). We therefore don’t really expect authors to submit manuscripts in JATS <code>xml</code>, but rather convert documents into this format after a manuscript has been accepted for publication. This conversion step is often time-consuming and labor-intensive.</p><p>More recently <code>html</code> has become the most interesting candidate for a canonical scholarly document format. The big advantage over <code>xml</code> is that <code>html</code> - or at least <code>html5</code> which is most popular today - is an attractive format for online authoring tools (that is why <code>html</code> is listed both as input and output format) The downside of this flexibility is that it is much harder to embed structure and metadata into <code>html5</code> compared to <code>xml</code>. There are initiatives such as <a href="http://schema.org/">schema.org</a> and <a href="https://github.com/oreillymedia/HTMLBook">HTMLBook</a> that hope to change that, but we aren’t quiet there yet.</p><p>Or maybe we should learn from Tolkien and give up on the idea of a canonical document format and rather spend our energy on building tools that make it easier to transition from one format to another. <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a> is such as tool, but can’t do all the required conversions, e.g. it can’t yet use <code>docx</code> as input. The downside here is that every file conversion runs the risk of loosing important information. But the increase in flexibility hopefully outweights these shortcomings.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Don't Reinvent the Wheel]]></title>
            <link>https://blog.martinfenner.org/posts/dont-reinvent-the-wheel</link>
            <guid>f93ac598-d96f-40ec-85ec-e917098eccfe</guid>
            <pubDate>Thu, 24 Jul 2014 15:33:00 GMT</pubDate>
            <description><![CDATA[In a post last week
[http://blog.martinfenner.org/2014/07/18/roads-not-stagecoaches/] I talked about
roads and stagecoaches, and how work on scholarly infrastructure can often be
more important than building customer-facing apps. One important aspect of that
infrastructure work is to not duplicate efforts.

Image by Cocoabiscuit on Flickr
[https://www.flickr.com/photos/jfgallery/5673321593/]A good example is
information (or metadata) about scholarly publications. I am the technical lead
for the ]]></description>
            <content:encoded><![CDATA[<p>In a <a href="http://blog.martinfenner.org/2014/07/18/roads-not-stagecoaches/">post last week</a> I talked about roads and stagecoaches, and how work on scholarly infrastructure can often be more important than building customer-facing apps. One important aspect of that infrastructure work is to not duplicate efforts.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/02/5673321593_e6a7faa36d_w.jpg" class="kg-image" alt><figcaption>Image by Cocoabiscuit <a href="https://www.flickr.com/photos/jfgallery/5673321593/">on Flickr</a></figcaption></figure><p>A good example is information (or metadata) about scholarly publications. I am the technical lead for the open source <a href="http://articlemetrics.github.io/">article-level metrics (ALM) software</a>. This software can be used in different ways, but most people use it for tracking the metrics of scholarly articles, with articles that have DOIs issued by CrossRef. The ALM software needs three pieces of information for every article: <strong><strong>DOI</strong></strong>, <strong><strong>publication date</strong></strong>, and <strong><strong>title</strong></strong>. This information can be entered via a web interface, but that is of course not very practical for adding dozens or hundreds of articles at a time. The ALM software has therefore long supported the import of multiple articles via a text file and the command line.</p><p>This approach is working fine for the ALM software <a href="http://articlemetrics.github.io/plos/">running at PLOS since 2009</a>, but is for example a problem if the ALM software runs as a service for multiple publishers. A more flexible approach is to provide an API to upload articles, and I’ve <a href="http://articlemetrics.github.io/docs/api/">added an API</a> for creating, updating and deleting articles in January 2014.</p><p>While the API is an improvement, it still requires the integration into a number of possibly very different publisher workflows, and you have to deal with setting up the permissions, e.g. so that publisher A can’t delete an article from publisher B.</p><p>The next ALM release (3.3) will therefore add a third approach to importing articles: using the <a href="http://api.crossref.org/">CrossRef API</a> to look up article information. Article-level metrics is about tracking already published works, so we really only care about articles that have DOIs registered with CrossRef and are therefore published. ALM is now talking to a single API, and this makes it much easier to do this for a number of publishers without writing custom code. Since ALM is an open source application already used by several publishers that aspect is important. And because we are importing, we have don’t have to worry about permissions. The only requirement is that CrossRef has the correct article information, and has this information as soon as possible after publication.</p><p>At this point I have a confession to make: I regularly use other CrossRef APIs, but wasn’t aware of <strong><strong>api.crossref.org</strong></strong> until fairly recently. That is sort of understandable since the reference platform was deployed only September last year. The documentation to get you started is on <a href="https://github.com/CrossRef/rest-api-doc/blob/master/rest_api.md">Github</a> and the version history shows frequent API updates (now at v22). The API will return all kinds of information, e.g.</p><ul><li>how many articles has publisher x published in 2012</li><li>percentage of DOIs of publisher Y that include at least one ORCID identifier</li><li>list all books with a Creative Commons CC-BY license that were published this year</li></ul><p>Funder (via FundRef) information is also included, but is still incomplete. Another interesting result is the number of <a href="http://blogs.plos.org/mfenner/2011/03/26/direct-links-to-figures-and-tables-using-component-dois/">component DOIs</a> (DOIs for figures, tables or other parts of a document) per year:</p><!--kg-card-begin: html--><iframe src="https://cf.datawrapper.de/Ze7et/1/" width="640" height="480"></iframe><!--kg-card-end: html--><p>For my specific use case I wanted an API call that returns all articles published by PLOS (or any other publisher) in the last day which I can then run regularly. To get all DOIs from a specific publisher, use their CrossRef member ID - DOI prefixes don’t work, as publishers can own more than one DOI prefix. To make this task a little easier I built a CrossRef member search interface into the ALM application:</p><figure class="kg-card kg-image-card"><img src="https://martinfenner.ghost.io/content/images/2021/02/crossref_api.png" class="kg-image" alt srcset="https://martinfenner.ghost.io/content/images/size/w600/2021/02/crossref_api.png 600w, https://martinfenner.ghost.io/content/images/size/w1000/2021/02/crossref_api.png 1000w, https://martinfenner.ghost.io/content/images/2021/02/crossref_api.png 1170w" sizes="(min-width: 720px) 720px"></figure><p>We can filter API responses by publication date, but it is a better idea to use the update date, as it is possible that the metadata have changed, e.g. a correction of the title. We also want to increase the number of results per page (using the <code>rows</code> parameter). The final API call for all DOIs updated by PLOS since the beginning of the week would be</p><pre><code>http://api.crossref.org/members/340/works?filter=from-update-date:2014-07-21,until-update-date:2014-07-24&amp;rows=1000</code></pre><p>The next step is of course to parse the JSON of the API response, and you will notice that CrossRef is using <a href="http://gsl-nagoya-u.net/http/pub/citeproc-doc.html">Citeproc JSON</a>. This is a standard JSON format for bibliographic information used internally by several reference managers for citation styles, but increasingly also by APIs and other places where you encounter bibliographic information.</p><p>Citeproc JSON is helpful for one particular problem with CrossRef metadata: the exact publication date for an article is not always known, and CrossRef (and similarly DataCite) only requires the publication year. Citeproc JSON can nicely handle partial dates, e.g. year-month:</p><pre><code>issued: {
  date-parts: [
    [
      2014,
      7
    ]
  ]
},</code></pre><p>I think that a similar approach will work for many other systems that require bibliographic information about scholarly content with CrossRef DOIs. If are not already using <strong><strong>api.crossref.org</strong></strong>, consider integrating with it, I find the API fast, well documented, easy to use - and CrossRef is very responsive to feedback. As you can always wish for more, I would like to see the following: fix the problem were some journal articles are missing the publication date (a required field, even if only the year), and consider adding the canonical URL to the article metadata (which ALM currently has to look up itself, and which is needed to track social media coverage of an article).</p><p><em>Update July 24, 2014: added chart with number of component DOIs per year</em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Build Roads not Stagecoaches]]></title>
            <link>https://blog.martinfenner.org/posts/build-roads-not-stagecoaches</link>
            <guid>068cf033-cea0-469d-9292-0316cfc0f661</guid>
            <pubDate>Fri, 18 Jul 2014 15:36:00 GMT</pubDate>
            <description><![CDATA[I attended the Open Knowledge Festival [http://2014.okfestival.org/] this week
and I had a blast. For three days (I also attended the fringe event csv,conf
[http://csvconf.com/] on Tuesday) I listed to wonderful presentations and was
involved in great discussions - both within sessions, but more importantly all
the informal discussions between and after sessions.

Of all the things that were discussed I want to pick one theme that resonated in
particular with me. It surfaced in many places, but ]]></description>
            <content:encoded><![CDATA[<p>I attended the <a href="http://2014.okfestival.org/">Open Knowledge Festival</a> this week and I had a blast. For three days (I also attended the fringe event <a href="http://csvconf.com/">csv,conf</a> on Tuesday) I listed to wonderful presentations and was involved in great discussions - both within sessions, but more importantly all the informal discussions between and after sessions.</p><figure class="kg-card kg-image-card"><img src="https://martinfenner.ghost.io/images/okfest-2014-logo.png" class="kg-image" alt></figure><p>Of all the things that were discussed I want to pick one theme that resonated in particular with me. It surfaced in many places, but was articulated particularly well by <a href="https://twitter.com/erichysen">Eric Hysen</a> - who heads the <a href="http://www.google.com/elections/ed/us">Google Politics &amp; Elections Group</a> - in his keynote yesterday (starting at 54:52, but please also watch the keynote by Neelie Kroes, Vice-President of the European Commission):</p><figure class="kg-card kg-embed-card"><iframe width="356" height="200" src="https://www.youtube.com/embed/0UNRZEsLxKc?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><p>In his keynote he described how travel from Cambridge to London in the 18th and early 19th century improved mainly as a result of better roads, made possible by changes in how these roads are financed. Translated to today, he urged the audience to think more about the infrastructure and less about the end products:</p><blockquote>Ecosystems, not apps – Eric Hysen</blockquote><p>On Tuesday at <a href="http://csvconf.com/#nickstenning">csv,conf</a>, <a href="https://twitter.com/nickstenning">Nick Stenning</a> - Technical Director of the Open Knowledge Foundation - talked about <a href="http://dataprotocols.org/data-packages/">data packages</a>, an evolving standard to describe data that are passed around betwen different systems. He used the metaphor of containers, and how they have dramatically changed the transportation of goods in the last 50 years. He <a href="https://github.com/nickstenning/put-it-in-a-box">argued</a> that the cost of shipping was in large part determined by the cost of loading and unloading, and the container has dramatically changed that equation. We are in a very similar situation with datasets, where most of the time is spent translating between different formats, joining things together that use different names for the same thing, etc.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/640px-Hafenarbeiter_bei_der_Verladung_von_Sackgut_-_MS_Rothenstein_NDL-_Port_Sudan_1960.png" class="kg-image" alt srcset="https://martinfenner.ghost.io/content/images/size/w600/2021/01/640px-Hafenarbeiter_bei_der_Verladung_von_Sackgut_-_MS_Rothenstein_NDL-_Port_Sudan_1960.png 600w, https://martinfenner.ghost.io/content/images/2021/01/640px-Hafenarbeiter_bei_der_Verladung_von_Sackgut_-_MS_Rothenstein_NDL-_Port_Sudan_1960.png 640w"><figcaption><a href="https://commons.wikimedia.org/wiki/File:Hafenarbeiter_bei_der_Verladung_von_Sackgut_-_MS_Rothenstein_NDL,_Port_Sudan_1960.png">Wikimedia Commons</a> image used in <a href="https://github.com/nickstenning/put-it-in-a-box">Nick Stelling's presentation</a></figcaption></figure><p>What the two presentations have in common is not only that they link the building of an open digital infrastructure to important transforming events in the history of transportation, but also the emphasis on the building blocks rather than the finished product. When I thought more about this I realized that these building blocks are exactly the projects I get most excited about, i.e. projects that develop standards or provide APIs or libraries. Some examples would be</p><ul><li><a href="http://orcid.org/">ORCID</a>: unique identifiers for scholarly authors</li><li><a href="http://citationstyles.org/">Citation Style Language</a>: a language to describe the formatting of citations and bibliographies</li><li><a href="https://github.com/jgm/pandoc">Pandoc</a>: a universal document converter</li><li><a href="http://ropensci.org/">rOpenSci</a>: packages for the statistical programming language R to access data repositories</li><li><a href="http://www.niso.org/topics/tl/altmetrics_initiative/">NISO Alternative Assessment Metrics</a>: standards and best practices for novel scholarly metrics</li><li><a href="http://www.re3data.org/">re3data</a>: a registry of research data repositories</li><li><a href="http://creativecommons.org/">Creative Commons</a>: copyright licenses for creative works</li><li><a href="https://github.com/articlemetrics/alm">ALM</a>: software to collect comprehensive information about the discussion of scholarly articles on the web</li></ul><p>This list doesn’t include all the generic software needed to build open science tools, with <strong>git</strong> being a perfect example. The last project is obviously the project I have been working on the past two years for PLOS, but I have tried to support the other projects mentioned in various ways from small code contributions to promotion via this blog and presentations, or direct work in these projects. But strangely enough, I haven’t really realized this until now.</p><p>Not surprisingly infrastructure, servers, libraries and other building blocks are exactly the areas where open source software has been most successful so far, and this is of course a core part of the UNIX philosophy of building parts that work well together rather than big monolithic programs that do everything.</p><h2 id="next">Next</h2><p>We need more <strong><strong>Open Science Infrastructure</strong></strong> and it is the stuff that I really care about. I think we need to better support those projects that build these essential building blocks via advice, cooperation, promotion, and financial support. I am willing to help with that effort, and I have started to think how I can best contribute.</p><p>On the other hand there are many great open science projects that don’t fall in this category, maybe even the majority of them. I wish them good luck, but I would advice them to think more about infrastructure, and whether there is a small area where they can focus on. It still amazes me how successful projects such as <strong><strong>Citation Style Language</strong></strong> and <strong><strong>Pandoc</strong></strong> have been with no or almost no funding and a very small core group of people doing the majority of the work. One critical ingredient is the total focus on a very specific problem that is both important and can be solved with specific actions. Too many open science projects want to solve too many problems at once, try to solve the exact same problems that many other parallel projects work on, don’t cooperate enough with those parallel projects, and require a critical mass of users to work.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Literate Blogging]]></title>
            <link>https://blog.martinfenner.org/posts/literate-blogging</link>
            <guid>17bdf428-a25e-45d8-b2a0-a9f26ec1596b</guid>
            <pubDate>Mon, 14 Apr 2014 15:39:00 GMT</pubDate>
            <description><![CDATA[> Literate programming is a methodology that combines a programming language with
a documentation language, thereby making programs more robust, more portable,
more easily maintained, and arguably more fun to write than programs that are
written only in a high-level language. The main idea is to treat a program as a
piece of literature, addressed to human beings rather than to a computer. The
program is also viewed as a hypertext document, rather like the World Wide Web.
Literatue Programming by]]></description>
            <content:encoded><![CDATA[<blockquote>Literate programming is a methodology that combines a programming language with a documentation language, thereby making programs more robust, more portable, more easily maintained, and arguably more fun to write than programs that are written only in a high-level language. The main idea is to treat a program as a piece of literature, addressed to human beings rather than to a computer. The program is also viewed as a hypertext document, rather like the World Wide Web.</blockquote><p>Literatue Programming by <a href="http://www-cs-faculty.stanford.edu/~uno/">Donald Knuth</a> (1983) is a seminal book that introduces the concept of literate programming. Using technology available in 2014 we can make a small but important change to the last sentence:</p><blockquote>The program is also viewed as a hypertext document on the World Wide Web.</blockquote><p>This blog post is an example for such a document. The page is written in <strong><strong>markdown</strong></strong> (markdown file available <a href="https://github.com/mfenner/mfenner.github.io/blob/source/_posts/2014-04-04-literate-blogging.Rmd">here</a>), and all embedded code was executed when this page was generated, i.e. when the markdown was converted to HTML and the blog post was published. To demonstrate this I have embedded code in three different languages below - the output is the second code block.</p><p>In R you have</p><pre><code>cat('Hello, R world!\n')</code></pre><pre><code>Hello, R world!</code></pre><p>Or Python</p><pre><code>print "Hello, Python world!"</code></pre><pre><code>Hello, Python world!</code></pre><p>Or Ruby</p><pre><code>puts 'Hello, Ruby world!'</code></pre><pre><code>Hello, Ruby world!</code></pre><p>You can also embed code within text blocks (inline), so that <code>3.48 * 723</code> becomes <strong><strong>2516.04</strong></strong>. Another important option is to generate figures using the embedded code, e.g. the following figure taken from a recent publication.</p><pre><code># code for figure 1: density plots for citation counts for PLOS Biology
# articles published in 2010

# load May 20, 2013 ALM report
alm &lt;- read.csv("data/alm_report_plos_biology_2013-05-20.csv", stringsAsFactors = FALSE)

# only look at research articles
alm &lt;- subset(alm, alm$article_type == "Research Article")

# only look at papers published in 2010
alm$publication_date &lt;- as.Date(alm$publication_date)
alm &lt;- subset(alm, alm$publication_date &gt; "2010-01-01" &amp; alm$publication_date &lt;=
    "2010-12-31")

# labels
colnames &lt;- dimnames(alm)[[2]]
plos.color &lt;- "#1ebd21"
plos.source &lt;- "scopus"

plos.xlab &lt;- "Scopus Citations"
plos.ylab &lt;- "Probability"

quantile &lt;- quantile(alm[, plos.source], c(0.1, 0.5, 0.9), na.rm = TRUE)

# plot the chart
opar &lt;- par(mai = c(0.5, 0.75, 0.5, 0.5), omi = c(0.25, 0.1, 0.25, 0.1), mgp = c(3,
    0.5, 0.5), fg = "black", cex.main = 2, cex.lab = 1.5, col = plos.color,
    col.main = plos.color, col.lab = plos.color, xaxs = "i", yaxs = "i")

d &lt;- density(alm[, plos.source], from = 0, to = 100)
d$x &lt;- append(d$x, 0)
d$y &lt;- append(d$y, 0)
plot(d, type = "n", main = NA, xlab = NA, ylab = NA, xlim = c(0, 100), frame.plot = FALSE)
polygon(d, col = plos.color, border = NA)
mtext(plos.xlab, side = 1, col = plos.color, cex = 1.25, outer = TRUE, adj = 1,
    at = 1)
mtext(plos.ylab, side = 2, col = plos.color, cex = 1.25, outer = TRUE, adj = 0,
    at = 1, las = 1)

par(opar)</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/figures/density_plot_example-1.svg" class="kg-image" alt="Figure 1. Citation counts for PLOS Biology articles published in 2010. Scopus citation counts plotted as a probability distribution for all 197 PLOS Biology research articles published in 2010. Data collected May 20, 2013. Median 19 citations; 10% of papers have at least 50 citations. From Fenner (2013)."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Figure 1. Citation counts for PLOS Biology articles published in 2010.</strong> Scopus citation counts plotted as a probability distribution for all 197 <em style="box-sizing: border-box;">PLOS Biology</em> research articles published in 2010. Data collected May 20, 2013. Median 19 citations; 10% of papers have at least 50 citations. From Fenner <span class="citation" data-cites="fenner2013" style="box-sizing: border-box;">(2013)</span>.</figcaption></figure><p>All this functionality is provided by <a href="http://yihui.name/knitr/">knitr</a>, a package for the R statistical programming language. knitr has been around for a while, but integration into the <a href="http://jekyllrb.com/">Jekyll</a> blogging platform is still fragile. Earlier this week at the <a href="https://github.com/ropensci/hackathon">rOpenSci hackathon</a> (more on this later) a group of us worked hard to improve this integration. We are still not completely done, but the source code is available <a href="https://github.com/ropensci/docs">here</a>. Most importantly, all the conversion happens on the server, and we are only using freely available tools. I have now enabled this functionality for this blog, so expect more code embedded examples in the future.</p><h2 id="references">References</h2><p>Fenner, M. (2013). What can article-level metrics do for you? <em>PLoS Biol</em>, <em>11</em>(10), e1001687. <a href="http://doi.org/10.1371/journal.pbio.1001687">doi:10.1371/journal.pbio.1001687</a></p><p>Knuth, D. E., Stanford University, &amp; Computer Science Department. (1983). <em>Literate programming</em>. Stanford, CA: Dept. of Computer Science, Stanford University.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Continuous Publishing]]></title>
            <link>https://blog.martinfenner.org/posts/continuous-publishing</link>
            <guid>8550cc6b-45e9-4621-aad6-fadf592f7681</guid>
            <pubDate>Mon, 10 Mar 2014 15:44:00 GMT</pubDate>
            <description><![CDATA[Earlier this week Björn Brembs wrote in a blog post (What Is The Difference
Between Text, Data And Code?
[http://bjoern.brembs.net/2014/03/what-is-the-difference-between-text-data-and-code/]
):

> To sum it up: our intellectual output today manifests itself in code, data and
text.
The post is about the importance of publication of data and software where
currently the rewards are stacked disproportionately in favor of text
publications. The intended audience is probably mainly other scientists (]]></description>
            <content:encoded><![CDATA[<p>Earlier this week Björn Brembs wrote in a blog post (<a href="http://bjoern.brembs.net/2014/03/what-is-the-difference-between-text-data-and-code/">What Is The Difference Between Text, Data And Code?</a>):</p><blockquote>To sum it up: our intellectual output today manifests itself in code, data and text.</blockquote><p>The post is about the importance of publication of data and software where currently <em>the rewards are stacked disproportionately in favor of text publications</em>. The intended audience is probably mainly other scientists (Björn is a neurobiologist) who are reluctant to publish data and/or code, but there is another interesting aspect to this.</p><p>Just as scientific publication increasingly means more than just text and includes data and software, we are also increasingly seeing tools and methodologies common in software development applied to scientific publishing. This in particular includes the ideas behind Open Source software (which shares many commonalities with Open Access and Open Science), but also tools like the git version control system (<a href="http://marciovm.com/i-want-a-github-of-science/">We Need a Github of Science</a>) or the markdown markdown language (<a href="http://blog.martinfenner.org/2012/12/13/a-call-for-scholarly-markdown/">A Call for Scholarly Markdown</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/Agile-vs-iterative-flow.jpg" class="kg-image" alt="Delivery, image from Wikimedia Commons."><figcaption>Delivery, image from <a href="http://commons.wikimedia.org/wiki/File:Agile-vs-iterative-flow.jpg" style="box-sizing: border-box; background: transparent; color: rgb(52, 152, 219); text-decoration: none;">Wikimedia Commons</a>.</figcaption></figure><p>Continuous Delivery is another concept increasingly popular in software development that has many implications on how research can be performed and reported. Martin Fowler describes it as:</p><blockquote>Continuous Delivery is a software development discipline where you build software in such a way that the software can be released to production at any time.</blockquote><p>The concept of frequent small releases is of course familiar to everyone practicing <a href="http://usefulchem.wikispaces.com/">Open Notebook Science</a>, writing science blogs, presenting preliminary data at conferences or publishing <a href="http://arxiv.org/">preprints</a>, and is even relevant to <a href="http://www.crossref.org/crossmark/">CrossMark</a>, a service that tracks corrections, enhancements and other changes of scholarly documents.</p><p>When you read the definition given by Martin Fowler carefully, you see that Continuous Delivery is about more than the frequency of software updates – it is in fact about improving the process of releasing software. The scientific publication is the corresponding event in science, and I think that nobody would argue with me that the experience publishing a paper is too complex, time-consuming and often frustrating. The focus here is not on the time it takes to do peer review, or the multiple revisions needed before a manuscript is accepted. I am talking about the pain submitting a manuscript, the back and forth regarding file formats, citation styles and other technical requirements, the reformatting of manuscripts, and also the time it takes from accepting a manuscript to finally publishing it online.</p><p>I would argue that the main reason publishing is so painful for everyone involved is that it is still very much a manual process. Just as software development is creative work, but still can benefit tremendously from tools such as automated tests and build tools, we can apply the same principles to scientific publishing. This means that everything that can be automated should be automated so that we can focus on those areas that need human judgement. The mistake that I think is commonly made is that automation for many publishers means automation for the publisher, with even more work for the author who submits a manuscript. A good example is that authors are increasingly asked to submit publication-ready manuscripts even though typesetting and desktop publishing is not their area of expertise and the manuscript text will be very different after one or more rounds of revision. The pain of processing manuscripts into something that can be published was summarized perfectly by typesetter and friend Kaveh Bazargan at the <a href="http://www.youtube.com/watch?feature=player_embedded&amp;v=CGkcsvofjdg">SpotOn London 2012 Conference</a> (via <a href="http://rossmounce.co.uk/2012/11/19/yet-another-solo12-recap-part2/">Ross Mounce’s blog</a>):</p><blockquote>It’s madness really. I’m here to say I shouldn’t be in business.</blockquote><p>The promise of Continuous Delivery for publishing is to develop tools and best practices that make the process of publication faster, with better quality, and less frustrating. Continuous Integration (<a href="http://martinfowler.com/articles/continuousIntegration.html">again Martin Fowler</a>) is an important part of Continuous Delivery and means frequently merging all developer working copies of a software project into a central repository, combined with running automated unit tests and software builds using an integration server.</p><p>We can apply Continuous Integration to scholarly documents - instead of automated tests and software builds we can automate the transformation of documents into <a href="http://blog.martinfenner.org/2013/12/12/from-markdown-to-jats-xml-in-one-step/">JATS XML</a> and other output formats, and we can automate the process of checking for required metadata, correct file formats for images, etc. And we can use the same software tools for this, many of which are freely available to Open Source projects.</p><p>As an example of how this can be done <a href="https://github.com/mfenner/jekyll-travis">I have integrated</a> the <a href="https://travis-ci.org/">Travis CI</a> Continuous Integration server with the book project <a href="http://book.openingscience.org/">Opening Science</a>. The recently published book is a dynamic book that hopefully is updated frequently in the coming months. Every time an editor approves a correction to the text - <a href="https://github.com/openingscience/book">hosted in markdown format on Github</a> - the Travis CI server is automatically triggered to build a new HTML version of the book and to push the new version to the book website. The Travis server is running the <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a> document converter to not only convert the changed document from markdown to HTML, but Pandoc will also insert and format references, and the <a href="http://jekyllrb.com/">Jekyll</a> site generator will build a nice website around the markdown files. Over time this build process can be extended to do other things as well, from <a href="http://blog.martinfenner.org/2013/07/02/auto-generating-links-to-data-and-resources/">auto-generating links to data and resources</a> to transforming the document into <a href="http://blog.martinfenner.org/2013/12/12/from-markdown-to-jats-xml-in-one-step/">other file formats</a> besides HTML.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Are static Websites the Future or the Past?]]></title>
            <link>https://blog.martinfenner.org/posts/are-static-websites-the-future-or-the-past</link>
            <guid>aedec2a6-2cea-4319-b72a-0ff1c0d03f5e</guid>
            <pubDate>Wed, 05 Mar 2014 15:47:00 GMT</pubDate>
            <description><![CDATA[Last week I had a little discussion on Twitter about a great blog post by Zach
Holman: Only 90s Web Developers Remember This
[http://zachholman.com/posts/only-90s-developers/]. The post is not only fun to
read, but also reminded me that it is now almost 20 years (1995) that I built my
first website - of course using some of the techniques (the one pixel gif!, the 
&nbsp; tag!) described in the post.

ScriptWeb logo 1995We started ScriptWeb back in 1995 as a central resource for
scripting on the ]]></description>
            <content:encoded><![CDATA[<p>Last week I had a little discussion on Twitter about a great blog post by Zach Holman: <a href="http://zachholman.com/posts/only-90s-developers/">Only 90s Web Developers Remember This</a>. The post is not only fun to read, but also reminded me that it is now almost 20 years (1995) that I built my first website - of course using some of the techniques (the one pixel gif!, the <code>&amp;nbsp;</code> tag!) described in the post.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/scriptwebtitle.gif" class="kg-image" alt><figcaption>ScriptWeb logo 1995</figcaption></figure><p>We started ScriptWeb back in 1995 as a central resource for scripting on the Mac (Applescript and Frontier). It was a nice collaborative effort and I was resposible for a directory of scripting additions (or osaxen), joining forces with MacScripter.net a few years later:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/osaxen.png" class="kg-image" alt><figcaption>Scripting Additions at Macscripter.net 2000</figcaption></figure><p>Since then I have built many other websites for fun and work, adapting to how technology changed over the years:</p><ul><li>1995: website running on a Mac Quadra 610 using the WebSTAR HTTP server and server side includes (<a href="https://en.wikipedia.org/wiki/Server_Side_Includes">SSI</a>)</li><li>1995: static site generation with outline navigation using Applescript. FTP to transfer files</li><li>1995: Visual HTML editors (Adobe PageMill 1.0)</li><li>1999: database server and application layer (too long ago to remember the technology)</li><li>2001: Open source database and application code with MySQL and PHP. CVS for version control</li><li>2001: web frameworks with PHP and MySQL: PostNuke and Xaraya</li><li>2005: more complex web application frameworks: Ruby on Rails. Subversion version control</li><li>2008: git for version control</li><li>2011: more complex frontend Javascript</li><li>2013: static site generator Jekyll</li></ul><p>Since last June this blog is running on Github pages and the site is generated with <a href="http://jekyllrb.com/">Jekyll</a>. Jekyll works really well to build static websites such as this blog, but I am increasingly using it for more complex projects, e.g. for the online version of a <a href="http://book.openingscience.org/">book on Open Science</a>.</p><p>What I find interesting in this timeline is that with Jekyll there is a shift in focus. Rather than building even more complex web pages that are generated dynamically by the server, we are going back to a two-stage process where the HTML pages are built first and then served as HTML, CSS and Javascript without any database or server application layer. Doesn’t sound too different from what we did in the 1990s. This approach obviously works well for content-heavy sites like this blog or book chapters, not so much for dynamically generated content that changes every few minutes, or where the page is put together from many different page fragments.</p><p>What I don’t know, and I am really interested to find out, is how well this scales to larger sites, specifically publisher websites that host thousands of scholarly journal articles - again content that is very text-heavy and doesn’t change that much. The potential benefits of replacing the paradigm of a database layer that holds all content with a paradigm that stores all content in files managed by git version control are clear: serving the content on the web becomes less complex, cheaper and faster. The tradeoff is of course that generating the static content becomes more complex and time-consuming, and it can become a challenge to mix the static content with dynamic content generated by servers as well as the user’s browser. For a now infamous example using this technology, look no further than <a href="http://www.huffingtonpost.com/john-pavley/obamacare-website-problems_b_4057618.html">Heathcare.gov</a>. I don’t know enough details to understand what went wrong, and it might have more to do with the scale of the project and the tight timeline to launch. For scholarly journal articles this might be a reasonable approach, as even when there is no longer a printed version of the journal, articles are still published on a specific date, and changing the content is a very formal process.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/3781208877_936e1a162c_z.jpg" class="kg-image" alt srcset="https://martinfenner.ghost.io/content/images/size/w600/2021/01/3781208877_936e1a162c_z.jpg 600w, https://martinfenner.ghost.io/content/images/2021/01/3781208877_936e1a162c_z.jpg 640w"><figcaption>Netscape Navigator 1. <a href="http://www.flickr.com/photos/bump/3781208877/">Flickr photo</a> by bump</figcaption></figure>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Six Misunderstandings about Scholarly Markdown]]></title>
            <link>https://blog.martinfenner.org/posts/six-misunderstandings-about-scholarly-markdown</link>
            <guid>8271bf5c-523e-4c4c-907f-a58f9fd09ae8</guid>
            <pubDate>Mon, 03 Mar 2014 15:50:00 GMT</pubDate>
            <description><![CDATA[In this post I want to talk about some of the misunderstandings I frequently
encounter when discussing markdown as a format for authoring scholarly documents
[http://blog.martinfenner.org/2013/06/17/what-is-scholarly-markdown/].

Scholars will always use Microsoft Word
Microsoft Word is of course what almost all authors use in the life sciences and
many other disciplines. One big reason for this is the file formats accepted my
manuscript submission systems. By limiting the options to Microsoft W]]></description>
            <content:encoded><![CDATA[<p>In this post I want to talk about some of the misunderstandings I frequently encounter when discussing <a href="http://blog.martinfenner.org/2013/06/17/what-is-scholarly-markdown/">markdown as a format for authoring scholarly documents</a>.</p><h2 id="scholars-will-always-use-microsoft-word">Scholars will always use Microsoft Word</h2><p>Microsoft Word is of course what almost all authors use in the life sciences and many other disciplines. One big reason for this is the file formats accepted my manuscript submission systems. By limiting the options to Microsoft Word (and maybe LaTeX), you make it impossible for authors to use other tools, even if they wanted to. Publishers should accept manuscripts in any reasonable file format, as I have <a href="http://blog.martinfenner.org/2013/11/17/the-grammar-of-scholarly-communication/">argued before</a>.</p><h2 id="my-markup-language-is-better-than-markdown">My markup language is better than markdown</h2><p>There are of course numerous alternatives to markdown, including <a href="http://txstyle.org/">Textile</a>, <a href="http://www.methods.co.nz/asciidoc/">AsciiDoc</a>, <a href="http://www.mediawiki.org/wiki/Help:Formatting">MediaWiki Markup</a> and <a href="http://docutils.sourceforge.net/docs/ref/rst/introduction.html">reStructuredText</a>. There will always be features that are better implemented in one of these languages, but I don’t think there is room for more than one major initiative for a scholarly markup language. And markdown has the right mix of features and broad support by tools and the community.</p><p>Related to this there is the argument against markdown that the format <a href="http://blog.codinghorror.com/the-future-of-markdown/">is a mess</a> and that there are too many versions (or flavors) of it. While that is certainly a big problem with markdown, I would argue that with <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a> we have a nice standard and reference implementation for Scholarly Markdown. Pandoc is constantly evolving, and the addition of support for arbitrary YAML metadata was the biggest new feature in 2013 for me.</p><h2 id="scholarly-markdown-is-too-complex-and-we-might-as-well-use-latex">Scholarly Markdown is too complex and we might as well use LaTeX</h2><p>“LaTeX is a high-quality typesetting system; it includes features designed for the production of technical and scientific documentation” (The LaTeX Team, 2014). Although LaTeX has solved many of the problems Scholarly Markdown tries to tackle a long time ago, it is still something else. LaTeX at its core is a typesetting system, which is not something Scholarly Markdown cares about for two reasons: a) the focus is on authoring documents, which are then submitted to other systems at publishers and elsewhere that are specialized in producing the final document, and b) the focus is on HTML and the web as this is where we want most of the interactions with scholarly documents to take place. This means that</p><ul><li>Markdown is a great input format to convert into other formats, including XML (see for example my <a href="https://github.com/mfenner/pandoc-jats">pandoc-jats</a>).</li><li>LaTeX will always be the best choice for some content, e.g. documents rich in mathematical formulas</li><li>If the ultimate goal was to produce high-quality PDF documents, Scholarly Markdown would be a bad choice. It is the right format for HTML and the related ePub.</li></ul><p>We have to be very careful that we keep the right balance of simplicity and features in Scholarly Markdown. This means that sometimes we should just include the LaTeX code, e.g. for math.</p><h2 id="scientists-need-a-wysiwyg-editor-and-then-the-file-format-doesn-t-matter">Scientists need a WYSIWYG Editor, and then the file format doesn’t matter</h2><p><a href="http://en.wikipedia.org/wiki/WYSIWYG">WYSIWYG</a> - What You See Is What You Get - is a user interface metaphor that is both a blessing and a curse. We desperately need better writing tools, and this of course also means user interfaces that help with that task. But the focus on creating a new authoring environment that focusses too much on WYSIWYG creates several problems:</p><ul><li>WYSIWYG is not always a good metaphor for scholarly documents. Typographic features such as fonts, line spacing, etc. are not something that belong into an authoring environment - this is done during the publishing step, as is the formatting of references according to a specific citation style.</li><li>WYSIWYG is for human interactions, but content in scholarly documents is increasingly created by computers. Two good examples are statistics and figures created in <a href="http://yihui.name/knitr/">R/knitr</a> or <a href="http://ipython.org/notebook.html">iPython Notebook</a>. Scholarly Markdown works perfectly with these workflows.</li><li>WYSIWYG authoring environments run the high risk of vendor lock-in. This is understandable if you run a startup and want to promote your tool, but is not in the best interest of the scholarly community.</li></ul><p>Version control via git is central to Scholarly Markdown, and this can also be challenging for a WYSIWYG environment. But there are many good examples of how to make this work.</p><h2 id="scientists-should-submit-their-manuscripts-in-jats-xml-the-standard-format-for-scholarly-documents">Scientists should submit their manuscripts in JATS XML, the standard format for scholarly documents</h2><p>At the end of the day most scholarly publications in the life sciences are converted into JATS XML. Unfortunately central aspects of the format (e.g. the required document structure or required attributes) are difficult to enforce in an authoring environment. Even if you build a tool that can nicely handle this, I’m not so sure we want to burden an author with this, especially since the manuscript will usually undergo a lot of changes before it is accepted and then published.</p><h2 id="the-future-is-html">The future is HTML</h2><p>Although the future for consuming scholarly documents is clearly HTML (and ePub), and there are great HTML editors, I’m not so sure that HTML will become the default for authoring environments. This is the reason why markdown and related markup languages were invented, and even with modern WYSIWYG editors working directly with HTML is not always the best choice. HTML has two problems: a) it is not as human-readable as markdown and therefore requires an additional layer for authoring, and b) it is not as structured as XML, which makes it difficult to create some of the rigid document structure required for scholarly documents. O’Reilly is trying to get more structure into HTML for print and digital books with <a href="https://github.com/oreillymedia/htmlbook">HTMLBook</a>, but with too much structure you might run into similar problems for authoring as discussed above for JATS XML. And of course you can include HTML in markdown documents.</p><h2 id="references">References</h2><p>The LaTeX Team. (2014). LaTeX - a document preparation system. <a href="http://www.latex-project.org/">http://www.latex-project.org/</a>. Retrieved from <a href="http://www.latex-project.org/">http://www.latex-project.org/</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[From Markdown to JATS XML in one Step]]></title>
            <link>https://blog.martinfenner.org/posts/from-markdown-to-jats-xml-in-one-step</link>
            <guid>bc1e02ba-67b3-444a-9dd4-f0a45ace76a6</guid>
            <pubDate>Thu, 12 Dec 2013 15:54:00 GMT</pubDate>
            <description><![CDATA[The Journal Article Tag Suite (JATS [http://jats.nlm.nih.gov/]) is a NISO
standard that defines a set of XML elements and attributes for tagging journal
articles. JATS is not only used for fulltext content at PubMed Central (and JATS
has evolved from the NLM Archiving and Interchange Tag Suite originally
developed for PubMed Central), but is also increasinly used by publishers.

For many publishers the version of record of an article is stored in XML, and
other formats (currently HTML, PDF and i]]></description>
            <content:encoded><![CDATA[<p>The Journal Article Tag Suite (<a href="http://jats.nlm.nih.gov/">JATS</a>) is a NISO standard that defines a set of XML elements and attributes for tagging journal articles. JATS is not only used for fulltext content at PubMed Central (and JATS has evolved from the NLM Archiving and Interchange Tag Suite originally developed for PubMed Central), but is also increasinly used by publishers.</p><p>For many publishers the <em>version of record</em> of an article is stored in XML, and other formats (currently HTML, PDF and increasingly ePub) are generated from this XML. Unfortunately the process of converting author-submitted manuscripts into JATS-compliant XML is time-consuming and costly, and this is a problem in particular for small publishers.</p><p>In a recent blog post (<a href="http://blog.martinfenner.org/2013/11/17/the-grammar-of-scholarly-communication/">The Grammar of Scholarly Communication</a>) I argued that publishers should accept manuscripts in any reasonable file format, including Microsoft Word, Open Office, LaTeX, Markdown, HTML and PDF. Readers of this blog know that I am a big fan of <a href="http://blog.martinfenner.org/tags.html#markdown-ref">markdown</a> for scholarly documents, but I am of course well aware that at the end of the day these documents have to be converted into JATS.</p><p>As a small step towards that goal I have today released the first public version of <a href="https://github.com/mfenner/pandoc-jats">pandoc-jats</a>, a <a href="http://johnmacfarlane.net/pandoc/README.html#custom-writers">custom writer for Pandoc</a> that converts markdown documents into JATS XML with a single command, e.g.</p><pre><code>pandoc -f example.md --filter pandoc-citeproc --bibliography=example.bib --csl=apa.csl -t JATS.lua -o example.xml</code></pre><p>Please see the <a href="https://github.com/mfenner/pandoc-jats">pandoc-jats</a> Github repository for more detailed information, but using this custom writer is as simple as downloading a single <code>JATS.lua</code>file. The big challenge is of course to make this custom writer work with as many documents as possible, and that will be my job the next few weeks. Two example JATS documents are below (both markdown versions of scholarly articles and posted on this blog as HTML):</p><ul><li>Nine simple ways to make it easier to (re)use your data (<a href="http://blog.martinfenner.org/2013/06/25/nine-simple-ways-to-make-it-easier-to-reuse-your-data/">HTML</a>, <a href="http://blog.martinfenner.org/files/10.7287.peerj.preprints.7v2.xml">JATS</a>)</li><li>What Can Article Level Metrics Do for You? (<a href="http://blog.martinfenner.org/2013/12/11/what-can-article-level-metrics-do-for-you/">HTML</a>, <a href="http://blog.martinfenner.org/files/10.1371.journal.pbio.1001687.xml">JATS</a>)</li></ul><p>Both JATS files were validated against the JATS DTD and XSD and showed no errors with the NLM XML StyleChecker - using the excellent <a href="https://github.com/PeerJ/jats-conversion">jats-conversion</a> conversion and validation tools written by Alf Eaton. Markdown is actually a nice file format to convert to XML - in contrast to HTML authors can’t for example put closing tags at the wrong places. And a Pandoc custom writer written in the Lua scripting language is an interesting alternative to XSLT transformations, the more common way to create JATS XML. The custom writer has not been tested with other Pandoc input formats besides markdown, of particular interest are of course HTML and LaTeX - Microsoft Word .docx is unfortunately only a Pandoc output format.</p><p>This is the first public release and there is of course a lot of room for improvement. Many elements and attributes are not yet supported - although <a href="http://orcid.org/blog/2013/03/22/orcid-how-more-specifying-orcid-ids-document-metadata">ORCID author identifiers</a> are of course included. Please help me improve this tool using the Github <a href="https://github.com/mfenner/pandoc-jats/issues">Issue Tracker</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Example article with embedded code and data]]></title>
            <link>https://blog.martinfenner.org/posts/example-article-with-embedded-code-and-data</link>
            <guid>0d59a9aa-6ec9-47c5-9a8b-77c0f1719443</guid>
            <pubDate>Wed, 11 Dec 2013 16:04:00 GMT</pubDate>
            <description><![CDATA[In October I published an essay on Article-Level Metrics (ALM) in PLOS Biology
(Fenner, 2013). The essay is a good introduction into Article-Level Metrics, and
I am proud that it is part of the Tenth Anniversary PLOS Biology Collection
[http://dx.doi.org/10.1371/issue.pcol.v06.i03]. Like all PLOS content, the
article was published with a Creative Commons attribution license
[http://blogs.plos.org/tech/creative-commons-for-science-interview-with-puneet-kishor/]
, allowing me to republish the arti]]></description>
            <content:encoded><![CDATA[<p>In October I published an essay on Article-Level Metrics (ALM) in PLOS Biology (Fenner, 2013). The essay is a good introduction into Article-Level Metrics, and I am proud that it is part of the <a href="http://dx.doi.org/10.1371/issue.pcol.v06.i03">Tenth Anniversary PLOS Biology Collection</a>. Like all PLOS content, the article was published with a <a href="http://blogs.plos.org/tech/creative-commons-for-science-interview-with-puneet-kishor/">Creative Commons attribution license</a>, allowing me to republish the article on this blog. I have now done so and the article is available <a href="http://blog.martinfenner.org/2013/12/11/what-can-article-level-metrics-do-for-you/">here</a>.</p><p>Of course I didn’t want to simply republish the article, but I wanted to publish an improved version. The article has five figures, four of them show visualizations of ALM data that were generated using R (the fifth figure is a table reproduced from another article). The PLOS article includes the ALM dataset and the R scripts used to generate the figures as <a href="http://dx.doi.org/10.1371/journal.pbio.1001687.s001">supplementary information</a>. What I have done now is to recreate the article as a single markdown file (available <a href="https://github.com/mfenner/blog/blob/master/_posts/2013-12-11-what-can-article-level-metrics-do-for-you.Rmd">here</a>) that has all R code embedded. Using R and <a href="http://yihui.name/knitr/">knitr</a> - and the <a href="http://blog.martinfenner.org/data/alm_report_plos_biology_2013-05-20.csv">CSV file with the ALM data</a> - everyone can now reproduce the figures from the paper by simply running the embedded code, and can dig deeper into the data.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/2013-12-11_figure_3.svg" class="kg-image" alt="Figure 3. Views vs. citations for PLOS Biology articles published in 2010."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Figure 3.</strong> Views vs.&nbsp;citations for PLOS Biology articles published in 2010.</figcaption></figure><p>This was a good opportunity to improve the accessibility of the article in other ways. Instead of the raster image formats PNG, JPEG and TIFF used by PLOS and almost every other publisher, I generated the figures in the vector format SVG. Not only does SVG produce images independent of device resolution and screen size (try to zoom in on the figure above), but SVG can also easily be manipulated in the browser since it is XML. This is beyond the scope of this blog post, but look at the <a href="http://d3js.org/">d3.js</a> Javascript library for great examples of how SVG can be dynamically generated and changed in the browser. <strong><strong>Figure 3</strong></strong> above could for example be enhanced so that the article title is displayed when you hover over one of the bubbles, or we could enable zooming to show more detail.</p><p>Like all content on this blog, the article was created using <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a>, and the bibliography was dynamically generated. This makes it easy to change the citation style, and I decided to use the <a href="http://www.apastyle.org/">APA Style</a> that shows the citations in the text as author-date rather than numbered as with the PLOS style (see the example citation in the first paragraph). The combined bibliography for all blog posts including the article can be downloaded in bibtex format <a href="http://blog.martinfenner.org/bibliography/references.bib">here</a>.</p><p>Lastly, I wanted to generate nicer HTML for a better online reading experience. I haven’t done anything fancy, but most publishers seem to focus on navigation around an article, so that very little screen real estate is left for the actual content of the article. I’ve tried to improve readability by reducing the navigation areas to a minimum, by using readable fonts in larger sizes: <a href="https://typekit.com/fonts/minion-pro">Adobe Minion Pro</a> for the body text and <a href="https://typekit.com/fonts/myriad-pro">Adobe Myriad Pro</a> for headings, tables and figure legends.</p><h2 id="references">References</h2><p>Fenner, M. (2013). What can article-level metrics do for you? <em>PLoS Biol</em>, <em>11</em>(10), e1001687. <a href="http://doi.org/10.1371/journal.pbio.1001687">doi:10.1371/journal.pbio.1001687</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What Can Article-Level Metrics Do for You?]]></title>
            <link>https://blog.martinfenner.org/posts/what-can-article-level-metrics-do-for-you</link>
            <guid>9cae933c-062d-4572-8d47-8a6c31f539a7</guid>
            <pubDate>Wed, 11 Dec 2013 15:58:00 GMT</pubDate>
            <description><![CDATA[Article-level metrics (ALMs) provide a wide range of metrics about the uptake of
an individual journal article by the scientific community after publication.
They include citations, usage statistics, discussions in online comments and
social media, social bookmarking, and recommendations. In this essay, we
describe why article-level metrics are an important extension of traditional
citation-based journal metrics and provide a number of example from ALM data
collected for PLOS Biology.

This is a]]></description>
            <content:encoded><![CDATA[<p><em>Article-level metrics (ALMs) provide a wide range of metrics about the uptake of an individual journal article by the scientific community after publication. They include citations, usage statistics, discussions in online comments and social media, social bookmarking, and recommendations. In this essay, we describe why article-level metrics are an important extension of traditional citation-based journal metrics and provide a number of example from ALM data collected for PLOS Biology.</em></p><p><em>This is an open-access article distributed under the terms of the Creative Commons Attribution License, authored by me and <a href="http://dx.doi.org/10.1371/journal.pbio.1001687">originally published Oct 22, 2013 in PLOS Biology</a>.</em></p><p>The scientific impact of a particular piece of research is reflected in how this work is taken up by the scientific community. The first systematic approach that was used to assess impact, based on the technology available at the time, was to track citations and aggregate them by journal. This strategy is not only no longer necessary â€” since now we can easily track citations for individual articles â€” but also, and more importantly, journal-based metrics are now considered a poor performance measure for individual articles (Campbell, 2008; Glänzel &amp; Wouters, 2013). One major problem with journal-based metrics is the variation in citations per article, which means that a small percentage of articles can skew, and are responsible for, the majority of the journal-based citation impact factor, as shown by Campbell (2008) for the 2004 <em>Nature</em> Journal Impact Factor. <strong><strong>Figure 1</strong></strong> further illustrates this point, showing the wide distribution of citation counts between <em>PLOS Biology</em> research articles published in 2010. <em>PLOS Biology</em> research articles published in 2010 have been cited a median 19 times to date in Scopus, but 10% of them have been cited 50 or more times, and two articles (Dickson, Wang, Krantz, Hakonarson, &amp; Goldstein, 2010; Narendra et al., 2010) more than 300 times. <em>PLOS Biology</em> metrics are used as examples throughout this essay, and the dataset is available in the supporting information (<strong><strong>Data S1</strong></strong>). Similar data are available for an increasing number of other publications and organizations.</p><pre><code># code for figure 1: density plots for citation counts for PLOS Biology
# articles published in 2010

# load May 20, 2013 ALM report
alm &lt;- read.csv("data/alm_report_plos_biology_2013-05-20.csv", stringsAsFactors = FALSE)

# only look at research articles
alm &lt;- subset(alm, alm$article_type == "Research Article")

# only look at papers published in 2010
alm$publication_date &lt;- as.Date(alm$publication_date)
alm &lt;- subset(alm, alm$publication_date &gt; "2010-01-01" &amp; alm$publication_date &lt;=
    "2010-12-31")

# labels
colnames &lt;- dimnames(alm)[[2]]
plos.color &lt;- "#1ebd21"
plos.source &lt;- "scopus"

plos.xlab &lt;- "Scopus Citations"
plos.ylab &lt;- "Probability"

quantile &lt;- quantile(alm[, plos.source], c(0.1, 0.5, 0.9), na.rm = TRUE)

# plot the chart
opar &lt;- par(mai = c(0.5, 0.75, 0.5, 0.5), omi = c(0.25, 0.1, 0.25, 0.1), mgp = c(3,
    0.5, 0.5), fg = "black", cex.main = 2, cex.lab = 1.5, col = plos.color,
    col.main = plos.color, col.lab = plos.color, xaxs = "i", yaxs = "i")

d &lt;- density(alm[, plos.source], from = 0, to = 100)
d$x &lt;- append(d$x, 0)
d$y &lt;- append(d$y, 0)
plot(d, type = "n", main = NA, xlab = NA, ylab = NA, xlim = c(0, 100), frame.plot = FALSE)
polygon(d, col = plos.color, border = NA)
mtext(plos.xlab, side = 1, col = plos.color, cex = 1.25, outer = TRUE, adj = 1,
    at = 1)
mtext(plos.ylab, side = 2, col = plos.color, cex = 1.25, outer = TRUE, adj = 0,
    at = 1, las = 1)

par(opar)</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/2013-12-11_figure_1.svg" class="kg-image" alt="Figure 1. Citation counts for PLOS Biology articles published in 2010. Scopus citation counts plotted as a probability distribution for all 197 PLOS Biology research articles published in 2010. Data collected May 20, 2013. Median 19 citations; 10% of papers have at least 50 citations."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Figure 1. Citation counts for PLOS Biology articles published in 2010.</strong> Scopus citation counts plotted as a probability distribution for all 197 <em style="box-sizing: border-box;">PLOS Biology</em> research articles published in 2010. Data collected May 20, 2013. Median 19 citations; 10% of papers have at least 50 citations.</figcaption></figure><p>Scientific impact is a multi-dimensional construct that can not be adequately measured by any single indicator (Bollen, Sompel, Hagberg, &amp; Chute, 2009; Glänzel &amp; Wouters, 2013; Schekman &amp; Patterson, 2013). To this end, PLOS has collected and displayed a variety of metrics for all its articles since 2009. The array of different categorised article-level metrics (ALMs) used and provided by PLOS as of August 2013 are shown in <strong><strong>Figure 2</strong></strong>. In addition to citations and usage statistics, i.e., how often an article has been viewed and downloaded, PLOS also collects metrics about: how often an article has been saved in online reference managers, such as Mendeley; how often an article has been discussed in its comments section online, and also in science blogs or in social media; and how often an article has been recommended by other scientists. These additional metrics provide valuable information that we would miss if we only consider citations. Two important shortcomings of citation-based metrics are that (1) they take years to accumulate and (2) citation analysis is not always the best indicator of impact in more practical fields, such as clinical medicine (Eck, Waltman, Raan, Klautz, &amp; Peul, 2013). Usage statistics often better reflect the impact of work in more practical fields, and they also sometimes better highlight articles of general interest (for example, the 2006 <em>PLOS Biology</em> article on the citation advantage of Open Access articles (Eysenbach, 2006), one of the 10 most-viewed articles published in <em>PLOS Biology</em>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/2013-12-11_figure_2.png" class="kg-image" alt="Figure 2. Article-level metrics used by PLOS in August 2013 and their categories. Taken from (Lin &amp; Fenner, 2013) with permission by the authors."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Figure 2. Article-level metrics used by PLOS in August 2013 and their categories.</strong> Taken from <span class="citation" data-cites="Lin2013" style="box-sizing: border-box;">(Lin &amp; Fenner, 2013)</span> with permission by the authors.</figcaption></figure><p>A bubble chart showing all 2010 <em>PLOS Biology</em> articles (<strong><strong>Figure 3</strong></strong>) gives a good overview of the year’s views and citations, plus it shows the influence that the article type (as indicated by dot color) has on an article’s performance as measured by these metrics. The weekly <em>PLOS Biology</em> publication schedule is reflected in this figure, with articles published on the same day present in a vertical line. <strong><strong>Figure 3</strong></strong> also shows that the two most highly cited 2010 <em>PLOS Biology</em> research articles are also among the most viewed (indicated by the red arrows), but overall there isn’t a strong correlation between citations and views. The most-viewed article published in 2010 in <em>PLOS Biology</em> is an essay on Darwinian selection in robots (Floreano &amp; Keller, 2010). Detailed usage statistics also allow speculatulation about the different ways that readers access and make use of published literature; some articles are browsed or read online due to general interest while others that are downloaded (and perhaps also printed) may reflect the reader’s intention to look at the data and results in detail and to return to the article more than once.</p><pre><code># code for figure 3: Bubblechart views vs. citations for PLOS Biology
# articles published in 2010.

# Load required libraries
library(plyr)

# load May 20, 2013 ALM report
alm &lt;- read.csv("../data/alm_report_plos_biology_2013-05-20.csv", stringsAsFactors = FALSE,
    na.strings = c("0"))

# only look at papers published in 2010
alm$publication_date &lt;- as.Date(alm$publication_date)
alm &lt;- subset(alm, alm$publication_date &gt; "2010-01-01" &amp; alm$publication_date &lt;=
    "2010-12-31")

# make sure counter values are numbers
alm$counter_html &lt;- as.numeric(alm$counter_html)

# lump all papers together that are not research articles
reassignType &lt;- function(x) if (x == "Research Article") 1 else 0
alm$article_group &lt;- aaply(alm$article_type, 1, reassignType)

# calculate article age in months
alm$age_in_months &lt;- (Sys.Date() - alm$publication_date)/365.25 * 12
start_age_in_months &lt;- floor(as.numeric(Sys.Date() - as.Date(strptime("2010-12-31",
    format = "%Y-%m-%d")))/365.25 * 12)

# chart variables
x &lt;- alm$age_in_months
y &lt;- alm$counter
z &lt;- alm$scopus

xlab &lt;- "Age in Months"
ylab &lt;- "Total Views"

labels &lt;- alm$article_group
col.main &lt;- "#1ebd21"
col &lt;- "#666358"

# calculate bubble diameter
z &lt;- sqrt(z/pi)

# calculate bubble color
getColor &lt;- function(x) c("#c9c9c7", "#1ebd21")[x + 1]
colors &lt;- aaply(labels, 1, getColor)

# plot the chart
opar &lt;- par(mai = c(0.5, 0.75, 0.5, 0.5), omi = c(0.25, 0.1, 0.25, 0.1), mgp = c(3,
    0.5, 0.5), fg = "black", cex = 1, cex.main = 2, cex.lab = 1.5, col = "white",
    col.main = col.main, col.lab = col)

plot(x, y, type = "n", xlim = c(start_age_in_months, start_age_in_months + 13),
    ylim = c(0, 60000), xlab = NA, ylab = NA, las = 1)
symbols(x, y, circles = z, inches = exp(1.3)/15, bg = colors, xlim = c(start_age_in_months,
    start_age_in_months + 13), ylim = c(0, ymax), xlab = NA, ylab = NA, las = 1,
    add = TRUE)
mtext(xlab, side = 1, col = col.main, cex = 1.25, outer = TRUE, adj = 1, at = 1)
mtext(ylab, side = 2, col = col.main, cex = 1.25, outer = TRUE, adj = 0, at = 1,
    las = 1)

par(opar)</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/2013-12-11_figure_3.svg" class="kg-image" alt="Figure 3. Views vs. citations for PLOS Biology articles published in 2010. All 304 PLOS Biology articles published in 2010. Bubble size correlates with number of Scopus citations. Research articles are labeled green; all other articles are grey. Red arrows indicate the two most highly cited papers. Data collected May 20, 2013."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Figure 3. Views vs.&nbsp;citations for PLOS Biology articles published in 2010.</strong> All 304 <em style="box-sizing: border-box;">PLOS Biology</em> articles published in 2010. Bubble size correlates with number of Scopus citations. Research articles are labeled green; all other articles are grey. Red arrows indicate the two most highly cited papers. Data collected May 20, 2013.</figcaption></figure><p>When readers first see an interesting article, their response is often to view or download it. By contrast, a citation may be one of the last outcomes of their interest, occuring only about 1 in 300 times a PLOS paper is viewed online. A lot of things happen in between these potential responses, ranging from discussions in comments, social media, and blogs, to bookmarking, to linking from websites. These activities are usually subsumed under the term â€œaltmetrics,â€ and their variety can be overwhelming. Therefore, it helps to group them together into categories, and several organizations, including PLOS, are using the category labels of Viewed, Cited, Saved, Discussed, and Recommended (<strong><strong>Figures 2 and 4</strong></strong>, see also (Lin &amp; Fenner, 2013)).</p><pre><code># code for figure 4: bar plot for Article-level metrics for PLOS Biology

# Load required libraries
library(reshape2)

# load May 20, 2013 ALM report
alm &lt;- read.csv("../data/alm_report_plos_biology_2013-05-20.csv", stringsAsFactors = FALSE,
    na.strings = c(0, "0"))

# only look at research articles
alm &lt;- subset(alm, alm$article_type == "Research Article")

# make sure columns are in the right format
alm$counter_html &lt;- as.numeric(alm$counter_html)
alm$mendeley &lt;- as.numeric(alm$mendeley)

# options
plos.color &lt;- "#1ebd21"
plos.colors &lt;- c("#a17f78", "#ad9a27", "#ad9a27", "#ad9a27", "#ad9a27", "#ad9a27",
    "#dcebdd", "#dcebdd", "#789aa1", "#789aa1", "#789aa1", "#304345", "#304345")

# use subset of columns
alm &lt;- subset(alm, select = c("f1000", "wikipedia", "researchblogging", "comments",
    "facebook", "twitter", "citeulike", "mendeley", "pubmed", "crossref", "scopus",
    "pmc_html", "counter_html"))

# calculate percentage of values that are not missing (i.e. have a count of
# at least 1)
colSums &lt;- colSums(!is.na(alm)) * 100/length(alm$counter_html)
exactSums &lt;- sum(as.numeric(alm$pmc_html), na.rm = TRUE)

# plot the chart
opar &lt;- par(mar = c(0.1, 7.25, 0.1, 0.1) + 0.1, omi = c(0.1, 0.25, 0.1, 0.1),
    col.main = plos.color)

plos.names &lt;- c("F1000Prime", "Wikipedia", "Research Blogging", "PLOS Comments",
    "Facebook", "Twitter", "CiteULike", "Mendeley", "PubMed Citations", "CrossRef",
    "Scopus", "PMC HTML Views", "PLOS HTML Views")
y &lt;- barplot(colSums, horiz = TRUE, col = plos.colors, border = NA, xlab = plos.names,
    xlim = c(0, 120), axes = FALSE, names.arg = plos.names, las = 1, adj = 0)
text(colSums + 6, y, labels = sprintf("%1.0f%%", colSums))

par(opar)</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/2013-12-11_figure_4.svg" class="kg-image" alt="Figure 4. Article-level metrics for PLOS Biology. Proportion of all 1,706 PLOS Biology research articles published up to May 20, 2013 mentioned by particular article-level metrics source. Colors indicate categories (Viewed, Cited, Saved, Discussed, Recommended), as used on the PLOS website."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Figure 4. Article-level metrics for PLOS Biology.</strong> Proportion of all 1,706 <em style="box-sizing: border-box;">PLOS Biology</em> research articles published up to May 20, 2013 mentioned by particular article-level metrics source. Colors indicate categories (Viewed, Cited, Saved, Discussed, Recommended), as used on the PLOS website.</figcaption></figure><p>All <em>PLOS Biology</em> articles are viewed and downloaded, and almost all of them (all research articles and nearly all front matter) will be cited sooner or later. Almost all of them will also be bookmarked in online reference managers, such as Mendeley, but the percentage of articles that are discussed online is much smaller. Some of these percentages are time dependent; the use of social media discussion platforms, such as Twitter and Facebook for example, has increased in recent years (93% of <em>PLOS Biology</em> research articles published since June 2012 have been discussed on Twitter, and 63% mentioned on Facebook). These are the locations where most of the online discussion around published articles currently seems to take place; the percentage of papers with comments on the PLOS website or that have science blog posts written about them is much smaller. Not all of this online discussion is about research articles, and perhaps, not surprisingly, the most-tweeted PLOS article overall (with more than 1,100 tweets) is a <em>PLOS Biology</em> perspective on the use of social media for scientists (Bik &amp; Goldstein, 2013).</p><p>Some metrics are not so much indicators of a broad online discussion, but rather focus on highlighting articles of particular interest. For example, science blogs allow a more detailed discussion of an article as compared to comments or tweets, and journals themselves sometimes choose to highlight a paper on their own blogs, allowing for a more digestible explanation of the science for the non-expert reader (Fausto et al., 2012). Coverage by other bloggers also serves the same purpose; a good example of this is one recent post on the OpenHelix Blog (“Video Tip of the Week: Turkeys and their genomes,” 2012) that contains video footage of the second author of a 2010 <em>PLOS Biology</em> article (Dalloul et al., 2010) discussing the turkey genome.</p><p>F1000Prime, a commercial service of recommendations by expert scientists, was added to the PLOS Article-Level Metrics in August 2013. We now highlight on the PLOS website when any articles have received at least one recommendation within F1000Prime. We also monitor when an article has been cited within the widely used modern-day online encyclopedia, Wikipedia. A good example of the latter is the Tasmanian devil Wikipedia page (“Tasmanian devil,” 2013) that links to a <em>PLOS Biology</em> research article published in 2010 (Nilsson et al., 2010). While a F1000Prime recommendation is a strong endorsement from peer(s) in the scientific community, being included in a Wikipedia page is akin to making it into a textbook about the subject area and being read by a much wider audience that goes beyond the scientific community.</p><p><em>PLOS Biology</em> is the PLOS journal with the highest percentage of articles recommended in F1000Prime and mentioned in Wikipedia, but there is only partial overlap between the two groups of articles because they focus on different audiences (<strong><strong>Figure 5</strong></strong>). These recommendations and mentions in turn show correlations with other metrics, but not simple ones; you can’t assume, for example, that highly cited articles are more likely to be recommended by F1000Prime, so it will be interesting to monitor these trends now that we include this information.</p><pre><code># code for figure 5: Venn diagram F1000 vs. Wikipedia for PLOS Biology
# articles

# load required libraries
library("plyr")
library("VennDiagram")

# load May 20, 2013 ALM report
alm &lt;- read.csv("../data/alm_report_plos_biology_2013-05-20.csv", stringsAsFactors = FALSE)

# only look at research articles
alm &lt;- subset(alm, alm$article_type == "Research Article")

# group articles based on values in Wikipedia and F1000
reassignWikipedia &lt;- function(x) if (x &gt; 0) 1 else 0
alm$wikipedia_bin &lt;- aaply(alm$wikipedia, 1, reassignWikipedia)
reassignF1000 &lt;- function(x) if (x &gt; 0) 2 else 0
alm$f1000_bin &lt;- aaply(alm$f1000, 1, reassignF1000)
alm$article_group = alm$wikipedia_bin + alm$f1000_bin
reassignCombined &lt;- function(x) if (x == 3) 1 else 0
alm$combined_bin &lt;- aaply(alm$article_group, 1, reassignCombined)
reassignNo &lt;- function(x) if (x == 0) 1 else 0
alm$no_bin &lt;- aaply(alm$article_group, 1, reassignNo)

# remember to divide f1000_bin by 2, as this is the default value
summary &lt;- colSums(subset(alm, select = c("wikipedia_bin", "f1000_bin", "combined_bin",
    "no_bin")), na.rm = TRUE)
rows &lt;- nrow(alm)

# options
plos.colors &lt;- c("#c9c9c7", "#0000ff", "#ff0000")

# plot the chart
opar &lt;- par(mai = c(0.5, 0.75, 3.5, 0.5), omi = c(0.5, 0.5, 1.5, 0.5), mgp = c(3,
    0.5, 0.5), fg = "black", cex.main = 2, cex.lab = 1.5, col = plos.color,
    col.main = plos.color, col.lab = plos.color, xaxs = "i", yaxs = "i")

venn.plot &lt;- draw.triple.venn(area1 = rows, area2 = summary[1], area3 = summary[2]/2,
    n12 = summary[1], n23 = summary[3], n13 = summary[2]/2, n123 = summary[3],
    euler.d = TRUE, scaled = TRUE, fill = plos.colors, cex = 2, fontfamily = rep("sans",
        7))

par(opar)</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/2013-12-11_figure_5.svg" class="kg-image" alt="Figure 5. PLOS Biology articles: sites of recommendation and discussion. Number of PLOS Biology research articles published up to May 20, 2013 that have been recommended by F1000Prime (red) or mentioned in Wikipedia (blue)."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Figure 5. PLOS Biology articles: sites of recommendation and discussion.</strong> Number of <em style="box-sizing: border-box;">PLOS Biology</em> research articles published up to May 20, 2013 that have been recommended by F1000Prime (red) or mentioned in Wikipedia (blue).</figcaption></figure><p>With the increasing availability of ALM data, there comes a growing need to provide tools that will allow the community to interrogate them. A good first step for researchers, research administrators, and others interested in looking at the metrics of a larger set of PLOS articles is the recently launched ALM Reports tool (“ALM Reports,” 2013). There are also a growing number of service providers, including <a href="http://altmetric.com/" rel="nofollow">Altmetric.com</a> (“<a href="http://altmetric.com/" rel="nofollow">Altmetric.com</a>,” 2013), ImpactStory (“ImpactStory,” 2013), and Plum Analytics (“Plum Analytics,” 2013) that provide similar services for articles from other publishers.</p><p>As article-level metrics become increasingly used by publishers, funders, universities, and researchers, one of the major challenges to overcome is ensuring that standards and best practices are widely adopted and understood. The National Information Standards Organization (NISO) was recently awarded a grant by the Alfred P. Sloan Foundation to work on this (“NISO Alternative Assessment Metrics (Altmetrics) Project,” 2013), and PLOS is actively involved in this project. We look forward to further developing our article-level metrics and to having them adopted by other publishers, which hopefully will pave the way to their wide incorporation into research and researcher assessments.</p><h3 id="supporting-information">Supporting Information</h3><p><strong><strong><a href="http://dx.doi.org/10.1371/journal.pbio.1001687.s001">Data S1</a>. Dataset of ALM for PLOS Biology articles used in the text, and R scripts that were used to produce figures.</strong></strong> The data were collected on May 20, 2013 and include all <em>PLOS Biology</em> articles published up to that day. Data for F1000Prime were collected on August 15, 2013. All charts were produced with R version 3.0.0.</p><h2 id="references">References</h2><p>ALM Reports. (2013). Retrieved from <a href="http://almreports.plos.org/">http://almreports.plos.org</a></p><p><a href="http://altmetric.com/" rel="nofollow">Altmetric.com</a>. (2013). Retrieved from <a href="http://www.altmetric.com/">http://www.altmetric.com/</a></p><p>Bik, H. M., &amp; Goldstein, M. C. (2013). An introduction to social media for scientists. <em>PLOS Biology</em>, <em>11</em>(4), e1001535. <a href="http://doi.org/10.1371/journal.pbio.1001535">doi:10.1371/journal.pbio.1001535</a></p><p>Bollen, J., Sompel, H. de, Hagberg, A., &amp; Chute, R. (2009). A Principal Component Analysis of 39 Scientific Impact Measures. <em>PLoS ONE</em>, <em>4</em>(6), e6022. <a href="http://doi.org/10.1371/journal.pone.0006022">doi:10.1371/journal.pone.0006022</a></p><p>Campbell, P. (2008). Escape from the impact factor. <em>Ethics in Science and Environmental Politics</em>, <em>8</em>, 5–7. Journal article. <a href="http://doi.org/10.3354/esep00078">doi:10.3354/esep00078</a></p><p>Dalloul, R. A., Long, J. A., Zimin, A. V., Aslam, L., Beal, K., Blomberg, L. A., … Reed, K. M. (2010). Multi-platform next-generation sequencing of the domestic turkey (Meleagris gallopavo): genome assembly and analysis. <em>PLOS Biology</em>, <em>8</em>(9). <a href="http://doi.org/10.1371/journal.pbio.1000475">doi:10.1371/journal.pbio.1000475</a></p><p>Dickson, S. P., Wang, K., Krantz, I., Hakonarson, H., &amp; Goldstein, D. B. (2010). Rare variants create synthetic genome-wide associations. <em>PLOS Biology</em>, <em>8</em>(1), e1000294. <a href="http://doi.org/10.1371/journal.pbio.1000294">doi:10.1371/journal.pbio.1000294</a></p><p>Eck, N. J. van, Waltman, L., Raan, A. F. J. van, Klautz, R. J. M., &amp; Peul, W. C. (2013). Citation analysis may severely underestimate the impact of clinical research as compared to basic research. <em>PLOS ONE</em>, <em>8</em>(4), e62395. <a href="http://doi.org/10.1371/journal.pone.0062395">doi:10.1371/journal.pone.0062395</a></p><p>Eysenbach, G. (2006). Citation advantage of open access articles. <em>PLOS Biology</em>, <em>4</em>(5), e157. <a href="http://doi.org/10.1371/journal.pbio.0040157">doi:10.1371/journal.pbio.0040157</a></p><p>Fausto, S., Machado, F. A., Bento, L. F. J., Iamarino, A., Nahas, T. R., &amp; Munger, D. S. (2012). Research blogging: indexing and registering the change in science 2.0. <em>PLOS ONE</em>, <em>7</em>(12), e50109. <a href="http://doi.org/10.1371/journal.pone.0050109">doi:10.1371/journal.pone.0050109</a></p><p>Floreano, D., &amp; Keller, L. (2010). Evolution of adaptive behaviour in robots by means of Darwinian selection. <em>PLOS Biology</em>, <em>8</em>(1), e1000292. <a href="http://doi.org/10.1371/journal.pbio.1000292">doi:10.1371/journal.pbio.1000292</a></p><p>Glänzel, W., &amp; Wouters, P. (2013). The dos and don’ts in individudal level bibliometrics. Retrieved from <a href="http://de.slideshare.net/paulwouters1/issi2013-wg-pw">http://de.slideshare.net/paulwouters1/issi2013-wg-pw</a></p><p>ImpactStory. (2013). Retrieved from <a href="http://impactstory.org/">http://impactstory.org/</a></p><p>Lin, J., &amp; Fenner, M. (2013). Altmetrics in Evolution: Defining and Redefining the Ontology of Article-Level Metrics. <em>Information Standards Quarterly</em>, <em>25</em>(2), 20. <a href="http://doi.org/10.3789/isqv25no2.2013.04">doi:10.3789/isqv25no2.2013.04</a></p><p>Narendra, D. P., Jin, S. M., Tanaka, A., Suen, D.-F., Gautier, C. A., Shen, J., … Youle, R. J. (2010). PINK1 is selectively stabilized on impaired mitochondria to activate Parkin. <em>PLOS Biology</em>, <em>8</em>(1), e1000298. <a href="http://doi.org/10.1371/journal.pbio.1000298">doi:10.1371/journal.pbio.1000298</a></p><p>Nilsson, M. A., Churakov, G., Sommer, M., Tran, N. V., Zemann, A., Brosius, J., &amp; Schmitz, J. (2010). Tracking marsupial evolution using archaic genomic retroposon insertions. <em>PLOS Biology</em>, <em>8</em>(7), e1000436. <a href="http://doi.org/10.1371/journal.pbio.1000436">doi:10.1371/journal.pbio.1000436</a></p><p>NISO Alternative Assessment Metrics (Altmetrics) Project. (2013). Retrieved from <a href="http://www.niso.org/topics/tl/altmetrics/initiative">http://www.niso.org/topics/tl/altmetrics/initiative</a></p><p>Plum Analytics. (2013). Retrieved from <a href="http://www.plumanalytics.com/">http://www.plumanalytics.com/</a></p><p>Schekman, R., &amp; Patterson, M. (2013). Reforming research assessment. <em>eLife</em>, <em>2</em>, e00855. <a href="http://doi.org/10.7554/eLife.00855">doi:10.7554/eLife.00855</a></p><p>Tasmanian devil. (2013). Retrieved from <a href="http://en.wikipedia.org/wiki/Tasmanian%5Cdevil">http://en.wikipedia.org/wiki/Tasmanian\devil</a></p><p>Video Tip of the Week: Turkeys and their genomes. (2012). Retrieved from <a href="http://blog.openhelix.eu/?p=14388">http://blog.openhelix.eu/?p=14388</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Opening Science - the Book]]></title>
            <link>https://blog.martinfenner.org/posts/opening-science-the-book</link>
            <guid>98d1309e-cc0d-4f2d-8d50-66be35f7d9ad</guid>
            <pubDate>Thu, 05 Dec 2013 16:06:00 GMT</pubDate>
            <description><![CDATA[Opening Science: The Evolving Guide on How the Internet is Changing Research,
Collaboration and Scholarly Publishing
[http://www.openingscience.org/get-the-book/] is a SpringerOpen book (using a 
Creative Commons Attribution-NonCommercial license
[http://book.openingscience.org/cases_recipes_howtos/creative_commons_licences])
that will be published in a few weeks. If you can’t wait for the book to be
published and/or you want to make comments or suggestions, go to the dynamic
book online version]]></description>
            <content:encoded><![CDATA[<p><a href="http://www.openingscience.org/get-the-book/">Opening Science: The Evolving Guide on How the Internet is Changing Research, Collaboration and Scholarly Publishing</a> is a SpringerOpen book (using a <a href="http://book.openingscience.org/cases_recipes_howtos/creative_commons_licences">Creative Commons Attribution-NonCommercial license</a>) that will be published in a few weeks. If you can’t wait for the book to be published and/or you want to make comments or suggestions, go to the dynamic book online version at <a href="http://book.openingscience.org/">http://book.openingscience.org</a>. I am an author or co-author of three chapters (<a href="http://book.openingscience.org/tools/reference_management">Reference Management</a>, <a href="http://book.openingscience.org/vision/altmetrics">Altmetrics and Other Novel Measures for Scientific Impact</a>, <a href="http://book.openingscience.org/cases_recipes_howtos/unique_identifiers_for_researchers">Unique Identifiers for Researchers</a>) and have helped put the dynamic book together. The book is generated from markdown files hosted in a <a href="https://github.com/openingscience/book/">public Github repo</a> using <a href="http://jekyllrb.com/">Jekyll</a> and <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a>, and we use <a href="http://prose.io/">Prose</a> to enable online editing of the content.</p><p>Using markdown, github, jekyll and pandoc is nothing new for blogs, but this is probably one of the first scholarly books using this workflow. The dynamic book is therefore still very much work in progress and feedback is greatly appreciated.</p><p>Another great example using a very similar workflow is the upcoming book <a href="http://adv-r.had.co.nz/">Advanced R Programming</a> by Hadley Wickham, but he is of course using R and <a href="http://yihui.name/knitr/">knitr</a> to create most of the markdown. In contrast to Hadley we stored the individual chapters as Jekyll posts rather than pages, as this better integrates with other Jekyll functionality, e.g. tags.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Grammar of Scholarly Communication]]></title>
            <link>https://blog.martinfenner.org/posts/the-grammar-of-scholarly-communication</link>
            <guid>31f71610-289f-4a49-891e-b49ed9112d4d</guid>
            <pubDate>Sun, 17 Nov 2013 16:10:00 GMT</pubDate>
            <description><![CDATA[Authoring of scholarly articles is a recurring theme in this blog since it
started in 2008. Authoring is still in desperate need for improvement, and
nobody has convincingly figured out how to solve this problem. Authoring
involves several steps, and it helps to think about them separately:

 * Writing. Manuscript writing, including formatting, collaborative authoring
 * Submission. Formatting a manuscript according to a publisher’s author
   guidelines, and handing it over to a publishing platf]]></description>
            <content:encoded><![CDATA[<p>Authoring of scholarly articles is a recurring theme in this blog since it started in 2008. Authoring is still in desperate need for improvement, and nobody has convincingly figured out how to solve this problem. Authoring involves several steps, and it helps to think about them separately:</p><ul><li><strong><strong>Writing</strong></strong>. Manuscript writing, including formatting, collaborative authoring</li><li><strong><strong>Submission</strong></strong>. Formatting a manuscript according to a publisher’s author guidelines, and handing it over to a publishing platform</li><li><strong><strong>Revision</strong></strong>. Changes made to a manuscript in the peer review process, or after publication</li></ul><p>Although authoring typically involves text, similar issues arise for other research outputs, e.g research data. And these considerations are also relevant for other forms of publishing, whether it is self-publication on a blog or website, or publishing of preprints and white papers.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/grammar.jpg" class="kg-image" alt="Flickr photo by citnaj."><figcaption>Flickr photo by <a href="http://www.flickr.com/photos/citnaj/1278021067/" style="box-sizing: border-box; background: transparent; color: rgb(52, 152, 219); text-decoration: none;">citnaj</a>.</figcaption></figure><p>For me the main challenge in authoring is to go from human-readable unstructured content to highly structured machine-readable content. We could make authoring simpler by either forgoing any structure and just publishing in any format we want, or we can force authors to structure their manuscripts according to a very specific set of rules. The former doesn’t seem to be an option, not only do we have a set of community standards that have evolved for a very long time (research articles for example have title, authors, results, references, etc.), but it also makes it hard to find and reuse scholarly research by others.</p><p>The latter option is also not really viable since most researchers haven’t learned to produce their research outputs in machine-readable highly standardized formats. There are some exceptions, e.g. <a href="http://www.consort-statement.org/">CONSORT</a> and other reporting standards in clinical medicine or the <a href="http://blogs.ch.cam.ac.uk/pmr/2012/01/23/brian-mcmahon-publishing-semantic-crystallography-every-science-data-publisher-should-watch-this-all-the-way-through/">semantic publishing in Crystallography</a>, but for the most part research outputs are too diverse to easily find a format that works for all of them. The current trend is certainly towards machine-readable rather than towards human-readable, but there is still a significant gap - scholarly articles are transformed from documents in Microsoft Word (or sometimes LaTeX) format into XML (for most biomedical research that means <a href="http://jats.nlm.nih.gov/publishing/">JATS</a>) using kludgy tools and lots of manual labor.</p><p>What solutions have been tried to overcome the limitations of our current authoring tools, and to make the process more enjoyable for authors and more productive for publishers?</p><ol><li>Do the conversion manually, still a common workflow.</li><li>Tools for publishers such as <a href="http://blogs.plos.org/mfenner/2009/05/01/extyles_interview_with_elizabeth_blake_and_bruce_rosenblum/">eXtyles</a>, <a href="http://www.shabash.net/merops/">Merops</a> - both commercial - or the evolving Open Source <a href="http://www.lib.umich.edu/mpach/modules">mPach</a> that convert Microsoft Word documents into JATS XML and do a lot of automated checks along the way.</li><li>Tools for authors that directly generate JATS XML, either as a Microsoft Word plugin (the <a href="http://blogs.nature.com/mfenner/2008/11/07/interview-with-pablo-fernicola">Article Authoring Add-In</a>, not actively maintained) in the browser (e.g. <a href="http://blogs.plos.org/mfenner/2009/02/27/lemon8_xml_interview_with_mj_suhonos/">Lemon8-XML</a>, not actively maintained), or directly in a publishing platform such as Wordpress (<a href="http://annotum.org/">Annotum</a>).</li><li>Forget about XML and use HTML5 has the canonical file format, e.g. as <a href="http://blogs.plos.org/mfenner/2011/03/19/a-very-brief-history-of-scholarly-html/">Scholarly HTML</a> or HTML5 specifications such as <a href="https://github.com/oreillymedia/HTMLBook/blob/master/specification.asciidoc">HTMLBook</a>. Please read Molly Sharp’s <a href="http://blogs.plos.org/tech/structured-documents-for-science-jats-xml-as-canonical-content-format/">blog post</a> for background information about HTML as an alternative to XML.</li><li>Use file formats for authoring that are a better fit for the requirements of scholarly authors, in particular <a href="http://blog.martinfenner.org/2012/12/13/a-call-for-scholarly-markdown/">Scholarly Markdown</a>.</li><li>Build online editors for scientific content that hide the underlying file format, and guide users towards a structured format, e.g. by not allowing input that doesn’t conform to specifications.</li></ol><p><strong><strong>Solution 1.</strong></strong> isn’t really an option, as it makes scholarly publishing unnecessarily slow and expensive. Typesetter Kaveh Bazergan has gone on record at the <a href="http://www.nature.com/spoton/2012/11/spoton-london-2012-a-global-conference/">SpotOn London Conference 2012</a> by saying that the current process is insane and that he wants to be “put out of business”.</p><p><strong><strong>Solution 2.</strong></strong> is probably the most commonly used workflow used by larger publishers today, but is very much centered around a Microsoft Word to XML workflow. LaTeX is a popular authoring environment in some disciplines, but still requires work to convert documents into web-friendly formats such as HTML and XML.</p><p><strong><strong>Solutions 3. to 5.</strong></strong> have never picked up any significant traction. Overall the progress in this area has been modest at best, and the mainstream of authoring today isn’t too different from 20 years ago. Although I have gone on record for saying that <a href="http://blog.martinfenner.org/tags.html#markdown-ref">Scholarly Markdown</a> has a lot of potential, the problem is much bigger than finding a single file format, and markdown will never be the solution for all authoring needs.</p><p><strong><strong>Solution 6.</strong></strong> is an area where a lot of exciting development is currently happening, examples include <a href="https://www.authorea.com/">Authorea</a>, <a href="https://www.writelatex.com/">WriteLateX</a>, <a href="https://www.sharelatex.com/">ShareLaTeX</a>. Although the future of scholarly authoring will certainly include online authoring tools (making it much easier to collaborate, one of the authoring pain points), we run the risk of locking in users into one particular authoring environment.</p><h3 id="going-forward">Going Forward</h3><p>How can we move forward? I would suggest the following:</p><ol><li>Publishers should accept manuscripts in any reasonable file format, which means at least Microsoft Word, Open Office, LaTeX, Markdown, HTML and PDF, but possibly more. This will create a lot of extra work for publishers, but will open the doors for innovation, both in the academic and commercial sector. We will never see significant progress in scholarly authoring tools if the submission step requires manuscripts to be in a single file format (Microsoft Word) - in particular since this file format is a general purpose word processsing format and not something designed specifically for scholarly content. And we want researchers to spend their time doing research and writing up their research, not formatting documents.</li><li>To handle this avalanche of unstructured documents, publishers need conversion tools that can transform all these documents into a format that can feed into their editorial and publishing workflows. A limited number of these tools exist already, but this will require a significant development effort. Again, opening up submissions to a variety of file formats will not only foster innovation in authoring tools, but also in document conversion tools.</li><li>We should think beyond XML. Many of the workflows designed today center around conversions from one XML format to another, e.g. Microsoft Word to JATS or <a href="http://www.tei-c.org/index.xml">TEI</a> (popular in the humanities), often using XLST transforms. Not only is XML difficult for humans to read or edit, but the web and many of the technologies built around it are moving away from XML towards HTML5 and JSON. XML is fine as an important output format for publishing, but maybe not the best format to hold everything together.</li><li>As we haven’t come up with a canoical file format for scholarly documents by now, we should give up that idea. XML is great for publisher workflows, but is not something humans can easily edit or read. PDF is still the most widely read format by humans, but is not a good intermediary format. LaTeX is too complex for authors outside of mathematics, physics and related fields, and is not built with web standards in mind. Markdown is promising, but doesn’t easily support highly structured content. And HTML5 and the related ePub are widely popular, but can be hard to edit without a visual editor, and currently don’t include enough standard metadata to support scholarly content out of the box.</li><li>The focus should not be on canonical file formats for scholarly documents, but on tools that understand the manuscripts created by researchers and can transform them into something more structured. As we have learned from document conversion tools such as <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a>, we can’t do this with a simple find and replace using regular expressions, but need a more structured approach. Pandoc is taking the input document (markdown, LaTeX or HTML) apart and is constructing an abstract syntax tree (<a href="http://en.wikipedia.org/wiki/Abstract_syntax_tree">AST</a>) of the document, using parsing expression grammar (<a href="http://en.wikipedia.org/wiki/Parsing_expression_grammar">PEG</a>), which includes a set of parsing rules. Parsing expression grammars are fairly new, <a href="http://bford.info/pub/lang/peg">first described by Bryan Ford</a> about 10 years ago, but in my mind are a very good fit for the formal grammar of scientific documents. It should be fairly straightforward to generate a variety of output formats from the AST (Pandoc can convert into more than 30 document formats), the hard part is the parsing of the input.</li></ol><p>All this requires a lot of work. Pandoc is a good model to start, but is written in Haskell, a functional programming language that not many people are familar with. For small changes Pandoc allows you to directly manipulate the AST (represented as JSON) using <a href="http://johnmacfarlane.net/pandoc/scripting.html">filters</a> written in Haskell or Python. And <a href="https://github.com/jgm/pandoc">custom writers</a> for other document formats can be written using <a href="http://www.lua.org/">Lua</a>, another interesting programming language that not many people know about. Lua is a fast and relatively easy to learn scripting language that can be easily embedded into other languages, and for similar reasons is also used to <a href="http://en.wikipedia.org/wiki/Wikipedia:Lua">extend the functionality of Wikipedia</a>. PEG parsers in other languages include <a href="http://treetop.rubyforge.org/">Treetop</a> (Ruby), <a href="http://pegjs.majda.cz/">PEG.js</a> (Javascript), and <a href="http://www.antlr.org/">ANTLR</a>, a popular parser generator that also includes PEG features.</p><p>But I think the effort to build a solid open source conversion tool for scholarly documents is worth it, in particular for smaller publishers and publishing platforms who can’t afford the commercial Microsoft Word to JATS conversion tools. We shouldn’t take any shortcuts - e.g. by focussing on XML and XLST transforms - and we can improve this tool over time, e.g. by starting with a few input and output formats. This tool will be valuable beyond authoring, as it can also be very helpful to convert published scholarly content into other formats such as ePub, and in text mining, which in many ways tries to solve many of the same problems. The <a href="http://johnmacfarlane.net/pandoc/scripting.html">Pandoc documentation</a> includes an example of extracting all URLs out of a document, and this can be modified to extract other content. In case you wonder whether I gave up on the idea of <a href="http://blog.martinfenner.org/tags.html#markdown-ref">Scholarly Markdown</a> - not at all. To me this is a logical next step, opening up journal submission systems to Scholarly Markdown and other evolving file formats. And Pandoc, one of the most interesting tools in this space, is a markdown conversion tool at its heart. The next steps could be the following:</p><ul><li>write a custom writer in Lua that generates JATS output from Pandoc</li><li>explore how difficult it would be to add Microsoft Word .docx as Pandoc input format</li><li>develop Pandoc filters relevant for scholarly documents (e.g. <a href="http://blog.martinfenner.org/2013/07/02/auto-generating-links-to-data-and-resources/">auto-linking accession numbers of biomedical databases</a>)</li></ul><hr>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What is holding us back?]]></title>
            <link>https://blog.martinfenner.org/posts/what-is-holding-us-back</link>
            <guid>d603c9b3-2089-4425-a680-e3c04fcbf5ce</guid>
            <pubDate>Mon, 11 Nov 2013 16:14:00 GMT</pubDate>
            <description><![CDATA[Last Friday and Saturday the 6th SpotOn London conference
[http://www.nature.com/spoton/event/spoton-london2013/] tool place at the
British Library. I had a great time with many interesting sessions and good
conversations both in and between sessions. But I might be biased, since I
helped organize the event, and in particular did help put the sessions for the
Tools strand [http://www.nature.com/spoton/?cat=11] together.

SpotOn London name tags. Flickr photo by keatl
[http://www.flickr.com/photo]]></description>
            <content:encoded><![CDATA[<p>Last Friday and Saturday the 6th <a href="http://www.nature.com/spoton/event/spoton-london2013/">SpotOn London conference</a> tool place at the British Library. I had a great time with many interesting sessions and good conversations both in and between sessions. But I might be biased, since I helped organize the event, and in particular did help put the <a href="http://www.nature.com/spoton/?cat=11">sessions for the Tools strand</a> together.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/blendology.jpg" class="kg-image" alt="SpotOn London name tags. Flickr photo by keatl."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">SpotOn London name tags</strong>. Flickr photo by <a href="http://www.flickr.com/photos/keatl/10739125344/in/photolist-hmYPPw-9CVkfd/" style="box-sizing: border-box; background: transparent; color: rgb(52, 152, 219); text-decoration: none;">keatl</a>.</figcaption></figure><p>The following blog post summarizes some of my thoughts before, during and after the conference, and I want to focus on innovation in scholarly publishing, or rather: what is holding us back?</p><h2 id="reason-1">Reason #1</h2><p>The <a href="http://www.nature.com/spoton/event/spoton-london-2013-whats-your-number-altmetrics-session/">#solo13alt</a> session on Saturday looked at <em>the role of altmetrics in the evaluation of scientific research</em>. I was one of the panelists and had summarized my ideas prior to the session in a <a href="http://blogs.plos.org/tech/evaluating-impact-whats-your-number/">blog post</a> written together with Jennifer Lin. It was an interesting session, although a bit too controversial for my taste. But it became obvious to me in this and a few other sessions that other obsession with quantitative assessment of science is increasingly dangerous. Other people have said this more eloquently:</p><ul><li>The mania for measurement - Stephen Curry in the <a href="http://www.nature.com/spoton/event/spoton-london-2013-whats-your-number-altmetrics-session/">#solo13alt</a> session</li><li>Why research assessment is out of control - <a href="http://www.theguardian.com/education/2013/nov/04/peter-scott-research-excellence-framework">Peter Scott</a></li><li>Universities are becoming metrics factories, driven by large corporates - <a href="http://blogs.ch.cam.ac.uk/pmr/2013/11/10/spoton2013-yet-another-wonderful-meeting/">Peter Murray-Rust</a></li><li>The ‘real’ revolution in science will come when the scientific egosystem gets rid of the credit-imperative - <a href="https://twitter.com/Villavelius/status/399157271793762304">Jan Velterop</a></li><li>Excellence by Nonsense: The Competition for Publications in Modern Science - <a href="http://book.openingscience.org/basics_background/excellence_by_nonsense/">Mathias Binswanger</a></li></ul><p>My job title is <em>Technical Lead Article-Level Metrics</em> so it might sound surprising that I say this. But we have to differentiate of what we do now and in the next few years - which is mainly to get away from the Journal Impact Factor to more reasonable metrics that look at individual articles and include other metrics besides citations - to where we want to be in 10 or more years. And for the latter it is essential that journal articles and other research outputs are valued for the research they contain, rather than serving as a currency for <em>merit</em> that can be exchanged into grants and acadmic advancement. This is a very difficult problem to solve and I have no answers yet. Going back to how science was conducted until about 50 years ago - as a small elite club that worked based on closed personal networks - is definitely not the answer.</p><h2 id="reason-2">Reason #2</h2><p>In <a href="http://www.nature.com/spoton/event/spoton-london-2013-keynote-1-boson-50-years-50003-scientists-understanding-our-universe-through-global-scientific-collaboration-and-open-access/">his keynote</a> Salvatore Mele from CERN explained to us that Open Access in High Energy Phsics is 50 years old, and that the culture of sharing preprints preceeded the <a href="http://arxiv.org/">ArXiv</a> e-prints service - scientists were mailing their manuscripts to each other at least 20 years before ArXiV launched in 1991. A similar culture doesn’t exist in the life sciences and therefore the preprint services for biologists launched this year (e.g. <a href="https://peerj.com/preprints/">PeerJ Preprints</a> and <a href="http://biorxiv.org/">bioRxiv</a>) will have a hard time gaining traction.</p><p>Email is one of those services that every researcher uses, and we should think much more about how we can create innovative services around email rather than only considerung new tools and services that are still used only by early adopters. AJ Cann had coordinated a workshop around email at SpotOn London that he called <a href="http://www.nature.com/spoton/event/spoton-london-2013-the-dark-art-of-dark-social-email-the-antisocial-medium-which-will-not-die-workshop/">the dark art of dark social: email, the antisocial medium that will not die</a>. I am still puzzled why most researchers prefer to receive tables of content by email rather than as a RSS feed, but we shouldn’t confuse what we get excited about as software developers and early adopters of online tools with what the mainstream scientist would be likely to use.</p><p>Another good example is <a href="http://royalsociety.org/policy/projects/science-public-enterprise/report/">data sharing</a>, a topic that was discussed in at least three SpotOn sessions. Even though most attendees at SpotOn London agreed that sharing of research data is important, it is obvious that this is currently not common practice in most scientific disciplines. Funders have created data sharing policies (e.g. <a href="http://www.nsf.gov/bfa/dias/policy/dmp.jsp">NSF</a> or the <a href="http://www.wellcome.ac.uk/About-us/Policy/Spotlight-issues/Data-sharing/">Wellcome Trust</a>), as <a href="http://dx.doi.org/10.1371/journal.pone.0067111">have publishers</a>, and many organizations are thinking about incentives for data sharing, including data journals such as <a href="http://www.nature.com/scientificdata/">Scientific Data</a> that will launch in 2014 and was presented by Ruth Wilson in the <a href="http://www.nature.com/spoton/event/spoton-london-2013-how-can-we-encourage-data-sharing-discussion/">motivations for data sharing</a> session. Even though incentives can help promote changes, I am pessimistic that something as central to the conduct of science as data sharing can be changed without more scientists being intrinsically motivated to do so. This is a much slower process that should start as early as possible during training, as pointed out by Kaitlin Thaney in the <a href="http://www.nature.com/spoton/event/spoton-london-2013-how-can-we-encourage-data-sharing-discussion/">#solo13carrot</a> session.</p><h2 id="reason-3">Reason #3</h2><p>In terms of the technology that is holding us back, I increasingly think that publisher manuscript submission systems may be the single most important place that is slowing down innovation. I participated in the first <a href="https://sites.google.com/site/beyondthepdf/">Beyond the PDF</a> workshop in 2011, and I think now that <strong><strong>Beyond the MTS (or manuscript tracking system)</strong></strong> might have been a better motto than <strong><strong>Beyond the PDF</strong></strong>, as many of the problems we discussed relate to typical editorial workflows we use today. These systems need to implement many of the ideas discussed at SpotOn London and other places, from opening up peer review (<a href="http://www.nature.com/spoton/event/spoton-london-2013-how-should-peer-review-evolve/">#solo13peer</a>) to making it easier to integrate research data into manuscripts (<a href="http://www.nature.com/spoton/event/spoton-london-2013-how-should-peer-review-evolve/">#solo13carrot</a>) and to ideas of how the scientific record should like in the digital age (<a href="http://www.nature.com/spoton/event/spoton-london-2013-what-should-the-scientific-record-look-like-in-the-digital-age-discussion/">#solo13digital</a>). In the latter panel we discussed both new authoring tools such as <a href="https://www.writelatex.com/">WriteLaTeX</a>, and new ideas of what a research object should look like and how the different parts are linked to each other. A major theme here was reproducibility highlighted both by Carol Goble (also see her <a href="http://www.slideshare.net/carolegoble/ismb2013-keynotecleangoble">ISMB/ECCB 2013 Keynote</a>) and Peter Kraker (see also his <a href="http://science.okfn.org/2013/10/18/its-not-only-peer-reviewed-its-reproducible/">Open Knowledge Foundation blog post</a>).</p><p>The problem with today’s manuscript submission systems is that they have grown so big and complex that any change is slow and cumbersome, rather than iterative and part of an ongoing dialogue. I don’t want to blame any single vendor of these systems, but rather suggest that we carefully re-evaluate the workflow from the manuscript written by one or more authors to the accepted manuscript. My personal interest is mainly in authoring tools, and I have recently written about and experimented with <a href="http://localhost:4000/tags.html#markdown-ref">Markdown</a>. This process of re-evaluating manuscript tracking systems is not simply about technology, but is rather about how we approach this problem as author, publisher, tool vendor and as a community.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What is the Value of Hack Days?]]></title>
            <link>https://blog.martinfenner.org/posts/what-is-the-value-of-hack-days</link>
            <guid>eb96ef52-422c-49cd-86eb-9c2d49967461</guid>
            <pubDate>Mon, 04 Nov 2013 16:18:00 GMT</pubDate>
            <description><![CDATA[This Friday and Saturday the SpotOn London Conference
[http://www.nature.com/spoton/event/spoton-london2013/] will take place at the
British Library in London. I am very excited, as I have come to this conference
since the first one in 2008
[https://twitter.com/McDawg/status/397068628102610945], and have helped organize
the event since 2009. The conference is about science communication in the
broadest sense, and has three strands that focus on science communication,
science policy and tools. Eq]]></description>
            <content:encoded><![CDATA[<p>This Friday and Saturday the <a href="http://www.nature.com/spoton/event/spoton-london2013/">SpotOn London Conference</a> will take place at the British Library in London. I am very excited, as I have come to this conference since the <a href="https://twitter.com/McDawg/status/397068628102610945">first one in 2008</a>, and have helped organize the event since 2009. The conference is about science communication in the broadest sense, and has three strands that focus on <em>science communication, science policy and tools</em>. Equally important as the sessions are of course the many highly engaging informal discussions of the 250 participants that take place between and after the sessions.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/spoton12_hack.jpg" class="kg-image" alt="Presenting from SpotOn London 2012 hackathon. One of the projects in 2012 was a collaborative commenting system. Picture from Flickr, taken by Lou Woodley."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Presenting from SpotOn London 2012 hackathon</strong>. One of the projects in 2012 was a collaborative commenting system. Picture from <a href="http://www.flickr.com/photos/25467658@N00/8252989528/" style="box-sizing: border-box; background: transparent; color: rgb(52, 152, 219); text-decoration: none;">Flickr</a>, taken by Lou Woodley.</figcaption></figure><p>SpotOn London sessions are also more conversations than presentations, as they usually have 2-4 panelists with ample time for discussion with the audience. I will take part in two panels:</p><ul><li><a href="http://www.nature.com/spoton/event/spoton-london-2013-what-the-hack-part-one-hackdays-session/">What the hack?!</a> (Friday 10:30 AM, hashtag <a href="https://twitter.com/search?q=%23solo13hack">#solo13hack</a>), with Peter Murray-Rust, Ross Mounce and Helen Jackson</li><li><a href="http://www.nature.com/spoton/event/spoton-london-2013-whats-your-number-altmetrics-session/">What’s your number? - Altmetrics session</a> (Saturday 10:30 AM, hashtag <a href="https://twitter.com/search?q=%23solo13alt">#solo13alt</a>), with Marie Boran, David Colquhoun, Jean Liu and Stephen Curry</li></ul><p>I will summarize my thoughts regarding the altmetrics session in another post, but want to talk about the first session in more detail. According to the <a href="http://en.wikipedia.org/wiki/Hackathon">English Wikipedia</a></p><blockquote>A <strong><strong>hackathon</strong></strong> (also known as a <strong><strong>hack day</strong></strong>, <strong><strong>hackfest</strong></strong> or <strong><strong>codefest</strong></strong>) is an event in which computer programmers and others involved in software development, including graphic designers, interface designers and project managers, collaborate intensively on software projects.</blockquote><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/Wikimedia_hackathon_020_-_Berlin_2012_03.jpg" class="kg-image" alt="Wikimedia Hackathon Berlin June 2012. Largest hackathon I have attended so far with 100 people. Photo by Gulliaume Paumier, CC-BY license."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Wikimedia Hackathon Berlin June 2012</strong>. Largest hackathon I have attended so far with 100 people. Photo by Gulliaume Paumier, CC-BY license.</figcaption></figure><p>It is too bad that we will have no hackathon at year’s SpotOn London for logistical reasons, but the session is a great opportunity to reflect on the value of science hackdays. It is clear that hackdays for scientific software have become popular, with almost too many opportunities to participate.</p><h3 id="what-i-like">What I like</h3><ul><li>Do stuff. And have plenty of time to do stuff instead sessions in short intervals</li><li>Hackdays let you do great team work</li><li>Learn about other interesting projects and meet people doing cool work</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/alm12_anti_gaming.png" class="kg-image" alt="ALM 2012 hackathon. Brainstorming board from anti-gaming group."><figcaption><a href="http://article-level-metrics.plos.org/alm-workshop-2012/hackathon/" style="box-sizing: border-box; background: transparent; color: rgb(52, 152, 219); text-decoration: none;"><strong style="box-sizing: border-box; font-weight: bold;">ALM 2012 hackathon</strong></a>. Brainstorming board from anti-gaming group.</figcaption></figure><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/group_foto.jpg" class="kg-image" alt="#hack4ac. Our team working on PLOS Author Contributions."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">#hack4ac</strong>. Our team working on <a href="http://hack4ac.com/plos-author-contributions/" style="box-sizing: border-box; background: transparent; color: rgb(52, 152, 219); text-decoration: none;">PLOS Author Contributions</a>.</figcaption></figure><h3 id="what-i-don-t-like">What I don’t like</h3><ul><li>Hackdays are very much targeted at intermediate to advanced software developers, and it is sometimes not easy for beginners to participate</li><li>Too much time spent setting up stuff</li><li>Some of the work done at hackdays can be better done in virtual collaborations over weeks or months</li><li>Not many projects make it beyond the hackday and actually turn into a useable product. One example where this is not true are the visualizations started at the ALM 2012 hackathon that were implemented by OJS in 2013 (<a href="http://dx.doi.org/10.3402/gha.v6i0.19283">see article for more</a>), and of course <a href="http://impactstory.org/">ImpactStory</a> that started at a hackathon at the <a href="http://beyond-impact.org/">Beyond Impact</a> conference in May 2011.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/alm_d3.png" class="kg-image" alt="ALM 2012 hackathon. Sparkline visualization implemented by OJS based on work at the workshop."><figcaption><a href="http://article-level-metrics.plos.org/alm-workshop-2012/hackathon/" style="box-sizing: border-box; background: transparent; color: rgb(52, 152, 219); text-decoration: none;"><strong style="box-sizing: border-box; font-weight: bold;">ALM 2012 hackathon</strong></a>. Sparkline visualization implemented by OJS based on work at the workshop.</figcaption></figure><h3 id="some-of-the-challenges">Some of the challenges</h3><ul><li>Coming up with projects where progress can be made in a day or two</li><li>Technology: WiFi access, access to servers for code deployment, collaboration tools, etc.</li><li>Come up with a good unifying theme, so that the various projects during the hackday relate to each other. The theme at <a href="http://hack4ac.com/">#hack4ac</a> was to demonstrate the value of the CC-BY license within academia.</li></ul><h3 id="some-ideas-to-improve-science-hackdays">Some ideas to improve science hackdays</h3><ul><li>Go beyond software development. We <a href="http://blogs.plos.org/tech/alm-data-challenge-metrics-for-a-standard-set-of-dois/">recently tried a data challenge using Altmetrics data</a>, and at a <a href="http://blog.martinfenner.org/2013/07/02/auto-generating-links-to-data-and-resources/">hackathon between IGSN, DataCite, PANGAEA and ORCID in July</a> we focussed on a high-level discussion of technical issues. There is a continuum towards the <a href="http://en.wikipedia.org/wiki/BarCamp">BarCamp</a> format, although I don’t like to drift too much from <em>doing</em> to <em>talking</em>. A good example of a workshop open to everyone and not just software developers is the SpotOn London workshop this Saturday on <a href="http://www.nature.com/spoton/event/spoton-london-2013-wikipedia-editing-workshop/">Wikipedia Editing</a> run by Brian Kelly and Toni Sant.</li><li>Meet before and after the hackathon. This can be done online, but it helps to focus on what can be achieved in the limited time available for a hackathon, and to follow up on projects that have just been started. But a hackathon is also a great opportunity to meet new people and new ideas, so meeting afterwards is more important than before.</li><li>Involve remote people. A lot of the fun of hackdays comes from sitting around a table and doing something together. But sometimes this is not possible for everyone, so think about remote participation where it makes sense.</li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Commenting on scientific papers]]></title>
            <link>https://blog.martinfenner.org/posts/commenting-on-scientific-papers</link>
            <guid>efacbd9d-fcd9-4ec5-a23d-075f0c1f88d6</guid>
            <pubDate>Fri, 25 Oct 2013 16:21:00 GMT</pubDate>
            <description><![CDATA[I think it is fair to say that commenting on scientific papers is broken. And
with commenting I mean online comments that are publicly available, not informal
discussions in journal clubs or at meetings. This definition would include
discussions of papers on social media such as Twitter or Facebook. Why do I
think that commenting is broken?

 * the number of papers with online comments is low. For PLOS Biology we have
   comments on the journal platform for 11% of articles, tweets for 14% of
   ]]></description>
            <content:encoded><![CDATA[<p>I think it is fair to say that commenting on scientific papers is broken. And with commenting I mean online comments that are publicly available, not informal discussions in journal clubs or at meetings. This definition would include discussions of papers on social media such as Twitter or Facebook. Why do I think that commenting is broken?</p><ul><li>the number of papers with online comments is low. For PLOS Biology we have comments on the journal platform for 11% of articles, tweets for 14% of articles and Facebook activity for 22% of articles (Fenner, 2013). The numbers for Twitter and Facebook are much higher for more recently published articles, but are nowhere close to every article having at least one comment.</li><li>even though there is a fair amount of social media activity around articles, the quality of the discussion is varied. Twitter for example seems to work mostly as an alerting service for interesting articles with little more than the title of the article in the tweet text and not much discussion.</li><li>when comments are made, they are really hard to find coming from the article. Unless they are made on the journal platform, or the publisher tracks article-level metrics and links out to these comments.</li></ul><p>What can be done to address these issues, i.e. increase the number of comments, increase the depth of the discussion, and make it easier to link comments to articles? Some of the thoughts that I and others have had include the following:</p><ul><li>lower the technical barriers for commenting by providing a common and familiar commenting platform with an attractive user interface. Many blogs (including this one) and <a href="http://elife.elifesciences.org/">some publishers</a> use Disqus, which is arguably the most popular third-party commenting platform.</li><li>develop new features that make commenting more attractive, including comments linked to specific sections of the text and notes that can be public, semi-public or private. See for example <a href="https://medium.com/about/5972c72b18f2">what Medium is doing</a>, check out <a href="http://hypothes.is/">Hypothes.is</a> and <a href="http://blog.peerj.com/post/62886292466/peerj-questions-a-new-way-to-never-publish-forget">PeerJ Questions</a>, or study what services such as <a href="http://stackoverflow.com/">Stackoverflow</a> are doing.</li><li>Link comments made in different places about the same object together, e.g. through Article-Level Metrics services.</li><li>create incentives for scientists to comment, e.g. through <a href="http://openbadges.org/">Mozilla Open Badges</a> or by making them part of a community.</li></ul><p>On Tuesday the US National Library of Medicine launched <a href="http://ncbiinsights.ncbi.nlm.nih.gov/2013/10/22/pubmed-commons-a-new-forum-for-scientific-discourse/">PubMed Commons</a> as a <em>New Forum for Scientific Discourse</em>:</p><blockquote>We hope that PubMed Commons will leverage the social power of the internet to encourage constructive criticism and high quality discussions of scientific issues that will both enhance understanding and provide new avenues of collaboration within the community.</blockquote><p>PubMed Commons is still a pilot project and in order to read or write comments you have to be a PubMed Commons participant and be signed in with your My NCBI account. PubMed Commons has some important features:</p><ul><li>PubMed is probably the place where most life sciences researchers search for literature. Having comments and discussion there makes perfect sense, and is probably a better place than a publisher platform that only targets particular journals. PubMed also has a reputation that is very different from social media tools that are popular, but not really familiar to most scientists.</li><li>Access to PubMed Commons is restricted to researchers, and this is one strategy to have the comments focus on scientific discourse. It has to be seen whether the process of registering for PubMed Commons (which currently is a bit more involved than most commenting systems) is a barrier for scientists to take part in the discussion, or whether it generates an audience that makes it more likely that scientists contribute.</li><li>For people signing in with their My NCBI account (I don’t know the percentage of PubMed users that do that on a regular basis), commenting is really easy and the interface is straightforward. The comment editor uses markdown, which makes it easy to format comments and to include links.</li></ul><p><em>10/26/13: added link to the recently launched <a href="http://blog.peerj.com/post/62886292466/peerj-questions-a-new-way-to-never-publish-forget">PeerJ Questions</a> which uses a question and answer format (thanks to Jason Hoyt for reminding me).</em></p><h2 id="references">References</h2><p>Fenner, M. (2013). What can article-level metrics do for you? <em>PLoS Biol</em>, <em>11</em>(10), e1001687. <a href="http://doi.org/10.1371/journal.pbio.1001687">doi:10.1371/journal.pbio.1001687</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What Can Article-Level Metrics Do for You?]]></title>
            <link>https://blog.martinfenner.org/posts/what-can-article-level-metrics-do-for-you-2</link>
            <guid>9fd59d1c-8d24-4879-93c3-cd37c9f60ecf</guid>
            <pubDate>Wed, 23 Oct 2013 16:23:00 GMT</pubDate>
            <description><![CDATA[Yesterday PLOS Biology published an essay by me: What Can Article Level Metrics
Do for You? [http://dx.doi.org/10.1371/journal.pbio.1001687] (Fenner, 2013). I
had help from many others in writing the essay, in particular PLOS Biology
editor Emma Ganley. I hope that the essay can help researchers get introduced to
article-level metrics, and I am honored that the essay is part of the PLOS
Biology 10th anniversary collection
[http://dx.doi.org/10.1371/journal.pbio.1001688].

The essay is an Open Ac]]></description>
            <content:encoded><![CDATA[<p>Yesterday PLOS Biology published an essay by me: <a href="http://dx.doi.org/10.1371/journal.pbio.1001687">What Can Article Level Metrics Do for You?</a> (Fenner, 2013). I had help from many others in writing the essay, in particular PLOS Biology editor Emma Ganley. I hope that the essay can help researchers get introduced to article-level metrics, and I am honored that the essay is part of the <a href="http://dx.doi.org/10.1371/journal.pbio.1001688">PLOS Biology 10th anniversary collection</a>.</p><p>The essay is an Open Access article published under a CC-BY license, so not only can everyone read it, but the text and figures can be freely reused, as long as proper attribution is provided, e.g. Fig. 5:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/venndiagram_plos_biology.png" class="kg-image" alt="PLOS Biology articles: sites of recommendation and discussion. Number of PLOS Biology research articles published until May 20, 2013 that have been recommended by F1000Prime (red) and/or mentioned in Wikipedia (blue). Taken from doi:10.1371/journal.pbio.1001687.g005"><figcaption><strong style="box-sizing: border-box; font-weight: bold;">PLOS Biology articles: sites of recommendation and discussion</strong>. Number of PLOS Biology research articles published until May 20, 2013 that have been recommended by F1000Prime (red) and/or mentioned in Wikipedia (blue). Taken from <a href="http://dx.doi.org/10.1371/journal.pbio.1001687.g005" style="box-sizing: border-box; background: transparent; color: rgb(52, 152, 219); text-decoration: none;">doi:10.1371/journal.pbio.1001687.g005</a></figcaption></figure><p>Although this is an essay and not a research article, I’ve added the data and R scripts used to generate the figures (1, 3-5) as <a href="http://dx.doi.org/10.1371/journal.pbio.1001687.s001">supporting information</a>. As I <a href="http://blog.martinfenner.org/2013/10/20/the-complete-article/">have said earlier</a>, I think it is important that an article contains more than the text. With the open source software <a href="http://www.r-project.org/">R</a> or <a href="http://www.rstudio.com/">RStudio</a> everyone can recreate the figures, and can look at the data underlying the figures in the essay. One can for example look into the data behind Fig. 5 to better understand how articles with F1000Prime recommendations <strong><strong>and</strong></strong> Wikipedia mentions differ from those <strong><strong>only</strong></strong> recommended in F1000Prime. Feel free to ask for help getting started in the comments.</p><p>Incidentally this is also my first PLOS article (my wife is way ahead of me with 5 research articles), so that I can finally look at PLOS article-level metrics as an author - after being the technical lead for this project since May 2012.</p><h2 id="references">References</h2><p>Fenner, M. (2013). What can article-level metrics do for you? <em>PLoS Biol</em>, <em>11</em>(10), e1001687. <a href="http://doi.org/10.1371/journal.pbio.1001687">doi:10.1371/journal.pbio.1001687</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Complete Article]]></title>
            <link>https://blog.martinfenner.org/posts/the-complete-article</link>
            <guid>b3200195-69bd-4488-ba19-6706a1ae9db9</guid>
            <pubDate>Sun, 20 Oct 2013 16:26:00 GMT</pubDate>
            <description><![CDATA[Open access to research data is becoming increasingly important, as manifested
by memos or press releases from the Wellcome Trust
[http://www.wellcome.ac.uk/About-us/Policy/Policy-and-position-statements/WTX035043.htm]
, the European Commission
[http://europa.eu/rapid/press-release_IP-12-790_en.htm], and the the Office of
Science and Technology Policy
[http://www.whitehouse.gov/blog/2013/02/22/expanding-public-access-results-federally-funded-research] 
(OSTP) from the White House.

Open access t]]></description>
            <content:encoded><![CDATA[<p>Open access to research data is becoming increasingly important, as manifested by memos or press releases from the <a href="http://www.wellcome.ac.uk/About-us/Policy/Policy-and-position-statements/WTX035043.htm">Wellcome Trust</a>, the <a href="http://europa.eu/rapid/press-release_IP-12-790_en.htm">European Commission</a>, and the <a href="http://www.whitehouse.gov/blog/2013/02/22/expanding-public-access-results-federally-funded-research">the Office of Science and Technology Policy</a> (OSTP) from the White House.</p><p>Open access to research data is important as this makes it easier for other researchers to reproduce the research, and to build upon the research by others by re-analysis of data or combination with other research data. In other words, <a href="http://royalsociety.org/policy/projects/science-public-enterprise/report/">Science as an open enterprise</a>.</p><p>The major challenge to open access to research data is that data sharing is not a widespread practice. Several strategies have been developed to create incentives for researchers to share research data, including services that make it easier to share research data (e.g. <a href="http://figshare.com/">figshare</a>, <a href="http://dataup.cdlib.org/">DataUp</a> and <a href="http://www.zenodo.org/">Zenodo</a>), <a href="http://www.knowledge-exchange.info/Default.aspx?ID=586">metrics for research data</a>, and data journals such as <a href="http://www.earth-system-science-data.net/">Earth System Science Data</a>, <a href="http://www.gigasciencejournal.com/">GigaScience</a> or the <a href="http://openarchaeologydata.metajnl.com/">Journal of open archaeology data</a>. Some of the sticks that have been tried in addition to the carrots above include data management plan requirements such as those <a href="http://www.nsf.gov/bfa/dias/policy/dmp.jsp">set forth by the National Science Foundation (NSF)</a> in 2011.</p><p>I would argue that all these carrots and sticks will eventually fall short, unless we redefine what the journal article (and similarly monograph) in the digital age should be about. Research data should become a required part of any research article, rather than an optional afterthought, or taking on a life on their own in a separate data journal.</p><p>The <em>complete article</em> - as I would like to call this journal article made fit for the digital age - should not only include the research data used to create figures and tables and reportes as results. Equally important are descriptions of reagents, workflows and software tools that go into much more detail compared to what is common practice today.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/complete_paper.png" class="kg-image" alt="Ingredients of the complete article"><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Ingredients of the complete article</strong></figcaption></figure><p>The <em>complete article</em> does not have to come as one big file. More likely the research data will be hosted at one or more data centers elsewhere. Authorship will turn into contributorship and will include all roles required to put the <em>complete article</em> together, including for example data collection and -analysis, and writing software needed to analyze the data. The <em>complete article</em> can be shorter or longer than the typical article today, important is not article length, but the combination of text, data, and description of reagents and analysis tools.</p><p>The <em>complete article</em> should also include (or link to) the text of the peer reviews and previous article versions, including preprints. This makes it much easier to understand the article (and the data) in context. The <em>complete article</em> should also link to article-level metrics post-publication for similar reasons.</p><p>This idea of a <em>complete article</em> is not too far away from the best practices used today, but it is important to make it the default for scientific publication. Too much of what we publish today is still centered around the concept of what can be printed on paper, and telling exciting stories that have impact counts more than telling complete stories that can be reproduced.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Challenges in automated DOI resolution]]></title>
            <link>https://blog.martinfenner.org/posts/challenges-in-automated-doi-resolution</link>
            <guid>629bdd07-70e5-4e32-bc76-3bf31ec5c061</guid>
            <pubDate>Sun, 13 Oct 2013 16:29:00 GMT</pubDate>
            <description><![CDATA[Yesterday we created a set of roughly 10,000 DOIs for journal articles published
in 2011 or 2012. We used these DOIs as a reference set in a data hackathon
[http://almdatachallenge.eventbrite.com/] around article-level
metrics/altmetrics - material for another blog post.

The random DOis were generated using the CrossRef RanDOIm service
[http://random.labs.crossref.org/], with article titles fetched from the 
CrossRef OpenURL API [http://labs.crossref.org/openurl/]. We didn’t have time to
proper]]></description>
            <content:encoded><![CDATA[<p>Yesterday we created a set of roughly 10,000 DOIs for journal articles published in 2011 or 2012. We used these DOIs as a reference set in a <a href="http://almdatachallenge.eventbrite.com/">data hackathon</a> around article-level metrics/altmetrics - material for another blog post.</p><p>The random DOis were generated using the <a href="http://random.labs.crossref.org/">CrossRef RanDOIm service</a>, with article titles fetched from the <a href="http://labs.crossref.org/openurl/">CrossRef OpenURL API</a>. We didn’t have time to properly parse the publication date and only used the publication year. We used the <code>crossref_r</code> and <code>crossref</code> functions from the rOpenSci <a href="http://ropensci.github.io/rplos/">rplos package</a> (and some extra help from Scott Chamberlain) to achieve this, the datasets were deposited to figshare and can be found <a href="http://dx.doi.org/10.6084/m9.figshare.821209">here</a> (2011) and <a href="http://dx.doi.org/10.6084/m9.figshare.821213">here</a> (2012).</p><p>The basic idea behind DOI names is summarized well in the <a href="http://en.wikipedia.org/wiki/Digital_object_identifier">Wikipedia entry</a>:</p><blockquote>A digital object identifier (DOI) is a character string (a “digital identifier”) used to uniquely identify an object such as an electronic document. Metadata about the object is stored in association with the DOI name and this metadata may include a location, such as a URL, where the object can be found. The DOI for a document is permanent, whereas its location and other metadata may change. Referring to an online document by its DOI provides more stable linking than simply referring to it by its URL, because if its URL changes, the publisher need only update the metadata for the DOI to link to the new URL.</blockquote><p>DOIs for journal articles should provide users with a URL specific for that journal article. This URL could point to a digital copy of the journal article in HTML or PDF format, or could point to a landing page (with an abstract or other basic metadata) for journal articles that require a subscription. This should work not only for humans using a web browser, but also for automated services using command line tools such as <a href="http://curl.haxx.se/">curl</a> as scientific infrastructure depends heavily on automation and computers talking to each other. In our use case we want to find content linking to a specific article, and as some services (e.g. social media) will use the URL and not DOI of an article, we need to find out that URL.</p><p>Unfortunately it was difficult to find a URL for many DOIs in our reference set using automated tools. All these DOIs resolve to URLs for human users using a web browser, but for automated tools there are a number of challenges:</p><h3 id="requiring-a-cookie">Requiring a cookie</h3><p>Some publishers require a cookie, and that can cause problems for automated tools. We can use the popular command line tool <code>curl</code> with the options <code>-L</code> to follow redirects and <code>-I</code> to only send the header (as we care about the location and not the content of the page).</p><pre><code>curl -I -L "http://dx.doi.org/10.1080/13658816.2010.531020"</code></pre><p>This command will lead us not to a page specific for that article, but to a “Cookie absent” page. You can work around this by having curl accept cookies:</p><pre><code>curl -I -L --cookie "tmp" "http://dx.doi.org/10.1080/13658816.2010.531020"</code></pre><p>Unfortunately not all tools do this. The way Facebook tracks likes, shares, comments, etc. is a prominent example.</p><h3 id="too-many-redirects">Too many redirects</h3><p>Some DOIs never resolve using a HEAD request, and curl stops after 50 redirects:</p><pre><code>curl -I -L "http://dx.doi.org/10.1097/SLA.0b013e318235e525"</code></pre><p>This error may relate to the “requiring a cookie” error above.</p><h3 id="method-not-allowed">Method not allowed</h3><p>Some DOis HEAD requests result in a “405 Method Not Allowed” error. The reason is that the journal platform doesn’t accept the HEAD request, but wants a GET instead.</p><pre><code>curl -I -L "http://dx.doi.org/10.1002/sam.10120"</code></pre><p>The <a href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html">HTTP 1.1 protocol</a> says about HEAD:</p><blockquote>The HEAD method is identical to GET except that the server MUST NOT return a message-body in the response. … This method is often used for testing hypertext links for validity, accessibility, and recent modification.</blockquote><p>We can work around this error by using a GET request, which unfortunately creates extra overhead and is not the recommended way to obtain this kind of information.</p><h3 id="empty-reply-from-server">Empty reply from server</h3><p>Some DOIs never resolve using a HEAD because curl reports “Empty reply from server” and we don’t get a HTTP 200 status code.</p><pre><code>curl -I -L "http://dx.doi.org/10.1016/j.cca.2011.04.012"</code></pre><p>You can again work around this by using the location information before the last redirect, but maybe resolving a DOI should not result in curl routinely throwing an error. It looks as if this error is related to “method not allowed”, as a GET request resolves to a landing page.</p><p>This problem is not specific to the <code>curl</code> tool, we get exactly the same error with <code>wget</code>:</p><pre><code>wget -S --spider "http://dx.doi.org/10.1016/j.cca.2011.04.012"</code></pre><h3 id="timeout-errors">Timeout errors</h3><p>Some DOI resolutions resulted in timeout errors, but this was temporary and much less frequent than the errors above.</p><h3 id="resource-not-found">Resource not found</h3><p>We didn’t specifically look into this error, which is a well-known problem with URLs. The DOI names we used were from 2011 and 2012, and it is known that <a href="http://en.wikipedia.org/wiki/Link_rot">link rot</a> is more common the older the resource is.</p><h3 id="content-negotiation">Content negotiation</h3><p>As Karl Ward has pointed out in the comments there are other ways to get to the URL from the DOI name, e.g. using content negotiation:</p><pre><code>curl -LH "Accept: application/vnd.crossref.unixref+xml" "http://dx.doi.org/10.1016/j.cca.2011.04.012"</code></pre><p>The URL is stored in the <code>doi_data/resource</code> attribute. The URL stored there is unfortunately not always the final landing page for the article, e.g. for the DOI name used in the example above.</p><h3 id="conclusions">Conclusions</h3><p>We created a reference set of 10,000 DOIs to collect metrics around them. The first conclusion from this exercise is that getting the URL for these articles is a challenge in many cases. This does not seem to relate to a permission problem for subscription content, but rather how the HTTP HEAD request is handled. Content negotiation is one alternative, but sometimes leads to different URLs for the landing page than where the user would get via the browser. We therefore have to rewrite our code to use GET requests and to better handle the scenarios above.</p><p><em>Update 10/13/13: Updated the title and the text to make it clear that I am not talking about DOIs that don’t resolve for human users, but rather about the problems automating this process using command-line tools.</em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Altmetrics coming of age? Not for Wikipedia]]></title>
            <link>https://blog.martinfenner.org/posts/altmetrics-coming-of-age-not-for-wikipedia</link>
            <guid>2f4b11bb-2bb2-4998-8dfa-dad98d706ecd</guid>
            <pubDate>Sat, 10 Aug 2013 16:31:00 GMT</pubDate>
            <description><![CDATA[Ten days ago Information Standards Quarterly (ISQ) published a special issue on
altmetrics [http://www.niso.org/publications/isq/2013/v25no2/]. I was the guest
editor for the five altmetrics articles, and in the editorial
[http://dx.doi.org/10.3789/isqv25no2.2013.01] that I titled Altmetrics have come
of age I argued that

> We no longer need to talk about whether it is possible to reliably collect
altmetrics, or whether this is valuable information that can complement
citations and usage statis]]></description>
            <content:encoded><![CDATA[<p>Ten days ago Information Standards Quarterly (ISQ) published a <a href="http://www.niso.org/publications/isq/2013/v25no2/">special issue on altmetrics</a>. I was the guest editor for the five altmetrics articles, and in the <a href="http://dx.doi.org/10.3789/isqv25no2.2013.01">editorial</a> that I titled <strong><strong>Altmetrics have come of age</strong></strong> I argued that</p><blockquote>We no longer need to talk about whether it is possible to reliably collect altmetrics, or whether this is valuable information that can complement citations and usage statistics.</blockquote><p>In June we have seen that the National Information Standards Organization (<a href="http://www.niso.org/home/">NISO</a>) was <a href="http://dx.doi.org/10.3789/isqv25no2.2013.07">awarded a grant</a> by the <a href="http://www.sloan.org/">Sloan Foundation</a> to develop standards and recommended best practices for altmetrics.</p><p>Unfortunately Wikipedia - which is of course an important source of altmetrics information and was also mentioned in the editorial - doesn’t think so. When you try to go to the <a href="https://en.wikipedia.org/w/index.php?title=Altmetrics&amp;redirect=no">Altmetrics</a> page on the English Wikipedia, you get this:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/wikipedia_redirect.png" class="kg-image" alt="Wikipedia doesn’t think Altmetrics need their own page"><figcaption>Wikipedia doesn’t think Altmetrics need their own page</figcaption></figure><p>In other words, you are redicted to a short section on the <a href="https://en.wikipedia.org/wiki/Impact_factor#Article_level_metrics_and_altmetrics">Impact Factor</a> page. I would go and start an altmetrics (and article-level metrics) page, but with my professional involvement in altmetrics it is difficult to write from a <a href="http://en.wikipedia.org/wiki/Wikipedia:Neutral_point_of_view">neutral point of view</a>, one of the core Wikipedia policies.</p><p><em>Update August 13, 2013: We now have a nice <a href="http://en.wikipedia.org/wiki/Altmetrics">altmetrics</a> Wikipedia page thanks to the hard work of <a href="http://en.wikipedia.org/wiki/User:Egonw">Egon Willighagen</a> and others.</em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[CSL is more than citation styles]]></title>
            <link>https://blog.martinfenner.org/posts/csl-is-more-than-citation-styles</link>
            <guid>7a66bcf9-958d-494a-b4c5-ae875db5231a</guid>
            <pubDate>Thu, 08 Aug 2013 16:33:00 GMT</pubDate>
            <description><![CDATA[According to the description [http://citationstyles.org/] on the Citation Style
Language (CSL) website, CSL is an open XML-based language to describe the
formatting of citations and bibliographies. We use reference managers such as 
Zotero, Mendeley, or Papers to format our references in manuscripts we submit
for publication, and underneath a CSL processor such as Citeproc-js
[https://bitbucket.org/fbennett/citeproc-js/wiki/Home] - together with a CSL
file for a particular citation style - is do]]></description>
            <content:encoded><![CDATA[<p>According to the <a href="http://citationstyles.org/">description</a> on the Citation Style Language (CSL) website, CSL <em>is an open XML-based language to describe the formatting of citations and bibliographies</em>. We use reference managers such as <strong>Zotero</strong>, <strong><strong>Mendeley</strong></strong>, or <strong><strong>Papers</strong></strong> to format our references in manuscripts we submit for publication, and underneath a CSL processor such as <a href="https://bitbucket.org/fbennett/citeproc-js/wiki/Home">Citeproc-js</a> - together with a CSL file for a particular citation style - is doing the work:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/02/csl.png" class="kg-image" alt><figcaption>Citation processing during manuscript writing</figcaption></figure><p>When the journal article is accepted the publisher takes the text with the formatted text citation and turns it into XML, a process that is error-prone and takes time:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/02/csl2.png" class="kg-image" alt><figcaption>Citation processing by the publisher</figcaption></figure><p>It is not hard to see that something is very wrong here:</p><ul><li>Authors are required to use a specific citation style (there are probably about 1,000 different citation styles and many more dependent styles) even though the publisher doesn’t directly use the formatted text. The publisher eLife <a href="http://www.elifesciences.org/elife-references/">accepts references in any format</a>.</li><li>Turning structured information into plain text and back into structured XML is always a bad idea. <a href="http://twitter.com/kaveh1000">Kaveh Bazargan</a> is a typesetter who has gone on record for saying that we should stop this nonsense and put him out of business.</li></ul><p>It is also obvious how the ideal workflow should look like:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/02/csl3.png" class="kg-image" alt srcset="https://martinfenner.ghost.io/content/images/size/w600/2021/02/csl3.png 600w, https://martinfenner.ghost.io/content/images/2021/02/csl3.png 768w" sizes="(min-width: 720px) 720px"><figcaption>Ideal workflow of citation processing</figcaption></figure><p>We go from structured content to structured content, and never use citations formatted as text as intermediary steps in the workflow.</p><p>What is surprising is that this is an ideal workflow and not something that publishers actually do. Most journal author instructions don’t even mention CSL styles (I work for PLOS and they are no exception). There are some issues to be solved, but they are all minor:</p><ul><li>The Citeproc JSON citation format isn’t really an official standard, but rather something invented for the most popular CSL processor, Citeproc-js.</li><li>People like to fight over standards, and there are always people you prefer bibtex, RIS, MODS or BibJSON over Citeproc JSON, or want authors to to use JATS XML.</li></ul><p>I would really like to push Citeproc JSON as a standard bibliographic exchange format for authors. There are several things I like about Citeproc JSON:</p><ul><li>It is the native format to format citations, so it is used internally by many reference managers anyway.</li><li>Citeproc JSON is really good in handling all the possible variations of author names. Putting all authors into a single text field as in bibtex requires a lot of trickery to get it right.</li><li>JSON is a standard serialization format and there are a kinds of libraries in different programming languages to do things like searching, sorting or finding of duplicates. And JSON is easily extensible, e.g. if we would want to add ORCID identifiers for authors.</li></ul><p>I have five suggestions to move forward:</p><ul><li>Make a specification for Citeproc JSON that is as clear as the CSL specification.</li><li>Consider extending the specification to include content other than citations. Ideally we should be able to add arbitrary <a href="http://blog.martinfenner.org/2013/06/29/metadata-in-scholarly-markdown/">metadata about a manuscript</a>.</li><li>Consider other serialization formats besides JSON. I particularly <a href="http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/">like YAML</a> as it is very similar to JSON, but human-readable, but other people might prefer XML. It is relatively easy to transform data between these serialization formats, in particular between JSON and YAML. In my <a href="http://blog.martinfenner.org/about.html">About page</a> I only need the <a href="https://github.com/nodeca/js-yaml">js-yaml</a> library and one extra line of code to use Citeproc YAML instead of Citeproc JSON (in the d3.js visualization).</li><li>Add Citeproc JSON (and YAML) support to reference managers. Zotero is already doing this, but it should be an easy to add feature if the reference manager is already using CSL internally (Mendeley and Papers).</li><li>Push publishers to accept Citeproc JSON with manuscript submissions.</li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What a publication timeline can tell you]]></title>
            <link>https://blog.martinfenner.org/posts/what-a-publication-timeline-can-tell-you</link>
            <guid>2fda06c5-a583-4d45-8376-3e580db9c488</guid>
            <pubDate>Tue, 06 Aug 2013 16:35:00 GMT</pubDate>
            <description><![CDATA[Now that I can automatically import my publications from my ORCID profile and
display them
[http://blog.martinfenner.org/2013/08/04/automatically-list-all-your-publications-in-your-blog/] 
in this blog, I also want to visualize them. I have started with d3.js code
[https://github.com/mfenner/blog/blob/master/_includes/by_year.js] that displays
the number of publications per year - using the list of my publications in
Citeproc JSON format. The chart is displayed on my About page
[http://blog.mart]]></description>
            <content:encoded><![CDATA[<p>Now that I can <a href="http://blog.martinfenner.org/2013/08/04/automatically-list-all-your-publications-in-your-blog/">automatically import my publications from my ORCID profile and display them</a> in this blog, I also want to visualize them. I have started with <a href="https://github.com/mfenner/blog/blob/master/_includes/by_year.js">d3.js code</a> that displays the number of publications per year - using the list of my publications in Citeproc JSON format. The chart is displayed on my <a href="http://blog.martinfenner.org/about.html">About page</a>, but I have also embedded the Javascript here:</p><p>I am a big fan of data visualizations because they can highlight something that you would otherwise miss. In this case I was really surprised to see how well my different academic jobs over the years (1991-1993, 1994-1998, 1998-2000, 2000-2005, 2005-2012) are reflected in my publication pattern. You clearly see the gaps between the jobs, indicating that I not only switched jobs, but also changed the research focus every time. The publications are listed chronologically on the <a href="http://blog.martinfenner.org/about.html">About page</a> page and you can look at the papers I wrote since my first publication in 1993. My publication pattern seems to indicate that I was never really on track for a typical academic career, so it should not be a surprise that I left academia in 2012.</p><p>There are at least two other visualizations I want to do: publications by type (journal article, book chapter, dataset, etc.), and author position with number of co-authors. You can reuse the Javascript code with small modifications (CSS and the JSON query) even if you are not running a Jekyll blog.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Automatically list all your publications in your blog]]></title>
            <link>https://blog.martinfenner.org/posts/automatically-list-all-your-publications-in-your-blog</link>
            <guid>d6eaf4b6-f664-433a-964d-e1e9b6441bab</guid>
            <pubDate>Sun, 04 Aug 2013 16:37:00 GMT</pubDate>
            <description><![CDATA[A common feature of blogs written by scientists is a listing of all their
publications. Publication lists are a great way to provide background
information about your research. Publication lists should provide links to the
fulltext versions of these publications, should be nicely formatted - e.g. using
a common citation style such as APA - and should be easy to maintain. A number
of tools for a variety of blogging platforms (including Wordpress and Jekyll)
are available to help with this task, b]]></description>
            <content:encoded><![CDATA[<p>A common feature of blogs written by scientists is a listing of all their publications. Publication lists are a great way to provide background information about your research. Publication lists should provide links to the fulltext versions of these publications, should be nicely formatted - e.g. using a common citation style such as APA - and should be easy to maintain. A number of tools for a variety of blogging platforms (including Wordpress and Jekyll) are available to help with this task, but maintaining the list of publications has remained difficult.</p><p>Publication lists are best maintained in a system built for this purpose. This could be either a reference manager, or a profile page in a social network for scientists. Even better suited for this task is your Open Researcher &amp; Contributor ID (<a href="http://orcid.org/">ORCID</a>) profile, as this service (<strong><strong>???</strong></strong>) directly integrates with a number of bibliographic databases and makes the profile information available via an open API.</p><h3 id="orcid-feed">ORCID Feed</h3><p>Last week I have started work on <a href="http://feed.labs.orcid-eu.org/">ORCID Feed</a>, a service that reformats the API response from ORCID into RSS, bibtex and formattted citations, making it easier for scientists to reuse the content stored in their ORCID profile. This service is still experimental, so please report any issues <a href="https://github.com/orcid-eu-labs/orcid-feed/issues">here</a>.</p><h3 id="jekyll-orcid">jekyll-orcid</h3><p>I have now added the final piece to automatically import my publications into this blog. <a href="https://github.com/mfenner/jekyll-orcid">jekyll-orcid</a> is a Jekyll plugin that automatically downloads all my publications from my ORCID profile via <strong><strong>ORCID Feed</strong></strong> and stores them in a subfolder of this blog, both in bibtex and Citeproc JSON format. It does this every time you regenerate your blog, so that the publication list will be automatically updated with new content. I can then use <a href="https://github.com/inukshuk/jekyll-scholar">jekyll-scholar</a>, a popular Jekyll plugin written by Sylvester Keil to generate a bibliography (<code>jekyll-orcid</code> automatically adds a YAML frontmatter section to the files so that jekyll-scholar can process it). I can format this auto-generated bibliography in a variety of ways - you can see the result in my <a href="http://blog.martinfenner.org/about.html">About</a> page where I also provide a download link of the bibtex file.</p><p>My publications are of course also available if I want to cite them in the text, e.g. our recent publication summarizing the main findings from the 2011 European Consensus Conference on germ-cell cancer (<strong><strong>???</strong></strong>), or last year’s case report on liver toxicity induced by the cancer drug imatinib (<strong><strong>???</strong></strong>).</p><p>Similar tools also exist for Wordpress, e.g. <a href="http://wordpress.org/plugins/papercite/">Papercite</a>, which can import the bibtex file directly from ORCID Feed.</p><h3 id="next">Next</h3><p>Now there is only one step missing to have your paper that was just published automatically appear in your publication list. Assuming you have provided your ORCID identifier when you submitted the paper, and the publisher has included your ORCID identifier in the metadata sent to CrossRef (both are already common practices), we only need CrossRef to automatically push that paper into your ORCID profile.</p><p>And once we have this workflow in place, we can automatically add additional information, including links to the fulltext paper in the institutional repository, copyright information, and metrics.</p><h2 id="references">References</h2>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Citeproc YAML for bibliographies]]></title>
            <link>https://blog.martinfenner.org/posts/citeproc-yaml-for-bibliographies</link>
            <guid>34df496a-1835-41c8-bb0a-f0d96365c130</guid>
            <pubDate>Tue, 30 Jul 2013 16:39:00 GMT</pubDate>
            <description><![CDATA[The standard local file formats for bibliographic data are probably bibtex and
RIS. They have been around for a long time, and are supported by all reference
managers and many other tools and services. Unfortunately these formats are far
from perfect:

 * neither bibtex nor RIS use a web-friendly data interchange format such as XML
   or JSON, which makes it harder to work with these formats
 * bibtex - and to a lesser extend RIS - don’t support all entry types that we
   need, e.g. datasets, or]]></description>
            <content:encoded><![CDATA[<p>The standard local file formats for bibliographic data are probably bibtex and RIS. They have been around for a long time, and are supported by all reference managers and many other tools and services. Unfortunately these formats are far from perfect:</p><ul><li>neither bibtex nor RIS use a web-friendly data interchange format such as XML or JSON, which makes it harder to work with these formats</li><li>bibtex - and to a lesser extend RIS - don’t support all entry types that we need, e.g. datasets, or new standards such as ORCID author identifiers</li><li>bibtex stores all authors in a single field, which makes author names hard to parse</li></ul><h3 id="bibtex">bibtex</h3><pre><code>@article{fenner2012a,
  title = {One-click science marketing},
  volume = {11},
  url = {http://dx.doi.org/10.1038/nmat3283},
  doi = {10.1038/nmat3283},
  number = {4},
  journal = {Nature Materials},
  publisher = {Nature Publishing Group},
  author = {Fenner, Martin},
  year = {2012},
  month = {mar},
  pages = {261-263}
}</code></pre><p>One obvious solution would be to store bibliographic data in XML or JSON. These formats have very good support in all programming languages, and they are the formats used by APIs on the web. There have been some efforts to standardize these formats for bibliographic data, e.g. <a href="http://www.bibjson.org/">BibJSON</a>, <a href="http://www.loc.gov/standards/mods/">MODS</a>, <a href="http://bibtexml.sourceforge.net/">BibTeX XML</a> or Endnote XML.</p><h3 id="bibtex-xml">BibTeX XML</h3><pre><code>&lt;bibtex:entry id='fenner2012a'&gt;
  &lt;bibtex:article&gt;
    &lt;bibtex:title&gt;One-click science marketing&lt;/bibtex:title&gt;
    &lt;bibtex:volume&gt;11&lt;/bibtex:volume&gt;
    &lt;bibtex:url&gt;http://dx.doi.org/10.1038/nmat3283&lt;/bibtex:url&gt;
    &lt;bibtex:doi&gt;10.1038/nmat3283&lt;/bibtex:doi&gt;
    &lt;bibtex:number&gt;4&lt;/bibtex:number&gt;
    &lt;bibtex:journal&gt;Nature Materials&lt;/bibtex:journal&gt;
    &lt;bibtex:publisher&gt;Nature Publishing Group&lt;/bibtex:publisher&gt;
    &lt;bibtex:person&gt;
      &lt;bibtex:first&gt;Martin&lt;/bibtex:first&gt;
      &lt;bibtex:last&gt;Fenner&lt;/bibtex:last&gt;
    &lt;bibtex:person&gt;&lt;bibtex:author/&gt;
    &lt;bibtex:year&gt;2012&lt;/bibtex:year&gt;
    &lt;bibtex:month&gt;mar&lt;/bibtex:month&gt;
    &lt;bibtex:pages&gt;261-263&lt;/bibtex:pages&gt;
  &lt;/bibtex:article&gt;
&lt;/bibtex:entry&gt;</code></pre><p>My problem with these formats is that they are made for computers talking to each other and not humans. I personally think that a file with bibliographic data should be human-readable, similar to why <a href="http://blog.martinfenner.org/2012/12/13/a-call-for-scholarly-markdown/">I like markdown</a> for writing scientific documents.</p><p>When you have too many standards and are not happy with any of them, you of course create a new standard.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://imgs.xkcd.com/comics/standards.png" class="kg-image" alt="How Standards Proliferate. Taken from http://xkcd.com/927/"><figcaption><strong style="box-sizing: border-box; font-weight: bold;">How Standards Proliferate</strong>. Taken from <a href="http://xkcd.com/927/" class="uri" style="box-sizing: border-box; background: transparent; color: rgb(52, 152, 219); text-decoration: none;">http://xkcd.com/927/</a></figcaption></figure><p>My suggestion for a new bibliographic file format is twofold: a) use YAML for data serialization and b) use CSL as data format. <a href="http://www.yaml.org/spec/1.2/spec.html">YAML</a> is a data format popular with Ruby Developers and is described on the <a href="http://yaml.org/">YAML website</a> as</p><blockquote>YAML is a human friendly data serialization standard for all programming languages.</blockquote><p>Something that not may people seem to know is that YAML is a superset of JSON and that <a href="http://yaml.org/spec/1.2/spec.html#id2759572">every JSON file is also a valid YAML file</a>. The main difference is the better human readability of YAML.</p><p><strong><strong>Citation Style Language</strong></strong> is described on the <a href="http://citationstyles.org/">CSL website</a> as</p><blockquote>CSL is an open XML-based language to describe the formatting of citations and bibliographies.</blockquote><p>Although some commercial applications still use proprietary citation styles, CSL has become the de facto standard, and is used by the reference managers <strong><strong>Zotero</strong></strong>, <strong><strong>Mendeley</strong></strong>, <strong><strong>Papers</strong></strong>, and others. This blog uses CSL via Pandoc and the <a href="http://code.google.com/p/citeproc-hs/">citeproc-hs</a> library. CSL processors need bibliographic data in a standard format. The popular <a href="https://bitbucket.org/fbennett/citeproc-js/wiki/Home">Citeproc-js</a> Javascript CSL processor by Frank Bennett for example uses JSON, but we might as well use YAML:</p><h3 id="citeproc-yaml">Citeproc YAML</h3><pre><code>- title: One-click science marketing
  volume: '11'
  URL: http://dx.doi.org/10.1038/nmat3283
  DOI: 10.1038/nmat3283
  issue: '4'
  container-title: Nature Materials
  publisher: Nature Publishing Group
  author:
  - family: Fenner
    given: Martin
    orcid: 0000-0003-1419-2405
  page: 261-263
  id: fenner2012a
  type: article-journal
  issued:
    date-parts:
      - 2012
      - 3</code></pre><p>I hope you agree that this format is not only structured and can be understood by computers, but is also very readable by humans. You may have noticed that I have inserted my ORCID, something that is very difficult to do with bibtex where all authors are stored in one text string (see above).</p><p>Careful readers of this blog will of course remember that <a href="http://blog.martinfenner.org/2013/06/29/metadata-in-scholarly-markdown/">I have written about</a> using YAML to store metadata about a blog post. We could now add bibliographic information to these metadata, either in the YAML frontmatter (if it is a Jekyll blog), or in a separate file. It should be straightforward to adapt the existing CSL processors to understand YAML since YAML and JSON are so similar. To get started with some Citeproc YAML, use the new (and still experimental) <strong><strong>ORCID Feed</strong></strong> Webservice with your ORCID and specify the <code>yml</code> format, e.g. <a href="http://feed.labs.orcid-eu.org/0000-0003-1419-2405.yml">http://feed.labs.orcid-eu.org/0000-0003-1419-2405.yml</a> for my publications.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[RSS Feeds for Scholarly Authors]]></title>
            <link>https://blog.martinfenner.org/posts/rss-feeds-for-scholarly-authors</link>
            <guid>502c138f-de35-4147-ae94-a226f16c51e3</guid>
            <pubDate>Fri, 26 Jul 2013 16:48:00 GMT</pubDate>
            <description><![CDATA[Open Researcher & Contributor ID (ORCID
[https://speakerdeck.com/mfenner/orcid-connecting-research-and-researchers-1])
provides a persistent identifier for researchers and lets them claim their
research outputs in the ORCID Registry. I have been involved with ORCID since
early 2010 and I am happy to see that nine months after launch 200,000
researchers have signed up for the service, and the organisation has more than 
70 member organizations [http://orcid.org/about/community/members].

Register]]></description>
            <content:encoded><![CDATA[<p>Open Researcher &amp; Contributor ID (<a href="https://speakerdeck.com/mfenner/orcid-connecting-research-and-researchers-1">ORCID</a>) provides a persistent identifier for researchers and lets them claim their research outputs in the ORCID Registry. I have been involved with ORCID since early 2010 and I am happy to see that nine months after launch 200,000 researchers have signed up for the service, and the organisation has more than <a href="http://orcid.org/about/community/members">70 member organizations</a>.</p><figure class="kg-card kg-embed-card"><iframe src="http://s3.datawrapper.de/BZBSQ/" frameborder="0" allowtransparency="true" allowfullscreen="allowfullscreen" webkitallowfullscreen="webkitallowfullscreen" mozallowfullscreen="mozallowfullscreen" oallowfullscreen="oallowfullscreen" msallowfullscreen="msallowfullscreen" width="600" height="400" style="box-sizing: border-box; color: rgb(0, 0, 0); font-family: ff-tisa-web-pro, Georgia, serif; font-size: 21px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"></iframe></figure><p><a href="https://orcid.org/register">Registering for an ORCID identifier</a> is easy, and can be done in a few minutes. Claiming works in the profile is also straighforward, and works by integration with CrossRef Search, Scopus, Web of Science, DataCite Metadata Search, and other services. Even though about 1.5 million works have been claimed by now, many users have still not claimed any works or added profile information in other ways.</p><p>These numbers should go up as more academic institutions sign up for ORCID and help their researchers create ORCIDs and claim works. In the meantime we need more incentives for researchers to add publications to their ORCID profile. Publication lists are a very good reason to add your papers and other research outputs to your ORCID profile.</p><h3 id="publication-lists">Publication Lists</h3><p>Every researcher maintains a list of his publications in some form. These publication lists are used for grant and job applications, for academic websites to attract collaborators and students, and more. Publication lists can be generated in many different ways, but I have never heard that someone finds this process fun or easy. The challenge is multiplied when the publication list is not generated for an individual, but for a research group, department or institution (my university goes through this process every year uisng RefWorks and produces an <a href="http://www.refworks.com/RefShare2?site=047931198213200000/RWWS6A619751/2013%20Hochschulbibliografie">annual institutional bibliography</a>).</p><p>Although the library usually takes care of the larger publication lists and can help researchers setting up their own lists, there still is much that needs to be done by individual researchers, and the process needs to be easier. Some recommendations are:</p><ul><li>don’t reinvent the wheel</li><li>use persistent identifiers</li><li>use standards</li><li>don’t worry about citation styles</li><li>keep everything upstream, not locally</li></ul><p>Don’t try to invent a new way of managing publication lists. Other people have worked on this problem before, and there are many tools available. This doesn’t mean you shouldn’t try something new, but please build it on top of all the infrastructure and services we have already.</p><p>Managing publication lists becomes much easier when you use persistent identifiers such as DOIs. They make it much easier to obtain metadata (e.g. authors, title, journal) and the fulltext version. Some disciplines use other identifiers, but a local identifier such as a URL is usually a bad idea.</p><p>Use standard protocols, standard file formats and standard metadata. BibTex and RIS are file formats for references that almost every piece of software handling references understands.</p><p>Citation styles come from a time when publications were printed on paper. They make no real sense anymore, and as a researcher you shouldn’t bother which one of 3000+ styles is the appropriate one.</p><p>The last recommendation is the most important one. Don’t try to manage publication lists in your local system, or your department, but rather do this as much upstream as possible. ORCID is an ideal service for this. But don’t try to manually add or edit publications in the ORCID registry, but rather claim them from CrossRef, DataCite or similar services, because these are the places that have authoritative information about publication. If you try to “fix” information (because all metadata can contain mistakes), nobody will notice. If something is wrong with your works, notify the publisher so that the CrossRef metadata can be updated.</p><h3 id="orcid-profiles-as-rss-feeds">ORCID Profiles as RSS Feeds</h3><p>ORCID is a good place to manage publication lists, but it is often not easy to get the information out of the system. The standard way is via a REST API (XML or JSON). This might work really well for a software developer who wants to connect his system to ORCID, but most researchers have other things to do.</p><p>RSS was invented to publish information about frequently updated works, and a good example are Tables of Content (TOC) for journals. RSS is also a great tool to manage publication lists, as it can be easily integrated into content management systems such as Wordpress or Drupal. There is a <a href="http://oxford.crossref.org/best_practice/rss/">Recommendation on RSS Feeds for Scholarly Publishers</a>, and we can apply the same guidelines to <strong><strong>RSS Feeds for Scholarly Authors</strong></strong>. With <a href="http://en.wikipedia.org/wiki/OPML">OPML</a> we also have a standard format to aggregate multiple RSS feeds, and this is true not only for journal RSS feeds, but also author RSS feeds.</p><p>Unfortunately there is one missing piece in this workflow: turning ORCID profiles into RSS feeds. At the <a href="http://occamstypewriter.org/trading-knowledge/2012/11/13/solo-hackday/">SpotOn London hackathon</a> last November I worked with <a href="http://twitter.com/easternblot">Eva Amsen</a> and <a href="http://twitter.com/graemedmoffat">Graeme Moffat</a> to hack this workflow together using available tools. But we really need a more mature solution. Until RSS feeds are provided by the core ORCID service - and there is so much other stuff to do right now that this will take time - the best solution might be a web service that turns ORCID profiles into scholarly RSS as described above for journal articles.</p><p>Today I finally came around implementing a first version of this - hacking together a Ruby Sinatra application hosted on Amazon Web Services (<a href="http://hack4ac.com/">#hack4ac</a> attendees know why). The application takes an ORCID ID (e.g. mine: <a href="http://feed.labs.orcid-eu.org/0000-0003-1419-2405.rss">http://feed.labs.orcid-eu.org/0000-0003-1419-2405.rss</a>) and returns an RSS feed. The first version just returns just the name and biography from the profile, but I only started working on this today. ORCID Feed can be found at <a href="http://feed.labs.orcid-eu.org/">http://feed.labs.orcid-eu.org</a> and the source code is available at <a href="https://github.com/mfenner/orcid-feed">Github</a>. Please add suggestions and comments to the Github issue tracker <a href="https://github.com/mfenner/orcid-feed/issues">here</a>.</p><p><strong><strong>Update 7/28/13</strong></strong>: <em>I’ve added publications to the output, and additional content types. Use them as extension (e.g. <code>.json</code>), as format parameter (e.g. <code>?format=rss</code>), or use an accept-header, e.g. <code>Accept: application/x-bibtex</code>. I’ve also added basic error checking with cleanup of names and removal of duplicates.</em></p><ul><li>html (the default): forward to profile on the ORCID website</li><li>rss - RSS feed</li><li>bib - bibtex file</li><li>json - Citeproc JSON</li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The trouble with keynotes]]></title>
            <link>https://blog.martinfenner.org/posts/the-trouble-with-keynotes</link>
            <guid>87320192-6f0b-42e4-b0b6-03907d8466ff</guid>
            <pubDate>Tue, 23 Jul 2013 16:50:00 GMT</pubDate>
            <description><![CDATA[A keynote is a presentation typically given at a start of a conference that sets
the central theme for the event. A keynote speaker usually has more time (45-60
min) than other presenters, and has the full attention of everyone attending the
conference. The keynotes at the conferences I attended the last several years
(mostly scholarly communication conferences) seem to work like this:

 * find a prominent speaker, ideally not a core member of the community
   attending the conference
 * tell hi]]></description>
            <content:encoded><![CDATA[<p>A keynote is a presentation typically given at a start of a conference that sets the central theme for the event. A keynote speaker usually has more time (45-60 min) than other presenters, and has the full attention of everyone attending the conference. The keynotes at the conferences I attended the last several years (mostly scholarly communication conferences) seem to work like this:</p><ul><li>find a prominent speaker, ideally not a core member of the community attending the conference</li><li>tell him to talk about something he knows a lot about, not necessarily a central theme of the conference</li><li>the keynote should be inspiring and eye-opening, instead of focussing on the conference</li></ul><p>The problem with this approach is that it focusses too much on the <em>prominent speaker</em> and it runs the risk of the keynote speaker talking about what he always talks about. Meaning that we don’t learn much if we have heard the keynote speaker before. Which is too bad, because keynotes should contain things that are unexpected and exciting.</p><p>One of the best keynotes I had the pleasure of listening to in the last several years was the one given by <a href="http://michaelnielsen.org/blog/michael-a-nielsen/">Michael Nielsen</a> at <a href="https://martinfenner.ghost.io/2013/07/23/the-trouble-with-keynotes/(http://www.nature.com/spoton/)">Science Online London 2011</a> (disclaimer: I was one of the conference organizers). Not only is Michael a very good speaker, but his presentation about <strong><strong>Open Science</strong></strong> fit perfectly into the conference, and it was clear that he had made the presentation specifically for this conference (with an audience that knows a lot about Open Science). One of the main themes of his presentation - the <em>collective action problem</em>, or to get started with something that benefits everyone, but where there is a cost doing the first step - is something I later picked up in a publication about the Open Researcher &amp; Contributor ID (Fenner, Gomez, &amp; Thorisson, 2011).</p><figure class="kg-card kg-embed-card kg-card-hascaption"><iframe src="http://player.vimeo.com/video/29784152" width="720" height="480" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen="" style="box-sizing: border-box; color: rgb(0, 0, 0); font-family: ff-tisa-web-pro, Georgia, serif; font-size: 21px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"></iframe><figcaption><em>Keynote by Michael Nielsen at the <a href="http://www.nature.com/spoton/">Science Online London 2011 Conference</a>, video recording and editing by <a href="http://river-valley.tv/keynote-solo2011/">River Valley TV</a>.</em></figcaption></figure><p>Luckily we increasingly have video recordings of keynote presentations available online, making it easier to listen to the good presentations. <a href="http://www.ted.com/tedx">TED and TEDx</a> have of course made the format of recordings of carefully prepared talks popular. For large scholarly and academic conferences the best starting point is <a href="http://river-valley.tv/">River Valley TV</a>. The <a href="http://www.mediatheque.lindau-nobel.org/">Lindau Nobel Laureate Meeting</a> has hundreds of presentations by Nobel laureates. And as video recording and streaming has become easier technically (e.g. with <a href="http://googleblog.blogspot.de/2011/09/google-92-93-94-95-96-97-98-99-100.html">Google Hangouts on Air</a>), recording good keynotes should become the norm and not the exception.</p><p><em>After several hundred blog posts here and elsewhere, this may well be my first blog post with embedded video.</em></p><h2 id="references">References</h2><p>Fenner, M., Gomez, C. G., &amp; Thorisson, G. A. (2011). Key issue: Collective action for the open researcher &amp; contributor iD (oRCID). <em>Serials: The Journal for the Serials Community</em>, <em>24</em>(3), 277–279. Retrieved from <a href="http://dx.doi.org/10.1629/24277">http://doi.org/10.1629/24277</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Creating charts with Datawrapper]]></title>
            <link>https://blog.martinfenner.org/posts/creating-charts-with-datawrapper</link>
            <guid>3db0ce74-7053-4f2a-a11c-1b779494c341</guid>
            <pubDate>Fri, 19 Jul 2013 16:54:00 GMT</pubDate>
            <description><![CDATA[Figures are an important part of any scientific document. While the kind of
figure commonly used obviously varies between disciplines, charts are an
important part of many publications. There are two problems in how charts are
currently used:

 * the data used to draw the chart are not available or difficult to obtain
 * charts are drawn as static images with no interactivity, e.g. to see the
   values of individual data points

Ross Mounce and others did a Figures → Data project at the recent h]]></description>
            <content:encoded><![CDATA[<p>Figures are an important part of any scientific document. While the kind of figure commonly used obviously varies between disciplines, charts are an important part of many publications. There are two problems in how charts are currently used:</p><ul><li>the data used to draw the chart are not available or difficult to obtain</li><li>charts are drawn as static images with no interactivity, e.g. to see the values of individual data points</li></ul><p>Ross Mounce and others did a <strong><strong>Figures → Data</strong></strong> project at the recent <a href="http://hacka4ac.com/">hack4ac</a> to extract data from figures, described in a <a href="http://rossmounce.co.uk/2013/07/09/hack4ac-recap/">blog post</a>. The experience was painful, even though they started with a <em>really</em> simple chart.</p><p>While we should of course <a href="http://datadryad.org/">publish all data associated with a paper</a>, the smarter strategy to overcome the two limitations above would be to embed the data used for a chart directly into the document. We have many tools that can accomplish this, and I have given an example using R in an <a href="http://blog.martinfenner.org/2013/06/17/what-is-scholarly-markdown/">earlier blog post</a>. The problem is the sometimes steep learning curve.</p><p>One approach is to build an easy-to use online tool, and <a href="http://datawrapper.de/">Datawrapper</a> is exactly that:</p><blockquote>An open source tool helping anyone to create simple, correct and embeddable charts in minutes.</blockquote><p>Datawrapper uses the <strong><strong>d3.js</strong></strong> and <strong><strong>Highcharts</strong></strong> Javascript libraries for data visualizations, and the service is easy to use. It took me for example about 15 min to generate the chart below. The data used for the chart are embedded (click <strong><strong>Get the data</strong></strong>) and you can hover over the chart to see the actual numbers by month.</p><!--kg-card-begin: html--><iframe src="https://cf.datawrapper.de/7PqqU/" width="720" height="480"></iframe><!--kg-card-end: html--><p>Most journal articles see the highest usage immediately after publication, and the light purple line shows this pattern for Darcy et al. (2009). The dark purple line for Moher et al. (2009) – published on the same day – on the other hand shows a highly unusual usage pattern, as the usage actually increases over time, starting about 1 1/2 years after publication. The article is a guideline for reporting systematic reviews and meta-analyses, and is now viewed more often than directly after publication four years ago.</p><p>Datawrapper does three things: it makes it easy to generate charts, it allows you to embed them directly into your webpage (using an <code>&lt;iframe&gt;</code> tag), and it is Open Source software (MIT license, Github repo <a href="https://github.com/datawrapper/datawrapper">here</a>) so that you can help improve the code and host this service on your own. DataWrapper was written in Javascript and PHP by a group of German journalists, and the main focus is data journalism where the service has become really <a href="http://blog.datawrapper.de/2013/datawrapper-crosses-mark-of-10-million-visits/">popular</a> with more than 3.5 million views of embedded charts in May 2013 alone.</p><p>Datawrapper is a perfect tool for science blogs and websites with scientific content, but it can also enhance the charts in scientific articles. We need a few additional chart types, error bars and more flexible labeling. And we might want to add a license picker, making it easy to add a Creative Commons license so that it is clear how the chart can be reused. Datawrapper is intended for online use, but the service can also save the charts as PNG or PDF. We would want to add saving to SVG (already used for online rendering) for easier embedding into the XML and ePub versions of articles.</p><h2 id="references">References</h2><p>D’Arcy, E., &amp; Moynihan, R. (2009). Can the relationship between doctors and drug companies ever be a healthy one? <em>PLoS Medicine</em>, <em>6</em>(7), e1000075. Retrieved from <a href="http://doi.org/10.1371/journal.pmed.1000075">http://doi.org/10.1371/journal.pmed.1000075</a></p><p>Moher, D., Liberati, A., Tetzlaff, J., &amp; Altman, D. G. (2009). Preferred reporting items for systematic reviews and meta-analyses: The pRISMA statement. <em>PLoS Medicine</em>, <em>6</em>(7), e1000097. Retrieved from <a href="http://doi.org/10.1371/journal.pmed.1000097">http://doi.org/10.1371/journal.pmed.1000097</a></p><hr>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Altmetrics: first we need the for what? and only then the how? OK?]]></title>
            <link>https://blog.martinfenner.org/posts/altmetrics-first-we-need-the-for-what-and-only-then-the-how-ok</link>
            <guid>c7fe345b-938c-486b-b398-674345e7e52f</guid>
            <pubDate>Tue, 09 Jul 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[Altmetrics track the impact of scholarly works in the social web. Article-Level
Metrics focuses on articles, but also looks at traditional citations and usage
statistics. The PLOS Article-Level Metrics
[https://web.archive.org/web/20170913082053/http://article-level-metrics.plos.org/] 
project was started in 2008. The altmetrics manifesto
[https://web.archive.org/web/20170913082053/http://altmetrics.org/manifesto/] 
was published in October 2010 and described the fundamental ideas. By October
20]]></description>
            <content:encoded><![CDATA[<figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20170913082053im_/http://blogs.plos.org/mfenner/files/2012/11/cute-500x132.png" class="kg-image" alt></figure><p>Altmetrics track the impact of scholarly works in the social web. Article-Level Metrics focuses on articles, but also looks at traditional citations and usage statistics. The <a href="https://web.archive.org/web/20170913082053/http://article-level-metrics.plos.org/">PLOS Article-Level Metrics</a> project was started in 2008. The <a href="https://web.archive.org/web/20170913082053/http://altmetrics.org/manifesto/">altmetrics manifesto</a> was published in October 2010 and described the fundamental ideas. By October 2011 we had a number of altmetrics tools, fueled by the Mendeley/PLOS API <a href="https://web.archive.org/web/20170913082053/http://blog.mendeley.com/design-research-tools/winners-of-the-first-binary-battle-apps-for-science-contest/">programming contest</a>. In 2012 the focus shifted from the fact that we can provide these numbers to a discussion of the many open questions. We could see this at the <a href="https://web.archive.org/web/20170913082053/http://blogs.plos.org/mfenner/2012/06/25/random-notes-from-the-altmetrics12-conference/">altmetrics12 conference </a>in June, and even more so at the <a href="https://web.archive.org/web/20170913082053/https://sites.google.com/site/altmetricsworkshop/">altmetrics workshop</a> hosted by PLOS last week in San Francisco.</p><p>Altmetrics can provide a large amount of information about the post-publication activity around an article (and other scholarly content), and this is exciting, but at the same time also somewhat overwhelming and scary. Some of the things that we as a community have to figure out include standards for collecting, aggregating and displaying altmetrics data, strategies to combat attempts to game these metrics, and finding appropriate ways for the different organizations providing altmetrics to work together as a community. These and other topics were discussed in great detail at the PLOS altmetrics workshop, and we made excellent progress not least thanks to the excellent moderation by <em>Cameron Neylon</em>. The third day of the workshop was a <a href="https://web.archive.org/web/20170913082053/https://sites.google.com/site/altmetricsworkshop/altmetrics-hackathon">hackathon</a>, and we were able to translate some of the ideas into prototypes of new tools.</p><p>The most important conclusion from the workshop for me personally was that we should really should focus on use cases. Altmetrics should help answer questions that we can’t answer today, and despite the promise, the various altmetrics tools still have a log way to go. A case in point is the promise that altmetrics can make it easier to find relevant scholarly content. We all use social media to help us find papers and other stuff, but integration of altmetrics into the traditional scholarly search tools is still missing. <a href="https://web.archive.org/web/20170913082053/http://rerank.it/">ReRank</a> is a cool prototype developed during the hackathon last Saturday, but we are still a long way from having altmetrics feeding directly into the relevance sorting of search results.</p><p>With these thoughts in the back of mind, I look forward to the <a href="https://web.archive.org/web/20170913082053/http://www.nature.com/spoton/event/spoton-london-2012-altmetrics-beyond-the-numbers/">altmetrics session</a> at the <a href="https://web.archive.org/web/20170913082053/http://www.nature.com/spoton/in/london/">SpotOn London conference</a> this Sunday afternoon. <em>Sarah Venis</em> from <a href="https://web.archive.org/web/20170913082053/http://www.msf.org/">Médecins sans Frontières</a> (MSF) will talk about the questions that she hopes altmetrics can answer for her organization. MSF is very interested to look beyond citations for the impact of their publications, as their primary target audience is not really the scholarly community, but rather people in need in various parts of the world. <em>Marie Boran</em> from the <a href="https://web.archive.org/web/20170913082053/http://www.deri.ie/about/team/member/marie_boran/">Digital Research Enterprise Institute</a> (DERI) is interested in using altmetrics as a recommendation tool to find researchers with similar interests. <em>Euan Adie</em> from <a href="https://web.archive.org/web/20170913082053/http://altmetric.com/">altmetric.com</a> and I (technical lead for the <a href="https://web.archive.org/web/20170913082053/http://article-level-metrics.plos.org/">PLOS Article-Level Metrics project</a>) will use our respective tools to try to answer some of these questions. For me altmetrics are primarily tools to tell a good story, and that is one reason why we picked the title <em>Altmetrics beyond the Numbers</em> for this session. The focus of the session will then shift to an open discussion, and I hope we can get some good answers to this and other questions.</p><p>A clear focus on use cases should go a long way to reduce that feeling of being overwhelmed by all the numbers that altmetrics can provide. If we have specific goals for which we need altmetrics, it becomes much easier to decide what numbers work best for us, what standards we need and whom to ask to collect this information. <a href="https://web.archive.org/web/20170913082053/http://www.nature.com/spoton/2012/11/spoton-london-2012-altmetrics-everywhere-but-what-are-we-missing-solo12impact/">AJ Cann</a> and <a href="https://web.archive.org/web/20170913082053/http://ukwebfocus.wordpress.com/2012/11/08/understanding-the-limits-of-altmetrics-slideshare-statistics/">Brian Kelly </a>have written two excellent blog post about the confusion that too many altmetrics numbers can create, and the workshop <a href="https://web.archive.org/web/20170913082053/http://www.nature.com/spoton/event/spoton-london-2012-assessing-social-media-impact/">Assessing Social Media Impact</a> during SpotOn London addresses some of these questions. Hackathons have played an important role in the history of altmetrics. I invite you to come to the <a href="https://web.archive.org/web/20170913082053/http://www.nature.com/spoton/event/spoton-london-2012-fringe-event-hackday/">SpotOn London hackathon</a> this Saturday if you have some cool ideas and want to get started with the help of others.</p><h3 id="other-reports-from-the-plos-article-level-metrics-aka-altmetrics-workshop">Other reports from the PLOS Article-Level Metrics (aka Altmetrics) Workshop</h3><ul><li><strong>Paul Groth</strong>: <a href="https://web.archive.org/web/20170913082053/http://thinklinks.wordpress.com/2012/11/05/trip-report-plos-article-level-metrics-workshop-and-hackathon/">Trip Report: PLOS Article Level Metrics Workshop and Hackathon</a></li><li><strong>Karthik Ram</strong>: <a href="https://web.archive.org/web/20170913082053/http://inundata.org/2012/11/08/plos-altmetrics-workshop/">PLOS Altmetrics workshop</a></li><li><strong>Carl Boettiger</strong>: <a href="https://web.archive.org/web/20170913082053/http://www.carlboettiger.info/2012/11/03/altmetrics-conference.html">Altmetrics Conference</a></li><li><strong>Pedro Beltrao</strong>: <a href="https://web.archive.org/web/20170913082053/http://pbeltrao.blogspot.de/2012/11/scholarly-metrics-with-heart.html">Scholarly metrics with a heart</a></li><li><strong>Ian Mulvany</strong>: <a href="https://web.archive.org/web/20170913082053/https://plus.google.com/u/0/photos/102755743034732738536/albums/5807181863066123265">a photo post</a></li><li><strong>Euan Adie</strong> (who couldn’t attend in person but followed remotely): <a href="https://web.archive.org/web/20170913082053/http://altmetric.com/blog/?p=316">Want some hackathon friendly altmetrics data? arXiv tweets dataset now up on figshare</a></li></ul><p><em>Please let me know if you see other reports of the workshop that I have missed.</em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Auto generating links to data and resources]]></title>
            <link>https://blog.martinfenner.org/posts/auto-generating-links-to-data-and-resources</link>
            <guid>84de2484-f805-4df2-b842-f9803c8cd8ea</guid>
            <pubDate>Tue, 02 Jul 2013 16:58:00 GMT</pubDate>
            <description><![CDATA[A few weeks ago Kafkas et al. (2013) published a paper looking at current
patterns of how datasets o biological databases are cited in research articles,
based on an analysis of the full text Open Access articles available from Europe
PMC. They identified data ctiations by:

 1. Accession numbers available in articles as publisher-supplied, structured
    content;
 2. Accession numbers identified in articles by text mining;
 3. References to articles from the ENA, UniProt and PDBe records.

They]]></description>
            <content:encoded><![CDATA[<p>A few weeks ago Kafkas et al. (2013) published a paper looking at current patterns of how datasets o biological databases are cited in research articles, based on an analysis of the full text Open Access articles available from Europe PMC. They identified data ctiations by:</p><ol><li>Accession numbers available in articles as publisher-supplied, structured content;</li><li>Accession numbers identified in articles by text mining;</li><li>References to articles from the ENA, UniProt and PDBe records.</li></ol><p>They could show that text mining doubles the number of structured annotations available in journal articles (from 2.26% to 5.15%), and that these structured annotations should be extended beyond the ENA, UniProt and PDB identifiers that their analysis focused on. ENA identifiers (for nucleotide sequences in GenBank, EMBL or DDBJ) make up the largest group, with 160,112 identifiers found in the 410,364 articles that were analyzed.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/journal.pone.0063184.g003.png" class="kg-image" alt="Database Citation in Full Text Biomedical Articles. Fig. 3 from (Kafkas et al., 2013)."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Database Citation in Full Text Biomedical Articles</strong>. Fig. 3 from <span class="citation" data-cites="Kafkas:2013fp" style="box-sizing: border-box;">(Kafkas et al., 2013)</span>.</figcaption></figure><p>Another result in the paper is that references to articles in these databases show little overlap with database links found in articles. One of the conclusions drawn by the author is that</p><blockquote>Text-mining can be used to extend structured data citation, and could be a basis for the development of services to help authors or editors to add structured content at the beginning of the publication process, rather than after the fact.</blockquote><p>Adding structured data citations during the authoring phase of a manuscript requires tools that make this process easier, providing auto-linking and verification of the without requiring extra input from the author. Scholarly Markdown is an ideal platform for these tools, as it is easier to extend than traditional word processors such as Microsoft Word. During a small workshop around persistent identifiers for data (<a href="http://datacite.org/">DataCite</a>), people (<a href="http://orcid.org/">ORCID</a>) and geological samples (<a href="http://www.geosamples.org/igsnabout">IGSN</a>) that took place yesterday and today at the <a href="http://www.gfz-potsdam.de/portal/gfz/cegit">GFZ Potsdam</a> I worked on a tool that does auto-linking for these identifiers:</p><ul><li>IGSN. <a href="http://www.geosamples.org/igsnabout">International Geosample Number</a></li><li>MGI identifiers for genetically modified mouse strains in the <a href="http://www.findmice.org/about">Internal Mouse Strain Resource</a></li><li>ENA. <a href="http://www.ebi.ac.uk/ena/about/about">Genbank / ENA / DDBJ nucleotide sequences</a></li><li>UniProt protein sequences from the <a href="http://www.uniprot.org/help/about">UniProt database</a></li><li>PDB. <a href="http://www.rcsb.org/pdb/static.do?p=home/faq.html">Protein Data Bank protein structure information</a></li></ul><p>The list includes the IGSN, the database identifiers studied by Kafkas et al (2013), and the MGI identifier for genetically altered mice. In the life sciences there is a long tradition - and requirement by journals - to use database identifiers for data, but identifiers for resources such as genetically modified mice are unfortunately not in common use.</p><p>This blog uses the Pandoc markdown processor and the Jekyll static website generator. The easiest way to implement this functionality was by writing a filter for the liquid templating engine used by Jekyll, and provide this filter as a Jekyll plugin. The Jekyll plugin can be found at <a href="https://github.com/mfenner/jekyll-scholmd">mfenner/jekyll-scholmd</a>. The plugin expects the name of the identifier, followed by a colon and optional space, followed by the identifier:</p><pre><code>GenBank:  M10090
IGSN:  JRH964436
MGI:  96922
UniProt:  P02144
PDB:  1mbn</code></pre><p>This input is automatically translated into <a href="http://www.ebi.ac.uk/ena/data/view/M10090">GenBank:M10090</a>, <a href="http://hdl.handle.net/10273/JRH964436">IGSN:JRH964436</a>, <a href="http://www.findmice.org/summary?gaccid/96922">MGI:96922</a>, and information about the human myoglobin protein (<a href="http://www.uniprot.org/uniprot/P02144">UniProt:P02144</a>, <a href="http://www.rcsb.org/pdb/explore/explore.do?structureId=1mbn">PDB:1mbn</a>) is generated in a similar fashion.</p><p>The plugin was written in a few hours today, and is my first Jekyll plugin. There is room for improvement, e.g. support for more identifiers, better regex matching, validation of the resulting links, and automated tag generation if an identifier is found. Ideally the auto-linking should happen in the markdown and not the HTML output, so that these structured database links are also available in other markdown outputs such as PDF. But this is another example how Scholarly Markdown can make it easier for researchers to author documents without requiring a fancy web-based user interface.</p><h2 id="references">References</h2><p>Kafkas, Ş., Kim, J.-H., &amp; McEntyre, J. R. (2013). Database Citation in Full Text Biomedical Articles. <em>PLoS ONE</em>. <a href="http://doi.org/10.1371/journal.pone.0063184">doi:10.1371/journal.pone.0063184</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Metadata in Scholarly Markdown]]></title>
            <link>https://blog.martinfenner.org/posts/metadata-in-scholarly-markdown</link>
            <guid>e902cfc5-f699-4a65-b974-2d549adb467f</guid>
            <pubDate>Sat, 29 Jun 2013 17:01:00 GMT</pubDate>
            <description><![CDATA[Scholarly documents often need metadata that describe them: typically author(s),
title and location (DOI or URL), but possibly many other things. For some
metadata it makes sense to store them in the document text, e.g. as is typically
done for citations. The problem is that this can make it hard to make the
metadata machine-readable. The worst place for metadata is of course outside of
the document, and unfortunately that it is the most common way of doing this.
Two examples:

 * Manuscript sub]]></description>
            <content:encoded><![CDATA[<p>Scholarly documents often need metadata that describe them: typically author(s), title and location (DOI or URL), but possibly many other things. For some metadata it makes sense to store them in the document text, e.g. as is typically done for citations. The problem is that this can make it hard to make the metadata machine-readable. The worst place for metadata is of course outside of the document, and unfortunately that it is the most common way of doing this. Two examples:</p><ul><li>Manuscript submission. Papers submitted to scholarly journals contain the metadata in the text, but authors are required to enter the information again into a webform. You can add metadata (<a href="http://office.microsoft.com/en-001/word-help/add-property-information-to-a-document-HA010163766.aspx">property information</a>) to Microsoft Word documents, but it seems that nobody is doing it.</li><li>PDFs and image files. Even though we have at least one good standard with <a href="http://blogs.plos.org/mfenner/2008/12/22/just_doi_it/">XMP</a> to store metadata in these documents, it is not a common practice. Information about these documents is therefore stored somewhere else and doesn’t automatically travel with them.</li></ul><p>The best place for metadata is the document itself, and the metadata should be stored in machine-readable format. Another requirement is flexibility in what we can store, and we shouldn’t limit ourselves to a predefined list. Pandoc for example allows only three attributes in the <a href="http://johnmacfarlane.net/pandoc/README.html">title block</a>:</p><pre><code>% title
% author(s) (separated by semicolons)
% date</code></pre><p>For Scholarly Markdown we have another requirement: the metadata should be writeable and readable by humans. <a href="http://en.wikipedia.org/wiki/YAML">YAML</a> is the perfect format for this. JSON is closely related to YAML (and is in fact a subset of YAML 1.2), but YAML can also be written with whitespace instead of curly braces. The static website generator Jekyll - which I use to parse the markdown for this blog into HTML - uses YAML at the beginning of markdown documents to store metadata, and we can easily extend this functionality. Carl Boettinger posted a <a href="http://blog.martinfenner.org/2013/06/21/what-flavor-is-scholarly-markdown/#comment-945513935">comment</a> yesterday saying that YAML support is on the Pandoc development roadmap.</p><p>Below is the YAML for (Ethan P. White, 2013), where I reposted a paper written in markdown:</p><pre><code>---
layout: post
title: "Nine simple ways to make it easier to (re)use your data"
tags: [example, citation]
authors:
 - name: Ethan P. White
   orcid: 0000-0001-6728-7745
   affiliation: Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341
 - name: Elita Baldrige
   orcid: 0000-0003-1639-5951
   affiliation: Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341
 - name: Zachary T. Brym
   affiliation: Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341
 - name: Kenneth J. Locey
   affiliation: Dept. of Biology, Utah State University, Logan, UT, USA, 84341
 - name: Daniel J. McGlinn
   affiliation: Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341
 - name: Sarah R. Supp
   affiliation: Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341
---</code></pre><p>In JSON the same information would look like this (and Jekyll is able to parse it, since JSON is a subset of YAML 1.2):</p><pre><code>---
{
  "layout": "post",
  "title": "Nine simple ways to make it easier to (re)use your data",
  "tags": [
    "example",
    "citation"
  ],
  "authors": [
    {
      "name": "Ethan P. White",
      "orcid": "0000-0001-6728-7745",
      "affiliation": "Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341"
    },
    {
      "name": "Elita Baldrige",
      "orcid": "0000-0003-1639-5951",
      "affiliation": "Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341"
    },
    {
      "name": "Zachary T. Brym",
      "affiliation": "Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341"
    },
    {
      "name": "Kenneth J. Locey",
      "affiliation": "Dept. of Biology, Utah State University, Logan, UT, USA, 84341"
    },
    {
      "name": "Daniel J. McGlinn",
      "affiliation": "Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341"
    },
    {
      "name": "Sarah R. Supp",
      "affiliation": "Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341"
    }
  ]
}
---</code></pre><p>You can see that the author information required for manuscript submission can easily be written in YAML (email addresses were removed to protect privacy). JSON is also possible for people where this is a better fit into their workflow, but it is more difficult to write for humans because of the curly braces, and because all strings need to be in double quotes.</p><p>Once the ORCID Registry <a href="http://orcid.org/blog/2013/06/27/orcid-plans-launch-affiliation-module-using-isni-and-ringgold-organization">adds affiliation</a> information, we no longer need to provide email and affiliation when submitting manuscripts. I have stored my own name, orcid, email and affiliation in my site configuration file so that I don’t have to provide this info for every blog post.</p><p>In this blog markdown files are currently only processed to HTML, and I store the metadata in HTML <code>meta</code> tags in a <a href="http://www.monperrus.net/martin/accurate+bibliographic+metadata+and+google+scholar">format</a> used by many sites and services, including Google Scholar - look at the source code of Ethan P. White et al. (2013) for an example. These metadata are also understood by the Greycite service built by Phil Lord and Lindsay Marshall (2012) that generates citation information for weblinks, adding important metadata such as title, authors and publication_date so that we can properly cite our blog post (Ethan P. White, 2013).</p><p>And I use the metadata to link the author names to their ORCID profile (if they have an ORCID) or email address, with the affiliation visible when you hover over the name. My own name is linked to the <a href="http://blog.martinfenner.org/about.html">About</a> page of this site, but with a little development effort I could automatically add all my publications (and other works) in my ORCID profile to that page.</p><p>Metadata are important, and Scholarly Markdown makes it easy to embed them.</p><p><em>Update 06/30/13: added JSON example to demonstrate the differences to YAML, and to show that Jekyll also works with JSON (used in this blog post, and tested with the examples above which produce identical HTML output). Also added two references, using the embedded HTML metadata and the Greycite service to generate citations in bibtex.</em></p><h2 id="references">References</h2><p>Ethan P. White, Z. T. B., Elita Baldrige. (2013). Nine simple ways to make it easier to (re)use your data. <em>Gobbledygook</em>. Retrieved from <a href="http://blog.martinfenner.org/2013/06/25/nine-simple-ways-to-make-it-easier-to-reuse-your-data">http://blog.martinfenner.org/2013/06/25/nine-simple-ways-to-make-it-easier-to-reuse-your-data</a></p><p>Lord, P., &amp; Marshall, L. (2012). Greycite: Citing the web. <em>An Exercise in Irrelevance</em>. Retrieved from <a href="http://www.russet.org.uk/blog/2071">http://www.russet.org.uk/blog/2071</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Nine simple ways to make it easier to (re)use your data]]></title>
            <link>https://blog.martinfenner.org/posts/nine-simple-ways-to-make-it-easier-to-re-use-your-data</link>
            <guid>94df384d-0c20-40ac-945c-5ede4042763a</guid>
            <pubDate>Tue, 25 Jun 2013 17:04:00 GMT</pubDate>
            <description><![CDATA[> This paper in markdown format was written by Ethan White et al. The markdown
file and the associated bibliogaphy and figure files are available from the 
Github repository of the paper [https://github.com/weecology/data-sharing-paper]
.
> I used this
[https://github.com/weecology/data-sharing-paper/commit/b5a73eb0942a18bb29810025a528aea48a8465e7] 
version, an earlier version was published as PeerJ Preprint
[http://dx.doi.org/10.7287/peerj.preprints.7v1]. Special thanks to Ethan White
for allow]]></description>
            <content:encoded><![CDATA[<blockquote>This paper in markdown format was written by Ethan White et al. The markdown file and the associated bibliogaphy and figure files are available from the <a href="https://github.com/weecology/data-sharing-paper">Github repository of the paper</a>.</blockquote><blockquote>I used <a href="https://github.com/weecology/data-sharing-paper/commit/b5a73eb0942a18bb29810025a528aea48a8465e7">this</a> version, an earlier version was published as <a href="http://dx.doi.org/10.7287/peerj.preprints.7v1">PeerJ Preprint</a>. Special thanks to Ethan White for allowing me to reuse this paper. The paper is used here as an example document to show how markdown can handle scholarly documents, in particular tables, figures and citations. The document was slightly modified from the orginal: added YAML frontmatter (needed by jekyll, author names are also stored there), and changed the anchor text for some links. This post is using the APA citation style. Please restrict your comments to issues related to Scholarly Markdown, for the content of the article contact Ethan directly.</blockquote><h2 id="abstract">Abstract</h2><p>Sharing data is increasingly considered to be an important part of the scientific process. Making your data publicly available allows original results to be reproduced and new analyses to be conducted. While sharing your data is the first step in allowing reuse, it is also important that the data be easy to understand and use. We describe nine simple ways to make it easy to reuse the data that you share and also make it easier to work with it yourself. Our recommendations focus on making your data understandable, easy to analyze, and readily available to the wider community of scientists.</p><h2 id="introduction">Introduction</h2><p>Sharing data is increasingly recognized as an important component of the scientific process (Whitlock, McPeek, Rausher, Rieseberg, &amp; Moore, 2010). The sharing of scientific data is beneficial because it allows replication of research results and reuse in meta-analyses and projects not originally intended by the data collectors (Poisot, Mounce, &amp; Gravel, 2013). In ecology and evolutionary biology, sharing occurs through a combination of formal data repositories like <a href="http://www.ncbi.nlm.nih.gov/genbank/">GenBank</a> and <a href="http://datadryad.org/">Dryad</a>, and through individual and institutional websites.</p><p>While data sharing is increasingly common and straightforward, much of the shared data in ecology and evolutionary biology are not easily reused because they do not follow best practices in terms of data structure, metadata, and licensing (M. B. Jones, Schildhauer, Reichman, &amp; Bowers, 2006). This makes it more difficult to work with existing data and therefore makes the data less useful than it could be (M. B. Jones et al., 2006; O. J. Reichman, Jones, &amp; Schildhauer, 2011). Here we provide a list of 9 simple ways to make it easier to reuse the data that you share.</p><p>Our recommendations focus on making your data understandable, easy to work with, and available to the wider community of scientists. They are designed to be simple and straightforward to implement, and as such represent an introduction to good data practices rather than a comprehensive treatment. We contextualize our recommendations with examples from ecology and evolutionary biology, though many of the recommendations apply broadly across scientific disciplines. Following these recommendations makes it easier for anyone to reuse your data including other members of your lab and even yourself.</p><h2 id="1-share-your-data">1. Share your data</h2><p>The first and most important step in sharing your data is to share your data. The recommendations below will help make your data more useful, but sharing it in any form is a big step forward. So, why should you share your data?</p><p>Data sharing provides substantial benefits to the scientific community (Fienberg &amp; Martin, 1985). It allows</p><ol><li>the results of existing analyses to be reproduced and improved upon (Fienberg &amp; Martin, 1985; Poisot et al., 2013),</li><li>data to be combined in meta-analyses to reach general conclusions (Fienberg &amp; Martin, 1985),</li><li>new approaches to be applied to the data and new questions asked using it (Fienberg &amp; Martin, 1985), and</li><li>approaches to scientific inquiry that couldn’t even be considered without broad scale data sharing (Hampton et al., 2013).</li></ol><p>As a result, data sharing is increasingly required by funding agencies (Poisot et al. (2013); e.g., <a href="http://www.nsf.gov/bfa/dias/policy/dmp.jsp">NSF</a>, <a href="http://grants.nih.gov/grants/guide/notice-files/NOT-OD-03-032.html">NIH</a>, <a href="http://www.nserc-crsng.gc.ca/Professors-Professeurs/FinancialAdminGuide-GuideAdminFinancier/Responsibilities-Responsabilites_eng.asp">NSERC</a>, <a href="http://www.fwf.ac.at/en/public_relations/oai/index.html">FWF</a>), journals (Whitlock et al., 2010), and potentially by law (e.g. <a href="http://doyle.house.gov/sites/doyle.house.gov/files/documents/2013%2002%2014%20DOYLE%20FASTR%20FINAL.pdf">FASTR</a>).</p><p>Despite these potential benefits to the community, many scientists are still reluctant to share data. This reluctance is largely due to perceived fears of 1) competition for publications based on the shared data, 2) technical barriers, and 3) a lack of recognition for sharing data (Hampton et al., 2013; Palmer et al., 2004). These concerns are often not as serious as they first appear, and the minimal costs associated with data sharing are frequently offset by individual benefits to the data sharer (Hampton et al., 2013; Parr &amp; Cummings, 2005). Many data sharing initiatives allow for data embargoes or limitations on direct competition that can last for several years while the authors develop their publications and thus avoid competition for deriving publications from the data. Also, logistical barriers to data sharing are diminishing as data archives become increasingly common and easy to use (Hampton et al., 2013; Parr &amp; Cummings, 2005). Datasets are now considered citable entities and data providers receive recognition in the form of increased citation metrics and credit on CVs and grant applications (Heather A Piwowar &amp; Vision, 2013; Heather A. Piwowar, Day, &amp; Fridsma, 2007; Poisot et al., 2013). In addition to increased citation rates, shared datasets that are documented and standardized are also more easily reused in the future by the original investigator. As a result, it is increasingly beneficial to the individual researcher to share data in the most useful manner possible.</p><h2 id="2-provide-metadata">2. Provide metadata</h2><p>The first key to using data is understanding it. Metadata is information about the data including how it was collected, what the units of measurement are, and descriptions of how to best use the data. Clear metadata makes it easier to figure out if a dataset is appropriate for a project. It also makes data easier to use by both the original investigators and by other scientists by making it easy to figure out how to work with the data. Without clear metadata, datasets can be overlooked or not used due to the difficulty of understanding the data (Fraser &amp; Gluck, 1999; A. S. Zimmerman, 2003), and the data becomes less useful over time (Michener, Brunt, Helly, Kirchner, &amp; Stafford, 1997).</p><p>Metadata can take several forms, including descriptive file and column names, a written description of the data, images (<em>i.e.,</em> maps, photographs), and specially structured information that can be read by computers. Good metadata should provide 1) the what, when, where, and how of data collection, 2) how to find and access the data, 3) suggestions on the suitability of the data for answering specific questions, 4) warnings about known problems or inconsistencies in the data, and 5) information to check that the data are properly imported, such as the number of rows and columns in the dataset and the total sum of numerical columns (Michener et al., 1997; Strasser, Cook, Michener, &amp; Budden, 2012; A. S. Zimmerman, 2003).</p><p>Just like any other scientific publication, metadata should be logically organized, complete, and clear enough to enable interpretation and use of the data (A. Zimmerman, 2007). Specific metadata standards exist (<em>e.g.,</em> Ecological Metadata Language <a href="http://knb.ecoinformatics.org/software/eml/">EML</a>, Directory Interchange Format <a href="http://gcmd.gsfc.nasa.gov/add/difguide/index.html">DIF</a>, Darwin Core <a href="http://rs.tdwg.org/dwc/">DWC</a> (Wieczorek et al., 2012), Dublin Core Metadata Initiative <a href="http://dublincore.org/metadata-basics/">DCMI</a>, Federal Geographic Data Committee <a href="http://www.fgdc.gov/metadata/geospatial-metadata-standards">FGDC</a> (O. J. Reichman et al., 2011; Whitlock, 2011). These standards are designed to provide consistency in metadata across different datasets and also to allow computers to interpret the metadata automatically. This allows broader and more efficient use of shared data (Brunt, McCartney, Baker, &amp; Stafford, 2002; M. B. Jones et al., 2006). While following these standards is valuable, the most important thing is to have metadata at all.</p><p>You don’t need to spend a lot of extra time to write good metadata. The easiest way to develop metadata is to start describing your data during the planning and data collection stages. This will help you stay organized, make it easier to work with your data after it has been collected, and make eventual publication of the data easier. If you decide to take the extra step and follow metadata standards, there are tools designed to make this easier including: <a href="http://knb.ecoinformatics.org/morpho%20portal.jsp">KNB Morpho</a>, <a href="http://geology.usgs.gov/tools/metadata/tools/doc/xtme.html">USGS xtme</a>, and <a href="http://www.fgdc.gov/metadata/documents/workbook_0501_bmk.pdf">FGDC workbook</a>.</p><h2 id="3-provide-an-unprocessed-form-of-the-data">3. Provide an unprocessed form of the data</h2><p>Often, the data used in scientific analyses are modified in some way from the original form in which they were collected. This is done to address the questions of interest in the best manner possible and to address common limitations associated with the raw data. However, the best way to process data depends on the question being asked and corrections for common data limitations often change as better approaches are developed. It can also be very difficult to combine data from multiple sources that have each been processed in different ways. Therefore, to make your data as useful as possible it is best to share the data in as raw a form as possible.</p><p>This is not to say that your data are best suited for analysis in the raw form, but providing it in the raw form gives data users the most flexibility. Of course, your work to develop and process the data is also very important and can be quite valuable for other scientists using your data. This is particularly true when correcting data for common limitations. Providing both the raw and processed forms of the data, and clearly explaining the differences between them in the metadata, is an easy way to include the benefits of both data forms. An alternate approach is to share the unprocessed data along with the code that process the data to the form you used for analysis. This allows other scientists to assess and potentially modify the process by which you arrived at the values used in your analysis.</p><h2 id="4-use-standard-data-formats">4. Use standard data formats</h2><p>Everyone has their own favorite tools for storing and analyzing data. To make it easy to use your data it is best to store it in a standard format that can be used by many different kinds of software. Good standard formats include the type of file, the overall structure of the data, and the specific contents of the file.</p><h3 id="use-standard-file-formats">Use standard file formats</h3><p>You should use file formats that are readable by most software and, when possible, are non-proprietary (Borer, Seabloom, Jones, &amp; Schildhauer, 2009; Strasser, Cook, Michener, Budden, &amp; Koskela, 2011; Strasser et al., 2012). Certain kinds of data in ecology and evolution have well established standard formats such as <a href="http://zhanglab.ccmb.med.umich.edu/FASTA/">FASTA</a> files for nucleotide or peptide sequences and the <a href="http://evolution.genetics.washington.edu/phylip/newicktree.html">Newick files</a> for phylogenetic trees. Use these well defined formats when they exist, because that is what other scientists and most existing software will be able to work with most easily.</p><p>Data that does not have a well defined standard format is often stored in tables. Tabular data should be stored in a format that can be opened by any type of software to increase reuseability of the data, i.e. text files. These text files use delimiters to indicate different columns. Commas are the most commonly used delimiter (i.e., comma-delimited text files with the .csv extension). Tabs can also be used as a delimiter, although problems can occur in displaying the data correctly when importing data from one program to another. In contrast to plain text files, proprietary formats such as those used by Microsoft Excel (e.g, .xls, .xlsx) can be difficult to load into other programs. In addition, these types of files can become obsolete, eventually making it difficult to open the data files at all if the newer versions of the software no longer support the original format (Borer et al., 2009; Strasser et al., 2011, 2012).</p><p>When naming files you should use descriptive names so that it is easy to keep track of what data they contain (Borer et al., 2009; Strasser et al., 2011, 2012). If there are multiple files in a dataset, name them in a consistent manner to make it easier to automate working with them. You should also avoid spaces in file names, which can cause problems for some software (Borer et al., 2009). Spaces in file names can be avoided by using camel case (e.g, RainAvg) or by separating the words with underscores (e.g., rain_avg).</p><h3 id="use-standard-table-formats">Use standard table formats</h3><p>Data tables are ubiquitous in ecology and evolution. Tabular data provides a great deal of flexibility in how to structure the data, which makes it easy to structure the data in a way that is difficult to (re)use. We provide three simple recommendations to help ensure that tabular data are properly structured to allow the data to be easily imported and analyzed by most data management systems and common analysis software, such as R and Python.</p><ul><li>Each row should represent a single observation (i.e., a record) and each column should represent a single variable or type of measurement (i.e., a field) (Borer et al., 2009; Strasser et al., 2011, 2012). This is the standard format for tables in the most commonly used database management systems and analysis packages and makes the data easy to work with in the most general way.</li><li>Every cell should contain only a single value (Strasser et al., 2012). For example, do not include units in the cell with the values (Figure 1) or include multiple measurements in a single cell, and break taxonomic information up into single components with one column each for family, genus, species, subspecies, etc. Violating this rule makes it difficult to process or analyze your data using standard tools, because there is no easy way for the software to treat the items within a cell as separate pieces of information.</li><li>There should only be one column for each type of information (Borer et al., 2009; Strasser et al., 2011, 2012). The most common violation of this rule is <a href="http://en.wikipedia.org/wiki/Cross_tabulation">cross-tab structured data</a>, where different columns contain measurements of the same variable (e.g., in different sites, treatments, etc.; Figure 1).</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/Data_formatting.jpg" class="kg-image" alt="Figure 1. Examples of how to restructure two common issues with tabular data. (a) Each cell should only contain a single value. If more than one value is present then the data should be split into multiple columns. (b) There should be only one column for each type of information. If there are multiple columns then the column header should be stored in one column and the values from each column should be stored in a single column."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Figure 1. Examples of how to restructure two common issues with tabular data</strong>. (a) Each cell should only contain a single value. If more than one value is present then the data should be split into multiple columns. (b) There should be only one column for each type of information. If there are multiple columns then the column header should be stored in one column and the values from each column should be stored in a single column.</figcaption></figure><p>While cross-tab data can be useful for its readability, and may be appropriate for data collection, this format makes it difficult to link the records with additional data (e.g., the location and environmental conditions at a site) and it cannot be properly used by most common database management and analysis tools (e.g., relational databases, dataframes in R and Python, etc.). If tabular data are currently in a cross-tab structure, there are tools to help restructure the data including functions in Excel, R (e.g., melt() function in the R package reshape; Wickham (2007)), and Python (e.g., melt() function in the <a href="http://pandas.pydata.org/">Pandas</a> Python module.</p><p>In addition to following these basic rules you should also make sure to use descriptive column names (Borer et al., 2009). Descriptive column names make the data easier to understand and therefore make data interpretation errors less likely. As with file names, spaces can cause problems for some software and should be avoided.</p><h3 id="use-standard-formats-within-cells">Use standard formats within cells</h3><p>In addition to using standard table structures it is also important to ensure that the contents of each cell don’t cause problems for data management and analysis software. Specifically, we recommend:</p><ul><li>Be consistent. For example, be consistent in your capitalization of words, choice of delimiters, and naming conventions for variables.</li><li>Avoid special characters. Most software for storing and analyzing data works best on plain text, and accents and other special characters can make it difficult to import your data (Borer et al., 2009; Strasser et al., 2012).</li><li>Avoid using your delimiter in the data itself (e.g., commas in the notes filed of a comma-delimited file). This can make it difficult to import your data properly. This means that if you are using commas as the decimal separator (as is often done in continental Europe) then you should use a non-comma delimiter (e.g., a tab).</li><li>When working with dates use the YYYY-MM-DD format (i.e., follow the <a href="http://www.iso.org/iso/support/faqs/faqs_widely_used_standards/widely_used_standards_other/iso8601">ISO 8601</a> data standard).</li></ul><h2 id="5-use-good-null-values">5. Use good null values</h2><p>Most ecological and evolutionary datasets contain missing or empty data values. Working with this kind of “null” data can be difficult, especially when the null values are indicated in problematic ways. Unfortunately, there are many different ways to indicate a missing/empty value, and very little agreement on which approach to use.</p><p>We recommend choosing a null value that is both compatible with most software and unlikely to cause errors in analyses (Table 1). The null value that is most compatible with the software commonly used by biologists is the blank (i.e., nothing; Table 1). Blanks are automatically treated as null values by R, Python, SQL, and Excel. They are also easily spotted in a visual examination of the data. Note that a blank involves entering nothing, it is not a space, so if you use this option make sure there aren’t any hidden spaces. There are two potential issues with blanks that should be considered:</p><ol><li>It can be difficult to know if a value is missing or was overlooked during data entry.</li><li>They can be confusing when spaces or tabs are used as delimiters in text files.</li></ol><p>NA and NULL are reasonable null values, but they are only handled automatically by a subset of commonly used software (Table 1). NA can also be problematic if it is also used as an abbreviation (e.g., North America, Namibia, <em>Neotoma albigula</em>, sodium, etc.). We recommend against using numerical values to indicate nulls (e.g., 999, -999, etc.) because they typically require an extra step to remove from analyses and can be accidentally included in calculations. We also recommend against using non-standard text indications (e.g., No data, ND, missing, —) because they can cause issues with software that requires consistent data types within columns). Whichever null value that you use, only use one, use it consistently throughout the data set, and indicate it clearly in the metadata.</p><!--kg-card-begin: html--><table style="box-sizing: border-box; border-collapse: collapse; border-spacing: 0px; max-width: 100%; background-color: rgb(255, 255, 255); font-size: 19px; font-family: ff-tisa-sans-web-pro, Arial, sans-serif; width: 750px; margin-bottom: 21px; color: rgb(0, 0, 0); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"><caption style="box-sizing: border-box; text-align: left; vertical-align: top; font-size: 19px; font-style: italic; line-height: 1.33; margin-bottom: 0.75em;"><strong style="box-sizing: border-box; font-weight: bold;">Tabel 1. Commonly used null values, limitations, compatibility with common software and a recommendation regarding whether or not it is a good option</strong>. Null values are indicated as being a null value for specific software if they work consistently and correctly with that software. For example, the null value “NULL” works correctly for certain applications in R, but does not work in others, so it is not presented as part of the table.</caption><colgroup style="box-sizing: border-box;"><col style="box-sizing: border-box; width: 0px;"><col style="box-sizing: border-box; width: 0px;"><col style="box-sizing: border-box; width: 0px;"><col style="box-sizing: border-box; width: 0px;"></colgroup><thead style="box-sizing: border-box;"><tr class="header" style="box-sizing: border-box;"><th style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: bottom; border-top: 0px; font-weight: bold;">Null values</th><th style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: bottom; border-top: 0px; font-weight: bold;">Problems</th><th style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: bottom; border-top: 0px; font-weight: bold;">Compatibility</th><th style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: bottom; border-top: 0px; font-weight: bold;">Recommendation</th></tr></thead><tbody style="box-sizing: border-box;"><tr class="odd" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">0</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Indistinguishable from a true zero</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Never use</p></td></tr><tr class="even" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">blank</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Hard to distinguish values that are missing from those overlooked on entry. Hard to distinguish blanks from spaces, which behave differently.</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">R, Python, SQL</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Best option</p></td></tr><tr class="odd" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">999, -999</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Not recognized as null by many programs without user input. Can be inadvertently entered into calculations.</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Avoid</p></td></tr><tr class="even" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">NA, na</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Can also be an abbreviation (e.g., North America), can cause problems with data type (turn a numerical column into a text column). NA is more commonly recognized than na.</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">R</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Good option</p></td></tr><tr class="odd" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">N/A</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">An alternate form of NA, but often not compatible with software</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Avoid</p></td></tr><tr class="even" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">NULL</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Can cause problems with data type</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">SQL</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Good option</p></td></tr><tr class="odd" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">None</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Can cause problems with data type</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Python</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Avoid</p></td></tr><tr class="even" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">No data</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Can cause problems with data type, contains a space</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Avoid</p></td></tr><tr class="odd" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Missing</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Can cause problems with data type</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Avoid</p></td></tr><tr class="even" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">-,+,.</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Can cause problems with data type</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Avoid</p></td></tr></tbody></table><!--kg-card-end: html--><h2 id="6-make-it-easy-to-combine-your-data-with-other-datasets">6. Make it easy to combine your data with other datasets</h2><p>Ecological and evolutionary data are often most valuable when combined with other kinds of data (e.g., taxonomic, environmental). You can make it easier to combine your data with other data sources by including the data that is common across many data sources (e.g., Latin binomials, latitudes and longitudes) It is common for data to include codes or abbreviations. For example, in ecology and evolution codes often appear in place of site locations or taxonomy. This is useful because it reduces data entry (e.g., DS instead of <em>Dipodomys spectabilis</em>) and redundancy (a single column for a species ID rather than separate columns for family, genus, and species). However, without clear definitions these codes can be difficult to understand and make it more difficult to connect your data with external sources. The easiest way to link your data to other datasets is to include additional tables that contain a column for the code and additional columns that describe the item in the standard way. For example, you might include a table with the species codes followed by their most current family, genus, and specific epithet. For site location, you could include a table with the site code followed by latitude and longitude. Linked tables can also be used to include additional information about your data, such as spatial extent, temporal duration, and other appropriate details.</p><h2 id="7-perform-basic-quality-control">7. Perform basic quality control</h2><p>Data, just like any other scientific product, should undergo some level of quality control (O. J. Reichman et al., 2011). This is true regardless of whether you plan to share the data because quality control will make it easier to analyze your own data and decrease the chance of making mistakes. However, it is particularly important for data that will be shared because scientists using the data won’t be familiar with quirks in the data and how to work around them.</p><p>At its most basic, quality control can consist of a few quick sanity checks of the data. More advanced quality control can include automated checks on data as it is entered and double-entry of data (Lampe &amp; Weiler, 1998; Paulsen, Overgaard, &amp; Lauritsen, 2012). This additional effort can be time consuming, but is valuable because it increases data accuracy by catching typographical errors, reader/recorder error, out-of-range values, and questionable data in general (Lampe &amp; Weiler, 1998; Paulsen et al., 2012).</p><p>Before sharing your data we recommend performing a quick “data review”. Start by performing some basic sanity checks on your data. For example:</p><ul><li>If a column should contain numeric values, check that there are no non-numeric values in the data.</li><li>Check that empty cells actually represent missing data, and not mistakes in data entry, and indicate that they are empty using the appropriate null values (see recommendation 6).</li><li>Check for consistency in unit of measurement, data type (e.g., numeric, character), naming scheme (e.g., taxonomy, location), etc.</li></ul><p>These checks can be performed by carefully looking at the data or can be automated using common programming and analysis tools like R or Python.</p><p>Then ask someone else to look over your metadata and data and provide you with feedback about anything they didn’t understand. In the same way that friendly reviews of papers can help catch mistakes and identify confusing sections of papers, a friendly review of data can help identify problems and things that are unclear in the data and metadata.</p><h2 id="8-use-an-established-repository">8. Use an established repository</h2><p>For data sharing to be effective, data should be easy to find, accessible, and stored where it will be preserved for a long time (Kowalczyk &amp; Shankar, 2011). To make your data (and associated code) visible and easily accessible, and to ensure a permanent link to a well maintained website, we suggest depositing your data in one of the major well-established repositories. This guarantees that the data will be available in the same location for a long time, in contrast to personal and institutional websites that do not guarantee the long-term persistence of the data. There are repositories available for sharing almost any type of biological or environmental data. Repositories that host specific data types, such as molecular sequences (e.g., DDBJ, GenBank, MG-RAST), are often highly standardized in data type, format, and quality control approaches. Other repositories host a wide array of data types and are less standardized (e.g., Dryad, KNB, PANGAEA). In addition to the repositories focused on the natural sciences there are also all purpose repositories where data of any kind can be shared (e.g., figshare).</p><p>When choosing a repository you should consider where other researchers in your discipline are sharing their data. This helps you quickly identify the community’s standard approach to sharing and increases the likelihood that other scientists will discover your data. In particular, if there is a centralized repository for a specific kind of data (e.g., GenBank for sequence data) then you should use that repository.</p><p>In cases where there is no <em>de facto</em> standard it is worth considering differences among repositories in terms of use, data rights, and licensing (Table 2) and whether your funding agency or journal has explicit requirements or restrictions related to repositories. We also recommend that you use a repository that allows your dataset to be easily cited. Most repositories will describe how this works, but an easy way to guarantee that your data are citable is to confirm that the repository associates it with a persistent identifier, the most popular of which is the digital object identifier (DOI). DOIs are permanent unique identifiers that are independent of physical location and site ownership. There are also online tools for finding good repositories for your data including <a href="http://databib.org/">Databib</a> and <a href="http://re3data.org/">re3data</a>.</p><!--kg-card-begin: html--><table style="box-sizing: border-box; border-collapse: collapse; border-spacing: 0px; max-width: 100%; background-color: rgb(255, 255, 255); font-size: 19px; font-family: ff-tisa-sans-web-pro, Arial, sans-serif; width: 750px; margin-bottom: 21px; color: rgb(0, 0, 0); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"><caption style="box-sizing: border-box; text-align: left; vertical-align: top; font-size: 19px; font-style: italic; line-height: 1.33; margin-bottom: 0.75em;"><strong style="box-sizing: border-box; font-weight: bold;">Table 2. Popular repositories for scientific datasets</strong>. This table does not include well-known molecular repositories (e.g.&nbsp;GenBank, EMBL, MG-RAST) that have become<span>&nbsp;</span><em style="box-sizing: border-box;">de facto</em><span>&nbsp;</span>standards in molecular and evolutionary biology. Consequently, several of these primarily serve the ecological community. These repositories are not exclusively used by members of specific institutions or museums, but accept data from the general scientific community.</caption><colgroup style="box-sizing: border-box;"><col style="box-sizing: border-box; width: 0px;"><col style="box-sizing: border-box; width: 0px;"><col style="box-sizing: border-box; width: 0px;"><col style="box-sizing: border-box; width: 0px;"><col style="box-sizing: border-box; width: 0px;"><col style="box-sizing: border-box; width: 0px;"></colgroup><thead style="box-sizing: border-box;"><tr class="header" style="box-sizing: border-box;"><th style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: bottom; border-top: 0px; font-weight: bold;">Repository</th><th style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: bottom; border-top: 0px; font-weight: bold;">License</th><th style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: bottom; border-top: 0px; font-weight: bold;">DOI</th><th style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: bottom; border-top: 0px; font-weight: bold;">Metadata</th><th style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: bottom; border-top: 0px; font-weight: bold;">Access</th><th style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: bottom; border-top: 0px; font-weight: bold;">Notes</th></tr></thead><tbody style="box-sizing: border-box;"><tr class="odd" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Dryad</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">CC0</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Yes</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Suggested</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Open</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Ecology &amp; evolution data associated with publications</p></td></tr><tr class="even" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Ecological Archives</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">No</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Yes</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Required</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Open</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Publishes supplemental data for ESA journals and stand alone data papers</p></td></tr><tr class="odd" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Knowledge Network for Biocomplexity</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">No</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Yes</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Required</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Variable</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Partners with ESA, NCEAS, DataONE</p></td></tr><tr class="even" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Paleobiology Database</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Various CC</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">No</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Optional</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Variable</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Paleontology specific</p></td></tr><tr class="odd" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Data Basin</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Various CC</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">No</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Optional</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Open</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">GIS data in ESRI files, limited free space</p></td></tr><tr class="even" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Pangaea</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Various CC</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Yes</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Required</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Variable</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Editors participate in QA/QC</p></td></tr><tr class="odd" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">figshare</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">CC0</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Yes</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Optional</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Open</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Also allows deposition of other research outputs and private datasets</p></td></tr></tbody></table><!--kg-card-end: html--><h2 id="9-use-an-established-and-liberal-license">9. Use an established and liberal license</h2><p>Including an explicit license with your data is the best way to let others know exactly what they can and cannot do with the data you shared. Following the <a href="http://pantonprinciples.org/">Panton Principles</a> we recommend:</p><ol><li>Using well established licenses in order to clearly communicate the rights and responsibilities of both the people providing the data and the people using it.</li><li>Using the most open license possible, because even minor restrictions on data use can have unintended consequences for the reuse of the data (Poisot et al., 2013; Schofield et al., 2009).</li></ol><p>The Creative Commons Zero license (CC0) places no restrictions on data use and is considered by many to be one of the best license for sharing data (e.g., (Poisot et al., 2013; Schofield et al., 2009), <a href="http://blog.datadryad.org/2011/10/05/why-does-dryad-use-cc0/">Why does Dryad use CC0</a>). Having a clear and open license will increase the chance that other scientists will be comfortable using your data.</p><h2 id="concluding-remarks">Concluding remarks</h2><p>Data sharing has the potential to transform the way we conduct ecological and evolutionary research (Fienberg &amp; Martin, 1985; Poisot et al., 2013; Whitlock et al., 2010). As a result, there are an increasing number of initiatives at the federal, funding agency, and journal levels to encourage or require the sharing of the data associated with scientific research (Heather A Piwowar &amp; Chapman, 2008; Poisot et al., 2013; Whitlock et al., 2010). However, making the data available is only the first step. To make data sharing as useful as possible it is necessary to make the data usable with as little effort as possible (M. B. Jones et al., 2006; O. J. Reichman et al., 2011). This allows scientists to spend their time doing science rather than cleaning up data.</p><p>We have provided a list of 9 practices that require only a small additional time investment but substantially improve the usability of data. These practices can be broken down into three major groups.</p><ol><li>Well documented data are easier to understand.</li><li>Properly formatted data are easier to use in a variety of software.</li><li>Data that is shared in established repositories with open licenses is easier for others to find and use.</li></ol><p>Most of these recommendations are simply good practice for working with data regardless of whether that data are shared or not. This means that following these recommendations (2-7) make the data easier to work with for anyone, including you. This is particularly true when returning to your own data for further analysis months or years after you originally collected or analyzed it. In addition, data sharing often occurs within a lab or research group. Good data sharing practices make these in-house collaborations faster, easier, and less dependent on lab members who may have graduated or moved on to other things.</p><p>By following these practices we can assure that the data collected in ecology and evolution can be used to its full potential to improve our understanding of biological systems.</p><h2 id="acknowledgments">Acknowledgments</h2><p>Thanks to Karthik Ram for organizing this special section and inviting us to contribute. Carly Strasser and Kara Woo recommended important references and David Harris and Carly Strasser provided valuable feedback on null values, all via Twitter. Carl Boettiger, Matt Davis, Daniel Hocking, Heinz Pampel, Karthik Ram, Thiago Silva, Carly Strasser, Tom Webb, and beroe (Twitter handle) provided value comments on the manuscript. Many of these comments were part of the informal review process facilitated by posting this manuscript as a preprint. The writing of this paper was supported by a CAREER grant from the U.S. National Science Foundation (DEB 0953694) to EPW.</p><h2 id="references">References</h2><p>Borer, E. T., Seabloom, E. W., Jones, M. B., &amp; Schildhauer, M. (2009). Some simple guidelines for effective data management. <em>Bulletin of the Ecological Society of America</em>, <em>90</em>(2), 205–214. Retrieved from <a href="http://dx.doi.org/10.1890/0012-9623-90.2.205">http://dx.doi.org/10.1890/0012-9623-90.2.205</a></p><p>Brunt, J. W., McCartney, P., Baker, K., &amp; Stafford, S. G. (2002). The future of ecoinformatics in long term ecological research. In <em>Proceedings of the 6th world multiconference on systemics, cybernetics and informatics: SCI</em> (pp. 14–18).</p><p>Fienberg, S. E., &amp; Martin, M. E. (1985). <em>Sharing research data</em>. Natl Academy Pr.</p><p>Fraser, B., &amp; Gluck, M. (1999). Usability of geospatial metadata or space-time matters. <em>Bulletin of the American Society for Information Science and Technology</em>, <em>25</em>(6), 24–28. Retrieved from <a href="http://dx.doi.org/10.1002/bult.134">http://dx.doi.org/10.1002/bult.134</a></p><p>Hampton, S. E., Strasser, C. A., Tewksbury, J. J., Gram, W. K., Budden, A. E., Batcheller, A. L., … Porter, J. H. (2013). Big data and the future of ecology. <em>Frontiers in Ecology and the Environment</em>, <em>11</em>(3), 156–162. Retrieved from <a href="http://dx.doi.org/10.1890/120103">http://dx.doi.org/10.1890/120103</a></p><p>Jones, M. B., Schildhauer, M. P., Reichman, O., &amp; Bowers, S. (2006). The new bioinformatics: Integrating ecological data from the gene to the biosphere. <em>Annual Review of Ecology, Evolution, and Systematics</em>, <em>37</em>(1), 519–54. Retrieved from <a href="http://dx.doi.org/10.1146/annurev.ecolsys.37.091305.110031">http://dx.doi.org/10.1146/annurev.ecolsys.37.091305.110031</a></p><p>Kowalczyk, S., &amp; Shankar, K. (2011). Data sharing in the sciences. <em>Annual Review of Information Science and Technology</em>, <em>45</em>(1), 247–294. Retrieved from <a href="http://dx.doi.org/10.1002/aris.2011.1440450113">http://dx.doi.org/10.1002/aris.2011.1440450113</a></p><p>Lampe, A., &amp; Weiler, J. (1998). Data capture from the sponsors’ and investigators’ perspectives: Balancing quality, speed, and cost. <em>Drug Information Journal</em>, <em>32</em>(4), 871–886.</p><p>Michener, W. K., Brunt, J. W., Helly, J. J., Kirchner, T. B., &amp; Stafford, S. G. (1997). Nongeospatial metadata for the ecological sciences. <em>Ecological Applications</em>, <em>7</em>(1), 330–342. Retrieved from <a href="http://dx.doi.org/10.1890/1051-0761(1997)007%5B0330:nmftes%5D2.0.co;2">http://dx.doi.org/10.1890/1051-0761(1997)007[0330:nmftes]2.0.co;2</a></p><p>Palmer, M. A., Bernhardt, E. S., Chornesky, E. A., Collins, S. L., Dobson, A. P., Duke, C. S., … Turner, M. G. (2004). Ecological science and sustainability for a crowded planet. Retrieved from <a href="http://www.esa.org/ecovisions/ppfiles/EcologicalVisionsReport.pdf">http://www.esa.org/ecovisions/ppfiles/EcologicalVisionsReport.pdf</a></p><p>Parr, C., &amp; Cummings, M. (2005). Data sharing in ecology and evolution. <em>Trends in Ecology &amp; Evolution</em>, <em>20</em>(7), 362–363. Retrieved from <a href="http://dx.doi.org/10.1016/j.tree.2005.04.023">http://dx.doi.org/10.1016/j.tree.2005.04.023</a></p><p>Paulsen, A., Overgaard, S., &amp; Lauritsen, J. M. (2012). Quality of data entry using single entry, double entry and automated forms processing–An example based on a study of patient-reported outcomes. <em>PloS ONE</em>, <em>7</em>(4), e35087. Retrieved from <a href="http://dx.doi.org/10.1371/journal.pone.0035087">http://dx.doi.org/10.1371/journal.pone.0035087</a></p><p>Piwowar, H. A., &amp; Chapman, W. W. (2008). A review of journal policies for sharing research data. In <em>ELPUB2008</em>.</p><p>Piwowar, H. A., &amp; Vision, T. J. (2013). Data reuse and the open data citation advantage. <em>PeerJ PrePrints</em>, <em>1</em>, e1. Retrieved from <a href="http://dx.doi.org/10.7287/peerj.preprints.1">http://dx.doi.org/10.7287/peerj.preprints.1</a></p><p>Piwowar, H. A., Day, R. S., &amp; Fridsma, D. B. (2007). Sharing detailed research data is associated with increased citation rate. <em>PLoS ONE</em>, <em>2</em>(3), e308. Retrieved from <a href="http://dx.doi.org/10.1371/journal.pone.0000308">http://dx.doi.org/10.1371/journal.pone.0000308</a></p><p>Poisot, T., Mounce, R., &amp; Gravel, D. (2013). Moving toward a sustainable ecological science: Don’t let data go to waste! Retrieved from <a href="https://github.com/tpoisot/DataSharingPaper/blob/master/DataSharing-MS.md">https://github.com/tpoisot/DataSharingPaper/blob/master/DataSharing-MS.md</a></p><p>Reichman, O. J., Jones, M. B., &amp; Schildhauer, M. P. (2011). Challenges and opportunities of open data in ecology. <em>Science</em>, <em>331</em>(6018), 703–705. Retrieved from <a href="http://dx.doi.org/10.1126/science.1197962">http://dx.doi.org/10.1126/science.1197962</a></p><p>Schofield, P. N., Bubela, T., Weaver, T., Portilla, L., Brown, S. D., Hancock, J. M., … Rosenthal, N. (2009). Post-publication sharing of data and tools. <em>Nature</em>, <em>461</em>(7261), 171–173. Retrieved from <a href="http://dx.doi.org/10.1038/461171a">http://dx.doi.org/10.1038/461171a</a></p><p>Strasser, C. A., Cook, R. B., Michener, W. K., Budden, A., &amp; Koskela, R. (2011). Promoting data stewardship through best practices. In <em>Proceedings of the environmental information management conference 2011 (eIM 2011)</em>. Oak Ridge National Laboratory (ORNL).</p><p>Strasser, C. A., Cook, R., Michener, W. K., &amp; Budden, A. (2012). Primer on data management: What you always wanted to know. DataONE. Retrieved from <a href="http://dx.doi.org/10.5060/D2251G48">http://dx.doi.org/10.5060/D2251G48</a></p><p>Whitlock, M. C. (2011). Data archiving in ecology and evolution: Best practices. <em>Trends in Ecology &amp; Evolution</em>, <em>26</em>(2), 61–65. Retrieved from <a href="http://dx.doi.org/10.1016/j.tree.2010.11.006">http://dx.doi.org/10.1016/j.tree.2010.11.006</a></p><p>Whitlock, M. C., McPeek, M. A., Rausher, M. D., Rieseberg, L., &amp; Moore, A. J. (2010). Data archiving. <em>The American Naturalist</em>, <em>175</em>(2), 145–146. <a href="http://doi.org/10.1086/650340">doi:10.1086/650340</a></p><p>Wickham, H. (2007). Reshaping data with the reshape package. <em>Journal of Statistical Software</em>, <em>21</em>(12). Retrieved from <a href="http://www.jstatsoft.org/v21/i12/paper">http://www.jstatsoft.org/v21/i12/paper</a></p><p>Wieczorek, J., Bloom, D., Guralnick, R., Blum, S., Döring, M., Giovanni, R., … Vieglais, D. (2012). Darwin core: An evolving community-developed biodiversity data standard. <em>PLoS ONE</em>, <em>7</em>(1), e29715. <a href="http://doi.org/10.1371/journal.pone.0029715">doi:10.1371/journal.pone.0029715</a></p><p>Zimmerman, A. (2007). Not by metadata alone: The use of diverse forms of knowledge to locate data for reuse. <em>International Journal on Digital Libraries</em>, <em>7</em>(1-2), 5–16. <a href="http://doi.org/10.1007/s00799-007-0015-8">doi:10.1007/s00799-007-0015-8</a></p><p>Zimmerman, A. S. (2003). <em>Data sharing and secondary use of scientific data: Experiences of ecologists</em> (PhD thesis). The University of Michigan.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Citations in Markdown Part 3]]></title>
            <link>https://blog.martinfenner.org/posts/citations-in-markdown-part-3</link>
            <guid>dd7d89a7-84c2-4e59-bdda-517e976f11d8</guid>
            <pubDate>Mon, 24 Jun 2013 17:08:00 GMT</pubDate>
            <description><![CDATA[After the post last week
[http://blog.martinfenner.org/2013/06/19/citations-in-scholarly-markdown/] and
the crazy discussion that followed I would understand that you feel you have
heard enough about citations in markdown. But I had the feeling last week that
something was still missing, and I have done some more thinking. What we have so
far:

 * Pandoc has nice support for citations, including Citation Style Language
   support (i.e. it is using the same 5000+ citation styles as Zotero, Mendel]]></description>
            <content:encoded><![CDATA[<p>After the <a href="http://blog.martinfenner.org/2013/06/19/citations-in-scholarly-markdown/">post last week</a> and the crazy discussion that followed I would understand that you feel you have heard enough about citations in markdown. But I had the feeling last week that something was still missing, and I have done some more thinking. What we have so far:</p><ul><li>Pandoc has nice support for citations, including Citation Style Language support (i.e. it is using the same 5000+ citation styles as Zotero, Mendeley and Papers).</li><li>Pandoc requires a separate file to store the citations, typically in bibtex format. This is fine for some people, but can make the workflow complicated for short documents or when several people work on the bibliography at the same time.</li><li>Citations are similar to links, and we can use links for almost all the functionality we need, making it much easier to add citations to a text. The problem is a) citations that don’t include a weblink, b) being able to do this offline, and c) where in the HTML to store the citation metadata.</li></ul><p>And I looked at how Wikipedia is <a href="http://en.wikipedia.org/wiki/Wikipedia:Citing_sources">doing this</a>, and they use a) links, b) citations and c) footnotes. If Wikipedia thinks that it can’t do without citations and do everything as links, then maybe we also shouldn’t enforce this for scholarly texts.</p><p>I think what we need is the best of both worlds. We should use the Pandoc citation workflow, as it is similar to what we are used to from other authoring environments, and we get good citation style support, including more complex formatting of references. Some reference managers already support copy/paste of Pandoc citation keys. The inclusion of a bibtex file with a scholarly markdown text is also a bonus, as it allows the automated extraction of citations, e.g. by manuscript submission systems.</p><p>We also want to support a simpler solution for shorter texts or when people don’t want to use a separate bibtex file. Here we would add the citations as links, ideally in a syntax very similar to Pandoc citation keys:</p><pre><code>Johnson [@Johnson2006] didn't agree with ...

[@Johnson2006]: http://dx.doi.org/10.1002/aris.201 "Data sharing in the sciences"</code></pre><p>We need to write a tool that parses the markdown before Pandoc, fetches the citation metadata for these links in bibtex format (e.g. using CrossRef Content Negotiation), and adds them to the existing bibtex file (or creates a new bibtex file). The next time the markdown is parsed, the citation is already “cached” in the bibtex file. Those people who don’t have such a tool would see the citation as link (<strong><strong>???</strong></strong>), with the essential information (DOI or URL) preserved so that a downstream tool can fetch the bibliographic information. Some people were worried about typos in DOIs and URLs. They can add additional information - e.g. the title of the paper - in double quotes to allow checking of the correct DOI.</p><p>This workflow now makes a lot of sense to me, as it uses existing solutions, but also allows for easy entering of citation information in a way similar to the <a href="http://carlboettiger.info/2012/05/30/knitcitations.html">knitcitations</a> and <a href="http://wordpress.org/plugins/kcite/">kcite</a> tools. As I use jekyll and am a Ruby developer, I will implement the citation parsing as a jekyll plugin.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What Flavor is Scholarly Markdown?]]></title>
            <link>https://blog.martinfenner.org/posts/what-flavor-is-scholarly-markdown</link>
            <guid>97197639-565a-4090-82da-63f3c8a8507e</guid>
            <pubDate>Fri, 21 Jun 2013 17:11:00 GMT</pubDate>
            <description><![CDATA[One important outcome of the recent Markdown for Science
[https://github.com/scholmd/scholmd/wiki/workshop] workshop was an overall
agreement that all the different implementations (or flavors) of markdown that
currently exist are a big problem for the adoption of Scholarly Markdown and
that we need [https://github.com/scholmd/scholmd/wiki/What-is-Markdown]:

> A reference implementation with documentation and tests
As described by Karthik Ram (31 flavors is great for ice cream but not markdown
]]></description>
            <content:encoded><![CDATA[<p>One important outcome of the recent <a href="https://github.com/scholmd/scholmd/wiki/workshop">Markdown for Science</a> workshop was an overall agreement that all the different implementations (or flavors) of markdown that currently exist are a big problem for the adoption of Scholarly Markdown and that we <a href="https://github.com/scholmd/scholmd/wiki/What-is-Markdown">need</a>:</p><blockquote>A reference implementation with documentation and tests</blockquote><p>As described by Karthik Ram (<a href="https://github.com/scholmd/scholmd/wiki/workshop">31 flavors is great for ice cream but not markdown</a>), <a href="http://blog.martinfenner.org/2012/12/13/a-call-for-scholarly-markdown/">me</a> and <a href="http://www.codinghorror.com/blog/2012/10/the-future-of-markdown.html">others</a>, there is really a large number of markdown implementations to choose from, including</p><ul><li>John Gruber’s <a href="http://daringfireball.net/projects/markdown/">original Markdown</a></li><li><a href="https://help.github.com/articles/github-flavored-markdown">Github-flavored Markdown</a></li><li><a href="http://michelf.ca/projects/php-markdown/extra/">PHP Markdown Extra</a></li><li><a href="http://johnmacfarlane.net/pandoc/">Pandoc</a></li><li><a href="http://fletcherpenney.net/multimarkdown/">MultiMarkdown</a></li></ul><p>These different flavors all serve their needs, but for Markdown to take off in the relatively small scholarly community it would be very helpful to come up with a reference implementation. But how do we get to that point?</p><ol><li>Think about the features we need for Scholarly Markdown and make this the reference implementation?</li><li>Organize a working group or committee that decides what is Scholarly Markdown?</li><li>Pick the Markdown flavor with the best developer support?</li><li>Figure out what markdown flavor has the widest support by tools relevant for scholars?</li><li>See what markdown flavor most scholars are currently using?</li></ol><p>I think as a starting point, and until we come up with something better, #5 makes the most sense. The number of markdown users among scholars is still small, but my guess would be that Pandoc is currently the most popular Markdown flavor among scholars. This blog uses Pandoc and the static site generator <a href="http://jekyllrb.com/">Jekyll</a>, and is hosted on <a href="http://pages.github.com/">Github Pages</a> - for the source code use the link in the footer. Please tell me in the comments what you are using (Markdown flavor and tools), and whether I am correct with my wild guess regarding Pandoc. And make sure your preferred tool is listed in the <a href="https://github.com/scholmd/scholmd/wiki/Tools-to-support-your-markdown-authoring">Tools to spport your markdown authoring</a> wiki page.</p><p>A reference Markdown document is also very helpful to move forward, as we can see what outputs in HTML, PDF (or other formats) our specific Markdown tools produce, and how they differ. This reference document should include citations, tables, figures, and other features typical for scholarly content. Ideally this is a paper written in Markdown and accepted for publication - proving the concept -, or it can be a published paper transformed into markdown, e.g. a paper by <a href="http://www.elifesciences.org/elife-now-supports-content-negotiation/">eLife</a>. Feel free to suggest a paper in the comments.</p><p>The idea of tests that came up in the Markdown workshop is also great. Ideally we have a set of tests that we (or someone else, e.g. a publisher) can run to make sure that the markdown in the document conforms with the reference implementation. This could also include basic checks for required metadata (title, author, publication date, etc.), and could optionally validate the citations as well.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Citations in Scholarly Markdown]]></title>
            <link>https://blog.martinfenner.org/posts/citations-in-scholarly-markdown</link>
            <guid>54b4a024-a0d7-4726-8130-5dd7c0e7a63a</guid>
            <pubDate>Wed, 19 Jun 2013 17:13:00 GMT</pubDate>
            <description><![CDATA[In the comments on Monday’s blog post
[http://blog.martinfenner.org/2013/06/17/what-is-scholarly-markdown/] about the
Markdown for Science workshop, Carl Boettiger [http://carlboettiger.info/] had
some good arguments against the proposal for how to do citations
[https://github.com/scholmd/scholmd/wiki/citations] that we came up with during
the workshop. As this is a complex topic, I decided to write this blog post.

Citations of the scholarly literature are an essential part of scholarly texts
a]]></description>
            <content:encoded><![CDATA[<p>In the comments on <a href="http://blog.martinfenner.org/2013/06/17/what-is-scholarly-markdown/">Monday’s blog post</a> about the Markdown for Science workshop, <a href="http://carlboettiger.info/">Carl Boettiger</a> had some good arguments against the proposal for how to do <a href="https://github.com/scholmd/scholmd/wiki/citations">citations</a> that we came up with during the workshop. As this is a complex topic, I decided to write this blog post.</p><p>Citations of the scholarly literature are an essential part of scholarly texts and therefore have to be supported by scholarly markdown. Both the <a href="http://johnmacfarlane.net/pandoc/README.html">Pandoc</a> and <a href="https://github.com/fletcher/MultiMarkdown/wiki/MultiMarkdown-Syntax-Guide">Multimarkdown</a> flavors of markdown support citations, using a bibtex file that contains citations, placeholders for citekeys – <code>[@smith04]</code> for Pandoc and <code>[#smith04]</code> for Multimarkdown – and the <a href="http://citationstyles.org/">Citation Style Language</a> for citation formatting (Pandoc). A very reasonable approach would therefore be to use this functionality, with a preference for Pandoc because of the Citation Style Language support. All reference managers can export to the bibtex format, and some of them (e.g. <a href="http://www.papersapp.com/papers/">Papers</a>) make it very easy to copy and paste citekeys.</p><p>Ten days after the workshop I’m not so sure anymore this is the best approach. For four reasons:</p><ol><li><strong><strong>YFNS</strong></strong>. This approach failed the YFNS (your friendly neighborhood scientist) test. We came up with this term during the workshop and it means that our ideas about authoring should make sense to the workflow of the average scientist. I thought that using citekeys is a good idea, but my wife (my YFNS) tells me that she never uses citekeys because there are just too many <code>[@smith04]</code>, and it is too easy get out of sync with the reference manager. She therefore prefers to put the complete reference information into the text while writing.</li><li><strong><strong>Snippets</strong></strong>. As I said <a href="http://blog.martinfenner.org/2013/06/17/what-is-scholarly-markdown/">previously</a>, I think that scholarly markdown has great potential not so much for writing full papers, but for all the little scientific documents we write on a daily basis. For this reason the citation information should ideally be embedded in the document if it is short, and that is difficult with bibtex (which is not human-readable).</li><li><strong><strong>Citations as links</strong></strong>. Carl Boettiger reminded me that I wrote a <a href="http://blogs.plos.org/mfenner/2010/12/11/citations-are-links-so-where-is-the-problem/">blog post in 2010</a> stating that citations are nothing else than links, and that we should treat them accordingly. He has written a tool (<a href="http://carlboettiger.info/2012/05/30/knitcitations.html">knitcitations</a>) for R that does just that, and Phil Lord and colleagues have written a similar tool (<a href="http://wordpress.org/plugins/kcite/">kcite</a>) for Wordpress. In 2010 I wrote a tool for Wordpress (<a href="http://wordpress.org/plugins/link-to-link/">Link to Link</a>) that takes a different approach but also treats citations as links. All that we need is the DOI (or URL) for the article.</li><li><strong><strong>Vendor lock-in</strong></strong>. Although a number of excellent reference managers are available now, users are still limited in their choices because everyone has to use the same reference manager when multiple authors work on the same document. This has always annoyed me. It would no longer be the case if we embed the citation information in the document in a standard format.</li></ol><p>Part of the motivation for using scholarly markdown is that we can come up with best practices that make sense for digital content and don’t need to support conventions from an era when articles were still printed on paper. Reference information in the form of volumes and pages, and 1000s of citation styles certainly have outlived their purpose. Citation styles are a particular pain point, as they are nothing more than a visual representation of a citation - we should care much more about the machine-readable metadata, in particular the DOI or other identifier.</p><p>The best practice for scholarly markdown could therefore be to treat citations as links, using DOIs or other standard identifiers (PMID, ArXiV, etc.) where possible. Because we typically want to list the citations as references at the end of the document, reference-style links should be preferred over inline links. From the <a href="http://daringfireball.net/projects/markdown/syntax#link">markdown syntax documentation</a>:</p><pre><code>This is [an example][id] reference-style link.

This is [an example](http://example.com/ "Title") inline link.
[id]: http://example.com/  "Optional Title Here"</code></pre><p>It might be tempting to use sequential numbers as id for the reference-style links, but the order of links can of course change during writing. It may make sense to think of the id in reference-style links as a citekey, and people should be free use that functionality of their reference manager. The citekey is used to link to the reference list at the bottom of the document, different from linking to the citekey in a separate bibtex file.</p><p>All of the above can be done in any text editor. This also includes the text editor that scholars spend most of their time with - their email program. Reference-style citations in an email are very readable, and also actionable since they are links and not text with bibliographic information.</p><p>One problem with this approach is of course that all links are inline in the resulting HTML, without a references section at the end of the document. This may be fine, as we can provide citation information in the title attribute, available upon hovering over the link (try hovering over <a href="http://dx.doi.org/10.1371/journal.pmed.0020124">this link</a>, the journal eLife is doing <a href="http://dx.doi.org/10.7554/eLife.00633">something similar</a>). The markdown could look like this (using the <em>Vancouver</em> citation style):</p><pre><code>[@Ioannidis2005]: http://dx.doi.org/10.1371/journal.pmed.0020124 "Ioannidis JPA. Why Most Published Research Findings Are False. PLoS Medicine. Public Library of Science; 2005;2(8):e124. Available from: http://dx.doi.org/10.1371/journal.pmed.0020124"</code></pre><p>The title attribute now of course uses a citation style, but this is optional information and can easily be reformatted as we have the DOI.</p><p>Or we break away from standard markdown and display reference-style links at the end of the document - similar to <a href="http://rephrase.net/box/word/footnotes/syntax/">footnotes</a>, which are also not part of standard markdown. But this is just a display issue that can be solved, and the solution might look different depending on whether the output is HTML, PDF or XML. This document for example contains 14 reference-style citations.</p><p>There is obviously a need for tools that make adding citations to scholarly markdown easier. This could be accomplished by relatively small changes to existing reference managers (enabling copy/paste of citations in reference-style markdown format), or by tools similar to the <a href="http://carlboettiger.info/2012/05/30/knitcitations.html">knitcitations</a> and <a href="http://wordpress.org/plugins/kcite/">kcite</a> mentioned above.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What is Scholarly Markdown?]]></title>
            <link>https://blog.martinfenner.org/posts/what-is-scholarly-markdown</link>
            <guid>c5c6f033-8b9a-4f40-a6e7-3d5ad0f5bc30</guid>
            <pubDate>Mon, 17 Jun 2013 17:16:00 GMT</pubDate>
            <description><![CDATA[One of the important discussions taking place at the Markdown for Science
[https://github.com/scholmd/scholmd/wiki] workshop last weekend was about the
definition of Scholarly Markdown. We came up with this
[https://github.com/scholmd/scholmd/wiki/What-is-Markdown]:

 1. Markdown that supports the requirements of scientific texts
 2. Markdown as format that glues open scientific text resources together
 3. A reference implementation with documentation and tests
 4. A community

We also agreed th]]></description>
            <content:encoded><![CDATA[<p>One of the important discussions taking place at the <a href="https://github.com/scholmd/scholmd/wiki">Markdown for Science</a> workshop last weekend was about the definition of Scholarly Markdown. We came up with <a href="https://github.com/scholmd/scholmd/wiki/What-is-Markdown">this</a>:</p><ol><li>Markdown that supports the requirements of scientific texts</li><li>Markdown as format that glues open scientific text resources together</li><li>A reference implementation with documentation and tests</li><li>A community</li></ol><p>We also agreed that <strong><strong>Scholarly Markdown</strong></strong> is a better term than <strong><strong>Markdown for Science</strong></strong>, as it also includes the Social Sciences and Humanities. And we agreed on a hashtag, <a href="https://twitter.com/search?q=%23scholmd&amp;src=typd">#scholmd</a>.</p><p>I like #3 and #4, and I was not surprised to see #1. #2 is one of the most important outcomes of the workshop for me personally, and was reflected in the discussion we had in the breakout session on <em>What is needed for Markdown to be adopted by the scientific community?</em> One important strategy is the following:</p><blockquote>We need an online tool that makes it easy for scholars to write scholarly markdown in a collaborative manner.</blockquote><p>We called this the <strong><strong>Google Docs for Scientists</strong></strong> (Google Docs is a good collaborative tool, but is lacking some important features required for scientific documents, e.g. integrated citation management). <a href="https://www.authorea.com/">Authorea</a> was mentioned as a promising example of this concept. It was also noted that some previous efforts failed, because the tool looked too different from Microsoft Word. But building such a tool wouldn’t really require markdown as a file format, and could for example also be done directly in HTML5. This would be a reasonable strategy, but in my mind is falling short because I think the problem we need to solve is more complicated than making collaboration easier with tools that look like Microsoft Word. We therefore also discussed a different strategy:</p><blockquote>We need multiple tools that make it easy for scholars to create scholarly markdown documents and openly share them. This collaborative work is not limited to authoring scholarly papers, but also includes shorter scholarly texts, e.g. experimental results, lab notebooks, lecture notes, blog posts and working papers. Ease of use is not only defined by the writing experience, but also how easy it is to share documents with others.</blockquote><p>This definition almost sounds like a definition for Open Science, and assumes that data - and increasingly software - are an integral part of reporting science. This makes <a href="https://www.scienceexchange.com/reproducibility">reproducibility</a> of scientific results much easier, and one nice example how this can be done is the integration of markdown into the R statistical software, using the <a href="http://yihui.name/knitr/">knitr package</a>. Using the <a href="http://article-level-metrics.plos.org/plos-alm-data/">May 2013 PLOS article-level metrics data</a> which are freely available for download, the R code below can be embedded into a markdown file and will produce the bar plot below when the markdown file is run in R (to try this yourself, download the ALM data and <a href="https://github.com/articlemetrics/plosOpenR/blob/master/barPlotSummary.Rmd">markdown file for this article</a>).</p><pre><code># Load required libraries
library(reshape2)

# Load the data from the bulk download, filter out DOIs that are not from PLoS journals
alm &lt;- read.csv("data-alm/alm_report_2013-05-20.csv", encoding = "UTF8", sep = ",", stringsAsFactors=FALSE, na.strings=c("0"))
alm &lt;- subset(alm, (substr(alm$doi,1,15) == "10.1371/journal"))
alm$publication_date &lt;- as.Date(alm$publication_date)
alm$counter_html &lt;- as.numeric(alm$counter_html)

# Options
plos.start_date &lt;- NA
plos.end_date &lt;- NA
plos.colors &lt;- c("#304345","#304345","#789aa1","#789aa1","#789aa1","#ad9a27","#ad9a27","#ad9a27","#ad9a27","#ad9a27","#ad9a27","#ad9a27")

# Aggregate notes and comments
alm$comments &lt;- as.numeric(alm$comments)

# Aggregate Mendeley
alm$mendeley &lt;- rowSums(subset(alm, select=c("mendeley_readers","mendeley_groups")), na.rm=TRUE)
alm$mendeley[alm$mendeley == 0] &lt;- NA

# Use subset of columns
alm &lt;- subset(alm, select=c("counter_html","pmc_html","crossref","scopus","pubmed","mendeley","citeulike","comments","researchblogging","facebook","twitter","wikipedia"))

# Calculate percentage of values that are not missing (i.e. have a count of at least 1)
colSums &lt;- colSums(!is.na(alm)) * 100 / length(alm$counter_html)
exactSums &lt;- sum(as.numeric(alm$pmc_html),na.rm =TRUE)

# Plot the chart.
opar &lt;- par(mar=c(1,7,2,1)+0.1,omi=c(1,0.3,1,1))
plos.names &lt;- c("PLoS HTML Views", "PMC HTML Views","CrossRef","Scopus","PubMed Citations", "Mendeley","CiteULike","PLoS Comments","Research Blogging","Facebook","Twitter","Wikipedia")
y &lt;- barplot(colSums,horiz=TRUE,col=plos.colors, border = NA, xlab=plos.names, xlim=c(0,120), axes=FALSE, names.arg=plos.names,las=1, adj=0)
text(colSums+6,y,labels=sprintf("%1.0f%%", colSums))</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/barplot-2013-06-17.svg" class="kg-image" alt="Proportion of articles covered by source. Article-level metrics for all 80,602 PLOS journal articles published until May 20, 2013."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Proportion of articles covered by source</strong>. Article-level metrics for all 80,602 PLOS journal articles published until May 20, 2013.</figcaption></figure><p>In a way this approach to scholarly markdown is much more difficult than building a nice online collaborative writing tool. But for me scholarly markdown is not about competing with Microsoft Word, it is about building something new that scholars want to use because it allows them to do something that is impossible with the existing tools. For the same reason my todo item at the end of the workshop was <em>think about document type where markdown shines</em>. The R example above is a great example where markdown shines. If you can think of additional examples, please add them to the comments.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Goodbye PLOS Blogs, Welcome Github Pages]]></title>
            <link>https://blog.martinfenner.org/posts/goodbye-plos-blogs-welcome-github-pages</link>
            <guid>f5d9c6e3-8b57-429a-b4d1-fbdc1197fdc4</guid>
            <pubDate>Sat, 15 Jun 2013 17:18:00 GMT</pubDate>
            <description><![CDATA[This is the last Gobbledygook [http://blogs.plos.org/mfenner] post on PLOS
Blogs, and at the same time the first post at the new Github blog location
[http://blog.martinfenner.org/]. I have been blogging at PLOS Blogs since the 
PLOS Blogs Network [http://blogs.plos.org/blogosphere/] was launched in
September 2010, so this step wasn’t easy. But I have two good reasons.

In May 2012 I started to work as technical lead for the PLOS Article-Level
Metrics [http://article-level-metrics.plos.org/] pro]]></description>
            <content:encoded><![CDATA[<p>This is the last <a href="http://blogs.plos.org/mfenner">Gobbledygook</a> post on PLOS Blogs, and at the same time the first post at the <a href="http://blog.martinfenner.org/">new Github blog location</a>. I have been blogging at PLOS Blogs since the <a href="http://blogs.plos.org/blogosphere/">PLOS Blogs Network</a> was launched in September 2010, so this step wasn’t easy. But I have two good reasons.</p><p>In May 2012 I started to work as technical lead for the <a href="http://article-level-metrics.plos.org/">PLOS Article-Level Metrics</a> project. Although this is contract work, and I also do other things - including spending 5% of my time as clinical researcher at Hannover Medical School - this created the awkward situation that I was never quite sure whether I was blogging as Martin Fenner or as someone working for PLOS. This was all in my head, as I never had any restrictions in my blogging from PLOS. With the recent launch of the <a href="http://blogs.plos.org/tech/">PLOS Tech Blog</a> there is now a good venue for the kind of topics I like to write about, and I have started to work on two posts for this new blog.</p><p>There will always be topics for which the PLOS Tech Blog is not a good fit, and for these posts I have launched the new personal blog at Github. But the main reason for this new blog is a technical one: I’m moving away from blogging on Wordpress to writing my posts in <a href="http://daringfireball.net/projects/markdown/">markdown</a> (a lightweight markup language), that are then transformed into static HTML pages using <a href="http://jekyllrb.com/">Jekyll</a> and <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a>. Last weekend I co-organized the workshop <a href="https://github.com/scholmd/markdown_science/wiki"><strong><strong>Scholarly Markdown</strong></strong></a> together with <a href="http://twitter.com/houshuang">Stian Håklev</a>. A full workshop report will follow in another post, but the discussions before, at and after the workshop convinced me that <strong><strong>Scholarly Markdown</strong></strong> has a bright future and that it is time to move more of my writing to markdown. At the end of the workshop each participant suggested a <a href="https://github.com/scholmd/markdown_science/wiki/Todo-list-from-workshop">todo item</a> that he/she would be working on, and my todo item was “Think about document type where MD shines”. Markdown might be good for writing scientific papers, but I think it really shines in shorter scientific documents that can easily be shared with others. And blog posts are a perfect fit.</p><p>The new site is work in progress. Over time I will copy over all old blog posts from PLOS Blogs, and will work on the layout as well as additional features. Special thanks to <a href="http://carlboettiger.info/">Carl Boettiger</a> for helping me to get started with Jekyll and Github pages.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[re3data.org: registry of research data repositories launched]]></title>
            <link>https://blog.martinfenner.org/posts/re3data-org-registry-of-research-data-repositories-launched</link>
            <guid>28d4e446-ec85-498c-80b0-598e467c03a4</guid>
            <pubDate>Sat, 01 Jun 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[Earlier this week re3data.org – the Registry of Research Data Repositories – 
officially launched
[https://web.archive.org/web/20171029050202/http://www.re3data.org/2013/05/re3data-org-launched/]
. The registry is nicely described in a preprint
[https://web.archive.org/web/20171029050202/http://dx.doi.org/10.7287/peerj.preprints.21v1] 
also published this week.

re3data.org offers researchers, funding organizations, libraries and publishers
and overview of the heterogeneous research data reposit]]></description>
            <content:encoded><![CDATA[<p>Earlier this week re3data.org – the Registry of Research Data Repositories – <a href="https://web.archive.org/web/20171029050202/http://www.re3data.org/2013/05/re3data-org-launched/">officially launched</a>. The registry is nicely described in a <a href="https://web.archive.org/web/20171029050202/http://dx.doi.org/10.7287/peerj.preprints.21v1">preprint</a> also published this week.</p><p><em>re3data.org offers researchers, funding organizations, libraries and publishers and overview of the heterogeneous research data repository landscape. Information icons help researchers to identify an adequate repository for the storage and reuse of their data.</em></p><p>I really like re3data.org, and that is not because I personally know several of the people involved in this project, or because they <a href="https://web.archive.org/web/20171029050202/http://blogs.plos.org/mfenner/2012/02/16/figshare-interview-with-mark-hahnel/">cited this blog</a> in their preprint. I think that we are just at the beginning of building the infrastructure needed for research data management, and re3data.org fills an important need. In my opinion it is not enough to provide lists of research data repositories, we need additional information that can help guide researchers in selecting an appropriate research data repository. re3data.org has addressed this nicely by providing a <a href="https://web.archive.org/web/20171029050202/http://blogs.plos.org/mfenner/2013/06/01/re3data-org-registry-of-research-data-repositories-launched/10.2312/re3.002">vocabulary for the registration and description of research data repositories</a>, and by creating a simple icon system:</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20171029050202im_/http://blogs.plos.org/mfenner/files/2013/06/figure2.png" class="kg-image" alt="The re3data icon system"></figure><p><em>Possible values for each icon. From <a href="https://web.archive.org/web/20171029050202/http://dx.doi.org/10.7287/peerj.preprints.21v1">http://dx.doi.org/10.7287/peerj.preprints.21v1</a></em></p><p>Future directions I would like re3data.org to take include:</p><ul><li><strong>Training and education.</strong> Researchers probably pick research data repositories mainly based on the familiarity of the repository within their community rather than the criteria developed by re3data.org. A lot more training and education is needed before researchers understand the importance of persistent identifiers, licenses and other criteria.</li><li><strong>Integration.</strong> re3data.org can make it easier to integrate into existing scientific infrastructure, e.g. by using persistent identifiers such as DOIs for research data repositories, or by providing an API that makes it easier for other services to integrate re3data.org.</li><li><strong>Governance</strong>. Whether or not scientific infrastructure such as re3data.org is accepted and used by the community depends on many factors, and governance is one of the most important ones. re3data.org should seek the support of other organizations, in particular from outside Germany. A governing board, re3data.org as an independent organization, and strategies to coordinate with similar efforts such as <a href="https://web.archive.org/web/20171029050202/http://www.databib.org/">Databib</a> are possible strategies.</li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Metrics and attribution: my thoughts for the panel at the ORCID-Dryad symposium on research attribution]]></title>
            <link>https://blog.martinfenner.org/posts/metrics-and-attribution-my-thoughts-for-the-panel-at-the-orcid-dryad-symposium-on-research-attribution</link>
            <guid>508d21c3-4237-4e98-9727-b1a6c09d6d5f</guid>
            <pubDate>Fri, 31 May 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[This Thursday I take part in a panel discussion at the Joint ORCID – Dryad
Symposium on Research Attribution
[https://web.archive.org/web/20171029102820/http://orcid.org/orcid-outreach-meeting-symposium-and-codefest-may-2013]
. Together with Trish Groves
[https://web.archive.org/web/20171029102820/http://www.bmj.com/about-bmj/editorial-staff/trish-groves] 
(BMJ) and Christine Borgman
[https://web.archive.org/web/20171029102820/http://polaris.gseis.ucla.edu/cborgman/Chriss_Site/Welcome.html] 
(UC]]></description>
            <content:encoded><![CDATA[<p>This Thursday I take part in a panel discussion at the <a href="https://web.archive.org/web/20171029102820/http://orcid.org/orcid-outreach-meeting-symposium-and-codefest-may-2013">Joint ORCID – Dryad Symposium on Research Attribution</a>. Together with <a href="https://web.archive.org/web/20171029102820/http://www.bmj.com/about-bmj/editorial-staff/trish-groves">Trish Groves</a> (BMJ) and <a href="https://web.archive.org/web/20171029102820/http://polaris.gseis.ucla.edu/cborgman/Chriss_Site/Welcome.html">Christine Borgman</a> (UCLA) I will discuss several aspects of attribution. Trish will speak about ethics, Christine will highlight problems, and I will add my perspective on metrics. This blog post summarizes the main points I want to make.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20171029102820im_/http://blogs.plos.org/mfenner/files/2013/05/A_Bicycle_in_Oxford.jpg" class="kg-image" alt="A_Bicycle_in_Oxford"></figure><p><em>Oxford. Source: <a href="https://web.archive.org/web/20171029102820/http://commons.wikimedia.org/wiki/File:A_Bicycle_in_Oxford.JPG">Wikimedia Commons</a></em></p><p>Scholarly metrics can be used in discovery tools, as business intelligence for funders, research organizations or publishers, and for research assessment. For all these scenarios – and in particular for research assessment – it is important to not only collect metrics for a particular journal publication, dataset or other research output, but to also link these metrics to the creators of that research output.  That is why unique identifiers for researchers, and ORCID in particular, are so important for scholarly metrics, and this is also reflected in the ORCID membership of organizations such as Thomson Reuters, Elsevier/Scopus, Altmetric or F1000Prime who provide metrics in a variety of ways.</p><h2 id="dora">DORA</h2><p>A good starting point for any discussion on metrics for research assessment is the San Francisco Declaration on Research Assessment (<a href="https://web.archive.org/web/20171029102820/http://am.ascb.org/dora/">DORA</a>) that was published las week, together with a set of editorials in several journals, including the <a href="https://web.archive.org/web/20171029102820/http://jcb.rupress.org/content/early/2013/05/14/jcb.201304162.full"><em>Journal of Cell Biology</em></a>, <a href="https://web.archive.org/web/20171029102820/http://www.molbiolcell.org/content/24/10/1505.full"><em>Molecular Biology of the Cell</em></a>, <a href="https://web.archive.org/web/20171029102820/http://www.nature.com/emboj/journal/vaop/ncurrent/full/emboj2013126a.html"><em>EMBO Journal</em></a>, <a href="https://web.archive.org/web/20171029102820/http://www.sciencemag.org/content/340/6134/787"><em>Science</em></a>, <a href="https://web.archive.org/web/20171029102820/http://jcs.biologists.org/content/early/2013/05/09/jcs.134460.full.pdf+html"><em>Journal of Cell Science</em></a>, and <a href="https://web.archive.org/web/20171029102820/http://elife.elifesciences.org/content/elife/2/e00855.full.pdf"><em>eLife</em></a>. The first three recommendations are a good starting point for the panel discussion:</p><ol><li><em>Do not use journal-based metrics, such as Journal Impact Factors, as a surrogate measure of the quality of individual research articles, to assess an individual scientist’s contributions, or in hiring, promotion, or funding decisions.</em></li><li><em>Be explicit about the criteria used in evaluating the scientific productivity of grant applicants and clearly highlight, especially for early-stage investigators, that the scientific content of a paper is much more important than publication metrics or the identity of the journal in which it was published.</em></li><li><em>For the purposes of research assessment, consider the value and impact of all research outputs (including datasets and software) in addition to research publications, and consider a broad range of impact measures including qualitative indicators of research impact, such as influence on policy and practice.</em></li></ol><h2 id="persistent-identifiers">Persistent Identifiers</h2><p>Before we can collect any metrics, we need persistent identifiers for research outputs. Most journal articles now come with a DOI, but we should make it easier for smaller publishers to use DOIs, as cost unfortunately is still an issue.</p><p>Persistent identifiers for data are a much more complex issue, as there a number of persistent identifiers out there (including DOIs, handles, ARKs and purls), in addition to all the domain-specific identifiers, e.g. for nucleotide sequence or protein structures. DataCite DOIs are probably the first choice for attribution, as this is their main use case and they have features that make attribution easier (e.g. familiar to researchers, funders and publishers, global resolver). There are many other use cases for identifiers for data (e.g. to identify temporary datasets in an ongoing experiment), and is of course possible to use several identifiers for the same dataset. CrossRef is of course also issuing DOIs for datasets on behalf of their members, and the publisher PLOS is for example using CrossRef <a href="https://web.archive.org/web/20171029102820/http://blogs.plos.org/mfenner/2011/03/26/direct-links-to-figures-and-tables-using-component-dois/">component DOIs</a> for figures and supplementary information associated with a journal article, and is <a href="https://web.archive.org/web/20171029102820/http://blogs.plos.org/plos/2013/01/easier-access-to-plos-data/">making them available via figshare</a>.</p><p>Particular challenges with persistent identifiers for research data include different versions of a dataset, and aggregation of datasets (e.g. whether we want to cite the aggregate dataset, or a particular subset). Persistent identifiers for other research outputs are an even bigger challenge, e.g. how to uniquely identify scientific software.</p><p>In addition to persistent identifiers for research outputs, we also need persistent identifiers for researchers. ORCID is obviously a good candidate, as it focusses on attribution (by allowing researchers to claim their research outputs and by integration in many researcher workflows). But it is clear that ORCID is not the only persistent identifiers for researchers, and that we need to link these identifiers, e.g. <a href="https://web.archive.org/web/20171029102820/http://orcid.org/blog/2013/04/22/orcid-and-isni-issue-joint-statement-interoperation-april-2013">ORCID and ISNI</a>.</p><p>Depending on how we want to aggregate the metrics we are interested in, we might also need persistent identifiers for institutions, for funding agencies and their grant IDs, and for resources such as particle accelerators or research vessels. Unfortunately much more work is needed in these areas.</p><h2 id="attribution">Attribution</h2><p>Attribution is then the next step, linking persistent identifiers for research outputs to their creators. Attribution is therefore essential for research assessment. The <a href="https://web.archive.org/web/20171029102820/http://www.force11.org/AmsterdamManifesto">Amsterdam Manifesto on Data Citation Principles</a> that came out of the Beyond the PDF 2 workshop in March are an excellent document, but are unfortunately missing the important step of linking persistent identifiers for data to the persistent identifiers of their creators.</p><p>One important issue related to attribution is the provenance of the claims. Has a researcher claimed authorship for a particular paper, is a data center linking creators to research data, or is a funder doing this? The ORCID registry is built around the concept of self-claims by authors, but will allow the other stakeholders to confirm these claims.</p><h2 id="metrics">Metrics</h2><p>Metrics for scholarly content fall into one of three categories:</p><ul><li>Citations</li><li>Usage stats</li><li>Altmetrics</li></ul><p>Altmetrics is a mixed bag of many different things, from sharing on social media such as Twitter or Facebook to more scholarly activities such as Mendeley bookmarks or F1000Prime reviews. I therefore expect the altmetrics category to over time further evolve into 2-3 sub-categories.</p><p>We are all familiar with citation-based metrics for journal articles. We currently see the long-overdue shift from journal-based citation metrics to article-level metrics (see #1 from the DORA statement above for the reasoning), and as the technical lead for the PLOS Article-Level Metrics project I of course welcome this shift in focus. We also see a trend towards opening up reference lists that will make citation-based metrics much more accessible, and the <a href="https://web.archive.org/web/20171029102820/http://opencitations.net/">JISC Open Citations</a> project by David Shotton and others is an important driver in this, as is the <a href="https://web.archive.org/web/20171029102820/http://openbiblio.net/">Open Bibliographic Data</a> project by OKFN. Until open bibliographic data become the norm, we have to deal with different citation counts from different sources. PLOS is collecting citations from Web of Science, Scopus, CrossRef and PubMed Central, and the citation counts are highly correlated overall (e.g. R2= 0.87 for CrossRef and Scopus citations for 2009 PLOS Biology papers), but for some papers differ substantially. Similar to persistent identifiers, reference lists of publications should become part of the open e-infrastructure for science and not depend on proprietary systems. This makes citation metrics more transparent and easier to compare, and fosters research and innovation, in particular by smaller organizations.</p><p>The data citation community has adopted the journal article citation model, and we are starting to see more citations to datasets. Even though data citations look similar to citations of journal articles, many essential tools and services still don’t properly handle datasets. The <a href="https://web.archive.org/web/20171029102820/http://wokinfo.com/products_tools/multidisciplinary/dci/">Web of Knowledge Data Citation Index</a> is an important step in the right direction, as is the<a href="https://web.archive.org/web/20171029102820/http://odin-project.eu/2013/05/13/new-orcid-integrated-data-citation-tool/"> new DataCite import tool for ORCID</a>. Something that we should pay closer attention to is the citation counts of the paper(s) associated with a dataset. Maybe the major scientific impact is in the data, but scientific practice still dictates to the cite the corresponding paper and not the dataset itself (one of the reasons we see data journals being launched). The DataCite metadata can contain the persistent identifier of the corresponding journal article, thus making it possible to associate the citation count of the corresponding paper with the dataset. This approach is particularly important for datasets that are always part of a paper, as is the case for Dryad. One important consideration is that contributor lists may differ between journal article and dataset, or between related datasets.</p><p>Another problem with data citation is that citation counts might not be the best way to reflect the scientific impact of a dataset. We are increasingly seeing usage stats for datasets, and DataCite for example has <a href="https://web.archive.org/web/20171029102820/http://www.datacite.org/node/76">started in January</a> to publish monthly stats for the most popular datasets by number of DOI resolutions. The #1 dataset in March was the raw data to a figure in a F1000Research article, <a href="https://web.archive.org/web/20171029102820/http://figshare.com/articles/Figure_7_raw_data_Effect_of_variable_exposure_to_PTHrP_1_36_on_bone_nodules_and_AP_activity_in_high_plating_density_cultures_/154685">hosted on figshare</a>.</p><p>Similar to citations we see a strong trend for usage stats to move from aggregate numbers for journals to article-level metrics. COUNTER has <a href="https://web.archive.org/web/20171029102820/http://www.projectcounter.org/pirus.html">released</a> a draft code of practice for their PIRUS (Publisher and Institutional Repository Usage Statistics) standard in February, and increasing numbers of publishers and repository infrastructure providers such as <a href="https://web.archive.org/web/20171029102820/http://www.irus.mimas.ac.uk/">IRUS-UK</a> and <a href="https://web.archive.org/web/20171029102820/http://www.dini.de/projekte/oa-statistik/english">OA-Statistics</a> are providing usage stats for individual articles.</p><p>One challenge with usage stats, in particular with Open Access content, is that an article or other research output might be available in more than one place, e.g. publisher (or data center), disciplinary repository and institutional repository. For PLOS articles we don’t know the aggregated usage stats from institutional repositories, but we know that 17% of HTML pageviews and 33% of PDF downloads happen not at the PLOS website, but at PubMed Central.</p><p>Altmetrics provide new challenges, but they are also a more recent development compared to usage stats and citations. Similar to usage stats they are easier to game than citations, and for some altmetrics sources (e.g. Twitter) standardization is still difficult. Altmetrics not necessarily measure impact, but sometimes rather reflect attention or self-promotion. We have just started to look into altmetrics beyond the numbers, e.g. who is tweeting, bookmarking or discussing a paper or dataset. Altmetrics provide the opportunity to show the broader social impact (as Mike Taylor from Elsevier explains it) of research, e.g. changing clinical practice or policies.</p><h2 id="contributions">Contributions</h2><p>One important aspect to attribution is contribution, i.e. what is the specific contribution by a researcher to a paper or other research output. An <a href="https://web.archive.org/web/20171029102820/http://projects.iq.harvard.edu/attribution_workshop">International Workshop on Contributorship and Scholarly Attribution</a> was held together with the May 2012 ORCID Outreach Meeting to discuss this topic. Authorship position (e.g. first author, last author) is used in some metrics, but overall the contributor role is still poorly appreciated in most metrics. David Shotton has proposed a <a href="https://web.archive.org/web/20171029102820/http://purl.org/spar/scoro/Shotton_SCoRO_and_SCoRF_Contributions-Workshop_Harvard_16May2012.pdf">Scholarly Contributions and Roles Ontology</a> (ScoRO), and is suggesting to split authorship credit in percentage points based on relative contributions, but I haven’t seen these numbers used in the context of metrics.</p><h2 id="conclusions">Conclusions</h2><p>Persistent identifiers for people, attribution and metrics are closely interrelated and we have seen a lot of exciting developments in this area in the last two years. The widespread adoption of ORCID identifiers by the research community will have a huge impact on scholarly metrics. But with all the excitement we should never forget that a) there will never be a single metric that can be used for research assessment, and b) that scientific content will always be more important than any metric. I look forward to a great panel discussions on Thursday, and welcome any feedback via comments, Twitter or email.</p><p><em>May 23, 2013: Post updated with minor corrections and additions.</em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[New DataCite / ORCID Integration Tool]]></title>
            <link>https://blog.martinfenner.org/posts/new-datacite-orcid-integration-tool</link>
            <guid>38250d43-50d0-4f1d-8c0b-026f554995a0</guid>
            <pubDate>Sat, 18 May 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[A new service
[https://web.archive.org/web/20171029102032/http://datacite.labs.orcid-eu.org/] 
allows researchers to add research datasets – and other content with DataCite
DOIs, including all figshare
[https://web.archive.org/web/20171029102032/http://figshare.com/] content – to
their ORCID [https://web.archive.org/web/20171029102032/http://about.orcid.org/] 
profile by integrating with the DataCite Metadata Store
[https://web.archive.org/web/20171029102032/http://search.datacite.org/ui]. The
t]]></description>
            <content:encoded><![CDATA[<p>A <a href="https://web.archive.org/web/20171029102032/http://datacite.labs.orcid-eu.org/">new service</a> allows researchers to add research datasets – and other content with DataCite DOIs, including all <a href="https://web.archive.org/web/20171029102032/http://figshare.com/">figshare</a> content – to their <a href="https://web.archive.org/web/20171029102032/http://about.orcid.org/">ORCID</a> profile by integrating with the <a href="https://web.archive.org/web/20171029102032/http://search.datacite.org/ui">DataCite Metadata Store</a>. The tool is an adaption (or fork) of the <a href="https://web.archive.org/web/20171029102032/http://search.crossref.org/">CrossRef Metadata Search</a> developed by <a href="https://web.archive.org/web/20171029102032/https://twitter.com/karlward">Karl Ward</a>, and was developed by <a href="https://web.archive.org/web/20171029102032/https://twitter.com/gthorisson">Gudmundur Thorisson</a> and myself as part of work in the EU-funded <a href="https://web.archive.org/web/20171029102032/http://odin-project.eu/">ODIN project</a>. More details can be found <a href="https://web.archive.org/web/20171029102032/http://odin-project.eu/2013/05/13/new-orcid-integrated-data-citation-tool/">here</a>.</p><p>There are many things I like about this new DataCite/ORCID integration tool:</p><ul><li>it makes it easier for researchers to get credit for their research outputs.</li><li>it shows the value of persistent identifiers for data, publications and people, and linking them together</li><li>it shows the Creative Commons licenses for DataCite content where this info is available, facilitating reuse of content</li><li>it demonstrates the power of open source (thanks CrossRef!), open collaboration, standard REST APIs, and lightweight programming (Sinatra/Ruby) and deployment (Vagrant, Amazon EC2, Rackspace) tools</li><li>it shows that we don’t need a single – often closed – system, but open services that build on top of each other using accepted community standards. Tools using the ORCID API can immediately reuse the new DataCite content, altmetrics provided by <a href="https://web.archive.org/web/20171029102032/http://impactstory.org/">ImpactStory</a> are a good example</li></ul><p>I want to explore some of these ideas in the panel <strong>Attribution: Managing Provenance, Ethics, and Metrics</strong> at the combined <a href="https://web.archive.org/web/20171029102032/http://orcid.org/orcid-outreach-meeting-symposium-and-codefest-may-2013">ORCID/Dryad Meeting</a> in Oxford next Thursday.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Announcing Markdown for Science Workshop on June 8th]]></title>
            <link>https://blog.martinfenner.org/posts/announcing-markdown-for-science-workshop-on-june-8th</link>
            <guid>189d5fd2-1c4e-4d0f-8fae-179c2631a602</guid>
            <pubDate>Wed, 08 May 2013 17:20:00 GMT</pubDate>
            <description><![CDATA[On Saturday June 8th – exactly a month from today – the PLOS San Francisco
offices will host a workshop/hackathon about using markdown for science. A lot
of people are experimenting with markdown for authoring scientific articles –
see blog posts here [http://blog.yoavram.com/markx/], here
[http://inundata.org/2012/06/01/markdown-and-the-future-of-collaborative-manuscript-writing/] 
or my post here
[http://blog.martinfenner.org/2012/12/13/a-call-for-scholarly-markdown/], and
the scientific manus]]></description>
            <content:encoded><![CDATA[<p>On Saturday June 8th – exactly a month from today – the PLOS San Francisco offices will host a workshop/hackathon about using markdown for science. A lot of people are experimenting with markdown for authoring scientific articles – see blog posts <a href="http://blog.yoavram.com/markx/">here</a>, <a href="http://inundata.org/2012/06/01/markdown-and-the-future-of-collaborative-manuscript-writing/">here</a> or my post <a href="http://blog.martinfenner.org/2012/12/13/a-call-for-scholarly-markdown/">here</a>, and the scientific manuscript <a href="https://github.com/weecology/data-sharing-paper/">here</a>.</p><p>Markdown is a simple markup language for text, and is primarily used for HTML content on the web, but can also be converted to PDF, LaTeX and others. One challenge with markdown is that there are a number of slightly different “flavors” out there, from the original markdown to multimarkdown, github-flavored markdown and pandoc. Some of the advanced formatting of scientific documents – tables, citations, math – is still a challenge for markdown.</p><p>Will markdown become our next authoring format for scientific content? Will there be yet another flavor, scholarly markdown? How will markdown writing tools be different from LaTeX tools or Microsoft Word? If you care about any of these questions and are in or near San Francisco, join us on for all full day on June 8th. Free registration is open at <a href="http://mdsci13.eventbrite.com/">http://mdsci13.eventbrite.com</a>. We are collecting workshop ideas at <a href="https://github.com/karthikram/markdown_science/wiki/workshop">https://github.com/karthikram/markdown_science/wiki/workshop</a>, the Twitter hashtag is #mdsci13.</p><p>This event is organized by <a href="http://twitter.com/houshuang">Stian Håklev</a> and myself, with generous support by a <a href="http://www.force11.org/node/4358">1K Challenge prize from Force11</a>, and hosting provided by PLOS.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Baby steps toward better metrics]]></title>
            <link>https://blog.martinfenner.org/posts/baby-steps-toward-better-metrics</link>
            <guid>0b290262-b4e8-4e97-9319-3dc6a3113f0b</guid>
            <pubDate>Tue, 16 Apr 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[Article-Level Metrics
[https://web.archive.org/web/20171029102027/http://article-level-metrics.plos.org/] 
provide new ways to look at the impact of scholarly research. Two important
concepts are a) to track metrics for individual scholarly articles instead of
using numbers aggregated by journal, and b) to go beyond citations and also
include usage stats and altmetrics.

Article-Level Metrics is also doing something else: instead of tracking impact
by year, it looks at usage, altmetrics and cita]]></description>
            <content:encoded><![CDATA[<p><a href="https://web.archive.org/web/20171029102027/http://article-level-metrics.plos.org/">Article-Level Metrics</a> provide new ways to look at the impact of scholarly research. Two important concepts are a) to track metrics for individual scholarly articles instead of using numbers aggregated by journal, and b) to go beyond citations and also include usage stats and altmetrics.</p><p>Article-Level Metrics is also doing something else: instead of tracking impact by year, it looks at usage, altmetrics and citations in real-time. There might have been technical reasons to do so 20 years ago, but there really is no longer any reason why scholarly impact should be tracked on a yearly basis in 2013. Unfortunately there is one big stumbling block:</p><blockquote><em><em>The publication date of a scholarly article is often difficult or impossible to obtain. Publication year may be the only available information.</em></em></blockquote><p>A good example is CrossRef. They provide a lot of interesting metadata about an article and make this information available in <a href="https://web.archive.org/web/20171029102027/http://search.crossref.org/">a very nice search interface</a>. But they only require the publisher to provide the publication year, information about the publication month and day is optional. There are many other examples of journals and services that just can’t tell you when exactly an article was published. This might have made sense when periodicals were printed on paper, but doesn’t work for digital content.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[You should be able to install my software in less than one hour – or why DevOps is important]]></title>
            <link>https://blog.martinfenner.org/posts/you-should-be-able-to-install-my-software-in-less-than-one-hour-or-why-devops-is-important</link>
            <guid>85d086e1-3da5-4090-a467-88271646de72</guid>
            <pubDate>Sun, 14 Apr 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[Cameron Neylon yesterday wrote a great blog post
[https://web.archive.org/web/20171029102958/http://cameronneylon.net/blog/whats-the-right-model-for-shared-scholarly-communications-infrastructure/] 
about appropriate business models for shared scholarly communications
infrastructure. This is an area I have also been thinking about a lot recently
[https://web.archive.org/web/20171029102958/http://blogs.plos.org/mfenner/2013/03/20/the-price-of-innovation-my-thoughts-for-beyond-the-pdf/]
, and in t]]></description>
            <content:encoded><![CDATA[<p>Cameron Neylon yesterday wrote a<a href="https://web.archive.org/web/20171029102958/http://cameronneylon.net/blog/whats-the-right-model-for-shared-scholarly-communications-infrastructure/"> great blog post</a> about appropriate business models for shared scholarly communications infrastructure. This is an area I have also been <a href="https://web.archive.org/web/20171029102958/http://blogs.plos.org/mfenner/2013/03/20/the-price-of-innovation-my-thoughts-for-beyond-the-pdf/">thinking about a lot recently</a>, and in this post I want to add a technical perspective (and an announcement) to the discussion.</p><p><a href="https://web.archive.org/web/20171029102958/http://en.wikipedia.org/wiki/DevOps">DevOps</a> is an important trend that brings software development and administration of IT infrastructure closer together. Agile software development, server virtualization, cloud infrastructure and software automation tools such as <a href="https://web.archive.org/web/20171029102958/http://www.opscode.com/chef/">Chef</a>, <a href="https://web.archive.org/web/20171029102958/https://puppetlabs.com/">Puppet</a> or <a href="https://web.archive.org/web/20171029102958/http://cfengine.com/">CFEngine</a> are an important pars of DevOps, but it is really the collaborative aspect of IT administrators working much closer with software developers what defines DevOps. The end result is often <a href="https://web.archive.org/web/20171029102958/http://readwrite.com/2013/03/27/devops-booms-in-the-enterprise">faster and more stable software releases</a>, and that is what is users and customers care about.</p><p>This makes DevOps particularly relevant for all areas where innovation is important, and that of course includes <a href="https://web.archive.org/web/20171029102958/http://science.okfn.org/tools-for-open-science/">tools and services for Open Science</a>. We not only need infrastructure that facilitates software development (with services like Github, among many others), but we also have to streamline IT administration. The question is not whether you do your development in Java, Python, Ruby, PHP or Javascript, but how well you integrate your software development and IT administration. The shift towards web-based tools has centralized software installation and updates, but these web-based services are becoming increasingly complex and difficult to set up and administer. Running an institutional respository, research information system or a journal is a complex task. The software may be freely available as open source (e.g. <a href="https://web.archive.org/web/20171029102958/http://www.dspace.org/">Dspace</a>, <a href="https://web.archive.org/web/20171029102958/http://vivoweb.org/">VIVO</a> or <a href="https://web.archive.org/web/20171029102958/http://pkp.sfu.ca/?q=ojs">Open Journal Systems</a>), but the resources required to run such a service still make this a big investment.</p><p>Two solutions to this dilemma are to pay either a vendor for installation and maintenance, or to use the software as a service (SaaS) that is hosted somewhere else. Why these two options are popular, they may not always be the best choices because they mean that you are locked in to a particular vendor or service provider, and that you may give expertise and direct access to your data away. I believe that these are helpful approaches for auxillary services, but that ideally the core services of a library, publisher or other provider of scientific infrastructure should not be outsourced. Developing software for scientific infrastructure that you want organizations to install locally should therefore always include work on integration with IT infrastructure, and just providing manual installation instructions isn’t good enough anymore.</p><p><a href="https://web.archive.org/web/20171029102958/http://blogs.plos.org/mfenner/tag/article-level-metrics/">Article-Level Metrics</a> (ALM) and the related altmetrics are becoming increasingly popular. The collection and display of this information is a complex process, as it requires the integration of information from several upstream APIs which may be temporarily unavailable, have changed their data format, or put up restrictions on how you can use the data. In turn this information has to be processed and aggregated, and then reliably be provided to downstream users. This kind of information gathering fits perfectly with a service provider model, and organizations such as <a href="https://web.archive.org/web/20171029102958/http://www.altmetric.com/">Altmetric</a>, <a href="https://web.archive.org/web/20171029102958/http://impactstory.org/">ImpactStory</a> and <a href="https://web.archive.org/web/20171029102958/http://www.plumanalytics.com/">Plum Analytics</a>. PLOS is collecting and displaying this information with <a href="https://web.archive.org/web/20171029102958/https://github.com/articlemetrics/alm">its own tool</a>. The simple reason is that PLOS started doing this several years before the services above became available, and none of them currently provide the same comprehensive set of information about citations, usage stats and altmetrics (although there are of course a lot of things they do better than the PLOS ALM application).</p><p>But there is also the question of whether Article-Level Metrics are a core service for every publisher and are best collected in-house. This not only makes it easier to collect information from some sources (e.g. usage stats or CrossRef citations), but also gives unrestricted access to the data in real-time. When I took over as technical lead for the PLOS Article-Level Metrics project last May, I therefore not only worked on improving the ALM application for PLOS, but we are also working hard on making it easier for other publishers to install and use the application. We want to provide an attractive alternative for organizations for which the service provider model is not the best option.</p><p>To that end I want to announce the latest feature which allows the automated installation of the PLOS ALM application on an Amazon Web Services (AWS) EC2 instance. This option is great not only for setting up an ALM production service, but because of the EC2 pricing model by hour (about $1 a day for a small EC2 instance) without setup costs is a great way to test-drive the application for a publisher, to analyze a particular set of papers from different publishers for a research project, or to set up a PLOS ALM server for a hackathon or workshop.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20171029102958im_/http://blogs.plos.org/mfenner/files/2013/04/vagrant_aws.png" class="kg-image" alt="vagrant_aws"></figure><p>There are of course many options to automate software deployment on a production server, including the PaaS (platform as a service) providers <a href="https://web.archive.org/web/20171029102958/https://www.heroku.com/">Heroku</a>, <a href="https://web.archive.org/web/20171029102958/http://www.cloudfoundry.com/">CloudFoundry</a> and <a href="https://web.archive.org/web/20171029102958/https://www.openshift.com/">OpenShift</a>, and the recently announced <a href="https://web.archive.org/web/20171029102958/http://aws.amazon.com/de/opsworks/">Amazon OpsWorks</a>. I am a big fan of the <a href="https://web.archive.org/web/20171029102958/http://www.vagrantup.com/">Vagrant</a> software development tool in combination with Chef for automation, and in March Vagrant added <a href="https://web.archive.org/web/20171029102958/http://www.hashicorp.com/blog/preview-vagrant-aws.html">support for Amazon AWS</a>. This makes deployment of the PLOS ALM application to AWS really simple:</p><ol><li>Install Vagrant and the <a href="https://web.archive.org/web/20171029102958/https://github.com/mitchellh/vagrant-aws">vagrant-aws plugin</a></li><li>Setup an Amazon Web Services Account</li><li>Check out the <a href="https://web.archive.org/web/20171029102958/https://github.com/articlemetrics/alm">PLOS ALM source code</a> from Github</li><li>run the command <strong>vagrant up –provider aws</strong></li></ol><p>Step #4 took 898 sec or about 15 min on my computer (see screenshot), and at the end I had a PLOS ALM server where I could access the admin dashboard via the web interface. If you are familiar with Amazon Web Services – you have to think about  the right size for the EC2 instance, an appropriate AMI, security groups, elastic IPs, and DNS service – then the whole process should be done in well under an hour. I will use this instance to load and analyze some articles from a publisher for a presentation next week. When I’m done, another command (<strong>vagrant destroy</strong>) will destroy this server and Amazon will stop billing me. During testing I have created and destroyed many servers, and the <a href="https://web.archive.org/web/20171029102958/http://www.hashicorp.com/blog/preview-vagrant-aws.html">vagrant-aws video</a> shows you how easy this process is.</p><p>At this stage the installation process is working (and has been working for a local Virtualbox install for many months), but needs testing and documentation. I therefore invite everyone interested in testing this out to contact me so that we can make this well-documented and working reliably.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Mendeley and Elsevier]]></title>
            <link>https://blog.martinfenner.org/posts/mendeley-and-elsevier</link>
            <guid>c40c0865-88e1-417c-8929-b372fd46c8a8</guid>
            <pubDate>Thu, 11 Apr 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[Earlier this week the rumors that started in January became official: Elsevier
is buying Mendeley
[https://web.archive.org/web/20170731044726/http://blog.mendeley.com/start-up-life/team-mendeley-is-joining-elsevier/] 
(see also here
[https://web.archive.org/web/20170731044726/http://elsevierconnect.com/elsevier-welcomes-mendeley/]
). A lot has been written
[https://web.archive.org/web/20170731044726/http://enjoythedisruption.com/post/47527556151/my-thoughts-on-mendeley-elsevier-why-i-left-to-sta]]></description>
            <content:encoded><![CDATA[<p>Earlier this week the rumors that started in January became official: <a href="https://web.archive.org/web/20170731044726/http://blog.mendeley.com/start-up-life/team-mendeley-is-joining-elsevier/">Elsevier is buying Mendeley</a> (see also <a href="https://web.archive.org/web/20170731044726/http://elsevierconnect.com/elsevier-welcomes-mendeley/">here</a>). A <a href="https://web.archive.org/web/20170731044726/http://enjoythedisruption.com/post/47527556151/my-thoughts-on-mendeley-elsevier-why-i-left-to-start">lot has been written</a> about this announcement, in particular about the fear that Mendeley as a product and organization will turn into something not as open and collaborative as before.</p><p>I first met Victor and Jan from Mendeley in 2008 and did an <a href="https://web.archive.org/web/20170731044726/http://blogs.plos.org/mfenner/2008/09/05/interview_with_victor_henning_from_mendeley/">interview with Victor</a> in September 2008. We worked together in the organization of two <a href="https://web.archive.org/web/20170731044726/http://www.scienceonlinelondon.org/">Science Online London conferences</a> (2009 and 2010, together with Nature.com and others), and my current job started with an entry for an <a href="https://web.archive.org/web/20170731044726/http://blog.mendeley.com/design-research-tools/winners-of-the-first-binary-battle-apps-for-science-contest/">API programming contest</a> co-organized by PLOS and Mendeley, with the <a href="https://web.archive.org/web/20170731044726/http://blogs.plos.org/mfenner/2011/09/28/announcing-sciencecard/">first lines of code written</a> in the Mendeley offices during the Science Online London 2011 hackathon. I wish Mendeley all the best with their new parent.</p><p>What this acquisition signals to me is that commercial publishers are now moving into the software tools for scientists business at full speed. They have always done this, but with <a href="https://web.archive.org/web/20170731044726/http://www.readcube.com/">ReadCube</a> by Digital Science (a Nature Publishing Group sister company) in 2011, the acquisition of <a href="https://web.archive.org/web/20170731044726/http://www.papersapp.com/">Papers</a> by Springer last year and now Mendeley, reference management now often means using a tool owned by a publisher – this market used to be dominated academic software such as <a href="https://web.archive.org/web/20170731044726/http://www.zotero.org/">Zotero</a> and commercial software vendors such as Thomson Reuters (<a href="https://web.archive.org/web/20170731044726/http://endnote.com/">Endnote</a>) or ProQuest (<a href="https://web.archive.org/web/20170731044726/http://www.refworks.com/">RefWorks</a>).</p><p>For me this trend signals that publishers have realized that we are moving into an Open Access publishing model, which in contrast to subscription publishing is not about owning the content, but about providing valuable services around content that is free to read and reuse.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Comment: the case for open preprints in biology]]></title>
            <link>https://blog.martinfenner.org/posts/comment-the-case-for-open-preprints-in-biology</link>
            <guid>f4784bc2-1df2-4825-8d75-c6acfdd21fbb</guid>
            <pubDate>Sat, 30 Mar 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[Last week Philippe Desjardins-Prouly et al. published the article The case for
open preprints in biology
[https://web.archive.org/web/20170731034700/http://dx.doi.org/10.6084/M9.FIGSHARE.655710] 
– naturally as a preprint on figshare. The article sees preprint servers as a
great opportunity for open science, and discusses the status of preprints in the
biological sciences. In this blog post I want to add some comments to the text.

E-BIOMED
What is now PubMed Central started out as E-BIOMED in 1]]></description>
            <content:encoded><![CDATA[<p>Last week Philippe Desjardins-Prouly et al. published the article <a href="https://web.archive.org/web/20170731034700/http://dx.doi.org/10.6084/M9.FIGSHARE.655710">The case for open preprints in biology</a> – naturally as a preprint on figshare. The article sees preprint servers as a great opportunity for open science, and discusses the status of preprints in the biological sciences. In this blog post I want to add some comments to the text.</p><h2 id="e-biomed">E-BIOMED</h2><p>What is now PubMed Central <a href="https://web.archive.org/web/20170731034700/http://www.nih.gov/about/director/pubmedcentral/ebiomedarch.htm">started out as E-BIOMED in 1999</a> and initially was envisioned to include a repository for preprints. It is important to look back at what happened then, and why the preprint repository was dropped from what then became PubMed Central. Harold Varmus talks a bit about this in this <a href="https://web.archive.org/web/20170731034700/http://poynder.blogspot.de/2006/06/interview-with-harold-varmus.html">interview</a> from 2006.</p><h2 id="nature-precedings">Nature Precedings</h2><p>The article talks about why biologists have not developed a culture of sharing preprints. It would be good to mention <a href="https://web.archive.org/web/20170731034700/http://precedings.nature.com/">Nature Precedings</a>, a preprint server for the life sciences started in 2007 that stopped taking new submissions in 2012. <a href="https://web.archive.org/web/20170731034700/http://retractionwatch.wordpress.com/2012/03/30/nature-precedings-to-stop-accepting-submissions-next-week-after-finding-model-unsustainable/">This blog post</a> on RetractionWatch cites the announcement by Nature Publishing Group (which doesn’t explain why the service was shut down), and there are a good number of interesting comments.</p><h2 id="ssrn">SSRN</h2><p>Preprints in other disciplines are mentioned in the text, in particular ArXiv, but also RePEc. I would also include <strong>SSRN</strong> (<a href="https://web.archive.org/web/20170731034700/http://www.ssrn.com/">Social Science Research Network</a>), which uses a different model, but is as important for the working paper and preprint culture in the social sciences as ArXiV is in physics/mathematics.</p><h2 id="google-scholar-metrics">Google Scholar Metrics</h2><p>In April 2012 Google launched <a href="https://web.archive.org/web/20170731034700/http://scholar.google.com/citations?view_op=top_venues&amp;hl=en&amp;vq=en">Google Scholar Metrics</a>, listing the top 100 publications (according to their h5-index) in several disciplines. Six out of the top 10 publications in physics/mathematics are ArXiV sections (arXiv Astrophysics (astro-ph) is #2), the <a href="https://web.archive.org/web/20170731034700/http://www.iza.org/en/webcontent/publications/papers">IZA Discussion Papers</a> are #1 in Social Sciences, and the <a href="https://web.archive.org/web/20170731034700/http://www.nber.org/papers.html">NBER Working Papers</a> are #1 in Economics, and arXiv Astrophysics (astro-ph) is #12 on the top 100 list for all disciplines (#1-5 are journals in biology and medicine: <em>Nature</em>, <em>New England Journal of Medicine</em>, <em>Science</em>, <em>Lancet</em>, <em>Cell</em>). All these metrics are a strong indicator that preprints can be highly cited.</p><h2 id="citation-advantage-of-preprints">Citation Advantage of Preprints</h2><p>Anne Gentil-Beccot et al. have written a nice paper (of course <a href="https://web.archive.org/web/20170731034700/http://arxiv.org/abs/0906.5418">available as preprint</a>) that shows that publication as preprint now only increases the citation rate for the corresponding peer-reviewed article published later, but also leads to much faster citations, with a peak immediately after publication.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20170731034700im_/http://blogs.plos.org/mfenner/files/2013/03/gentil-beccot.png" class="kg-image" alt="gentil-beccot"><figcaption>Average number of citations per article per month as a function of the time of the citation relative to the time of publication. From <a href="https://web.archive.org/web/20170731034700/http://arxiv.org/abs/0906.5418">http://arxiv.org/abs/0906.5418</a></figcaption></figure><h2 id="scoap3">SCOAP3</h2><p>The Sponsoring Consortium for Open Access Publications in High Energy Physics (<a href="https://web.archive.org/web/20170731034700/http://scoap3.org/">SCOAP3</a>) is working on turning the majority of peer-reviewed publications in high energy physics into gold open access. It is important to understand that the high energy physics community feels that they need peer-reviewed journal articles in addition to ArXiv.</p><h2 id="preprint-culture-in-clinical-medicine">Preprint Culture in Clinical Medicine</h2><p>It is a little known fact that there is a strong preprint culture in clinical medicine. I have written about this topic in <a href="https://web.archive.org/web/20170731034700/http://blogs.plos.org/mfenner/2010/10/16/in-which-i-suggest-a-preprint-archive-for-clinical-trials/">October 2010</a>. Clinical trials have to be registered before starting the trial, and information about the trial is publicly available in <a href="https://web.archive.org/web/20170731034700/http://clinicaltrials.gov/">clinicaltrials.gov</a> and other registries. Results are presented in conferences (as poster or oral presentation), at which stage it becomes public information. The peer-reviewed paper – with a few exceptions – follows much later, sometimes even after drug approval by the FDA (in the blog post I used the <a href="https://web.archive.org/web/20170731034700/http://dx.doi.org/10.1016/S0140-6736(10)61389-X">TROPIC</a> trial as example). The problem is of course that information in oral presentations and posters is incomplete and difficult to find. But publication of a clinical trial in a peer-reviewed journal is more about giving credit to the researchers involved (similar to SCOAP3 in high energy physics) than about spreading the knowledge. Peer review is not an appropriate filter for whether or not a new drug or drug combination should be used to treat patients – the approval process by regulatory authorities is much more extensive than any peer review can ever be.</p><h2 id="preprint-culture-in-biology">Preprint culture in Biology</h2><p>The paper mentions several reasons why the field of biology has essentially no preprint culture. One argument against preprints is that it would be easier to steal ideas. Although I agree with the authors that preprints are a great way to establish precedence, there is a big difference between research based on years of work using expensive equipment (as is often the case in high energy physics but also some other fields), and research that can be reproduced in a few weeks. In the latter case it is possible that someone else is faster in publishing the peer-reviewed paper. Another difference is the community: “stealing” ideas from someone else is probably more difficult in smaller scientific communities, and some scientific communities are more competitive and less collaborative than others.</p><p>Another concern about preprints raised in the paper is the Ingelfinger rule, i.e. the uncertainty that a journal would accept a manuscript if already published as a preprint. This concern is fortunately unfounded regarding most publishers, and the paper includes a table listing the preprint policies of important publishers in biology.</p><p>I would like to add two other reasons why the preprint culture is probably not established in biology. Preprints are competition for the peer-reviewed journal article and scholarly publishers might not be particularly interested in encouraging a preprint culture. A lot has fortunately changed since E-BIOMED in 1999.</p><p>Finally, whereas some disciplines use preprints and working papers to communicate, in biology the preferred way to communicate research findings before publication of a peer-reviewed paper is the oral presentation. What we may need is a service that makes it easy to upload and share scientific presentations. We for example already have Slideshare, Speaker Deck as generic tools, and SciVee, fishare aimed at scientists. Speaker Deck <a href="https://web.archive.org/web/20170731034700/http://blogs.plos.org/mfenner/2012/05/29/speaker-deck-for-sharing-presentations/">is currently my favorite tool</a> and is a Github product (Github has been mentioned in the manuscript as an option for hosting preprints). Maybe what is missing is a killer combination of features in a new or existing service – persistent identifiers, uploading of background material (text, data, software, video) in addition to the slides, non-textual search, cooperation with conference organizers, etc. – for presentation sharing to take off as a way to establish a preprint culture in biology.</p><p><em>Update 4/4/13: Yesterday PeerJ launched a new preprint service for life sciences research. Read <a href="https://web.archive.org/web/20170731034700/http://blogs.scientificamerican.com/guest-blog/2013/04/03/who-killed-the-preprint-and-could-it-make-a-return/">this blog post</a> for details, and this <a href="https://web.archive.org/web/20170731034700/http://blog.mendeley.com/open-access/is-the-time-right-for-a-preprint-server-for-life-science/">post</a> on the Mendeley blog.</em></p><h2 id="references">References</h2><p>Desjardins-Proulx, P., White, E. P., Adamson, J., Ram, K., Poisot, T., &amp; Gravel, D. (2013). <em>The case for open preprints in biology</em>. <a href="https://doi.org/10.6084/M9.FIGSHARE.655710">https://doi.org/10.6084/M9.FIGSHARE.655710</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using d3.js to visualize Article-Level Metrics over time]]></title>
            <link>https://blog.martinfenner.org/posts/using-d3-js-to-visualize-article-level-metrics-over-time</link>
            <guid>bf2eaf10-e7f2-439f-bb44-1daea383caa8</guid>
            <pubDate>Tue, 26 Mar 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[PLOS Article-Level Metrics (ALM) are a great set of data (available via API and
as monthly data dump
[https://web.archive.org/web/20170731170128/http://article-level-metrics.plos.org/plos-alm-data/]
) for some nice data visualizations. I have recently become a big fan of the 
d3.js [https://web.archive.org/web/20170731170128/http://d3js.org/] javascript
library, and have now used d3 to look at some ALM data over time.

I like simple visualizations without too many labels or axes, and wanted to d]]></description>
            <content:encoded><![CDATA[<p>PLOS Article-Level Metrics (ALM) are a great set of data (available via API and as <a href="https://web.archive.org/web/20170731170128/http://article-level-metrics.plos.org/plos-alm-data/">monthly data dump</a>) for some nice data visualizations. I have recently become a big fan of the <a href="https://web.archive.org/web/20170731170128/http://d3js.org/">d3.js</a> javascript library, and have now used d3 to look at some ALM data over time.</p><p>I like simple visualizations without too many labels or axes, and wanted to do a visualization inspired by <a href="https://web.archive.org/web/20170731170128/http://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0001OR">sparklines</a> ever since we discussed this idea in our <em>altviz</em> breakout group at the <a href="https://web.archive.org/web/20170731170128/http://article-level-metrics.plos.org/alm-workshop-2012/hackathon/#altviz">ALM workshop hackathon</a> in November 2012 (kudos in particular to Juan Alperin, Karthik Ram and Carl Boettiger). In the chart below every column represents the numbers for a given month, with alternating colors for the years (the article was published November 2009).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20170731170128im_/http://blogs.plos.org/mfenner/files/2013/03/sparklines4-e1364338936408.png" class="kg-image" alt="sparklines4"><figcaption>CiteULike bookmarks, usage stats from PLOS website and blog posts for article <strong>Article-Level Metrics and the Evolution of Scientific Impact</strong> by month, available at <a href="https://web.archive.org/web/20170731170128/http://dx.doi.org/10.1371/journal.pbio.1000242">http://dx.doi.org/10.1371/journal.pbio.1000242</a>.</figcaption></figure><p>You can see a pattern that is probably typical for many articles independent of the absolute numbers: most pageviews and downloads happen in the weeks after publication, as does academic bookmarking and science blogging.</p><p>The second example shows a very different pattern. This is not only the <a href="https://web.archive.org/web/20170731170128/http://alm.plos.org/sources/counter">most-downloaded</a> PLOS article, but  the distribution of downloads over time is very different, with the number of monthly downloads actually higher the last two years (this article was published in August 2005). We also see a few spikes in the usage stats, probably indicating events that triggered usage. Academic bookmarking was most active from 2009 to 2011 and not right after publication, although that might also have to do with the relative popularity of CiteULike over time.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20170731170128im_/http://blogs.plos.org/mfenner/files/2013/03/sparklines5-500x262.png" class="kg-image" alt="sparklines5"><figcaption>CiteULike bookmarks, usage stats from PLOS website and blog posts for article <strong>Why Most Published Research Findings Are False</strong> by month, available at <a href="https://web.archive.org/web/20170731170128/http://dx.doi.org/10.1371/journal.pmed.0020124">http://dx.doi.org/10.1371/journal.pmed.0020124</a>.</figcaption></figure><p>Citation data are unfortunately more difficult to get with exact publication dates (why is that so difficult?), but we can at least look at CrossRef numbers by year for the same article.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20170731170128im_/http://blogs.plos.org/mfenner/files/2013/03/sparklines31-e1364340861413.png" class="kg-image" alt="sparklines3"><figcaption>CiteULike bookmarks, usage stats from PLOS website and blog posts for article <strong>Why Most Published Research Findings Are False</strong> by year, available at <a href="https://web.archive.org/web/20170731170128/http://dx.doi.org/10.1371/journal.pmed.0020124">http://dx.doi.org/10.1371/journal.pmed.0020124</a>.</figcaption></figure><p>The citation numbers by year are still increasing (the last bar is for 2013), indicating that this article is still of general interest 8 years after publication. This would probably be unusual for a life sciences research article, but the article is an essay looking at common pitfalls in the statistical analysis of research data.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[New version of Article-Level Metrics app released]]></title>
            <link>https://blog.martinfenner.org/posts/new-version-of-article-level-metrics-app-released</link>
            <guid>a7442b7e-a2b4-46c4-bded-b8cb3be8d856</guid>
            <pubDate>Thu, 21 Mar 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[On Tuesday we released the latest version of the PLOS Article-Level Metrics
application
[https://web.archive.org/web/20170731155505/http://article-level-metrics.plos.org/]
. As always, the source code is available at Github
[https://web.archive.org/web/20170731155505/https://github.com/articlemetrics].
The changes in this version focus on improving API performance, making it easier
to install the application, and RSS feeds for the most popular articles by
source and publication date (e.g. the mo]]></description>
            <content:encoded><![CDATA[<p>On Tuesday we released the latest version of the <a href="https://web.archive.org/web/20170731155505/http://article-level-metrics.plos.org/">PLOS Article-Level Metrics application</a>. As always, the source code is available at <a href="https://web.archive.org/web/20170731155505/https://github.com/articlemetrics">Github</a>. The changes in this version focus on improving API performance, making it easier to install the application, and RSS feeds for the most popular articles by source and publication date (e.g. <a href="https://web.archive.org/web/20170731155505/http://alm.plos.org/sources/twitter.rss?days=7">the most tweeted papers published in the last 7 days</a>). See the <a href="https://web.archive.org/web/20170731155505/https://github.com/articlemetrics/alm/wiki/2.6">Github Wiki page</a>for more details, in the Wiki you also find the development <a href="https://web.archive.org/web/20170731155505/https://github.com/articlemetrics/alm/wiki/Roadmap">roadmap</a> and the <a href="https://web.archive.org/web/20170731155505/https://github.com/articlemetrics/alm/issues">issue tracker</a> for feature suggestions.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Price of Innovation – my Thoughts for Beyond the PDF]]></title>
            <link>https://blog.martinfenner.org/posts/the-price-of-innovation-my-thoughts-for-beyond-the-pdf</link>
            <guid>f6eb230d-9862-4622-9227-d69871a9b2b8</guid>
            <pubDate>Wed, 20 Mar 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[The Beyond the PDF Conference
[https://web.archive.org/web/20170731155123/http://www.force11.org/taxonomy/term/27] 
is currently taking place in Amsterdam. Unfortunately I am unable to attend in
person this time (I took part in the first Beyond the PDF in January 2011), but
I was watching the livestream
[https://web.archive.org/web/20170731155123/http://www.force11.org/beyondthepdf2/live] 
of the Business Case
[https://web.archive.org/web/20170731155123/http://blogs.plos.org/mfenner/2013/03/20/t]]></description>
            <content:encoded><![CDATA[<p>The <a href="https://web.archive.org/web/20170731155123/http://www.force11.org/taxonomy/term/27">Beyond the PDF Conference</a> is currently taking place in Amsterdam. Unfortunately I am unable to attend in person this time (I took part in the first Beyond the PDF in January 2011), but I was watching the <a href="https://web.archive.org/web/20170731155123/http://www.force11.org/beyondthepdf2/live">livestream</a> of the <a href="https://web.archive.org/web/20170731155123/http://blogs.plos.org/mfenner/2013/03/20/the-price-of-innovation-my-thoughts-for-beyond-the-pdf/ww.force11.org/Business_Case">Business Case</a> panel disucssion yesterday afternoon.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20170731155123im_/http://farm1.staticflickr.com/120/291798596_dbfcfc26d7_m.jpg" class="kg-image" alt="the globe of science and innovation"><figcaption>Globe of Science and Innovation at CERN, <a href="https://web.archive.org/web/20170731155123/http://www.flickr.com/photos/nico_h/291798596/">Flickr photo</a> by <strong>nico h</strong>.</figcaption></figure><p>How to pay for the development of new scientific infrastructure and tools is something that I think a lot about science moving away from academia to become a developer of scientific software last year. I would assume three things:</p><ul><li>there are a lot of great ideas out there to improve scholarly communication</li><li>there is enough money out there to pay for improvements in scholarly communication</li><li>we are frustrated because progress is much slower than we anticipate</li></ul><p>If we have enough great ideas and enough money, but don’t see the results we expect, something must be going wrong. A simple answer would be that it is different people and organizations that have the ideas from those that have the money, but I don’t think that this is the reason. My suspicion is that there is a deeper problem, and that the approach we take to scholarly innovation is broken. Below is how innovation is approached by the major players:</p><ul><li>individual scientists and/or software developers come up with great ideas, but don’t get past the prototype stage because of limited resources</li><li>academic tools and infrastructure are built as part of a funded project (anywhere from 6 months to a few years), but there are no resources to turn this into a service that is persistent beyond the project</li><li>publishers and large academic institutions have the resources to build these tools. They are often less innovative because of their size</li><li>funders pay for projects (see above), but rarely for infrastructure, and they rarely get involved in innovative projects themselves</li><li>commercial organizations can quickly bring great ideas to market (in particular small startups), but it is often unclear how their services are paid for in the long run</li></ul><p>At the end of the day it seems that we have a lot of great ideas, but many of them never reach critical mass, and an even smaller number has long-term sustainability. I can think of a number of great projects that have never gained traction, and of a number of great tools and services where I have no idea how their development and service is paid for. The idea to get to a large number of users no matter what it costs, and figure out the business plan later is popular with internet startups, but dangerous when we care about tools we want to still use two years from now. Two projects that are not specific to science, but are important for science and have made this work are <strong>Wikipedia</strong> and <strong>Github</strong>. From the long list of tools for scientists I would not pick <strong>Mendeley</strong> or <strong>figshare</strong> (both great services, but still in search of sustainability), but <strong>ArXiV</strong> and <strong>Papers</strong>.It also doesn’t help that most scientists are a conservative bunch when it comes to technology, and that the scientific market is fairly small compared to the overall number of users. Another big challenge is to innovate in an open environment, i.e. to make the innovation available to as many people as possible without barriers of access. Some of my personal conclusions from all this are the following:</p><ul><li>we should acknowledge that we have an innovation problem, and it is not simply solved by getting more money</li><li>we have a collaboration problem, too many people are doing similar things without talking to each other and working together</li><li>scientific infrastructure and tools cost money. We need the right people to pay (ideally not the individual researcher), fair prices and intelligent business models</li><li>funders should reconsider how they pay for scientific infrastructure, as the project-based approach is broken</li><li>large organizations (commercial, non-commercial and academic) should think about their approach to innovation, in particular how they support innovation outside of their organization</li></ul><p>You can follow the Beyond the PDF <a href="https://web.archive.org/web/20170731155123/http://www.force11.org/beyondthepdf2/live">livestream</a> today or follow the Twitter hashtag <a href="https://web.archive.org/web/20170731155123/https://twitter.com/search?q=%23btpdf2&amp;src=hash">#btpdf2</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Some Thoughts on Beyond the Paper]]></title>
            <link>https://blog.martinfenner.org/posts/some-thoughts-on-beyond-the-paper</link>
            <guid>8d562ed8-7d13-457b-a8ef-ad560e553684</guid>
            <pubDate>Wed, 20 Mar 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[Today the journal Nature has released a special on the Future of Publishing
[https://web.archive.org/web/20170731155331/http://www.nature.com/news/specials/scipublishing/index.html]
. It includes a lot of interesting reading, but I want to focus on the comment 
Beyond the Paper
[https://web.archive.org/web/20170731155331/http://dx.doi.org/10.1038/495437a] 
by Jason Priem. In the comment Jason describes his vision of the future of
scholarly communication, a future where many of today’s roles for ]]></description>
            <content:encoded><![CDATA[<p>Today the journal <em>Nature</em> has released a special on the <a href="https://web.archive.org/web/20170731155331/http://www.nature.com/news/specials/scipublishing/index.html">Future of Publishing</a>. It includes a lot of interesting reading, but I want to focus on the comment <a href="https://web.archive.org/web/20170731155331/http://dx.doi.org/10.1038/495437a">Beyond the Paper</a> by Jason Priem. In the comment Jason describes his vision of the future of scholarly communication, a future where many of today’s roles for articles and journals will be replaced by the <em>decoupled journal</em> and online tools taking the lead in dissemination and filtering of scholarly content.</p><p>Jason makes a strong case for this vision, and takes his time to also discuss the concerns and challenges. He doesn’t have the space to discuss in more detail how we get to that future, and in particular what the role of researchers, publishers, libraries and funders be in that transition.</p><p>Jason’s vision will probably be overwhelming for many researchers, and might not directly address what is probably the biggest issue for most researchers: funding for grants and jobs is limited, and the processes we use to select for good science and good scientists are inefficient and often arbitrary. Most students entering graduate school will not be able to have a career in academia, and most academics will say that they spend far too much time with evaluations – of their own work and the work of others. It is unclear to me how we can get from the current system – where one misstep such as denied grant or submission to the wrong journal can mean the end of a career – to the system that Jason envisions. The current climate doesn’t really foster experimentation by researchers and I am interested to understand how researchers can take part in this process of change.</p><p>The vision of the decoupled journal is very threatening for some of the stakeholders of the current scholarly communication ecosystem, in particular publishers and libraries. Every journal publisher and library knows that it has to reinvent itself to survive the digital transformation, but a vision that is build around a new ecosystem of service providers needs to be clear how publishers and libraries can be part of the transformation process.</p><p>Lastly, I disagree with the notion that <em>today’s publication silos will be replaced by a set of decentralized, interoperable services that are built on a core infrastructure of open data and evolving standards — like the Web itself.</em> I would argue that both scholarly communication and the web in general have a tendency for centralization, and that scientific infrastructure needs to be interoperable first and decentralized second. Without a focus on interoperability the future of scholarly communication will not be open and in the hands of many, but will be a race to become one of the dominant players in this new ecosystem, and we might end up with not 1000s of libraries and publishers but just a handful of technology companies holding the keys to our scientific infrastructure.</p><h2 id="references">References</h2><p>Priem, J. (2013). Scholarship: Beyond the paper <em>Nature, 495</em> (7442), 437-440 DOI: <a href="https://web.archive.org/web/20170731155331/http://dx.doi.org/10.1038/495437a">10.1038/495437a</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Bye-bye Google Reader]]></title>
            <link>https://blog.martinfenner.org/posts/bye-bye-google-reader</link>
            <guid>360b1702-d93d-4c32-bcea-0b8e3681bd33</guid>
            <pubDate>Thu, 14 Mar 2013 00:00:00 GMT</pubDate>
            <description><![CDATA[Yesterday Google announced that they will shut down Google Reader
[https://web.archive.org/web/20170731151455/http://googleblog.blogspot.de/2013/03/a-second-spring-of-cleaning.html] 
July 1st. In a way this announcement didn’t surprise me, as my own use of RSS
readers has gone down in favor of news readers such as Flipboard
[https://web.archive.org/web/20170731151455/http://blogs.plos.org/mfenner/2010/09/05/flipboard-plos-blogs-on-the-ipad/] 
and using Twitter as a discovery tool. And built-in s]]></description>
            <content:encoded><![CDATA[<p>Yesterday Google announced that they will <a href="https://web.archive.org/web/20170731151455/http://googleblog.blogspot.de/2013/03/a-second-spring-of-cleaning.html">shut down Google Reader</a> July 1st. In a way this announcement didn’t surprise me, as my own use of RSS readers has gone down in favor of news readers such as <a href="https://web.archive.org/web/20170731151455/http://blogs.plos.org/mfenner/2010/09/05/flipboard-plos-blogs-on-the-ipad/">Flipboard</a> and using Twitter as a discovery tool. And built-in support for RSS had slowly been depreciated in web browsers such as Firefox (version 4, 2011) and Safari (version 6, 2012).</p><p>Although RSS (and the related Atom) may never have caught on with the typical web user, it is an essential tool for scholarly content. It is the best format to subscribe to journal table of contents, much more suitable than email alerts. <a href="https://web.archive.org/web/20170731151455/http://www.journaltocs.ac.uk/">JournalTOCs</a> is a good place to get started, but most publishers prominently display the RSS icon. RSS is also great for searches you want to do regularly and is supported by PLOS, <a href="https://web.archive.org/web/20170731151455/http://www.nlm.nih.gov/bsd/disted/pubmedtutorial/040_060.html">PubMed</a>, and others. Because it is a machine-readable format, it is also used by many websites to automatically read in article information. RSS feeds for journal table of contents differ in format, but CrossRef in 2009 has posted <a href="https://web.archive.org/web/20170731151455/http://oxford.crossref.org/best_practice/rss/">recommendations</a> for publishers.</p><p>Google Reader is of course only one of many RSS readers, so this announcement shouldn’t have any immediate impact. Nevertheless it is probably another sign that the web is moving away from RSS, and that we should start to think about alternatives for distributing tables of content. Or that we should use different strategies for finding interesting articles that have recently been published, e.g. follow the article recommendations in your social network.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Additional Markdown we need in Scholarly Texts]]></title>
            <link>https://blog.martinfenner.org/posts/additional-markdown-we-need-in-scholarly-texts</link>
            <guid>8ec4bfc6-eb75-4484-ae36-dcd8fbe76592</guid>
            <pubDate>Tue, 18 Dec 2012 17:34:00 GMT</pubDate>
            <description><![CDATA[Following up from my post last week [/2012/12/13/a-call-for-scholarly-markdown/]
, below is a suggested list of features that should be supported in documents
written in scholarly markdown. Please provide feedback via the comments, or by
editing the Wiki version I have set up here
[https://github.com/mfenner/scholarly-markdown/wiki]. Listed are features that
go beyond the standard markdown syntax
[http://daringfireball.net/projects/markdown/syntax].

The goals of scholarly markdown are

 1. to s]]></description>
            <content:encoded><![CDATA[<p>Following up from <a href="https://martinfenner.ghost.io/2012/12/13/a-call-for-scholarly-markdown/">my post last week</a>, below is a suggested list of features that should be supported in documents written in scholarly markdown. Please provide feedback via the comments, or by editing the Wiki version I have set up <a href="https://github.com/mfenner/scholarly-markdown/wiki">here</a>. Listed are features that go beyond the <a href="http://daringfireball.net/projects/markdown/syntax">standard markdown syntax</a>.</p><p>The goals of scholarly markdown are</p><ol><li>to support writing of complete scholarly articles,</li><li>don’t make the syntax more complicated than it is today, and</li><li>don’t rely on HTML as the fallback mechanism.</li></ol><p>In practice this means that scholarly markdown should support most, but not all scholarly texts – documents that are heavy in math formulas, have complicated tables, etc. may be better written with LaTeX or Microsoft Word. It also means that scholarly markdown will probably contain only limited semantic markup, as this is difficult to do with a lightweight markup language and much easier with XML or a binary file format.</p><h2 id="cover-page">Cover Page</h2><p>Optional metadata about a document. Typically used for title, authors (including affiliation), and publication date, but should be flexible enough to handle any kind of metadata (keywords, copyright, etc.).</p><pre><code>---
layout: post
title: "Additional Markdown we need in Scholarly Texts"
tags: [markdown]
authors:
 - name: Martin Fenner
   orcid: 0000-0003-1419-2405
copyright: http://creativecommons.org/licenses/by/3.0/deed.en
---</code></pre><h2 id="typography">Typography</h2><p>Scholarly markdown should support superscript and subscript text, and should provide an easy way to enter greek ζ letters.</p><h2 id="tables">Tables</h2><p>Tables should work as anchors (i.e. you can link to them) and table captions should support styled text. Unless the table is very simple, tables are probably better written as CSV files with another tool, and then imported into the scholarly markdown document similar to figures.</p><!--kg-card-begin: html--><table style="box-sizing: border-box; border-collapse: collapse; border-spacing: 0px; max-width: 100%; background-color: rgb(255, 255, 255); font-size: 19px; font-family: ff-tisa-sans-web-pro, Arial, sans-serif; width: 750px; margin-bottom: 21px; color: rgb(0, 0, 0); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"><caption style="box-sizing: border-box; text-align: left; vertical-align: top; font-size: 19px; font-style: italic; line-height: 1.33; margin-bottom: 0.75em;"><strong style="box-sizing: border-box; font-weight: bold;">This is the table caption</strong>. We can explain the table here.</caption><colgroup style="box-sizing: border-box;"><col style="box-sizing: border-box; width: 0px;"><col style="box-sizing: border-box; width: 0px;"><col style="box-sizing: border-box; width: 0px;"></colgroup><thead style="box-sizing: border-box;"><tr class="header" style="box-sizing: border-box;"><th style="box-sizing: border-box; padding: 8px; text-align: center; vertical-align: bottom; border-top: 0px; font-weight: bold;">Centered Header</th><th style="box-sizing: border-box; padding: 8px; text-align: right; vertical-align: bottom; border-top: 0px; font-weight: bold;">Right Aligned</th><th style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: bottom; border-top: 0px; font-weight: bold;">Left Aligned</th></tr></thead><tbody style="box-sizing: border-box;"><tr class="odd" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: center; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);">First</td><td style="box-sizing: border-box; padding: 8px; text-align: right; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);">12.0</td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);">Example of a row that spans multiple lines.</td></tr><tr class="even" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: center; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);">Second</td><td style="box-sizing: border-box; padding: 8px; text-align: right; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);">5.0</td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);">Here’s another one. Note the blank line between rows.</td></tr></tbody></table><!--kg-card-end: html--><h2 id="figures">Figures</h2><p>Figures in scholarly works are separated from the text, and have a figure caption (which can contain styled text). Figures should work as anchors (i.e. you can link to them). Figures can be in different file formats, including TIFF and PDF, and those formats have to be converted into web-friendly formats when exporting to HTML (e.g. PNG and SVG).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/set-operations-illustrated-with-venn-diagrams.png" class="kg-image" alt><figcaption>Set operations illustrated with Venn diagrams. Example taken <a href="https://texample.net/tikz/examples/set-operations-illustrated-with-venn-diagrams/">TeXample.net</a>.</figcaption></figure><h2 id="citations-and-links">Citations and Links</h2><p>Scholarly articles typically don’t have inline links, but rather citations. The external links (both scholarly identifiers such as DOIs and regular web URLs) are collected in a bibliography at the end of the document, and the citations in the text link to this bibliography. This functionality is similar to footnotes.</p><p>Citations should include a citation key in the text, e.g. <code>[@kowalczyk2011]</code>, parsed as (Kowalczyk &amp; Shankar, 2011), and a separate bibliography file in BibTeX (or RIS) format that contains references for all citations. Inserting citations and creating the bibliography can best be done with a reference manager.</p><p>Cross-links – i.e. links within a document – are important for scholarly texts. It should be possible to link to section headers (e.g. the beginning of the discussion section), figures and tables.</p><h2 id="math">Math</h2><p>Complicated math is probably best done in a different authoring environment, but simple formulas, both inline 2‾√x and block elements</p><p>ddxarctan(sin(x2))=−2cos(x2)x−2+(cos(x2))2</p><p>should be supported by scholarly markdown.</p><h2 id="comments">Comments</h2><p>Comments are important for multi-author documents and if reviewer feedback should be included. Comments should be linked to a particular part of a document to provide context, or attached at the end of a document for general comments. It would also be helpful to “comment out” parts of a document, e.g. to indicate parts that are incomplete and need more work. Revisions of a markdown document are best handled using a version control system such as git.</p><h2 id="references">References</h2><p>Kowalczyk, S., &amp; Shankar, K. (2011). Data sharing in the sciences. <em>Annual Review of Information Science and Technology</em>, <em>45</em>(1), 247–294. Retrieved from <a href="http://doi.org/10.1002/aris.2011.1440450113">http://doi.org/10.1002/aris.2011.1440450113</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Call for Scholarly Markdown]]></title>
            <link>https://blog.martinfenner.org/posts/a-call-for-scholarly-markdown</link>
            <guid>fd7b416a-1cbd-4ca0-8932-03ab2e144464</guid>
            <pubDate>Thu, 13 Dec 2012 17:37:00 GMT</pubDate>
            <description><![CDATA[Markdown is a lightweight markup language, originally created by John Gruber for
writing content for the web. Other popular lightweight markup languages are
Textile and Mediawiki. Whereas Mediawiki markup is of course popular thanks to
the ubiquitous Wikipedia, Markdown seems to have gained momentum among scholars.
Markdown really focuses on writing content, many of the features of today’s word
processors are just a distraction (e.g. fonts, line spacing or style sheets).
Adding markup for docume]]></description>
            <content:encoded><![CDATA[<p>Markdown is a lightweight markup language, originally created by John Gruber for writing content for the web. Other popular lightweight markup languages are Textile and Mediawiki. Whereas Mediawiki markup is of course popular thanks to the ubiquitous Wikipedia, Markdown seems to have gained momentum among scholars. Markdown really focuses on writing content, many of the features of today’s word processors are just a distraction (e.g. fonts, line spacing or style sheets). Adding markup for document structure (e.g. title, authors or abstract) on the other hand is overly complicated with tools such as Microsft Word.</p><p>Fortunately or unfortunately there are several versions (or flavors) of Markdown. The original specification by John Gruber hasn’t been updated for years. Github uses Markdown with some minor modifications. Multimarkdown and Pandoc provide features important for scholarly content, e.g. citations, superscript and tables.</p><ul><li>Markdown</li><li>Github-flavored Markdown</li><li>Multimarkdown</li><li>Pandoc</li></ul><p>The Pandoc flavor of Markdown probably comes closest to the requirements of a scholar, but still has limitations, e.g. support for metadata and tables isn’t very flexible. I propose that we as a community create a new Scholarly Markdown flavor, which takes into account most of the use cases important for scholarly content.</p><p>One of the big advantages of Markdown is that the format can not only be translated to HTML, but also to other formats, and Pandoc is particularly good in translating to and from many different formats. We want to make sure that Scholarly Markdown not only translates into nice Scholarly HTML (with good support for HTML5 tags relevant for scholars), but also into Microsot Word, LaTeX and PDF, as these are the formats typically required by manuscript tracking systems.</p><p>Some of the features required for Scholarly Markdown include:</p><ul><li>Superscript and subscript</li><li>Highlighting text (supporting the HTML tag <code>&lt;mark&gt;</code>)</li><li>Captions for tables and figures (with support for the HTML tags <code>&lt;caption&gt;</code> and <code>&lt;figcaption&gt;</code>)</li><li>Support for document sections (the HTML5 tags <code>&lt;article&gt;</code>, <code>&lt;header&gt;</code>, <code>&lt;footer&gt;</code>, <code>&lt;section&gt;</code>)</li><li>Good table support</li><li>Math support</li><li>Good citation support</li><li>Support for comments and annotations</li></ul><p>Multimarkdown and Pandoc of course already support many of these features. Tables and citations are two examples where it is important to not only support them, but support them in a non-intrusive way that doesn’t get in the way of the flow of writing.</p><p>BTW, this wouldn’t be the first community flavor for Markdown. The screenwriting community has done this already with <a href="http://fountain.io/">Fountain</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ORCID has launched. What’s next?]]></title>
            <link>https://blog.martinfenner.org/posts/orcid-has-launched-whats-next</link>
            <guid>29af87a7-69de-4901-97bf-5a1002adea31</guid>
            <pubDate>Mon, 22 Oct 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Last week has been busy. I went to Berlin for the launch of the Open Researcher
& Contributor ID (ORCID)
[https://web.archive.org/web/20170518120701/http://orcid.org/] service. ORCID
allows researchers to obtain a persistent identifier that can be used to claim
publications and other scholarly works. I’m 0000-0003-1419-2405
[https://web.archive.org/web/20170518120701/http://orcid.org/0000-0003-1419-2405]
, and we put the ID (and the QR code linking to the profile on the ORCID
website) on the nam]]></description>
            <content:encoded><![CDATA[<p>Last week has been busy. I went to Berlin for the launch of the <a href="https://web.archive.org/web/20170518120701/http://orcid.org/">Open Researcher &amp; Contributor ID (ORCID)</a> service. ORCID allows researchers to obtain a persistent identifier that can be used to claim publications and other scholarly works. I’m <a href="https://web.archive.org/web/20170518120701/http://orcid.org/0000-0003-1419-2405">0000-0003-1419-2405</a>, and we put the ID (and the QR code linking to the profile on the ORCID website) on the name tags for the ORCID Outreach Meeting last Wednesday (Geek alert: I also have received a T-shirt with my name, ORCID and QR code).</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20170518120701im_/http://blogs.plos.org/mfenner/files/2012/10/nametag-500x283.png" class="kg-image" alt></figure><p>I was invited to work with ORCID in early 2010 after writing about the initiative that was started in November 2009 on this blog (<a href="https://web.archive.org/web/20170518120701/http://blogs.plos.org/mfenner/2010/01/03/orcid_or_how_to_build_a_unique_identifier_for_scientists_in_10_easy_steps/">ORCID or how to build a unique identifier for scientists in 10 easy steps</a>). And now, after three years and a lot of work by a lot of people, ORCID is real and everyone can use the system. As a researcher, you can go to the ORCID website and register.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20170518120701im_/http://blogs.plos.org/mfenner/files/2012/10/orcid-500x445.png" class="kg-image" alt></figure><p>Obtaining a number is of course not very interesting in itself, few people get excited about the fact of having a 16-digit unique identifier. What ORCID is really about is claiming your publications and other scholarly works, and <strong>Connecting Research and Researchers</strong> is the slogan of the organization. In the ORCID system you can now claim publications found in the CrossRef database, and other work types will be added over time.</p><p>What I’m particularly interested in is the claiming of research datasets. Everyone wants to give researchers better credit for the data they have produced, transformed and annotated, but data citation is still not a widespread practice. I am therefore very excited to be involved in the <a href="https://web.archive.org/web/20170518120701/http://www.odin-project.eu/">ORCID and DataCite Interoperability Network</a> (ODIN), a EU-funded project that had its kickoff meeting last week in Berlin.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20170518120701im_/http://blogs.plos.org/mfenner/files/2012/10/odin-500x319.png" class="kg-image" alt></figure><p>In the ODIN project we will work closely with <a href="https://web.archive.org/web/20170518120701/http://www.datacite.org/">DataCite</a>, an organization that provides digital object identifiers (DOIs) for research data. One of the many things I like about ODIN is that social sciences is one of the disciplines where we will build a proof of concept (with the British Library, the other discipline is high-energy physics and CERN). We also want to understand how to best link researchers, data and publications. <a href="https://web.archive.org/web/20170518120701/http://datadryad.org/">Dryad</a> is an ODIN project partner and obviously has a lot of experience linking biological datasets to publications, and we will discuss how to integrate ORCID identifiers into the workflow.</p><p>Unique identifiers for researchers are of course also an essential part of any work on article-level metrics. I <a href="https://web.archive.org/web/20170518120701/http://blogs.plos.org/mfenner/2012/06/25/random-notes-from-the-altmetrics12-conference/">spoke about this</a> at the altmetrics12 conference in June, and I’m excited that we can now finally start linking things together. <a href="https://web.archive.org/web/20170518120701/http://impactstory.org/">ImpactStory</a> was one of the ORCID launch partners, and I demoed their ORCID integration last week in Berlin.</p><p><a href="https://web.archive.org/web/20170518120701/http://sciencecard.org/">ScienceCard</a> is a fork of the open source <a href="https://web.archive.org/web/20170518120701/http://article-level-metrics.plos.org/">PLOS Article-Level Metrics application</a>, and is a project I started <a href="https://web.archive.org/web/20170518120701/http://blogs.plos.org/mfenner/2011/11/20/sciencecard-named-finalist-in-mendeleyplos-api-binary-battle/">about a year ago</a>. ScienceCard allows researchers to list all their publications, and the metrics associated with them. With the launch of ORCID I was able to finally add one important missing piece. Through automatic lookup of the ORCID identifier and retrieval of the publications claimed in the ORCID profile is has become much easier to create and maintain a ScienceCard profile – it shouldn’t take more than 5 min and a few mouse clicks (collecting all metrics takes longer because that happens in the background). I added ORCID integration to ScienceCard over the weekend, using the <a href="https://web.archive.org/web/20170518120701/http://dev.orcid.org/resources">free public ORCID API</a>.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20170518120701im_/http://blogs.plos.org/mfenner/files/2012/10/sciencecard-500x438.png" class="kg-image" alt></figure><p>ScienceCard is a great tool to explore how research impact can be collected and displayed, and I appreciate feedback in the form of feature requests and bug reports, ideally in the <a href="https://web.archive.org/web/20170518120701/https://github.com/mfenner/alm/issues">GitHub issue tracker</a> of the project. This will also provide very valuable feedback to improve the PLOS Article-Level Metrics application, as they use almost the same code base. The API is for example completely the same, <a href="https://web.archive.org/web/20170518120701/https://github.com/ropensci/rplos">rplos</a> and other tools using the PLOS ALM API can be used with ScienceCard by just changing the URL. Another example is the <a href="https://web.archive.org/web/20170518120701/http://wordpress.org/extend/plugins/plos-alm-widget/">PLOS ALM WordPress Widget</a>, with minor modifications it can be also be used with ScienceCard, allowing a researcher to display the metrics for his publications from PLOS and other sources on his blog. The upcoming <a href="https://web.archive.org/web/20170518120701/https://sites.google.com/site/altmetricsworkshop/">Altmetrics workshop and hackathon</a> (November 1-3 in San Francisco) will be a great opportunity to explore this further.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Announcing the ScienceCard Relaunch]]></title>
            <link>https://blog.martinfenner.org/posts/announcing-the-sciencecard-relaunch</link>
            <guid>0ca4a671-19ae-44a8-9390-8421e76b4caf</guid>
            <pubDate>Wed, 19 Sep 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Almost exactly a year ago (in the hackathon of the Science Online London 2011
conference) I started the ScienceCard project
[https://web.archive.org/web/20160402053034/http://blogs.plos.org/mfenner/2011/09/28/announcing-sciencecard/]
. ScienceCard is a fork of the Open Source PLOS Article-Level Metrics (ALM)
code, personalizing the Article-Level Metrics.

A lot has happened in the last 12 months, most importantly that I started to
work for PLOS as technical lead for the Article-Level Metrics pro]]></description>
            <content:encoded><![CDATA[<p>Almost exactly a year ago (in the hackathon of the Science Online London 2011 conference) I <a href="https://web.archive.org/web/20160402053034/http://blogs.plos.org/mfenner/2011/09/28/announcing-sciencecard/">started the ScienceCard project</a>. ScienceCard is a fork of the Open Source PLOS Article-Level Metrics (ALM) code, personalizing the Article-Level Metrics.</p><p>A lot has happened in the last 12 months, most importantly that I started to work for PLOS as technical lead for the Article-Level Metrics project in May. In July, version 2.0 of the PLOS ALM application was released, and the code made <a href="https://web.archive.org/web/20160402053034/https://github.com/articlemetrics/alm">available on Github</a>. This not only means that everyone can install his own ALM application (assuming some familiarity with the Ruby and Rails web framework), but that we can fork the code and modify it.</p><p>Although my focus is now clearly on improving the PLOS application, it didn’t feel right to shut down the <a href="https://web.archive.org/web/20160402053034/http://sciencecard.org/">ScienceCard</a> project. I really like the idea of personalized Article-Level Metrics (something I spoke about at the <a href="https://web.archive.org/web/20160402053034/http://blogs.plos.org/mfenner/2012/06/25/random-notes-from-the-altmetrics12-conference/">altmetrics12</a> conference). So I sat down the last two weekends to upgrade ScienceCard to the PLOS ALM 2.0 code (the source code of my fork can be found <a href="https://web.archive.org/web/20160402053034/https://github.com/mfenner/alm">here</a>). With the <a href="https://web.archive.org/web/20160402053034/http://about.orcid.org/content/orcid-launch-plan-announced">imminent launch</a> of the Open Researcher &amp; Contributor ID (ORCID) service next month I dropped the functionality to get all articles by a particular person from Microsoft Academic Search. For now you can add your articles by DOI or PubMed ID (currently only articles from PubMed), but you can now also add interesting articles not authored by you. This makes ScienceCard a nice tool to try out the PLOS ALM code without installing the software. Please remember that all metrics are collected in the background, so it can take a few hours until they show up for newly added articles.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20160402053034im_/http://blogs.plos.org/mfenner/files/2012/09/sciencecard_new-500x312.png" class="kg-image" alt title="sciencecard_new"></figure><p>ScienceCard also shows the power of open source software. Open source doesn’t simply mean free software, it means that you can modify the code if your requirements are different. For ScienceCard I added authentication via Twitter (required to add articles, I don’t want to deal with usernames and passwords), a simple lookup by DOI or PubMed ID (something not needed if you are publisher of the article and have that information), and comments and likes. Article-Level Metrics is not about collecting numbers, it is about capturing the <a href="https://web.archive.org/web/20160402053034/http://blogs.bmj.com/bmj/2011/04/06/richard-smith-what-is-post-publication-peer-review/">activity surrounding an article post-publication</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Bye bye Nature Network, welcome SciLogs.com]]></title>
            <link>https://blog.martinfenner.org/posts/bye-bye-nature-network-welcome-scilogs-com</link>
            <guid>8da63a54-3f0a-4ceb-9efa-e281125c90d7</guid>
            <pubDate>Thu, 26 Jul 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[The science blogging network Nature Network is moving to a new home
[https://web.archive.org/web/20161022002312/http://blogs.nature.com/ofschemesandmemes/2012/07/26/a-new-era-for-the-nature-network-blogs]
. Today SciLogs.com
[https://web.archive.org/web/20161022002312/http://www.scilogs.com/] launched as
new home for Nature Network bloggers. I have been blogging at Nature Network for
three years, starting with my first blog post (Open access may become mandatory
for NIH-funded research
[https://]]></description>
            <content:encoded><![CDATA[<figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20161022002312im_/http://blogs.plos.org/mfenner/files/2012/07/nature-network.jpeg" class="kg-image" alt title="nature-network"></figure><p>The science blogging network Nature Network is <a href="https://web.archive.org/web/20161022002312/http://blogs.nature.com/ofschemesandmemes/2012/07/26/a-new-era-for-the-nature-network-blogs">moving to a new home</a>. Today <a href="https://web.archive.org/web/20161022002312/http://www.scilogs.com/">SciLogs.com</a> launched as new home for Nature Network bloggers. I have been blogging at Nature Network for three years, starting with my first blog post (<a href="https://web.archive.org/web/20161022002312/http://blogs.plos.org/mfenner/2007/08/03/open_access_may_become_mandatory_for_nih_funded_research/">Open access may become mandatory for NIH-funded research</a>) almost exactly 5 years ago to the day. My blog moved to PLOS BLOGS in September 2010 and all my old Nature Network content can be found here at PLOS BLOGS.</p><p>Blogging at Nature Network has changed my life in many ways. Thank you Matt and Corie (and later Lou) to make this possible.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What Users do with PLOS ONE Papers]]></title>
            <link>https://blog.martinfenner.org/posts/what-users-do-with-plos-one-papers</link>
            <guid>9188bc59-49b3-483e-be9e-99baaba95f80</guid>
            <pubDate>Tue, 24 Jul 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Inspired by four recent blog posts and their comments (Comments at journal
websites: just turn them off
[https://web.archive.org/web/20170423155530/http://nsaunders.wordpress.com/2012/07/13/comments-at-journals-websites-just-turn-them-off/]
, Open Access and The Dramatic Growth of PLoS ONE
[https://web.archive.org/web/20170423155530/http://figshare.com/blog/Open_Access_and_The_Dramatic_Growth_of_PLoS_ONE/41]
, No Comment?
[https://web.archive.org/web/20170423155530/http://blogs.plos.org/everyone]]></description>
            <content:encoded><![CDATA[<p>Inspired by four recent blog posts and their comments (<a href="https://web.archive.org/web/20170423155530/http://nsaunders.wordpress.com/2012/07/13/comments-at-journals-websites-just-turn-them-off/">Comments at journal websites: just turn them off</a>, <a href="https://web.archive.org/web/20170423155530/http://figshare.com/blog/Open_Access_and_The_Dramatic_Growth_of_PLoS_ONE/41">Open Access and The Dramatic Growth of PLoS ONE</a>, <a href="https://web.archive.org/web/20170423155530/http://blogs.plos.org/everyone/2012/07/23/no-comment/">No Comment?</a>, <a href="https://web.archive.org/web/20170423155530/http://perlsteinlab.com/round-table/if-you-email-it-they-will-comment">If you email it, they will comment</a>), I created a graphic to show what users do with PLoS ONE papers. As always, the data behind the graphic are <a href="https://web.archive.org/web/20170423155530/http://www.plosone.org/static/almInfo.action">openly available</a>. I think that the number of times a paper is informally discussed (comments, Facebook, science blogs, etc.) should be much larger compared to the numer of formal citations. The challenge is of course to have technology that captures all these discussions – this is much more difficult than for bookmarks or citations, and is obviously what <a href="https://web.archive.org/web/20170423155530/http://altmetrics.org/manifesto/">altmetrics</a> is all about. The blog posts I link to above also express another feeling: that there are still too many barriers for scientists to take part in the informal discussion of scholarly research on the web, in particular as comments on journal websites. Hat tip to <a href="https://web.archive.org/web/20170423155530/http://www.davidmccandless.com/">David McCandless</a> for inspiration.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20170423155530im_/http://blogs.plos.org/mfenner/files/2012/07/PLoS-ONE-summary-466x500.png" class="kg-image" alt title="PLoS ONE summary"></figure><p><em>Update 08/02/12: The publication of the dataset used in this chart was delayed, but the data are now available at the <a href="https://web.archive.org/web/20170423155530/http://www.plosone.org/static/almInfo.action">link</a> provided.</em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Neelie Kroes talks Open Science]]></title>
            <link>https://blog.martinfenner.org/posts/neelie-kroes-talks-open-science</link>
            <guid>553527dc-105e-4764-9091-f1b793f2ca04</guid>
            <pubDate>Sat, 21 Jul 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Earlier this week the European Commission announced
[https://web.archive.org/web/20160404225257/http://blogs.ec.europa.eu/neelie-kroes/open-science/] 
new measures towards open science. As part of the announcement interviews of
three scientists with European Commission Vice President Neelie Kroes were
posted on YouTube. Here is a summary:]]></description>
            <content:encoded><![CDATA[<p>Earlier this week the European Commission <a href="https://web.archive.org/web/20160404225257/http://blogs.ec.europa.eu/neelie-kroes/open-science/">announced</a> new measures towards open science. As part of the announcement interviews of three scientists with European Commission Vice President Neelie Kroes were posted on YouTube. Here is a summary:</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[More fun with Visualizations]]></title>
            <link>https://blog.martinfenner.org/posts/more-fun-with-visualizations</link>
            <guid>38f108c5-bd3f-425c-995d-21b9baa35261</guid>
            <pubDate>Fri, 20 Jul 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[This has been another week working on visualizations. I have summarized some of
the results in a blog post
[https://web.archive.org/web/20160528080207/http://api.plos.org/2012/07/20/example-visualizations-using-the-plos-search-and-alm-apis/] 
over at the PLoS API website. One of my current favorites is the dot chart. PLoS
Computational Biology publishes a collection of Ten Simple Rules
[https://web.archive.org/web/20160528080207/http://www.ploscollections.org/article/browseIssue.action?issue=inf]]></description>
            <content:encoded><![CDATA[<p>This has been another week working on visualizations. I have summarized some of the results in a <a href="https://web.archive.org/web/20160528080207/http://api.plos.org/2012/07/20/example-visualizations-using-the-plos-search-and-alm-apis/">blog post</a> over at the PLoS API website. One of my current favorites is the dot chart. PLoS Computational Biology publishes a <a href="https://web.archive.org/web/20160528080207/http://www.ploscollections.org/article/browseIssue.action?issue=info:doi/10.1371/issue.pcol.v03.i01">collection of Ten Simple Rules</a>. The dot chart below summarizes the HTML pageviews, PDF downloads and Mendeley readers for this collection (click on the image for a larger size).</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20160528080207im_/http://blogs.plos.org/mfenner/files/2012/07/dotchart2-500x386.png" class="kg-image" alt title="dotchart2"></figure><p>On Wednesday I gave a presentation about Article-Level Metrics, using many of the same visualizations. You can find the slides over at <a href="https://web.archive.org/web/20160528080207/https://speakerdeck.com/u/mfenner/p/article-level-metrics">Speaker Deck</a> (my new favorite to upload presentation slides).</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Europe PubMed Central coming in November]]></title>
            <link>https://blog.martinfenner.org/posts/europe-pubmed-central-coming-in-november</link>
            <guid>d4756127-88a1-4f09-9c20-780f5a38ff73</guid>
            <pubDate>Mon, 16 Jul 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[The European Research Council
[https://web.archive.org/web/20160528080726/http://erc.europa.eu/] on Friday 
announced
[https://web.archive.org/web/20160528080726/http://erc.europa.eu/sites/default/files/press_release/files/EuropePMC_press_release_WT_ERC_FINAL.pdf] 
that they will participate in the UK PubMed Central (UKPMC) open access
repository service. They become the third European funder to join UKPMC, and the
existing UKPMC funders have agreed to rebrand UKPMC as Europe PubMed Central
(abb]]></description>
            <content:encoded><![CDATA[<figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20160528080726im_/http://blogs.plos.org/mfenner/files/2012/07/epmc.gif" class="kg-image" alt title="epmc"></figure><p>The <a href="https://web.archive.org/web/20160528080726/http://erc.europa.eu/">European Research Council</a> on Friday <a href="https://web.archive.org/web/20160528080726/http://erc.europa.eu/sites/default/files/press_release/files/EuropePMC_press_release_WT_ERC_FINAL.pdf">announced</a> that they will participate in the UK PubMed Central (UKPMC) open access repository service. They become the third European funder to join UKPMC, and the existing UKPMC funders have agreed to rebrand UKPMC as Europe PubMed Central (abbreviated to EPMC?) on November 1st.</p><p>More information about these changes can be found on the <a href="https://web.archive.org/web/20160528080726/http://ukpmc.blogspot.de/2012/07/european-research-council-renews-its.html">UKPMC blog</a> and in the <a href="https://web.archive.org/web/20160528080726/http://www.wellcome.ac.uk/News/Media-office/Press-releases/2012/WTVM055890.htm">Wellcome Trust press release</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Visualizing tweets linking to a paper]]></title>
            <link>https://blog.martinfenner.org/posts/visualizing-tweets-linking-to-a-paper</link>
            <guid>6bf8af26-d49b-4449-b400-92ff2a077086</guid>
            <pubDate>Sat, 14 Jul 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[DNA Barcoding the Native Flowering Plants and Conifers of Wales
[https://web.archive.org/web/20170216233248/http://dx.doi.org/10.1371/journal.pone.0037945] 
has been one of the most popular new PLoS ONE papers in June. In the paper
Natasha de Vere et al. describe a DNA barcode resource that covers the 1143
native Welsh flowering plants and conifers.

My new job as technical lead for the PLoS Article Level Metrics (ALM) project
[https://web.archive.org/web/20170216233248/http://article-level-metr]]></description>
            <content:encoded><![CDATA[<p><a href="https://web.archive.org/web/20170216233248/http://dx.doi.org/10.1371/journal.pone.0037945">DNA Barcoding the Native Flowering Plants and Conifers of Wales</a> has been one of the most popular new <em>PLoS ONE</em> papers in June. In the paper Natasha de Vere <em>et al.</em> describe a DNA barcode resource that covers the 1143 native Welsh flowering plants and conifers.</p><p>My new job as technical lead for the <a href="https://web.archive.org/web/20170216233248/http://article-level-metrics.plos.org/">PLoS Article Level Metrics (ALM) project</a> involves thinking about how we can best display the ALM collected for this and other papers. We want these ALM to tell us something important and/or interesting, and it doesn’t hurt if the information is displayed in a visually appealing way. There are many different ways this can be done, but here I want to focus on <strong>Twitter</strong> and <strong>CiteULike</strong>, the only two data sources where PLoS is currently storing every single event (tweet or CiteULike bookmark) with a date. Usage data (HTML and XML views, PDF downloads) are aggregated on a monthly basis, and PLoS doesn’t store the publication dates of citations.</p><p>We know from the <a href="https://web.archive.org/web/20170216233248/http://blogs.plos.org/mfenner/2011/12/20/crowdometer-or-trying-to-understand-tweets-about-journal-papers/">work of Gunter Eysenbach</a> and others that most tweets linking to scholarly papers are written in the first few days after publication. It therefore makes sense to display this information on a timeline covering the first 30 days after publication, and the tweets about the de Vere paper follow the same pattern.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20170216233248im_/http://blogs.plos.org/mfenner/files/2012/07/sparklines.png" class="kg-image" alt title="sparklines"></figure><p>I like the simplicity of sparklines. It would be interesting to also map the 274 Facebook <strong>Likes, Comments</strong> and <strong>Shares</strong>, but we don’t have date information for them. The same is true for the 9 Mendeley readers and groups.</p><p>Another way to display the time course of tweets (or bookmarks) is to use a calendar heat map (the paper was published June 6).</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20170216233248im_/http://blogs.plos.org/mfenner/files/2012/07/calendarPlot.png" class="kg-image" alt title="calendarPlot"></figure><p>The chart looks a little bit empty, a calendar heat map probably works better for information with many daily data points. I would appreciate feedback on how these visualizations can be improved.</p><p>The charts were created with data from the <a href="https://web.archive.org/web/20170216233248/http://api.plos.org/">PLoS ALM API</a> and the statistical computing package <a href="https://web.archive.org/web/20170216233248/http://www.r-project.org/">R</a>, the source code is available <a href="https://web.archive.org/web/20170216233248/https://github.com/articlemetrics/plosOpenR/blob/master/sparkLines.R">here</a> and <a href="https://web.archive.org/web/20170216233248/https://github.com/articlemetrics/plosOpenR/blob/master/calendarPlot.R">here</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Random notes from the altmetrics12 conference]]></title>
            <link>https://blog.martinfenner.org/posts/random-notes-from-the-altmetrics12-conference</link>
            <guid>f5db2039-c880-4d74-8984-739598f31579</guid>
            <pubDate>Thu, 21 Jun 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Last week I attended the altmetrics12
[https://web.archive.org/web/20160528073512/http://altmetrics.org/altmetrics12/] 
workshop in Chicago. You can read all 11 abstracts here
[https://web.archive.org/web/20160528073512/http://altmetrics.org/altmetrics12/program/]
, and the conference had good Twitter coverage (using the hashtag #altmetrics12
[https://web.archive.org/web/20160528073512/https://twitter.com/search/%23altmetrics12]
), at least until Twitter had a total blackout around 12 PM our tim]]></description>
            <content:encoded><![CDATA[<p>Last week I attended the <a href="https://web.archive.org/web/20160528073512/http://altmetrics.org/altmetrics12/">altmetrics12</a> workshop in Chicago. You can read all 11 abstracts <a href="https://web.archive.org/web/20160528073512/http://altmetrics.org/altmetrics12/program/">here</a>, and the conference had good Twitter coverage (using the hashtag <a href="https://web.archive.org/web/20160528073512/https://twitter.com/search/%23altmetrics12">#altmetrics12</a>), at least until Twitter had a total blackout around 12 PM our time.</p><p>All but two presenters used slides – I have uploaded <a href="https://web.archive.org/web/20160528073512/https://speakerdeck.com/u/mfenner/p/altmetrics-will-be-taken-personally-at-plos">my presentation</a> to Speaker Deck. Kelli Barr used the blackboard to explain that</p><ul><li>filtering (via altmetrics or any other means) by definition always selects out content and therefore runs against the democratization of science</li><li>peer review is a black box. altmetrics is also a black box, only much bigger</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20160528073512im_/http://blogs.plos.org/mfenner/files/2012/06/barr.jpg" class="kg-image" alt title="barr"><figcaption>Kelli Barr used the blackboard for two important points in <a href="https://web.archive.org/web/20160528073512/http://altmetrics.org/altmetrics12/barr/">her talk</a>.</figcaption></figure><p>altmetrics12 was one of the best conferences that I have attended recently. The intensity of the discussions was palpable. My only regrets are that there wasn’t more time for discussions, but many of us convened in the bar afterwards.</p><p>We were off to a very strong start with two excellent keynote presentations by Johann Bollen ( Altmetrics: from usage data to social media) and Gregg Gordon (<a href="https://web.archive.org/web/20160528073512/http://ssrnblog.com/2012/05/18/alternative-is-the-new-grey/">Alternative is the new Grey</a>). Johann emphasized why it is both important and fascinating to study how science is actually working. He stressed that science is a gift economy, where the currency is acknowledgement of influence in the form of citations. He sees two major problems with the present citation-based analysis of scientific impact: a) data and b) the metrics. Citation data are very domain specific (with very different citation practices in different disciplines), and delayed (several years after publication of the research they are citing). The problem with current citation-based metrics is that they ignore the network effect in science. Johann then went on to explain the <a href="https://web.archive.org/web/20160528073512/http://mesur.informatics.indiana.edu/">MESUR</a> project, which studies the patterns of scientific activity on a very large scale (1 billion usage events, 500 million citations, 50 million papers).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20160528073512im_/http://www.plosone.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0004803.g005&amp;representation=PNG_I" class="kg-image" alt title="Map of Science"><figcaption>Map of science from <a href="https://web.archive.org/web/20160528073512/http://dx.doi.org/10.1371/journal.pone.0004803">2009 PLoS ONE paper</a>.</figcaption></figure><p>Johann then briefly explained the results of another <a href="https://web.archive.org/web/20160528073512/http://dx.doi.org/10.1371/journal.pone.0006022">2009 PLoS ONE paper</a> where he analyzed 39 metrics with principal component analysis. He found two major components in the metrics analysis: counting vs. social influence and fast vs. slow.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20160528073512im_/http://www.plosone.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0006022.g002&amp;representation=PNG_I" class="kg-image" alt title="PCA"><figcaption>Correlation of 37 metrics mapped onto first two principal components in another <a href="https://web.archive.org/web/20160528073512/http://dx.doi.org/10.1371/journal.pone.0006022">2009 PLoS ONE paper</a>.</figcaption></figure><p>Johann then told the interesting story behind a <a href="https://web.archive.org/web/20160528073512/http://arxiv.org/abs/1010.3003">2010 paper</a> where he showed that <strong>Twitter mood can predict the stock market</strong>. The paper was rejected by all publishers and was finally published on ArXiV in October 2010. It immediately became very popular in terms of downloads and media attention (and was eventually <a href="https://web.archive.org/web/20160528073512/http://dx.doi.org/10.1016/j.jocs.2010.12.007">published</a> in the <em>Journal of Computational Science</em> in March 2011).</p><p>Gregg Gordon has summarized his keynote in a May <a href="https://web.archive.org/web/20160528073512/http://ssrnblog.com/2012/05/18/alternative-is-the-new-grey/">blogpost</a>, so I will focus on a few highlights. Gregg Gordon runs the Social Science Research Network (<a href="https://web.archive.org/web/20160528073512/http://ssrn.com/">SSRN</a>), a leading resource for sharing social sciences research. Gregg thinks that altmetrics can provide the compass to navigate the map of science described earlier. He told us some very interesting anecdotes of users trying to game SSRN by inflating their download counts (e.g. “downloads for donuts”), and mentioned a <a href="https://web.archive.org/web/20160528073512/http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1346397">research paper</a>analyzing gaming at SSRN. Download counts are apparently taken very seriously by SSRN authors and are also used for hiring decisions. SSRN not only has written software to protect against gaming, but also has a person constantly looking over these numbers (Gregg feels that computer algorithms alone are not enough).</p><p>The two keynotes were followed by 11 short (10-15 minute) presentations, including two presentations about the PLoS <a href="https://web.archive.org/web/20160528073512/http://article-level-metrics.plos.org/">Article-Level Metrics</a> project. Jennifer Lin spoke about <a href="https://web.archive.org/web/20160528073512/http://altmetrics.org/altmetrics12/lin/">anti-gaming mechanisms</a> and I emphasized the importance of <a href="https://web.archive.org/web/20160528073512/http://altmetrics.org/altmetrics12/fenner/">personalizing altmetrics</a> to fully understand the “network of science”. All presentation abstracts are available <a href="https://web.archive.org/web/20160528073512/http://altmetrics.org/altmetrics12/program/">online</a>.</p><p>After the lunch break (and some interesting discussions) we continued with demos of various altmetrics applications and users, including <a href="https://web.archive.org/web/20160528073512/http://total-impact.org/">Total Impact</a>, <a href="https://web.archive.org/web/20160528073512/http://www.plumanalytics.com/">Plum Analytics</a>, <a href="https://web.archive.org/web/20160528073512/http://www.altmetric.com/">altmetric.com</a>, the<a href="https://web.archive.org/web/20160528073512/http://alm.plos.org/"> PLoS Article-Level Metrics</a> application, <a href="https://web.archive.org/web/20160528073512/http://knodeinc.com/">Knode</a>, <a href="https://web.archive.org/web/20160528073512/http://academia.edu/">Academia.edu</a>, <a href="https://web.archive.org/web/20160528073512/http://www.biomedcentral.com/">BioMed Central</a> and <a href="https://web.archive.org/web/20160528073512/http://www.ubiquitypress.com/">Ubiquity Press</a> (example <a href="https://web.archive.org/web/20160528073512/http://openarchaeologydata.metajnl.com/article/intensive-survey-data-from-antikythera-greece/">here</a>).</p><p>In the last our and a half we split up into several smaller group to discuss issues relevant for altmetrics. I was in the <strong>standards</strong> group and we all agreed that it is too early to sep up rigid standards for this evolving field, but not too early to start the discussion. The two standards experts in our group (Todd Carpenter from <a href="https://web.archive.org/web/20160528073512/http://www.niso.org/home/">NISO</a> and David Baker from <a href="https://web.archive.org/web/20160528073512/http://casrai.org/">CASRAI</a>) were of course very helpful with the discussion in our group. In the closing group discussion the breakout groups reported their major discussion points (which will hopefully be written up by someone). We also learned that the NIH is considering changing the Biosketch format for CVs and is looking for input via a <a href="https://web.archive.org/web/20160528073512/http://grants.nih.gov/grants/guide/notice-files/NOT-OD-12-115.html">Request for Information</a> (RFI) until June 29.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Speaker Deck for Sharing Presentations]]></title>
            <link>https://blog.martinfenner.org/posts/speaker-deck-for-sharing-presentations</link>
            <guid>91015ca7-5601-4c86-aaa4-3e31e8571228</guid>
            <pubDate>Tue, 29 May 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[It has become common practice to make presentation slides available for those
unable to attend in person, or for more in-depth review later. The most popular
service to do this is of course Slideshare
[https://web.archive.org/web/20160404225654/http://www.slideshare.net/].
Slideshare is a fine service, but the website has become fairly cluttered over
the years, and visuals are of course important when it comes to presentations.

Speaker Deck
[https://web.archive.org/web/20160404225654/http://spe]]></description>
            <content:encoded><![CDATA[<p>It has become common practice to make presentation slides available for those unable to attend in person, or for more in-depth review later. The most popular service to do this is of course <a href="https://web.archive.org/web/20160404225654/http://www.slideshare.net/">Slideshare</a>. Slideshare is a fine service, but the website has become fairly cluttered over the years, and visuals are of course important when it comes to presentations.<br><br><a href="https://web.archive.org/web/20160404225654/http://speakerdeck.com/">Speaker Deck</a> is an alternative to Slideshare with a focus on “simplicity and beauty”. The service has the features you would expect:</p><ul><li>upload presentations (currently in PDF only)</li><li>view presentations, including fullscreen mode</li><li>share presentations, including downloads and embedding</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20160404225654im_/http://blogs.plos.org/mfenner/files/2012/05/speakerdeck.png" class="kg-image" alt title="speakerdeck"><figcaption><a href="https://web.archive.org/web/20160404225654/https://speakerdeck.com/u/mfenner/p/future-formats-representing-the-next-generation-of-scholarly-articles?slide=51">Slide 51</a> from a presentation I did with Steve Pettifer at UKSG in March.</figcaption></figure><p>Speaker Deck was <a href="https://web.archive.org/web/20160404225654/http://orderedlist.com/blog/articles/share-presentations-without-the-mess/">announced last September</a> and is free to use. It would be helpful if Speaker Deck allowed the upload of Powerpoint or Keynote files, making it easier to reuse the slides. But the big item on my wish list is an improvement of the social activities enabled around a presentation. I want to see download counts, comments, number of tweets and Facebook likes, and I want an API for them. Ordered List, the company behind Speakerdeck, was <a href="https://web.archive.org/web/20160404225654/http://orderedlist.com/blog/articles/ordered-list-acquired-by-github/">acquired by Github</a> in December, turning Speaker Deck into a Github product. This makes me confident that we will see these Speaker Deck improvements rather sooner than later.</p><p>To see a few Speaker Deck presentations, go to <a href="https://web.archive.org/web/20160404225654/https://speakerdeck.com/u/mfenner">my presentations</a> or the <a href="https://web.archive.org/web/20160404225654/https://speakerdeck.com/c/science">Science category</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[PLoS Article-Level Metrics: Interview with Martin Fenner]]></title>
            <link>https://blog.martinfenner.org/posts/plos-article-level-metrics-interview-with-martin-fenner</link>
            <guid>808211bd-0183-46a4-a619-b08c439a042b</guid>
            <pubDate>Mon, 23 Apr 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[This blog occasionally does interviews with people providing interesting tools
for scholars. These interviews
[https://web.archive.org/web/20161226124822/http://blogs.plos.org/mfenner/category/interviews/] 
have always been among my favorite blog posts. This now is obviously an
interview with myself, but I felt this is the best format to explain some
important news.

Flickr photo
[https://web.archive.org/web/20161226124822/http://www.flickr.com/photos/stevec77/429781913/] 
my stevec77.Starting M]]></description>
            <content:encoded><![CDATA[<p>This blog occasionally does interviews with people providing interesting tools for scholars. These <a href="https://web.archive.org/web/20161226124822/http://blogs.plos.org/mfenner/category/interviews/">interviews</a> have always been among my favorite blog posts. This now is obviously an interview with myself, but I felt this is the best format to explain some important news.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/02/429781913_9524791cff_w.jpg" class="kg-image" alt><figcaption><a href="https://web.archive.org/web/20161226124822/http://www.flickr.com/photos/stevec77/429781913/">Flickr photo</a> my stevec77.</figcaption></figure><p>Starting May 16 I will be working full-time as technical lead for the PLoS <a href="https://web.archive.org/web/20161226124822/http://article-level-metrics.plos.org/">Article Level Metrics</a> (ALM) project. I will help with development of the <a href="https://web.archive.org/web/20161226124822/http://code.google.com/p/alt-metrics/">PLoS ALM application</a>, and will do community developer outreach for this project.</p><p>The PLoS ALM application is written in <a href="https://web.archive.org/web/20161226124822/http://rubyonrails.org/">Ruby on Rails</a>, an open-source web framework I have been working with since 2005. The ALM project was launched in 2009, and I first learned about ALM in a July 2009 presentation by Pete Binfield at <a href="https://web.archive.org/web/20161226124822/http://blogs.plos.org/mfenner/2009/07/10/i_was_at_scibarcamp_palo_alto/">SciBarCamp Palo Alto</a>. A month later I did an <a href="https://web.archive.org/web/20161226124822/http://blogs.plos.org/mfenner/2009/07/10/i_was_at_scibarcamp_palo_alto/">interview with Pete</a> about Article Level Metrics and PLoS ONE.</p><h3 id="what-is-article-level-metrics">What is Article Level Metrics?</h3><p>Article Level Metrics <em>place transparent and  comprehensive information about the usage and reach of published articles onto the articles themselves, so that the entire academic community can assess their value </em>(from the <a href="https://web.archive.org/web/20161226124822/http://article-level-metrics.plos.org/">PLoS ALM website</a>). A <a href="https://web.archive.org/web/20161226124822/http://dx.doi.org/10.1371/journal.pbio.1000242">November 2009 paper</a> by Cameron Neylon and Shirley Wu gives a more detailed introduction. And a <a href="https://web.archive.org/web/20161226124822/http://www.slideshare.net/kristenratan/metrics-the-new-black">recent presentation</a> by Kristen Ratan, PLoS Director of Product Management, given at the <a href="https://web.archive.org/web/20161226124822/http://www.nfais.org/page/361-program-2012-nfais-annual-conference">2012 NFAIS meeting</a>, provides an update for 2012.</p><p>Article Level Metrics is part of the larger <a href="https://web.archive.org/web/20161226124822/http://altmetrics.org/manifesto/">altmetrics</a> movement, which also looks at metrics for other scholarly works besides journal articles.</p><h3 id="does-this-mean-that-you-will-be-moving-to-san-francisco">Does this mean that you will be moving to San Francisco?</h3><p>For personal reasons I will continue to live in Hannover, Germany and work from home as a contractor with occasional trips to San Francisco.</p><h3 id="will-you-miss-treating-cancer-patients-and-doing-cancer-research">Will you miss treating cancer patients and doing cancer research?</h3><p>Absolutely. This has not been an easy decision. To make the transition easier, I will continue to spend 10% of my time at Hannover Medical School. I will no longer be seeing patients, but this will allow me to conclude the RADIT <a href="https://web.archive.org/web/20161226124822/http://clinicaltrials.gov/ct2/show/NCT01242631">clinical trial</a> for testicular cancer patients where I am the principal investigator.</p><p>I hope to continue doing research in the new position, but with a focus on information science. There are for example still a lot of things we don’t know about altmetrics. A more detailed analysis of our recent <a href="https://web.archive.org/web/20161226124822/http://blogs.plos.org/mfenner/2012/02/19/crowdsourcing-the-analysis-of-scholarly-tweets/">CrowdoMeter</a> project (a crowdsourced analysis of tweets linking to scholarly papers) would be a good start.</p><h3 id="what-will-happen-with-sciencecard">What will happen with ScienceCard?</h3><p><a href="https://web.archive.org/web/20161226124822/http://sciencecard.org/">ScienceCard</a> is a website that collects author level metrics and was my entry into the <a href="https://web.archive.org/web/20161226124822/http://blogs.plos.org/mfenner/2011/11/20/sciencecard-named-finalist-in-mendeleyplos-api-binary-battle/">Mendeley/PLoS Binary Battle API contest</a> last fall. ScienceCard is based on the PLoS ALM code (which is open source and <a href="https://web.archive.org/web/20161226124822/http://code.google.com/p/alt-metrics/">available via Google Code</a>). I will decide in the coming months what to do with ScienceCard. This depends mainly on how much author level metrics make sense in the PLoS ALM project.</p><h3 id="and-what-will-happen-to-your-other-scholarly-communication-activities">And what will happen to your other scholarly communication activities?</h3><p>There is no reason not to continue my other activities, including involvement in the Open Researcher &amp; Contributor ID (<a href="https://web.archive.org/web/20161226124822/http://about.orcid.org/">ORCID</a>) initiative, and using WordPress as a <a href="https://web.archive.org/web/20161226124822/http://blogs.plos.org/mfenner/tag/wordpress/">tool to write and publish manuscripts</a>.</p><h3 id="what-are-your-future-plans-for-this-blog">What are your future plans for this blog?</h3><p>I plan to continue this blog in a very similar format, and I will have more time for more in-depth articles. And of course I will indicate a conflict of interest when I write about Article Level Metrics.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Marketing for Scientists]]></title>
            <link>https://blog.martinfenner.org/posts/marketing-for-scientists</link>
            <guid>a460316d-af6b-4183-b3ca-c5889e220d0c</guid>
            <pubDate>Tue, 27 Mar 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[The April issue
[https://web.archive.org/web/20160528080410/http://www.nature.com/nmat/journal/v11/n4/index.html] 
of Nature Materials contains three articles that discuss marketing strategies
for scientists. The Editorial
[https://web.archive.org/web/20160528080410/http://dx.doi.org/10.1038/nmat3300] 
(“The scientific marketplace”) introduces the topic and explains why scientists
should consider marketing their work. The issue also contains an interview
[https://web.archive.org/web/201605280804]]></description>
            <content:encoded><![CDATA[<p>The <a href="https://web.archive.org/web/20160528080410/http://www.nature.com/nmat/journal/v11/n4/index.html">April issue</a> of <em>Nature Materials</em> contains three articles that discuss marketing strategies for scientists. The <a href="https://web.archive.org/web/20160528080410/http://dx.doi.org/10.1038/nmat3300">Editorial</a> (“The scientific marketplace”) introduces the topic and explains why scientists should consider marketing their work. The issue also contains an <a href="https://web.archive.org/web/20160528080410/http://dx.doi.org/10.1038/nmat3276">interview</a> (“The m word”) with astrophysicist Marc Kuchner who published a book titled <em><a href="https://web.archive.org/web/20160528080410/http://marketingforscientists.com/">Marketing for Scientists</a>. </em>Finally, there is a <a href="https://web.archive.org/web/20160528080410/http://dx.doi.org/10.1038/nmat3283">commentary</a> (“One-click science marketing”) by me discussing strategies and tools that scientists can use to promote their work. The commentary also includes links to pages by <a href="https://web.archive.org/web/20160528080410/http://www.scivee.tv/user/4">Phil Bourne</a> (SciVee), <a href="https://web.archive.org/web/20160528080410/http://www.mendeley.com/profiles/jonathan-eisen">Jonathan Eisen</a> (Mendeley), <a href="https://web.archive.org/web/20160528080410/http://rrresearch.fieldofscience.com/">Rosie Redfield</a> (blog), <a href="https://web.archive.org/web/20160528080410/http://johnhawks.net/weblog">John Hawks</a> (blog) and <a href="https://web.archive.org/web/20160528080410/http://cameronneylon.net/">Cameron Neylon</a> (personal webpage).</p><p>Scientists may feel uncomfortable about marketing their work, but we all are doing it already. We know that giving a presentation at a key meeting can be a boost for our career, and we know about the importance of maintaining an academic homepage listing our research interests and publications. And people reading this blog will understand that a science blog can be a powerful marketing tool.</p><p>Feel free to add your thoughts and suggestions in the comments.</p><h2 id="references">References</h2><p>Fenner, M. (2012). One-click science marketing. Nature Materials, 11(4), 261–263. <a href="https://doi.org/10.1038/nmat3283">https://doi.org/10.1038/nmat3283</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Why I still like FriendFeed, why Twitter is important and other thoughts about Altmetrics]]></title>
            <link>https://blog.martinfenner.org/posts/why-i-still-like-friendfeed-why-twitter-is-important-and-other-thoughts-about-altmetrics</link>
            <guid>01536fe0-9c7a-4379-88c0-083675356400</guid>
            <pubDate>Mon, 05 Mar 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Altmetrics [https://web.archive.org/web/20160528083343/http://altmetrics.org/] –
tools to assess the impact of scholarly works based on alternative online
measures such as bookmarks, links, blog posts, etc. –have become a regular topic
in this blog. The altmetrics manifesto
[https://web.archive.org/web/20160528083343/http://altmetrics.org/manifesto/] 
was published in October 2010, and in the last 18 months we have seen a number
of interesting new altmetrics services
[https://web.archive.org/web]]></description>
            <content:encoded><![CDATA[<p><a href="https://web.archive.org/web/20160528083343/http://altmetrics.org/">Altmetrics</a> – tools to assess the impact of scholarly works based on alternative online measures such as bookmarks, links, blog posts, etc. –have become a regular topic in this blog. The <a href="https://web.archive.org/web/20160528083343/http://altmetrics.org/manifesto/">altmetrics manifesto</a> was published in October 2010, and in the last 18 months we have seen a <a href="https://web.archive.org/web/20160528083343/http://altmetrics.org/tools/">number of interesting new altmetrics services</a>, including the <a href="https://web.archive.org/web/20160528083343/http://blogs.plos.org/blog/tag/sciencecard/">ScienceCard</a> service that I started six months ago. ScienceCard has been a very interesting learning experience, because I not only had to write the software, but also think about my perspective on altmetrics. Some of my recent thoughts are listed below.</p><p><strong>FriendFeed still is a great model for a scholarly service</strong><br>I have stopped using FriendFeed a few months ago in favor of Twitter, but I still very much like the design of the service. I think that the concept should also work very well for altmetrics, and I have therefore continued work on an <a href="https://web.archive.org/web/20160528083343/http://en.wikipedia.org/wiki/Activity_stream">activity stream</a> for ScienceCard. At <a href="https://web.archive.org/web/20160528083343/http://sciencecard.org/works">http://sciencecard.org/works</a> you find a listing of all recent scholarly works of your ScienceCard friends, and now you can like/comment/share them. ScienceCard friends are the people you follow on Twitter who also have a ScienceCard account and sharing is possible via Mendeley and CiteULike. Comments are still in the testing stage, and Twitter integration is also in the works. I also want to add more scholarly content including Slideshare presentations and blog posts, although the latter are really hard to do in an automated way.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20160528083343im_/http://blogs.plos.org/mfenner/files/2012/03/activitystream.png" class="kg-image" alt title="activitystream"></figure><p>Altmetrics tools should not only present scholarly works and their metrics, but they should also allow users to interact with them via sharing, commenting, etc.</p><p><strong>Altmetrics is about search</strong><br>Altmetrics is really about two related concepts: reputation and discovery. ScienceCard tries to summarize the metrics available about a particular researcher, although much more work needs to be done to show that these numbers correlate with reputation. The discovery aspect of altmetrics is at least as important, and this means that these alternative metrics should help provide better search results. Some bibliographic databases let you sort your search results by number of citations – the problem is of course that citations can have a delay of several years. Download counts, social bookmarks, Twitter links, etc. on the other hand can give information about the impact of a paper within days of publication. The search results can be improved further by personalizing them based on what the friends in your social network are publishing, bookmarking or discussing.</p><p><strong>Altmetrics is expensive</strong><br>It is great to see so many altmetrics grassroots projects. Unfortunately it is resource-intensive and therefore costly to collect altmetrics, in particular if it is almost real-time numbers such as Twitter citations. I’m afraid that many people (including myself) will have a hard time providing this service in a sustainable way. There are two possible solutions: providing altmetrics as a commercial service, and providing altmetrics as a collaborative effort. Although I understand the reasoning behind commercial services, I would very much prefer the open and collaborative approach. Collaboration could mean that several people and/or organizations join forces and run an altmetrics service together, but it could also mean that we break altmetrics into smaller services connected via programming interfaces. A typical altmetrics service has at least these functions:</p><ul><li>an interface to add journal articles and other scholarly objects, whether it is manual input by users or via an API</li><li>a searchable database with metadata about scholarly objects</li><li>a background service that collects metrics from other sources</li><li>a public interface that displays metrics for particular scholarly objects and collections</li><li>an interface for visualization and analysis of aggregate numbers</li></ul><p>I don’t think that all five functions (and possibly more) necessarily have to be provided by the same service. I’m sure that there are enough people that are only interested in providing interesting ways to add scholarly objects, or in visualization and analysis. The background service is probably the most boring and resource-intensive part.</p><p><strong>I want second-order metrics</strong><br>Second-order metrics means the metrics for the scholarly works citing a particular paper or for the person bookmarking or tweeting a scholarly work. This approach would add valuable information and is obviously similar to the PageRank algorithm for web links. Unfortunately this approach also creates a lot of extra work, as this means collecting the metrics (or at least some metrics) for all citing works. Again something that makes altmetrics expensive.</p><p><strong>Twitter is important</strong><br>Many of the <a href="https://web.archive.org/web/20160528083343/http://blogs.lse.ac.uk/impactofsocialsciences/2012/02/09/more-tweets-more-citations/">recent altmetrics discussions</a> have really been about the role of Twitter in scholarly communication. A lot of people are excited about the potential of Twitter to help discover interesting scholarly works, and to allow this within days after publication. It is still too early to know for sure whether <a href="https://web.archive.org/web/20160528083343/http://blogs.plos.org/mfenner/2011/12/20/crowdometer-or-trying-to-understand-tweets-about-journal-papers/">highly tweeted papers will be cited more often later on</a>. Better Twitter integration is high on my to do list for ScienceCard.</p><p><strong>Yet another bibliographic database</strong><br>The core function of altmetrics is to build a database with metadata about scholarly objects, including a variety of metrics. There are of course a large number of bibliographic databases already out there, so maybe existing databases can also be extended to include metrics. Ideally the existing database should not be restricted to particular kinds of scholarly works (e.g. journal articles) or disciplines, should be free to use and should allow reuse of the data with an appropriate license. There are several candidates that fit this description, includind <a href="https://web.archive.org/web/20160528083343/http://bibsoup.net/">BibSoup</a> run by the Open Knowledge Foundation and possibly also the <a href="https://web.archive.org/web/20160528083343/http://www.zotero.org/">Zotero </a>database allowing synchronization with the desktop reference manager. Both services focus on biobliographic collections uploaded by users, whereas my idea of an altmetrics database relies on disambiguated authors and scholarly works.</p><p><strong>There are too many altmetrics</strong><br>It is great that altmetrics increases the variety of available metrics, but too many different metrics can be confusing to users. Although it is interesting that the number of citations for the same work vary widely between PubMed, Web of Science, Scopus, Microsoft Academic Search, Pubmed and Google Scholar, the typical user is probably only interested in one citation count. The same is true for usage metrics and number of social bookmarks. I think it would be helpful to consolidate the metrics about a scholarly work to maybe five numbers, including views/downloads, bookmarks, citations, comments, and tweets. This doesn’t mean that the other information shouldn’t be collected, just that the numbers will be consolidated for display.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Few Questions about Science Spam]]></title>
            <link>https://blog.martinfenner.org/posts/a-few-questions-about-science-spam</link>
            <guid>5972ef88-a456-487f-880a-af63686d8f26</guid>
            <pubDate>Fri, 24 Feb 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[This was another week with a fair amount of spam in my email inbox. We all
receive email spam
[https://web.archive.org/web/20161026234722/http://www.youtube.com/watch?v=anwy2MPT5RE] 
on a regular basis and most of us have probably also received science spam:
invitations to scientific conferences about topics we are not working on,
invitations to submit articles to journals not covering your field, and
information about lab supplies we never had asked for. Although I’m of course
aware that spam i]]></description>
            <content:encoded><![CDATA[<p>This was another week with a fair amount of spam in my email inbox. We all receive email <a href="https://web.archive.org/web/20161026234722/http://www.youtube.com/watch?v=anwy2MPT5RE">spam</a> on a regular basis and most of us have probably also received science spam: invitations to scientific conferences about topics we are not working on, invitations to submit articles to journals not covering your field, and information about lab supplies we never had asked for. Although I’m of course aware that spam is now a fact of online life, I don’t quiet understand how this science spam works.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20161026234722im_/http://farm1.staticflickr.com/206/519906069_de5953764a_m.jpg" class="kg-image" alt="SPAM"><figcaption><a href="https://web.archive.org/web/20161026234722/http://www.flickr.com/photos/ajc1/519906069/in/photostream/">Flickr photo</a> by <a href="https://web.archive.org/web/20161026234722/http://scienceoftheinvisible.blogspot.com/">AJ Cann</a>.</figcaption></figure><p><strong>1. How do science spammers get my email address?</strong><br>Most science spam is not really targeted towards my research interests. From this I conclude that these spammers automatically harvest email addresses of researchers, or they buy these lists. One potential source to harvest <a href="https://web.archive.org/web/20161026234722/http://blogs.plos.org/mfenner/2011/07/13/did-you-receive-spam-because-you-published-a-paper/">email addresses is PubMed</a> and other bibliographic databases, but I don’t know whether this is actually done.</p><p><strong>2. Is even a small percentage of researchers responding to this science spam?</strong><br>Science spam is as uninteresting to me as any other spam, and I can’t really imagine a colleague submitting a manuscript to a journal marketed this way. But the idea behind spam is that there is a – admittedly very small – conversion rate.</p><p><strong>3. When will we start to see more social media science spam?</strong><br>I think it is only a question of time before we see more science spam on Twitter, Google+ and Facebook – I already receive a small amount of spam through these channels.</p><p><strong>4. Is there anything an individual researcher can other than using good spam filters in his email program?</strong><br>Would it for example help if we hide our email address as much as possible? Should we deny publishers the permission to post our email address in journal articles, and should places like PubMed hide them? Do publisher organizations such as OASPA (Open Access Scholarly Publishers Association) enforce their <strong><a href="https://web.archive.org/web/20161026234722/http://www.oaspa.org/conduct.php">Member Code of Conduct</a>,</strong> which clearly state that a<em>ny direct marketing activities publishers engage in shall be appropriate and unobtrusive.</em></p><p><strong>5. Will it get worse?</strong><br>I know this answer. Yes. This blog has seen more than 60,000 spam comments already, most of the spam in my email is filtered out before I even see it, but I think it will get much worse – via email and other channels.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Crowdsourcing the analysis of scholarly tweets]]></title>
            <link>https://blog.martinfenner.org/posts/crowdsourcing-the-analysis-of-scholarly-tweets</link>
            <guid>f170c4cc-6acd-4936-a349-3ed27f2ceb97</guid>
            <pubDate>Sun, 19 Feb 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[In December Euan Adie and I started the CrowdoMeter projec
[https://web.archive.org/web/20161027000313/http://blogs.plos.org/mfenner/2011/12/20/crowdometer-or-trying-to-understand-tweets-about-journal-papers/]
t, an analysis of the semantic content of tweets linking to scholarly papers.
Because classifying almost 500 tweets is a lot of work, we turned this into a
crowdsourcing project. We got help from 36 people, who did 953 classifications,
and we discussed the preliminary results (available he]]></description>
            <content:encoded><![CDATA[<p>In December Euan Adie and I <a href="https://web.archive.org/web/20161027000313/http://blogs.plos.org/mfenner/2011/12/20/crowdometer-or-trying-to-understand-tweets-about-journal-papers/">started the CrowdoMeter projec</a>t, an analysis of the semantic content of tweets linking to scholarly papers. Because classifying almost 500 tweets is a lot of work, we turned this into a crowdsourcing project. We got help from 36 people, who did 953 classifications, and we discussed the preliminary results (available <a href="https://web.archive.org/web/20161027000313/http://crowdometer.org/ratings">here</a>) at the <a href="https://web.archive.org/web/20161027000313/http://scienceonline2012.com/">ScienceOnline2012</a> conference.</p><p>There is no reason to stop the crowdsourcing here, so we have <a href="https://web.archive.org/web/20161027000313/http://hdl.handle.net/10779/01c28ce592291e7e294ed328208a5869">uploaded the result set</a> to <a href="https://web.archive.org/web/20161027000313/http://blogs.plos.org/mfenner/2012/02/16/figshare-interview-with-mark-hahnel/">figshare</a> and invite everybody to help us with the data analysis. For this purpose I have created a <a href="https://web.archive.org/web/20161027000313/https://github.com/mfenner/crowdometer">public repository</a> on Github which contains not only the source code for the CrowdoMeter website, but also all data – the same dataset made available on figshare. I have written a <a href="https://web.archive.org/web/20161027000313/https://github.com/mfenner/crowdometer/blob/crowdometer/public/assets/shares_author.R">first R script</a> that produces the following pie chart:</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20161027000313im_/http://blogs.plos.org/mfenner/files/2012/02/authors.png" class="kg-image" alt title="authors"></figure><p>The figure doesn’t look all that exciting, but there is some calculation involved. There are different numbers of classifications per tweet and sometimes there is disagreement: true means at least 50% of classifications were true. It would be great if we find people willing to help with data analysis, preferably using <a href="https://web.archive.org/web/20161027000313/http://rstudio.org/">R</a> and contributing their scripts to the Github repository. Please send me an email or contact me <a href="https://web.archive.org/web/20161027000313/http://twitter.com/#!/mfenner">via Twitter</a> if you need write access to the repository.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Reference Manager Papers now available for Windows]]></title>
            <link>https://blog.martinfenner.org/posts/reference-manager-papers-now-available-for-windows</link>
            <guid>8ccc97d6-1d3c-411b-9f91-3671a8e1cdfe</guid>
            <pubDate>Fri, 17 Feb 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Regular readers of this blog know that I’m a big fan of the reference manager 
Papers
[https://web.archive.org/web/20161026235642/http://www.mekentosj.com/papers/]–
three years ago we even had a poetry contest
[https://web.archive.org/web/20161026235642/http://blogs.plos.org/mfenner/2009/02/19/papers_for_iphone_released_time_for_more_poetry] 
when the iPhone version was first released. The strength of Papers has always
been the very nice user interface, and Papers 2 released last March
[https://]]></description>
            <content:encoded><![CDATA[<p>Regular readers of this blog know that I’m a big fan of the reference manager <a href="https://web.archive.org/web/20161026235642/http://www.mekentosj.com/papers/">Papers </a>– three years ago we even had a <a href="https://web.archive.org/web/20161026235642/http://blogs.plos.org/mfenner/2009/02/19/papers_for_iphone_released_time_for_more_poetry">poetry contest</a> when the iPhone version was first released. The strength of Papers has always been the very nice user interface, and Papers 2 <a href="https://web.archive.org/web/20161026235642/http://blogs.plos.org/mfenner/2011/03/08/papers-2-the-reference-manager-made-with-love/">released last March</a> was a major update that added many more reference types, collaboration and a word processor plugin.</p><p>The first five years Papers has been only available for Mac and iPhone/iPad, but this week mekentosj.com released a pre-release version for Windows. Papers for Windows is a collaboration with <a href="https://web.archive.org/web/20161026235642/http://www.scimatic.com/node/386">Scimatic Software</a>, and a <a href="https://web.archive.org/web/20161026235642/http://pfw.mekentosj.com/kb/roadmap/mac-vs-windows-comparison-compatibility">comparison of the Mac and Windows </a>versions is here. A 30 day trial version is <a href="https://web.archive.org/web/20161026235642/http://www.mekentosj.com/papers/">available for download</a> from mekentosj.com.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Figshare: Interview with Mark Hahnel]]></title>
            <link>https://blog.martinfenner.org/posts/figshare-interview-with-mark-hahnel</link>
            <guid>8d943c76-1bbc-4653-85ba-036956ca7321</guid>
            <pubDate>Thu, 16 Feb 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[figshare allows researchers to publish all of their research outputs in seconds
in an easily citable, sharable and discoverable manner. The service was started
by Mark Hahnel last year while still a PhD student. Mark joined Digital Science
[https://web.archive.org/web/20161023171633/http://www.digital-science.com/] to
work on figshare in September and last month relaunched a much improved version
of the service. I asked Mark a few questions about figshare below. I also
uploaded two datasetst to ]]></description>
            <content:encoded><![CDATA[<figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20161023171633im_/http://blogs.plos.org/mfenner/files/2012/02/me1.png" class="kg-image" alt title="me"></figure><p>fig<strong>share</strong> <em>allows researchers to publish all of their research outputs in seconds in an easily citable, sharable and discoverable manner</em>. The service was started by Mark Hahnel last year while still a PhD student. Mark joined <a href="https://web.archive.org/web/20161023171633/http://www.digital-science.com/">Digital Science</a> to work on fig<strong>share</strong> in September and last month relaunched a much improved version of the service. I asked Mark a few questions about fig<strong>share</strong> below. I also uploaded two datasetst to fig<strong>share</strong> and made them publicly available:</p><ul><li><a href="https://web.archive.org/web/20161023171633/http://hdl.handle.net/10779/1c48a305c08c717fea3f6fe1687b3eff">CrowdoMeter Tweets</a> – all 467 tweets used in the CrowdoMeter project</li><li><a href="https://web.archive.org/web/20161023171633/http://hdl.handle.net/10779/01c28ce592291e7e294ed328208a5869">CrowdoMeter Classifications</a> – all 953 classifications from the CrowdoMeter project.</li></ul><h2 id="1-what-is-figshare">1. What is figshare?</h2><p><a href="https://web.archive.org/web/20161023171633/http://figshare.com/">figshare</a> is a repository where users can make all of their research outputs available in a citable, sharable and discoverable manner. figshare allows users to upload any file format so that figures, datasets and media can be disseminated in a way that the current scholarly publishing model does not allow.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20161023171633im_/http://blogs.plos.org/mfenner/files/2012/02/screenshot.png" class="kg-image" alt title="screenshot"></figure><h2 id="2-what-is-the-right-content-for-figshare-and-what-should-rather-go-somewhere-else">2. What is the right content for figshare? And what should rather go somewhere else?</h2><p>Every experiment that is completed without error in the methods is valuable. Researchers investigate things because the question is interesting and supposedly unanswered. This means that other researchers will at some point ask that same question. Just because the hypothesis didn’t turn out to be true, doesn’t mean that this data should be thrown away.</p><p>With fig<strong>share</strong>, you can share your negative results or results that you were not planning to publish. You can make raw data available or supplementary material that journals cannot handle linked to from the published article. You can even make your papers and posters available and citable. People have uploaded whole chapters of their PhD thesis to share with the world and make sure that all of their hard work is not wasted.</p><p>Currently we are not focusing on handling massive datasets, whilst this is an aim for the future. These edgecases seem to be better handled by journals set up specifically for publishing these huge files, such as the excellent <a>GigaScience</a>.</p><h2 id="3-you-relaunched-figshare-in-january-what-has-changed-to-the-previous-version">3. You relaunched figshare in January. What has changed to the previous version?</h2><p>The old site was a proof of concept based on Mediawiki software. The new site has been completely built from scratch so that it is rapidly extendable in terms of features and scale. This means that we can adapt quickly and easily to the needs of researchers. As well as being much more intuitive, the biggest new feature for fig<strong>share</strong> is the private space.</p><p>Whilst we would like everyone to make all of their research objects available, we appreciate that some researchers would like to keep research private for many reasons. Because of this we set about giving users their own private repository to store their research objects. These objects can be uploaded in seconds and all objects are initially held in the private space, from where they can be made publicly available when the user decides. All research is easily tagged and categorisable, so that researchers can filter through their many files to find the one they were looking for in no time at all.</p><h2 id="4-how-important-is-the-user-interface-for-figshare-what-particular-features-do-you-like-the-most">4. How important is the user interface for figshare? What particular features do you like the most?</h2><p>The user interface is essential, researchers are busy enough as it it. They haven’t got the time to attend training course on how to use a repository. If this was the case with facebook, no one would use facebook. For this reason fig<strong>share</strong> is stupidly simple and allows users to get their research onto the site in seconds, even the PIs. At this point they can choose to make it publicly available and immediately citable, sharable and discoverable, or keep it private – securely hosted, taggable and accessible when they need it, from anywhere in the world.</p><p>This is something we are constantly aiming to improve and we not only welcome feedback, we are actively seeking the thoughts of researchers on how we can make this a seemless part of their research process.</p><h2 id="5-do-you-have-plans-for-a-desktop-version-of-figshare-e-g-to-watch-folders-for-new-figshare-content">5. Do you have plans for a desktop version of figshare, e.g. to watch folders for new figshare content?</h2><p>We do! By creating a desktop uploader, the process becomes even more intuitive for researchers, allowing them to make backups of their research in the cloud with no effort expenditure. Research data management is something I personally was terrible at. My research was organised into folders based on the month and the year I did that work. I lost days trying to find files that ‘I know I worked on sometime last summer’. Hopefully a desktop uploader will add to the current simple management system so that researchers like me have no excuse when it comes to losing (often expensive) results files.</p><h2 id="6-how-is-figshare-different-from-data-repositories">6. How is figshare different from data repositories?</h2><p>fig<strong>share</strong> is not limited as many repositories are. It caters for all research domains, no matter where your research is carried out. Institutional repositories are often limited to members of said institution. Also, the majority of institutional repositories are built specifically for papers. The Dryad repository has been doing some great work making the datasets behind published articles available under CC0. fig<strong>share</strong> is not limited by the normal constraints of publishing, data generated in the lab can be shared and made available to the world as a citable object the same day.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20161023171633im_/http://blogs.plos.org/mfenner/files/2012/02/usermetrics.png" class="kg-image" alt title="usermetrics"></figure><p>fig<strong>share</strong> also gives the researchers the credit for their research. By adding metrics to the public uploads, and putting the cumulative metrics of a researcher’s uploads on their profile, users can see the true impact and reach of the hard work they put in.</p><h2 id="7-you-currently-use-handles-for-figshare-content-what-are-your-thoughts-on-persistent-identifiers-are-the-plans-to-use-dois">7. You currently use handles for figshare content. What are your thoughts on persistent identifiers? Are the plans to use DOIs?</h2><p>Persistent identifiers are essential for the long term availability of research outputs. One of the reasons I set up figshare was because I wanted to cite a video in my thesis. Research data on <a href="https://web.archive.org/web/20161023171633/http://figshare.com/blog/A%20YouTube%20%20for%20Scientists/11">YouTube</a> is not easily citable, by adding persitent identifiers and an organised citation structure, videos as well as any other file format can be easily cited. All citations can be exported to <a href="https://web.archive.org/web/20161023171633/http://mendeley.com/">Mendeley</a>, <a href="https://web.archive.org/web/20161023171633/http://www.endnote.com/">Endnote</a> and <a href="https://web.archive.org/web/20161023171633/http://www.refman.com/">RefMan</a>with one click. We use handles at the moment, but have noticed that researchers tend to be more familiar with DOI’s and so will be making the move over to them shortly. We’re currently working with <a href="https://web.archive.org/web/20161023171633/http://datacite.org/">DataCite</a> through the British Library to get this set up.</p><h2 id="8-how-does-figshare-guarantee-long-term-preservation-of-uploaded-data">8. How does figshare guarantee long-term preservation of uploaded data?</h2><p>The long term persistence of this research data is essential. The research is backed up locally as soon as it is uploaded. We are currently in talks with <a href="https://web.archive.org/web/20161023171633/http://www.portico.org/">Portico</a> to back up all data and further guarantee this long term persistence.</p><h2 id="9-what-are-your-responsibilities-at-figshare-what-did-you-do-before-figshare">9. What are your responsibilities at figshare? What did you do before Figshare?</h2><p>I’m basically responsible for making the platform as useful for researchers as possible. I also love going to talk at Universities and conferences to researchers directly, to hear their thoughts/opinions. As a former life science researcher (I finished my PhD in stem cell biology at Imperial College London in September), I understand that what makes sense in a rational world does not make sense in the scientific world. You just have to look at the established model of scientific research dissemination to understand that. fig<strong>share</strong> is useful whether you want to make all of your research outputs available or none. Science would just be a lot more efficient and dynamic if they did.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Personalized Journal]]></title>
            <link>https://blog.martinfenner.org/posts/the-personalized-journal</link>
            <guid>a4b430d7-9c91-4e7b-94d6-e60c108b6fc7</guid>
            <pubDate>Fri, 10 Feb 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Earlier this week I wrote a guest post
[https://web.archive.org/web/20161026234447/http://blogs.lse.ac.uk/impactofsocialsciences/2012/02/09/more-tweets-more-citations/] 
for the Impact of Social Sciences blog. In the post I talk about a recent paper
correlating tweets and citations (also discussed on this blog
[https://web.archive.org/web/20161026234447/http://blogs.plos.org/mfenner/2011/12/20/crowdometer-or-trying-to-understand-tweets-about-journal-papers/]
). But the main argument I try to mak]]></description>
            <content:encoded><![CDATA[<p>Earlier this week I wrote a <a href="https://web.archive.org/web/20161026234447/http://blogs.lse.ac.uk/impactofsocialsciences/2012/02/09/more-tweets-more-citations/">guest post</a> for the <strong>Impact of Social Sciences</strong> blog. In the post I talk about a recent paper correlating tweets and citations (<a href="https://web.archive.org/web/20161026234447/http://blogs.plos.org/mfenner/2011/12/20/crowdometer-or-trying-to-understand-tweets-about-journal-papers/">also discussed on this blog</a>). But the main argument I try to make is that tweets are a powerful filter for personalized scholarly content:</p><blockquote><em><em>A few years from now the “personalized journal” will have replaced the traditional journal as the primary means to discover new scholarly papers with impact to our work.</em></em></blockquote>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Zotero 3.0 Released]]></title>
            <link>https://blog.martinfenner.org/posts/zotero-3-0-released</link>
            <guid>938846c0-6596-4409-9f4b-e39ae6d987ca</guid>
            <pubDate>Tue, 31 Jan 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Zotero 3.0 was officially released
[https://web.archive.org/web/20161026235801/http://www.zotero.org/blog/zotero-3-0-is-here/] 
today. The big change in version 3.0 of the reference manager is a standalone
version that runs outside the Firefox browser. The first beta
[https://web.archive.org/web/20161026235801/http://blogs.plos.org/mfenner/2011/08/23/zotero-3-0-beta-released-works-with-chrome-and-safari/] 
was released in August 2011.]]></description>
            <content:encoded><![CDATA[<p>Zotero 3.0 was <a href="https://web.archive.org/web/20161026235801/http://www.zotero.org/blog/zotero-3-0-is-here/">officially released</a> today. The big change in version 3.0 of the reference manager is a standalone version that runs outside the Firefox browser. The <a href="https://web.archive.org/web/20161026235801/http://blogs.plos.org/mfenner/2011/08/23/zotero-3-0-beta-released-works-with-chrome-and-safari/">first beta</a> was released in August 2011.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20161026235801im_/http://blogs.plos.org/mfenner/files/2012/01/zotero.png" class="kg-image" alt title="zotero"></figure>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Say Hello to F1000 Research]]></title>
            <link>https://blog.martinfenner.org/posts/say-hello-to-f1000-research</link>
            <guid>96e77700-adc9-43f5-ad4e-bb0da7b45935</guid>
            <pubDate>Mon, 30 Jan 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Faculty 1000 today announced
[https://web.archive.org/web/20161010225753/http://f1000research.com/2012/01/30/f1000-research-join-us-and-shape-the-future-of-scholarly-communication-2/] 
 F1000 Research, a new fully Open Access publishing program across biology and
medicine, that will start publishing later this year. The default open access
license is CC-BY, and CC0 for data.

Important features include:

 1. immediate publication
 2. open, post-publication peer review
 3. revisioning of work
 4.]]></description>
            <content:encoded><![CDATA[<p>Faculty 1000 today <a href="https://web.archive.org/web/20161010225753/http://f1000research.com/2012/01/30/f1000-research-join-us-and-shape-the-future-of-scholarly-communication-2/">announced</a> <strong>F1000 Research</strong>, <em>a new fully Open Access publishing program across biology and medicine, that will start publishing later this year</em>. The default open access license is CC-BY, and CC0 for data.</p><p>Important features include:</p><ol><li>immediate publication</li><li>open, post-publication peer review</li><li>revisioning of work</li><li>raw data repository</li><li>article format and content is not predefined</li></ol><p>F1000 Research is a <em>publishing program</em> rather than a journal. The format for F1000 Research hasn’t been finalized, and F1000 welcomes comments at the <a href="https://web.archive.org/web/20161010225753/http://f1000research.com/">blog</a> or via the <a href="https://web.archive.org/web/20161010225753/https://twitter.com/#!/F1000Research">Twitter</a> account.</p><p>My main question about F1000 Research: is this a preprint archive similar to <a href="https://web.archive.org/web/20161010225753/http://arxiv.org/">ArXiv</a> and <a href="https://web.archive.org/web/20161010225753/http://precedings.nature.com/">Nature Precedings</a>? I’m a big fan of preprint archives, and I think that – contrary to what most people think – they <a href="https://web.archive.org/web/20161010225753/http://blogs.plos.org/mfenner/2010/10/16/in-which-i-suggest-a-preprint-archive-for-clinical-trials/">should work particularly well</a> for clinical trial data.</p><p>Update (1/30/12): <a href="https://web.archive.org/web/20161010225753/http://retractionwatch.wordpress.com/2012/01/30/an-arxiv-for-all-of-science-f1000-launches-new-immediate-publication-journal/">RetractionWatch</a> and <a href="https://web.archive.org/web/20161010225753/http://blogs.nature.com/news/2012/01/f1000-launches-fast-open-science-publishing-for-biology-and-medicine.html">Nature News</a> also cover this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Altmetrics – Where Do We Go From Here?]]></title>
            <link>https://blog.martinfenner.org/posts/altmetrics-where-do-we-go-from-here</link>
            <guid>63154125-ab99-461e-93c9-f57c2366da55</guid>
            <pubDate>Tue, 24 Jan 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[The ScienceOnline2012
[https://web.archive.org/web/20161027000038/http://scienceonline2012.com/] 
conference last week again was a wonderful experience. This was my third time in
North Carolina, and I had many great conversations in the sessions, hallways –
and bars. One of many highlights was a lunch meeting with fellow PLoS bloggers
and staffers:

Flickr photo by briandcrawford
[https://web.archive.org/web/20161027000038/http://www.flickr.com/photos/brian_and_dawn/]
.Together with Euan Adie
[h]]></description>
            <content:encoded><![CDATA[<p>The <a href="https://web.archive.org/web/20161027000038/http://scienceonline2012.com/">ScienceOnline2012</a> conference last week again was a wonderful experience. This was my third time in North Carolina, and I had many great conversations in the sessions, hallways – and bars. One of many highlights was a lunch meeting with fellow PLoS bloggers and staffers:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20161027000038im_/http://farm8.staticflickr.com/7146/6732330879_726113a81d.jpg" class="kg-image" alt="PLoS Crew at Science Online2"><figcaption>Flickr photo by <a href="https://web.archive.org/web/20161027000038/http://www.flickr.com/photos/brian_and_dawn/">briandcrawford</a>.</figcaption></figure><p>Together with <a href="https://web.archive.org/web/20161027000038/http://twitter.com/Stew">Euan Adie</a> I moderated a session on Friday:</p><h2 id="using-altmetrics-tools-to-track-the-scholarly-impact-of-your-research-">Using altmetrics tools to track the scholarly impact of your research.</h2><p>We started the session by asking several people in the audience to demonstrate their altmetrics tools: <a href="https://web.archive.org/web/20161027000038/http://altmetric.com/">altmetric.com</a> (Euan Adie), <a href="https://web.archive.org/web/20161027000038/http://readermeter.org/">ReaderMeter</a> (Dario Taraborelli), <a href="https://web.archive.org/web/20161027000038/http://total-impact.org/">Total Impact</a> (Jason Priem), <a href="https://web.archive.org/web/20161027000038/http://article-level-metrics.plos.org/">PLoS Article-Level Metrics</a> (Jennifer Lin), and <a href="https://web.archive.org/web/20161027000038/http://sciencecard.org/">ScienceCard</a> (me). We briefly showed our <a href="https://web.archive.org/web/20161027000038/http://crowdometer.org/">CrowdoMeter</a> project where we crowdsourced the meaning of tweets about scholarly papers.</p><p>The discussion covered many interesting aspects. I would like to focus on three of them.</p><h3 id="gaming">Gaming</h3><p>Altmetrics are still fairly new, and therefore not many people try to the cheat yet (but almost 1% of tweets in the <a href="https://web.archive.org/web/20161027000038/http://crowdometer.org/ratings">CrowdoMeter dataset</a> were already spam). I’m sure that this will change over time, and some metrics will be more prone to gaming than others. Gaming is a particular problem for usage stats, as it is difficult to impossible to verify them. Metrics provided by the producer of a research object (author or publisher) will be more susceptible to gaming than metrics from an independent source. Anonymous metrics (e.g. Mendeley readers) are more susceptible to gaming than metrics that list the source of every citation (e.g. CiteULike bookmarks).</p><h3 id="context">Context</h3><p>Altmetrics is currently at a stage where we collect various metrics, but don’t really know what these numbers mean. Does 1,000 downloads, 10 Mendeley bookmarks or 50 tweets mean that the paper has impact? And how do we compare altmetrics from different disciplines? Does it make a difference if a <a href="https://web.archive.org/web/20161027000038/http://www.mathunion.org/general/prizes/fields/details/">Fields Medalist</a> blogs about your paper (an example given in the session)? I think that the most interesting metrics are those that take into account who is citing the work, being it a regular citation, a social bookmark or a social media comment. This is of course how Google <a href="https://web.archive.org/web/20161027000038/http://de.wikipedia.org/wiki/PageRank">PageRank</a> works for webpages, and how <a href="https://web.archive.org/web/20161027000038/http://www.eigenfactor.org/">Eigenfactor</a> ranks scholarly journals. The context can be further improved by including the social networks of the person looking for information, e.g. how many people I follow on Twitter have bookmarked this particular paper.</p><h3 id="scope">Scope</h3><p>The tools discussed in the ScienceOnline session all have a particular approach for gathering altmetrics: altmetrics over a given time period (<em>altmetric.com</em>), altmetrics for content produced by a particular publisher (<em>PLoS ALM</em>), altmetrics for a given researcher (<em>ReaderMeter</em> and <em>ScienceCard</em>), and altmetrics produced for a given dataset on demand (<em>Total-Impact</em>). One obvious advantage of this approach is that it reduces the number of datasets needed to run the service. Unfortunately this is an arbitrary distinction, and it falls apart when you use a PageRank approach and also look at the metrics of citing sources.</p><h3 id="conclusions">Conclusions</h3><p>I think that altmetrics has made tremendous progress in 2011, but that there is a lot of work to do in 2012. I’m very interested in altmetrics based on PageRank, but also want to take social networks into consideration. This is of course how finding information on the web works – scholarly communication is just a subset. Unfortunately this approach requires a massive database of scholarly citations, something that is impossible to do for the small part-time altmetrics projects mentioned at the beginning of the post.</p><p>I’m less interested in usage metrics because they are so prone to gaming and will probably become problematic in a few years, and I want to focus on a reasonable number of altmetrics. I hope that there will never be a single “altmetric”, but I also don’t think that we need 20 different altmetrics for every scholarly work. A lot of interesting work ahead for my ScienceCard project.</p><p>I’m looking forward to the altmetrics session at ScienceOnline2013.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Sloan Foundation funds Columbia and Mendeley to develop a Citation-Style Language Editor]]></title>
            <link>https://blog.martinfenner.org/posts/sloan-foundation-funds-columbia-and-mendeley-to-develop-a-citation-style-language-editor</link>
            <guid>4cf81991-64e7-4928-b322-f4972e60fbe4</guid>
            <pubDate>Thu, 19 Jan 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[The Sloan Foundation has awarded a $125,000 grant to Columbia University and
Mendeley to fund the development of a Citation-Style Language
[https://web.archive.org/web/20161026235632/http://blogs.plos.org/mfenner/2010/09/24/citation-style-language-an-interview-with-rintze-zelle-and-ian-mulvany/] 
(CSL) editor. CSL is a XML-based language to format citations and
bibliographies, and is used by the reference managers Zotero, Mendeley and
Papers, and in many other places. Even though more than 1000 ]]></description>
            <content:encoded><![CDATA[<p>The Sloan Foundation has awarded a $125,000 grant to Columbia University and Mendeley to fund the development of a <a href="https://web.archive.org/web/20161026235632/http://blogs.plos.org/mfenner/2010/09/24/citation-style-language-an-interview-with-rintze-zelle-and-ian-mulvany/">Citation-Style Language</a> (CSL) editor. CSL is a XML-based language to format citations and bibliographies, and is used by the reference managers Zotero, Mendeley and Papers, and in many other places. Even though more than 1000 citation styles are available at <a href="https://web.archive.org/web/20161026235632/http://citationstyles.org/">CitationStyles.org</a> (and built into the tools using CSL), it has until now been fairly hard to create new citation styles – editing XML files is not everyones idea of having fun. The CSL editor will be made available as open source software.</p><p>Building a dedicated CSL editor will be a tremendous boost to the format. The press release is <a href="https://web.archive.org/web/20161026235632/http://www.prnewswire.com/news-releases/mendeley-teams-up-with-columbia-university-libraries-to-develop-a-citation-style-language-editor-through-125000-sloan-foundation-award-137669218.html">here</a>.</p><p>In related news, Mekentosj is <a href="https://web.archive.org/web/20161026235632/http://news.mekentosj.com/2012/01/a-serial-for-a-style/">giving away</a> a Papers 2 serial number for every citation style submitted to CitationStyles.org.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Altmetrics to go – mobile version of ScienceCard available]]></title>
            <link>https://blog.martinfenner.org/posts/altmetrics-to-go-mobile-version-of-sciencecard-available</link>
            <guid>14aed51e-4e9d-470c-acff-bacaab825378</guid>
            <pubDate>Sun, 08 Jan 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[ScienceCard [https://web.archive.org/web/20161027000751/http://sciencecard.org/] 
is a web service that collects all scientific articles published by an author
and displays their aggregate article-level metrics. Yesterday I added a mobile
version to ScienceCard, simply browse to ScienceCard with your mobile phone or
go to http://mobile.sciencecard.org
[https://web.archive.org/web/20161027000751/http://mobile.sciencecard.org/].
This is a first version based on my work with jQuery Mobile on Crowdo]]></description>
            <content:encoded><![CDATA[<p><a href="https://web.archive.org/web/20161027000751/http://sciencecard.org/">ScienceCard</a> is a web service that collects all scientific articles published by an author and displays their aggregate article-level metrics. Yesterday I added a mobile version to ScienceCard, simply browse to ScienceCard with your mobile phone or go to <a href="https://web.archive.org/web/20161027000751/http://mobile.sciencecard.org/">http://mobile.sciencecard.org</a>. This is a first version based on my work with jQuery Mobile on <a href="https://web.archive.org/web/20161027000751/http://blogs.plos.org/mfenner/2012/01/04/crowdometer-goes-mobile/">CrowdoMeter</a>, but I think ScienceCard works really well on a small screen.My ScienceCard looks like <a href="https://web.archive.org/web/20161027000751/http://mobile.sciencecard.org/mfenner">this</a>. Further down the screen are the articles I have published and the collective metrics of these papers.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20161027000751im_/http://blogs.plos.org/mfenner/files/2012/01/sciencecard2.png" class="kg-image" alt title="sciencecard2"></figure><p>Other ScienceCard users have much more impressive metrics, both in traditional citations, and in altmetrics such as PDF downloads of papers published with PLoS:</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20161027000751im_/http://blogs.plos.org/mfenner/files/2012/01/sciencecard3.png" class="kg-image" alt title="sciencecard3"></figure><p>Metrics of an individual paper (<a href="https://web.archive.org/web/20161027000751/http://doi.org/dm9">http://doi.org/dm9</a>), with links to the journal and metrics (e.g. CiteULike bookmarks) look like this:</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20161027000751im_/http://blogs.plos.org/mfenner/files/2012/01/sciencecard.png" class="kg-image" alt title="sciencecard"></figure><p>One idea behind ScienceCard is to make the collection and display of this kind of information as simple as possible. I hope this is another small step in the right direction.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[CrowdoMeter goes Mobile]]></title>
            <link>https://blog.martinfenner.org/posts/crowdometer-goes-mobile</link>
            <guid>52c903d9-c4fc-43a7-addc-fe37ec168759</guid>
            <pubDate>Wed, 04 Jan 2012 00:00:00 GMT</pubDate>
            <description><![CDATA[Two weeks ago Euan Adie from altmetric.com
[https://web.archive.org/web/20170107002621/http://altmetric.com/] and myself 
launched
[https://web.archive.org/web/20170107002621/http://blogs.plos.org/mfenner/2011/12/20/crowdometer-or-trying-to-understand-tweets-about-journal-papers/] 
the website CrowdoMeter
[https://web.archive.org/web/20170107002621/http://crowdometer.org/], a
crowdsourcing project that tries to classify tweets about scholarly articles
using the Citation Typing Ontology (CiTO). D]]></description>
            <content:encoded><![CDATA[<p>Two weeks ago Euan Adie from <a href="https://web.archive.org/web/20170107002621/http://altmetric.com/">altmetric.com</a> and myself <a href="https://web.archive.org/web/20170107002621/http://blogs.plos.org/mfenner/2011/12/20/crowdometer-or-trying-to-understand-tweets-about-journal-papers/">launched</a> the website <a href="https://web.archive.org/web/20170107002621/http://crowdometer.org/">CrowdoMeter</a>, a crowdsourcing project that tries to classify tweets about scholarly articles using the Citation Typing Ontology (CiTO). Despite the holidays we have gotten off to a good start with currently 597 classifications by 56 different users, already covering 93% of the tweets we wanted to classify. We will discuss the results of this project at the <a href="https://web.archive.org/web/20170107002621/http://scienceonline2012.com/">ScienceOnline2012</a> conference in two weeks, but the most important findings can also be watched in real-time <a href="https://web.archive.org/web/20170107002621/http://crowdometer.org/ratings">here</a>.</p><p>To our knowledge this is the first time that CiTO has been used for the systematic classification of tweets, and the preliminary results seem to confirm what we and others had thought, i.e. that most tweets contain little semantic information and often only retweet the title of a paper. But not only do we now have numbers to confirm this, but we can also make some interesting additional observations. We find for example that only 1% of tweets disagree with the statements made in a paper – most Twitter users don’t seem to care telling others about papers they dislike or disagree with.</p><p>This project is far from over, ideally we want 3-5 classifications per tweet or an additional 1,000 classifications. It is a challenge to build a website so that enough people want to help with this project. One idea is to make the classifications as simple as possible, and to help further with this we today launched a mobile version of CrowdoMeter. Simply browse to <a href="https://web.archive.org/web/20170107002621/http://crowdometer.org/">http://crowdometer.org</a> with your iPhone or Android phone, sign in via your Twitter account, and you should see something similar to this:</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20170107002621im_/http://blogs.plos.org/mfenner/files/2012/01/crowdometer.png" class="kg-image" alt title="crowdometer"></figure><p>CrowdoMeter uses <a href="https://web.archive.org/web/20170107002621/http://jquerymobile.com/">jQuery Mobile</a>, a touch-optimized Javascript framework for smartphones and tablets. There are still some minor issues, but in general jQuery Mobile is a great tool to optimize a website for mobile users. Users are presented with 10 random tweets they haven’t classified yet, and see a simple classification screen when clicking (touching) a tweet:</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20170107002621im_/http://blogs.plos.org/mfenner/files/2012/01/crowdometer1.png" class="kg-image" alt title="crowdometer1"></figure><p>It should not take longer than 15 minutes to classify 15-25 tweets, and this would be a tremendous help for the project.</p><p>The CrowdoMeter results page displayed the same information as in the desktop version of the website, the charts are produced by the <a href="https://web.archive.org/web/20170107002621/http://www.highcharts.com/">Highcharts</a> Javascript library (and again jQuery).</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20170107002621im_/http://blogs.plos.org/mfenner/files/2012/01/crowdometer2.png" class="kg-image" alt title="crowdometer2"></figure><p>I’m interested to see how well the crowdsourcing for CrowdoMeter will work in the coming weeks. We hope to finish the data gathering part in January. If this project generates enough interest I could imagine doing another crowdsourcing project, maybe again using the Citation Typing Ontology, but this time for blog posts about scholarly papers.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How to formally cite a blog post]]></title>
            <link>https://blog.martinfenner.org/posts/how-to-formally-cite-a-blog-post</link>
            <guid>11aaf102-e576-41af-a1bc-b4cb7fb9eaad</guid>
            <pubDate>Thu, 21 Jul 2011 00:00:00 GMT</pubDate>
            <description><![CDATA[Other blog posts often provide important background material for your own posts,
and they are typically cited by inline links
[https://web.archive.org/web/20170913072431/http://blogs.plos.org/mfenner/2011/01/23/beyond-the-pdf-%E2%80%A6-is-epub/] 
in the text. But sometimes we need more formal citations, e.g. when citing blog
posts in a journal article or when providing a bibliography. But how do you
properly cite a blog post?

Flickr picture
[https://web.archive.org/web/20170913072431/http://www]]></description>
            <content:encoded><![CDATA[<p>Other blog posts often provide important background material for your own posts, and they are typically cited by <a href="https://web.archive.org/web/20170913072431/http://blogs.plos.org/mfenner/2011/01/23/beyond-the-pdf-%E2%80%A6-is-epub/">inline links</a> in the text. But sometimes we need more formal citations, e.g. when citing blog posts in a journal article or when providing a bibliography. But how do you properly cite a blog post?</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20170913072431im_/http://farm5.static.flickr.com/4049/4534468843_176c790812.jpg" class="kg-image" alt="APA"></figure><p><em><a href="https://web.archive.org/web/20170913072431/http://www.flickr.com/photos/ccacnorthlib/4534468843/">Flickr picture</a> by CCAC North Library.</em></p><p>The <a href="https://web.archive.org/web/20170913072431/http://www.apastyle.org/">APA Style</a> – from the Publication Manual of the American Psychological Association – suggests the following format:</p><p><strong>Fenner, M.H.</strong> (2011, January 23). Beyond the PDF … is ePub [Web log post]. Retrieved from <a href="https://web.archive.org/web/20170913072431/http://blogs.plos.org/mfenner/2011/01/23/beyond-the-pdf-%E2%80%A6-is-epub/">http://blogs.plos.org/mfenner/2011/01/23/beyond-the-pdf-…-is-epub/</a></p><p>This is a good start, but I think the citation should include the name of the blog. The <a href="https://web.archive.org/web/20170913072431/http://www.chicagomanualofstyle.org/tools_citationguide.html">Chicago Manual of Style</a> does this:</p><p><strong>Fenner, M.H., </strong>“Beyond the PDF … is ePub,” Gobbledygook, January 23, 2011, <a href="https://web.archive.org/web/20170913072431/http://blogs.plos.org/mfenner/2011/01/23/beyond-the-pdf-%E2%80%A6-is-epub/">http://blogs.plos.org/mfenner/2011/01/23/beyond-the-pdf-…-is-epub/</a></p><p>The <a href="https://web.archive.org/web/20170913072431/http://www.library.illinois.edu/learn/tutorials/mla.html">MLA Style</a> also mentions the Publisher, and the date the blog post was accessed (in addition to the publication date):</p><p><strong>Fenner, M.H.</strong> “Beyond the PDF … is ePub”. <em>Gobbledygook</em>. PLoS Blogs. January 23, 2011. Web. July 21, 2011.  <a href="https://web.archive.org/web/20170913072431/http://blogs.plos.org/mfenner/2011/01/23/beyond-the-pdf-%E2%80%A6-is-epub/">http://blogs.plos.org/mfenner/2011/01/23/beyond-the-pdf-…-is-epub/</a></p><p>This is a little bit too much for me. There is an argument to use the accession date for web content, but having two dates can be confusing and the publication date is more important. I personally prefer a citation format that looks very similar to a journal article citation (and comes closest to the Chicago Manual of Style):</p><p><strong>Fenner, M.H. </strong>Beyond the PDF … is ePub. <em>Gobbledygook</em>, January 23, 2011. <a href="https://web.archive.org/web/20170913072431/http://blogs.plos.org/mfenner/2011/01/23/beyond-the-pdf-%E2%80%A6-is-epub/">http://blogs.plos.org/mfenner/2011/01/23/beyond-the-pdf-…-is-epub/</a></p><p>If we want to include blog posts in a reference list, we also have to think about the formatting. In <strong>BibTeX</strong>a blog entry would look like this:</p><p>@misc{ author = {Fenner, Martin}, title = {Beyond the PDF … is ePub}, journal = {Gobbledygook}, type = {Blog}, number = {January 23}, year = {2011}, howpublished = {\url{http://blogs.plos.org/mfenner/2011/01/23/beyond-the-pdf-…-is-epub/}}</p><p><strong>BibTeX</strong> doesn’t have an entry type for blog posts, so we have to use @misc and use the type field. @unpublished is an alternative, but that is a strange name. We have to put the URL into the howpublished field, which is also awkward.</p><p><a href="https://web.archive.org/web/20170913072431/http://www.refman.com/support/risformat_intro.asp">RIS</a> is another popular format, and knows about blogs and URLs:</p><p>TY  - BLOG AU  - Fenner, Martin TI  - Beyond the PDF … is ePub JF  - Gobbledygook PY  - 2011/01/23 UR  - http://blogs.plos.org/mfenner/2011/01/23/beyond-the-pdf-…-is-epub/ ER  -</p><p><strong>BibTeX</strong> and <strong>RIS</strong> both predate the web. A modern reference format should use HTML or XML so that it can be embedded directly in the blog post. The recently announced <a href="https://web.archive.org/web/20170913072431/http://blogs.plos.org/mfenner/2011/06/07/schema-org-for-scholarly-html/">Schema.org</a> micro data format is a strong candidate for this <a href="https://web.archive.org/web/20170913072431/http://scholarlyhtml.org/">Scholarly HTML</a>, and it has a <a href="https://web.archive.org/web/20170913072431/http://schema.org/BlogPosting">BlogPosting</a> format:</p><p>&lt;div itemscope itemtype="http://schema.org/BlogPosting"&gt; &lt;span itemprop="author" itemscope itemtype="http://schema.org/Person"&gt; &lt;span itemprop="name"&gt;Fenner, Martin&lt;/span&gt; &lt;/span&gt; &lt;span itemprop="name"&gt;Beyond the PDF … is ePub&lt;/span&gt; &lt;span itemprop="publisher"&gt;Gobbledygook&lt;/span&gt; &lt;time datetime="2011-01-23"&gt;01/23/11&lt;/time&gt; &lt;a href="http://blogs.plos.org/mfenner/2011/01/23/beyond-the-pdf-…-is-epub/" itemprop="url"&gt;http://blogs.plos.org/mfenner/2011/01/23/beyond-the-pdf-…-is-epub/&lt;/a&gt; &lt;/div&gt;</p><p>There are other ways to describe this blog post with <strong>Schema.org</strong>, but the best practices are still evolving. How this BlogPosting above looks in a blog obviously depends on the CSS styling used, without any styling it looks like this:</p><p>Fenner, Martin<br>Beyond the PDF … is ePub<br>Gobbledygook<br>01/23/11<br><a href="https://web.archive.org/web/20170913072431/http://blogs.plos.org/mfenner/2011/01/23/beyond-the-pdf-%E2%80%A6-is-epub/">http://blogs.plos.org/mfenner/2011/01/23/beyond-the-pdf-…-is-epub/</a></p><p>With a little work on the CSS for this blog, the citation could look exactly as shown above, but with added semantic information that is understood by search engines and other tools. It is clear to me that HTML-based standards such as Schema.org are the future for embedding citation information in blog posts. We need better tools to make this process as painless as possible.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Why can’t I reuse these tables and figures?]]></title>
            <link>https://blog.martinfenner.org/posts/why-cant-i-reuse-these-tables-and-figures-2</link>
            <guid>2d4ae654-a0f2-4db3-8e74-1cb8257025e3</guid>
            <pubDate>Thu, 30 Sep 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Tables and figures contain the data of a scientific paper in condensed (and
often visually appealing) form. This is why they are among the first thing we
look at, and why they are often reused when we discuss the paper in a
presentation or blog post.

From https://doi.org/10.1371/journal.ppat.1001042
[https://web.archive.org/web/20120525130956/http://dx.doi.org/10.1371/journal.ppat.1001042]
. Image credit: Thomas J. Hannan, Washington University.Electronic publication
has dramatically simplified]]></description>
            <content:encoded><![CDATA[<p>Tables and figures contain the data of a scientific paper in condensed (and often visually appealing) form. This is why they are among the first thing we look at, and why they are often reused when we discuss the paper in a presentation or blog post.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120525130956im_/http://www.plospathogens.org/images/home/ppat.v06.i08.jpg" class="kg-image" alt title="PLoS Pathogens"><figcaption><em><em>From </em>https://doi.org/<em><a href="https://web.archive.org/web/20120525130956/http://dx.doi.org/10.1371/journal.ppat.1001042">10.1371/journal.ppat.1001042</a>. Image credit: Thomas J. Hannan, Washington University.</em></em></figcaption></figure><p>Electronic publication has dramatically simplified the reuse of tables and figures, and therefore reuse has become very common – you probably find reused material in most presentations given in academic institutions or at conferences.</p><p>Most authors will probably be happy that their results are disseminated, and reuse is likely to lead to more people reading the full paper and citing the work.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120525130956im_/http://blogs.plos.org/mfenner/files/2010/09/bmccancer-e1285879074865.jpg" class="kg-image" alt title="bmccancer"><figcaption>From https://doi.org/<a href="https://web.archive.org/web/20120525130956/http://dx.doi.org/10.1186/1471-2407-10-286">10.1186/1471-2407-10-286</a>.</figcaption></figure><p>But this reuse has two problems. The first problem is copyright. Many journals own the copyright of the papers they publish and don’t allow reuse without prior permission. Unfortunately copyright is a complicated issue, and also differs between countries. Most researchers assume “<a href="https://web.archive.org/web/20120525130956/http://www.copyright.gov/fls/fl102.html">Fair use</a>“ when they reuse material, but this might not apply to all situations, e.g. presentations at conferences. And many researchers don’t understand that they have often given away the copyright to their own works, so that they can’t show a figure from one of their papers without permission.</p><p>Many publishers have automated the process of obtaining permissions for copyrighted work, e.g. using the <a href="https://web.archive.org/web/20120525130956/http://www.copyright.com/viewPage.do?pageCode=pu4-n">Rightslink</a> system of the Copyright Clearance Center. But it still requires a considerable investment in time (and often money) to obtain all permissions, especially since these are usually one-time permissions only. This combination of unawareness of the details of copyright law and the required extra work means that many researchers probably don’t obtain permissions prior to reuse.</p><p>The solution to the copyright problems is obviously to use material with a <a href="https://web.archive.org/web/20120525130956/http://creativecommons.org/">Creative Commons</a> license whenever possible, as I have done in this blog post. And most Open Access papers are published under this license, so there is plenty of material to choose from.</p><p>But there is also a second problem with reusing tables and figures. They were designed to be part of a paper and often look terrible in a presentation, particularly tables.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120525130956im_/http://blogs.plos.org/mfenner/files/2010/09/plosone-e1285879245843.png" class="kg-image" alt title="plosone"><figcaption><em><em>From </em>https://doi.org/</em><a href="https://web.archive.org/web/20120525130956/http://dx.doi.org/10.1371/journal.pone.0006022"><em><em>10.1371/journal.pone.0006022</em></em></a>.</figcaption></figure><p>The solution to this problem is to provide the data behind the table or figure, so that the information can be displayed in a way that makes sense in a presentation. Here we usually have to reduce the amount of information, but it could also mean that we remix the content with other sources. The Creative Commons licenses discussed above are<a href="https://web.archive.org/web/20120525130956/http://pantonprinciples.org/"> not appropriate</a> for data. Whenever possible, scientific data should be placed in the public domain.</p><p>It is important to distinguish the publication of table and figure data from the publication of the <a href="https://web.archive.org/web/20120525130956/http://blogs.openaccesscentral.com/blogs/bmcblog/entry/bmc_research_notes_wants_your">whole research dataset</a>. The open questions with the latter (e.g. standard data formats, appropriate repositories, archiving) don’t apply to the former. This means that publishers could start providing these data immediately. I’m confident that they would see an increase in paper downloads and citations. But more importantly I hope this would lead to better presentations in seminars and at conferences.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Why can’t I reuse these tables and figures?]]></title>
            <link>https://blog.martinfenner.org/posts/why-cant-i-reuse-these-tables-and-figures</link>
            <guid>a7fc9701-ecd0-492f-8d77-daab2b95de07</guid>
            <pubDate>Thu, 30 Sep 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Tables and figures contain the data of a scientific paper in condensed (and
often visually appealing) form. This is why they are among the first thing we
look at, and why they are often reused when we discuss the paper in a
presentation or blog post.

From doi:10.1371/journal.ppat.1001042
[https://web.archive.org/web/20170913082049/http://dx.doi.org/10.1371/journal.ppat.1001042]
. Image credit: Thomas J. Hannan, Washington University.

Electronic publication has dramatically simplified the reuse]]></description>
            <content:encoded><![CDATA[<p>Tables and figures contain the data of a scientific paper in condensed (and often visually appealing) form. This is why they are among the first thing we look at, and why they are often reused when we discuss the paper in a presentation or blog post.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20170913082049im_/http://www.plospathogens.org/images/home/ppat.v06.i08.jpg" class="kg-image" alt title="PLoS Pathogens"></figure><p><em>From doi:<a href="https://web.archive.org/web/20170913082049/http://dx.doi.org/10.1371/journal.ppat.1001042">10.1371/journal.ppat.1001042</a>. Image credit: Thomas J. Hannan, Washington University.</em></p><p>Electronic publication has dramatically simplified the reuse of tables and figures, and therefore reuse has become very common – you probably find reused material in most presentations given in academic institutions or at conferences.</p><p>Most authors will probably be happy that their results are disseminated, and reuse is likely to lead to more people reading the full paper and citing the work.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20170913082049im_/http://blogs.plos.org/mfenner/files/2010/09/bmccancer-e1285879074865.jpg" class="kg-image" alt title="bmccancer"></figure><p><em>From doi:</em><a href="https://web.archive.org/web/20170913082049/http://dx.doi.org/10.1186/1471-2407-10-286"><em>10.1186/1471-2407-10-286</em></a><em>.</em></p><p>But this reuse has two problems. The first problem is copyright. Many journals own the copyright of the papers they publish and don’t allow reuse without prior permission. Unfortunately copyright is a complicated issue, and also differs between countries. Most researchers assume “<a href="https://web.archive.org/web/20170913082049/http://www.copyright.gov/fls/fl102.html">Fair use</a>” when they reuse material, but this might not apply to all situations, e.g. presentations at conferences. And many researchers don’t understand that they have often given away the copyright to their own works, so that they can’t show a figure from one of their papers without permission.</p><p>Many publishers have automated the process of obtaining permissions for copyrighted work, e.g. using the <a href="https://web.archive.org/web/20170913082049/http://www.copyright.com/viewPage.do?pageCode=pu4-n">Rightslink</a> system of the Copyright Clearance Center. But it still requires a considerable investment in time (and often money) to obtain all permissions, especially since these are usually one-time permissions only. This combination of unawareness of the details of copyright law and the required extra work means that many researchers probably don’t obtain permissions prior to reuse.</p><p>The solution to the copyright problems is obviously to use material with a <a href="https://web.archive.org/web/20170913082049/http://creativecommons.org/">Creative Commons</a> license whenever possible, as I have done in this blog post. And most Open Access papers are published under this license, so there is plenty of material to choose from.</p><p>But there is also a second problem with reusing tables and figures. They were designed to be part of a paper and often look terrible in a presentation, particularly tables.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20170913082049im_/http://blogs.plos.org/mfenner/files/2010/09/plosone-e1285879245843.png" class="kg-image" alt title="plosone"></figure><p><em>From doi:</em><a href="https://web.archive.org/web/20170913082049/http://dx.doi.org/10.1371/journal.pone.0006022"><em>10.1371/journal.pone.0006022</em></a>.</p><p>The solution to this problem is to provide the data behind the table or figure, so that the information can be displayed in a way that makes sense in a presentation. Here we usually have to reduce the amount of information, but it could also mean that we remix the content with other sources. The Creative Commons licenses discussed above are<a href="https://web.archive.org/web/20170913082049/http://pantonprinciples.org/"> not appropriate</a> for data. Whenever possible, scientific data should be placed in the public domain.</p><p>It is important to distinguish the publication of table and figure data from the publication of the <a href="https://web.archive.org/web/20170913082049/http://blogs.openaccesscentral.com/blogs/bmcblog/entry/bmc_research_notes_wants_your">whole research dataset</a>. The open questions with the latter (e.g. standard data formats, appropriate repositories, archiving) don’t apply to the former. This means that publishers could start providing these data immediately. I’m confident that they would see an increase in paper downloads and citations. But more importantly I hope this would lead to better presentations in seminars and at conferences.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Paper retractions do not induce citation mutations]]></title>
            <link>https://blog.martinfenner.org/posts/paper-retractions-do-not-induce-citation-mutations</link>
            <guid>a94dacae-4c14-4fa4-94d4-2a8149b88e0e</guid>
            <pubDate>Sun, 26 Sep 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Dear Christian Specht,

Thank you very much for your detailed response
[https://web.archive.org/web/20120525055229/http://www.the-scientist.com/news/display/57698/] 
in The Scientist to our previous letter
[https://web.archive.org/web/20120525055229/http://blogs.plos.org/mfenner/2010/09/20/letter-to-the-scientist/] 
regarding citation mutations. You clarified several issues that were raised in 
your original study
[https://web.archive.org/web/20120525055229/http://www.the-scientist.com/news/disp]]></description>
            <content:encoded><![CDATA[<p>Dear Christian Specht,</p><p>Thank you very much for <a href="https://web.archive.org/web/20120525055229/http://www.the-scientist.com/news/display/57698/">your detailed response</a> in <em><em>The Scientist</em></em> to <a href="https://web.archive.org/web/20120525055229/http://blogs.plos.org/mfenner/2010/09/20/letter-to-the-scientist/">our previous letter</a> regarding citation mutations. You clarified several issues that were raised in <a href="https://web.archive.org/web/20120525055229/http://www.the-scientist.com/news/display/57689/">your original study</a>, particularly that citation mutation rates have dropped significantly in the last 10 years (probably due to the more widespread use of reference management software), and that some citation mutations (e.g. 680→685 in Laemmli 1970) might be introduced not by citing authors, but by the citation database.</p><p>You rightfully point out that citation mutations indicate a much bigger problem: <em><em>authors often do not read the publications cited in their work</em></em>. I am not aware of any available direct data, but in an ongoing study Richard Grant is looking at this question (<a href="https://web.archive.org/web/20120525055229/http://blog.the-scientist.com/2010/09/23/mutatis-citandi/">Do you read the papers you cite?</a>). The preliminary data that Richard kindly made available indicate that more than 85% of authors indeed read the papers they cite.</p><p>In another study made aware to us by <a href="https://web.archive.org/web/20120525055229/http://twitter.com/noahWG/status/25410352261">Noah Gray</a>, Neale et al. (Neale 2009) used an elegant experimental design to address the same question. They studied lethal acquired mutations – retracted papers that in theory should no longer be cited – as an estimate of how diligent authors were reading the papers they cite.</p><p>The authors compared the number of citations of 102 retracted papers (starting 12 months after the retraction) to a control group of papers and found no difference in the average number of citations (26 vs. 27). Content analysis of a subset of citing papers indicated that more than 50% of citations used the retracted paper to support their own findings and less than 5% of citing papers mentioned the retraction.</p><p>These findings not only seem to contradict the preliminary findings by Richard Grant, but also indicate that journal publishers and citation databases may not be properly communicating paper retractions.</p><h3 id="references">References</h3><p><strong><strong>Laemmli UK</strong></strong>. Cleavage of Structural Proteins during the Assembly of the Head of Bacteriophage T4. <em><em>Nature</em></em>. 1970;227:680-685. https://doi.org/<a href="https://web.archive.org/web/20120525055229/http://dx.doi.org/10.1038/227680a0">10.1038/227680a0</a></p><p><strong><strong>Neale AVV, Dailey RK, Abrams J</strong></strong>. Analysis of citations to biomedical articles affected by scientific misconduct. <em><em>Science and Engineering Ethics</em></em>. 2010;16(2):251-261. https://doi.org/<a href="https://web.archive.org/web/20120525055229/http://dx.doi.org/10.1007/s11948-009-9151-4">10.1007/s11948-009-9151-4</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Citation Style Language: An Interview with Rintze Zelle and Ian Mulvany]]></title>
            <link>https://blog.martinfenner.org/posts/citation-style-language-an-interview-with-rintze-zelle-and-ian-mulvany</link>
            <guid>01b3b686-c4ed-42a3-9943-8da2f2c812bc</guid>
            <pubDate>Fri, 24 Sep 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Citation styles are one of the greater mysteries for the novice manuscript
writer. There are numerous ways that authors, title, journal, etc. can be
arranged and formatted (see examples below), and in bibliographies citations can
be ordered either alphabetically or by order of appearance in the text.

Laemmli UK. Cleavage of Structural Proteins during the Assembly of the Head of
Bacteriophage T4. Nature. 1970;227:680-685.

U. K. Laemmli (1970). ‘Cleavage of Structural Proteins during the Assembl]]></description>
            <content:encoded><![CDATA[<p>Citation styles are one of the greater mysteries for the novice manuscript writer. There are numerous ways that authors, title, journal, etc. can be arranged and formatted (see examples below), and in bibliographies citations can be ordered either alphabetically or by order of appearance in the text.</p><p><strong><strong>Laemmli UK</strong></strong>. Cleavage of Structural Proteins during the Assembly of the Head of Bacteriophage T4. <em><em>Nature</em></em>. 1970;227:680-685.</p><p><strong><strong>U. K. Laemmli</strong></strong> (1970). ‘Cleavage of Structural Proteins during the Assembly of the Head of Bacteriophage T4′. <em><em>Nature</em></em> <strong><strong>227</strong></strong>(5259):680-685.</p><p><strong><strong>U. K. Laemmli</strong></strong>, <em><em>Nature </em></em><strong><strong>227</strong></strong>, 680 (1970).</p><p>Because of this complexity, it has long become impractical to format citations manually, and formatting of citations and bibliographies is one of the main reasons for using reference management software. I interviewed Rintze Zelle (scientist and open source contributor) and Ian Mulvany (vice president of new product development at Mendeley) to better understand citation styles in general and the open source Citation Style Language (CSL) in particular. CSL co-developers Bruce D’Arcus and Frank Bennett provided important feedback.</p><h3 id="1-what-is-the-citation-style-language"><strong><strong>1. What is the Citation Style Language?</strong></strong></h3><p>Rintze Zelle: Scientific literature depends heavily on proper referencing. However, when writing a manuscript, manually editing citations and bibliographies is time consuming and error prone. Citation styles also differ between scientific journals, so authors often have to switch citation styles when they submit their manuscript to a different journal than originally anticipated.</p><p>The <a href="https://web.archive.org/web/20120525124720/http://citationstyles.org/">Citation Style Language</a> (CSL) is an open XML based language meant to automate the formatting of citations and bibliographies. When provided with the metadata (title, year, authors, etc.) of the cited items (journal articles, books, etc.), and a CSL style, a CSL processor can automatically generate the bibliography and in-text citations. CSL is currently used by <a href="https://web.archive.org/web/20120525124720/http://www.zotero.org/">Zotero</a> and <a href="https://web.archive.org/web/20120525124720/http://www.mendeley.com/">Mendeley</a>, and both programs offer word processor plug-ins for <a href="https://web.archive.org/web/20120525124720/http://office.microsoft.com/en-us/word/">Microsoft Word</a> and <a href="https://web.archive.org/web/20120525124720/http://www.openoffice.org/">OpenOffice.org</a>. Several other projects are working on or exploring CSL support.</p><h3 id="2-do-we-really-need-hundreds-of-citation-styles">2. Do we really need hundreds of citation styles?</h3><p><strong><strong>Rintze Zelle</strong></strong>: We have asked ourselves this question many times over. Some variability is certainly warranted: numeric, author-date and footnote styles are very different, and each type has its own advantages and disadvantages. However, small variations in citation styles result in a situation where almost every journal or publisher has its unique style. We think that even with the use of automated tools like CSL, reducing the number of citation styles in use could result in significant cost and time savings in scientific publishing. But this is a problem beyond the scope of CSL, so our goal is simply to support all the variability that currently exists in citation styles.</p><p><strong><strong>Ian Mulvany</strong></strong>: After working for many years at Springer, and then Nature, I was well aware that most large publishers just push submitted manuscripts out to companies in India where the formatting of the paper happens. The input format and citation formatting really doesn’t matter to most publishers. They just tear the submitted manuscript to pieces and rebuild it in their chosen XML schema.</p><p>However, most people using citations are not actually submitting manuscripts for publication, but rather are writing term papers, or theses, or reports. So the weird thing is that citations started off as a required identifier for the literature. Google Scholar and HTTP URIs such as the DOI have almost totally made formatted citations redundant as identifiers, and yet there is still a huge user need to be able to format citations according to a huge variety of styles, and since that need is going to continue for quite a long time, it’s a need that we have to support.</p><h3 id="3-what-is-the-difference-between-csl-and-other-citation-style-systems"><strong><strong>3. What is the difference between CSL and other citation style systems?</strong></strong></h3><p>Rintze Zelle: Our main “competition” arguably comes from <a href="https://web.archive.org/web/20120525124720/http://www.bibtex.org/">BibTeX</a> and <a href="https://web.archive.org/web/20120525124720/http://www.endnote.com/">EndNote</a>/<a href="https://web.archive.org/web/20120525124720/http://www.refman.com/">Reference Manager</a>. BibTeX is a popular choice for those working with the <a href="https://web.archive.org/web/20120525124720/http://www.latex-project.org/">LaTeX</a> typesetting system, but the user base of LaTeX is relatively small and mostly limited to the sciences. EndNote and Reference Manager are commercial tools offered by Thomson Reuters. While large collections of citation styles are available for each program, the use of these styles is limited to licensed users.</p><p>CSL was designed with three main goals:</p><ol><li>to create an open system that is independent of the operating system, application or document format,</li><li>to cover the full range of citation formatting rules in use, extending from the sciences to fields in the humanities as well as law,</li><li>to free end users from the complex task of formatting citations.</li></ol><p>Citation styles should be freely available, up to date, and complete. Switching between styles should be easy, and citation output should automatically localize to the desired language.</p><p>We think we’ve come quite far toward reaching these goals with our most recent release, CSL 1.0. The <a href="https://web.archive.org/web/20120525124720/http://citationstyles.org/downloads/specification.html">CSL 1.0 specification</a> covers a wide range of citation rules, and offers advanced features like automatic localization of date formats, terms and punctuation, support for in-field rich text and extensive support for the rendering and disambiguation of names. The first standalone CSL 1.0 processor (the JavaScript <a href="https://web.archive.org/web/20120525124720/http://bitbucket.org/fbennett/citeproc-js/">citeproc-js</a>) is currently being integrated into both Zotero and Mendeley, and is receiving attention for deployment on both the client and the server.</p><h3 id="4-can-you-tell-me-how-csl-was-developed">4. Can you tell me how CSL was developed?</h3><p><strong><strong>Rintze Zelle</strong></strong>: CSL is the brainchild of Bruce D’Arcus, an associate professor of Geography at Miami University of Ohio. The language was initially implemented for integration into OpenOffice.org, but only became popular in 2006 when Zotero, the first reference manager to use CSL, was released. In these early days major contributions were made to CSL by Zotero developer Simon Kornblith. Subsequently, the Zotero project successfully fostered an active user community, with many users contributing styles to a growing repository of CSL styles.</p><p>The year 2008 was a watershed of new developments. Mendeley was released, the second reference manager to use CSL for its citation formatting. Andrea Rossato released the first standalone CSL processor (<a href="https://web.archive.org/web/20120525124720/http://code.haskell.org/citeproc-hs/">citeproc-hs</a>) for use with the <a href="https://web.archive.org/web/20120525124720/http://johnmacfarlane.net/pandoc/">Pandoc</a> text processing system. Also in 2008, two Zotero users who enjoyed the program but felt that CSL could be further improved joined Bruce in CSL development: myself, at that time a PhD researcher in biotechnology at Delft University of Technology in the Netherlands, and Frank Bennett, Jr., an associate law professor at Nagoya University in Japan. Together with Andrea, our different academic and geographic backgrounds proved very useful in CSL development. In preparation for major backward-incompatible changes, CSL 0.8 was released in 2009, and in Spring 2010 CSL 1.0 saw the light of day. The <a href="https://web.archive.org/web/20120525124720/http://citationstyles.org/2010/03/22/citation-style-language-1-0/">1.0 release</a> was accompanied by a move to a new website at <a href="https://web.archive.org/web/20120525124720/http://citationstyles.org/">citationstyles.org</a>, and included improved documentation in the form of a full language specification. CSL development has now calmed down a bit as we await the integration of CSL 1.0 by Zotero and Mendeley.</p><p><strong><strong>Ian Mulvany</strong></strong>: We at Mendeley have been using the Citation Style Language for quite a while now. We think it is an amazing project and we are very strongly committed to working with the CSL community in encouraging uptake. We get a lot of feedback from our users and one area that they constantly run into problems with is the need to be able to format a citation in just such a manner. The CSL project is the best way for us to be able to support the needs of our users with these kinds of requests. Our developers have been pushing patches upstream to the <a href="https://web.archive.org/web/20120525124720/http://bitbucket.org/fbennett/citeproc-js/wiki/Home">citeproc-js</a> project, particularly <a href="https://web.archive.org/web/20120525124720/http://www.mendeley.com/profiles/carles-pina/">Carles Pina</a>.</p><p>We have added a cut and paste stylebox on our article pages. If you have a look at a <a href="https://web.archive.org/web/20120525124720/http://www.mendeley.com/research/karhunenloeve-eigenvalue-problems-cosmology-we-tackle-large-data-sets/">sample paper</a> you will now see a little citeproc-js driven “Cite this document” box that lets you copy and paste formatted citations in several popular citation styles. We have also been supporting the creation of a <a href="https://web.archive.org/web/20120525124720/http://bitbucket.org/csledit/csl-wysiwyg-editor">WISYWIG citation style editor</a>. The status of the project is that most of the code is complete and we just need to work on getting it integrated into our client, and figuring out the best way to manage the creation of more styles, and how that will work with the CSL community.</p><p>One of the things that we have been discussing with Bruce D’Arcus is how to manage the redistribution of new styles, and how to make sure that corrupt styles don’t propagate, and that people get the style that they are looking for. If people want to contribute there is a lot of activity on the <a href="https://web.archive.org/web/20120525124720/https://lists.sourceforge.net/lists/listinfo/xbiblio-devel">mailing list</a> of the CSL project. One thing we think we hope Mendeley can help with is reporting usage statistics on specific style files, so at least people can find the most popular version of a CSL file for a given style.</p><h3 id="5-where-can-a-user-find-more-csl-citation-styles-is-it-easy-to-modify-a-csl-style">5. Where can a user find (more) CSL citation styles? Is it easy to modify a CSL style?</h3><p><strong><strong>Rintze Zelle</strong></strong>: While anyone is free to write and host their own CSL styles, most CSL styles that are in use are available through the <a href="https://web.archive.org/web/20120525124720/http://www.zotero.org/styles">Zotero Style Repository</a> (many of these styles are licensed under a <a href="https://web.archive.org/web/20120525124720/http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons license</a>). We have to admit that editing CSL styles currently requires some technical skill and knowledge of XML. This hasn’t kept members of the Zotero user community from creating over a thousand CSL styles, but we do recognize that user friendly editing of styles is a very important feature. We therefore applaud Mendeley’s effort to create an online CSL editor.</p><h3 id="6-should-publishers-care-about-csl">6. Should publishers care about CSL?</h3><p><strong><strong>Ian Mulvany</strong></strong>: As I pointed out the big publishers don’t care about the submission format, but they have not really done a good job of communicating that to their editorial boards. Smaller publishers don’t have the resources to totally reformat submissions, and beyond academic publishing there are a huge number of people who just need to format citations. There is a huge waste of people’s time in reformatting papers for submissions, in fixing styles according to changing requirements from departments, when what should matter is the content. I’d love to get to a point where every publisher accepted the same type of XML input, and our authoring tools all created content conforming to that input format. Citations should be a DOI or other HTTP URI that can be rendered into the appropriate format using CSL and an API.</p><p><em><em>Martin Fenner: The Open Access publishers BioMed Central and PLoS plan to add a CSL style download link to their author instruction pages. I hope that more publishers follow this example.</em></em></p><h3 id="7-do-you-want-to-talk-about-future-plans-for-csl">7. Do you want to talk about future plans for CSL?</h3><p><strong><strong>Rintze Zelle</strong></strong>: We’re very excited about the work Zotero and Mendeley developers are doing to update their programs to support CSL 1.0. The update path should be relatively smooth for users as styles can be automatically updated to the CSL 1.0 format, although styles will often need to be edited to take full advantage of all the new features. Zotero Everywhere <a href="https://web.archive.org/web/20120525124720/http://www.zotero.org/blog/zoteros-next-big-step/">was announced</a> earlier this week and will include a web citation formatting service based on citeproc-js.</p><p>There are two things we consider crucial to the further development of CSL: one, we still lack an easy way for users to modify existing styles, although we’re hopeful that <a href="https://web.archive.org/web/20120525124720/http://bitbucket.org/csledit/csl-wysiwyg-editor/">Mendeley’s CSL editor</a> will soon fill this gap. Secondly, we feel there is a need for a more full-featured online style repository which allows users to find their style of choice, to add comments, and to propose style changes.</p><p>The goal of CSL is to make citing formatting easier at a general level, across all fields and in all languages. This can only be achieved through a collaborative endeavor, and here we think publishers also share some responsibility. By providing high quality item metadata through robust standards (like <a href="https://web.archive.org/web/20120525124720/http://unapi.info/">unAPI</a> or <a href="https://web.archive.org/web/20120525124720/http://ocoins.info/">COinS</a>), by freely providing clear, correct and complete style guidelines or opting for a standard citation style (like APA, Chicago or <a href="https://web.archive.org/web/20120525124720/http://www.mhra.org.uk/Publications/Books/StyleGuide/download.shtml">MHRA</a>), and perhaps even by creating and hosting their own CSL styles, publishers can make our work, and that of authors, so much easier.</p><p>With broad participation and support, we believe that CSL can benefit all fields of scholarship in a similar way Oren Patashnik’s BibTeX helped the sciences, by (further) streamlining the publication process, improving access to metadata of materials of all kinds, and by allowing scholars to spend more of their time on the core of their research.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Zotero soon is Everywhere]]></title>
            <link>https://blog.martinfenner.org/posts/zotero-soon-is-everywhere</link>
            <guid>22172c41-4ee6-46f1-8076-621eb793c5f9</guid>
            <pubDate>Wed, 22 Sep 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Whereas most reference managers are either standalone applications and/or
web-based, Zotero works as a Firefox plugin. This approach has many advantages,
but is not for everybody. So today Zotero announced Zotero Everywhere
[https://web.archive.org/web/20120603054912/http://www.zotero.org/blog/zoteros-next-big-step/]
:

 * A browser pluging for Microsoft Internet Explorer, Apple Safari and Google
   Chrome, and standalone desktop version for Windows, Mac and Linux
 * An expanded Zotero web API t]]></description>
            <content:encoded><![CDATA[<p>Whereas most reference managers are either standalone applications and/or web-based, Zotero works as a Firefox plugin. This approach has many advantages, but is not for everybody. So today Zotero announced <a href="https://web.archive.org/web/20120603054912/http://www.zotero.org/blog/zoteros-next-big-step/">Zotero Everywhere</a>:</p><ul><li>A browser pluging for Microsoft Internet Explorer, Apple Safari and Google Chrome, and standalone desktop version for Windows, Mac and Linux</li><li>An expanded Zotero web API that gives developers full read and write access to all of Zotero’s features.</li></ul><p>Zotero Everywhere goes beyond allowing users to use Zotero with other browsers. The enhanced API will give developers the chance to build an infrastructure around Zotero, from mobile applications to integration with other services. No dates were given about the availability of Zotero Everywhere. The project is funded by the Andrew W. Mellon Foundation.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Letter to The Scientist]]></title>
            <link>https://blog.martinfenner.org/posts/letter-to-the-scientist</link>
            <guid>68c13cf5-6b15-44cc-8a8c-86059106e065</guid>
            <pubDate>Mon, 20 Sep 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Dear Scientist,

last week you published an interesting article by Christian Specht about 
Mutations of citations
[https://web.archive.org/web/20120525054056/http://www.the-scientist.com/news/display/57689/]
. Dr. Specht found more than 600 wrong citations for the paper by Laemmli
(Laemmli 1970), which has been cited at least 88633 times according to Scopus.

Laemmli UK. Cleavage of Structural Proteins during the Assembly of the Head of
Bacteriophage T4. Nature. 1970;227:680-685. https://doi.org]]></description>
            <content:encoded><![CDATA[<p>Dear Scientist,</p><p>last week you published an interesting article by <strong><strong>Christian Specht</strong></strong> about <a href="https://web.archive.org/web/20120525054056/http://www.the-scientist.com/news/display/57689/">Mutations of citations</a>. Dr. Specht found more than 600 wrong citations for the paper by Laemmli (Laemmli 1970), which has been cited at least 88633 times according to Scopus.</p><p><strong><strong>Laemmli UK</strong></strong>. Cleavage of Structural Proteins during the Assembly of the Head of Bacteriophage T4. <em><em>Nature</em></em>. 1970;227:680-685. https://doi.org/<a href="https://web.archive.org/web/20120525054056/http://dx.doi.org/10.1038/227680a0">10.1038/227680a0</a></p><p>I was intrigued by the “sequence alignment” in Fig. 1a which clearly demonstrated that point mutations at 2<strong><strong>2</strong></strong>7 and 68<strong><strong>0</strong></strong> are particularly common, and that some mutations are inherited between overlapping groups of scientists. Of particular interest is the “complete nonsense mutation” that attributes the citation to the journal <em><em>Science</em></em>.</p><p>However, the author failed to demonstrate that the citation mutations had a paper-not-found phenotype or whether they were simply silent mutations. Missing is also an analysis of whether the mutations</p><ul><li>originated with the paper authors (who by now should all be using reference managers that automatically import citations),</li><li>were introduced by the publisher during manuscript production (many journals use tools such as <a href="https://web.archive.org/web/20120525054056/http://blogs.nature.com/mfenner/2009/05/01/extyles-interview-with-elizabeth-blake-and-bruce-rosenblum">eXtyles</a> to check and fix citations in manuscripts), or</li><li>first appeared in the scientific databases that stored the citations (Specht used Web of Science).</li></ul><p>Of particular interest would be whether there is a decrease in mutation rate over time, as automated tools have increased the fidelity of the citation process, and whether any citation style was particularly prone to mutations (no citation style uses checksums). As a researcher I suggest that the burden of proofreading should rest not with paper authors, and that journal and database publishers invest in appropriate citation repair mechanisms. And please use the DOI, even the paper by Laemmli (Laemmli 1970) has one.</p><h2 id="references">References</h2><p><strong><strong>Laemmli UK</strong></strong>. Cleavage of Structural Proteins during the Assembly of the Head of Bacteriophage T4. <em><em>Nature</em></em>. 1970;227:680-685. https://doi.org/<a href="https://web.archive.org/web/20120525054056/http://dx.doi.org/10.1038/227680a0">10.1038/227680a0</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Update of Reference Manager Overview Chart]]></title>
            <link>https://blog.martinfenner.org/posts/update-of-reference-manager-overview-chart</link>
            <guid>556ee8e8-20da-4930-9fae-9fab7452a255</guid>
            <pubDate>Sun, 19 Sep 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[In March 2009
[https://web.archive.org/web/20120603140937/http://blogs.nature.com/mfenner/2009/03/15/reference-manager-overview] 
I posted an overview chart of how popular reference managers have implemented
some important features. I’ve since updated this chart several times, and the
chart probably has outgrown the format of a blog post. For that reason I now
made the chart available

 * on a dedicated page with an easy to remember URL (http://bit.ly/refman
   [https://web.archive.org/web/20120]]></description>
            <content:encoded><![CDATA[<p>In <a href="https://web.archive.org/web/20120603140937/http://blogs.nature.com/mfenner/2009/03/15/reference-manager-overview">March 2009</a> I posted an overview chart of how popular reference managers have implemented some important features. I’ve since updated this chart several times, and the chart probably has outgrown the format of a blog post. For that reason I now made the chart available</p><ul><li>on a dedicated page with an easy to remember URL (<a href="https://web.archive.org/web/20120603140937/http://bit.ly/refman">http://bit.ly/refman</a>),</li><li>as PDF file for downloading and printing,</li><li>under a <a href="https://web.archive.org/web/20120603140937/http://creativecommons.org/licenses/by/3.0/">Creative Commons Attribution</a> license (like all content here on PLoS Blogs) for redistribution and reuse.</li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[And who are you?]]></title>
            <link>https://blog.martinfenner.org/posts/and-who-are-you</link>
            <guid>5d929cab-6f43-4b5c-aaac-daf9716a9f80</guid>
            <pubDate>Fri, 17 Sep 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Dear reader,

two weeks ago I moved this blog from Nature Network
[https://web.archive.org/web/20120525131414/http://blogs.nature.com/mfenner] to 
PLoS Blogs. I hope that most of my old readers have followed me here, and that I
might even have a few new readers.

Flickr image by grewlike.

Ed Yong
[https://web.archive.org/web/20120525131414/http://blogs.discovermagazine.com/notrocketscience/] 
recently talked about how he interacts with his many, many blog readers
[https://web.archive.org/web/20]]></description>
            <content:encoded><![CDATA[<p>Dear reader,</p><p>two weeks ago I moved this blog from <a href="https://web.archive.org/web/20120525131414/http://blogs.nature.com/mfenner">Nature Network</a> to <strong><strong>PLoS Blogs</strong></strong>. I hope that most of my old readers have followed me here, and that I might even have a few new readers.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120525131414im_/http://farm1.static.flickr.com/67/204929063_d90b9a9726.jpg" class="kg-image" alt="The Reader 1770-72"></figure><p><em><em>Flickr image by grewlike.</em></em></p><p><a href="https://web.archive.org/web/20120525131414/http://blogs.discovermagazine.com/notrocketscience/">Ed Yong</a> recently <a href="https://web.archive.org/web/20120525131414/http://www.onemanandhisblog.com/archives/2010/09/science_online_bloggers_commenters_and_t.html">talked about how he interacts with his many, many blog readers</a>. One important strategy he uses is to ask his readers for feedback in regular intervals – he calls it delurking. AJ Cann has picked up the idea, and <a href="https://web.archive.org/web/20120525131414/http://www.microbiologybytes.com/blog/2010/09/15/you-like/">on Wednesday asked the readers of his blog MicrobiologyBytes</a> a few questions. I like them and have blatantly reused most of them:</p><ul><li>Who are you and what’s your background?</li><li>How long have you been reading Gobbledygook?</li><li>Where do you read Gobbledygook – website or RSS?</li><li>What do you like best about Gobbledygook?</li><li>How could I make Gobbledygook better for you?</li></ul><p>It would be kind if you could provide some feedback in the comments section below.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Starting a reading list for Gobbledygook]]></title>
            <link>https://blog.martinfenner.org/posts/starting-a-reading-list-for-goobledygook</link>
            <guid>22d21c1b-7401-4135-bf9b-acd588d10f88</guid>
            <pubDate>Thu, 16 Sep 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[All science bloggers do a lot of reading for background information, or write
blog posts based on a (newly published) paper, blog post or news item. So I
thought that it would be a good idea to collect those references in a single
place.

Flickr photo by margolove.Reading lists are perfect for this, and they are easy
to create and maintain with web-based reference managers. Reading lists are used
in teaching, e.g. to provide a list of required reading material for a class.
But I can also see a n]]></description>
            <content:encoded><![CDATA[<p>All science bloggers do a lot of reading for background information, or write blog posts based on a (newly published) paper, blog post or news item. So I thought that it would be a good idea to collect those references in a single place.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120604134625im_/http://farm2.static.flickr.com/1359/1252522330_78b53d7e16.jpg" class="kg-image" alt="Day 14 - Visual Representation of a Reading List"><figcaption>Flickr photo by margolove.</figcaption></figure><p>Reading lists are perfect for this, and they are easy to create and maintain with web-based reference managers. Reading lists are used in teaching, e.g. to provide a list of required reading material for a class. But I can also see a number of benefits for science blogs:</p><ul><li>they help the blogger to organize his background material for writing</li><li>they help the reader find and keep referenced material</li><li>they can provide additional reading not mentioned in the blog post</li></ul><p>There are several good tools for reading lists. I decided to use <a href="https://web.archive.org/web/20120604134625/http://www.citeulike.org/">CiteULike</a>, because what I really want here is a social bookmarking tool that understands references. My Goobledygook reading list is <a href="https://web.archive.org/web/20120604134625/http://www.citeulike.org/group/13987/library">here</a>, the RSS feed is <a href="https://web.archive.org/web/20120604134625/http://www.citeulike.org/rss/group/13987/library">here</a>, and the most recent items in the reading list are also shown in a new sidebar to the right. I started with references to papers, but plan to also include blog posts and other web resources.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Peer Review at the Scholarly Kitchen]]></title>
            <link>https://blog.martinfenner.org/posts/peer-review-at-the-scholarly-kitchen</link>
            <guid>d9e2c91c-8d44-4734-be5c-1cf318c7e3e1</guid>
            <pubDate>Tue, 14 Sep 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[The Scholarly Kitchen
[https://web.archive.org/web/20120525093945/http://scholarlykitchen.sspnet.org/] 
is a group blog started by the Society for Scholarly Publishing
[https://web.archive.org/web/20120525093945/http://www.sspnet.org/] in 2008. The
blog posts by authors Kent Anderson, Phil Davis, David Crotty, Michael Clarke,
etc. are an always interesting – and often thought-provoking – read about
scholarly publishing. Two recent posts looked at peer review.

The “Burden” of Peer Review
[https:]]></description>
            <content:encoded><![CDATA[<p><a href="https://web.archive.org/web/20120525093945/http://scholarlykitchen.sspnet.org/">The Scholarly Kitchen</a> is a group blog started by the <a href="https://web.archive.org/web/20120525093945/http://www.sspnet.org/">Society for Scholarly Publishing</a> in 2008. The blog posts by authors Kent Anderson, Phil Davis, David Crotty, Michael Clarke, etc. are an always interesting – and often thought-provoking – read about scholarly publishing. Two recent posts looked at peer review.</p><p><a href="https://web.archive.org/web/20120525093945/http://scholarlykitchen.sspnet.org/2010/08/31/the-burden-of-peer-review/">The “Burden” of Peer Review</a><br>In this blog post <strong><strong>David Crotty</strong></strong> argues that we overestimate the amount of time the typical researcher spends doing peer review. In his informal survey most researchers review 1-3 papers per month, and those reviewing many more often do so voluntarily (e.g. because they sit on an editorial board). I haven’t yet checked whether there are any formal surveys on the workload of peer review.</p><p><a href="https://web.archive.org/web/20120525093945/http://scholarlykitchen.sspnet.org/2010/09/10/post-publication-review-when-the-dialog-of-science-has-become-a-monologue/">Post-publication Review: Is the Dialog of Science Really a Monologue?</a><br>In August the BMJ <a href="https://web.archive.org/web/20120525093945/http://dx.doi.org/10.1136/bmj.c3926">published a paper</a> that looked at the adequacy of author replies to electronic letters to the editor. The cohort study found that authors are reluctant to respond to criticism to their work. In a <a href="https://web.archive.org/web/20120525093945/http://dx.doi.org/10.1136/bmj.c3803">companion editorial</a>, David Schriger and Douglas Altman write about the possible reasons for this inadequate uptake of post-publication peer review, and that we need a change in culture to value public discussion<em><em><em><em><em><em>. </em></em></em></em></em></em><strong><strong>Philip David</strong></strong> summarized the two papers and concluded that <em><em>post-publication review may continue to be spotty and unreliable</em></em>.</p><p>I would disagree with Philip Davis about the conclusions that can be drawn from the cohort study. We all know that many papers receive few if any online comments. But we should rather think about where we could be 3-5 years from now, and how to get there. A recent editorial by Thomas Liesegang in the <em><em>Journal of Ophtalmology </em></em>is very relevant to this discussion (<a href="https://web.archive.org/web/20120525093945/http://dx.doi.org/10.1016/j.ajo.2009.11.015">Peer review should continue after publication</a>, link via <a href="https://web.archive.org/web/20120525093945/http://ese-bookshelf.blogspot.com/2010/09/b-peer-review-should-continue-after.html">EASE Journal Blog</a>). And Richard Smith gives a wonderful and much broader definition of post-publication peer review in a <a href="https://web.archive.org/web/20120525093945/http://www.bmj.com/content/341/bmj.c3803.full/reply#bmj_el_240798">response</a> to the BMJ editorial:</p><blockquote>I would define post-publication review as the process whereby scientists and others decide whether a piece of work matters or not. I suggest that this doesn’t happen much through debate in the correspondence pages of journals, but rather through scientists and other consumers of research recommending others to pay attention to a piece of research, conducting other studies off the back of it, absorbing it into systematic reviews, beginning to act on its conclusions, throwing it in the bin, and taking a thousand other actions that constitute the “market of ideas.”</blockquote>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Data is the new soil]]></title>
            <link>https://blog.martinfenner.org/posts/data-is-the-new-soil</link>
            <guid>0d365b34-4ec8-4de5-938a-f746fefa234c</guid>
            <pubDate>Mon, 13 Sep 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[One of the main themes at the Science Online London Conference
[https://web.archive.org/web/20120525101008/http://www.scienceonlinelondon.org/programme.php] 
last weekend was data, with several sessions devoted to publishing primary
research data, connecting scientific data from various resources, and a
scientific experiment taking place before and during the conference (Green
Chain
reaction
[https://web.archive.org/web/20120525101008/http://scienceonlinelondon.wikidot.com/topics:green-chain-rea]]></description>
            <content:encoded><![CDATA[<p>One of the main themes at the <a href="https://web.archive.org/web/20120525101008/http://www.scienceonlinelondon.org/programme.php">Science Online London Conference</a> last weekend was <strong><strong>data</strong></strong>, with several sessions devoted to publishing primary research data, connecting scientific data from various resources, and a scientific experiment taking place before and during the conference (<a href="https://web.archive.org/web/20120525101008/http://scienceonlinelondon.wikidot.com/topics:green-chain-reaction">Green Chain reaction</a>).</p><p><a href="https://web.archive.org/web/20120525101008/http://www.informationisbeautiful.net/visualizations/">David McCandless</a> talked about Data Visualization, a fascinating way to filter and make sense of the enormous amounts of data that we produce. This is of course also very relevant to science. Those that couldn’t attend the conference, or were in one of the parallel sessions like myself (<a href="https://web.archive.org/web/20120525101008/http://blogs.plos.org/mfenner/2010/09/07/orcid-as-unique-author-identifier-what-is-it-good-for-and-should-we-worry-or-be-happy/">speaking about author identifiers</a>), can watch this video of a <a href="https://web.archive.org/web/20120525101008/http://www.ted.com/talks/david_mccandless_the_beauty_of_data_visualization.html">TED talk</a> that David gave in July:</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Lanyrd: a new Twitter mashup for conferences]]></title>
            <link>https://blog.martinfenner.org/posts/lanyrd-a-new-twitter-mashup-for-conferences</link>
            <guid>8771a7ea-2d90-434f-b935-9d64677931b0</guid>
            <pubDate>Sun, 12 Sep 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Lanyrd [https://web.archive.org/web/20120525041249/http://lanyrd.com/] is a new
social directory for conferences launched just a few days ago. It integrates
with Twitter to allow you to list conferences you are organizing, speaking at,
attending, or just following. It is a social directory in the sense that
everyone can edit this information (I don’t know who started the Science Online
London 2010
[https://web.archive.org/web/20120525041249/http://lanyrd.com/2010/science-online-london/] 
page), ]]></description>
            <content:encoded><![CDATA[<p><a href="https://web.archive.org/web/20120525041249/http://lanyrd.com/">Lanyrd</a> is a new social directory for conferences launched just a few days ago. It integrates with Twitter to allow you to list conferences you are organizing, speaking at, attending, or just following. It is a social directory in the sense that everyone can edit this information (I don’t know who started the <a href="https://web.archive.org/web/20120525041249/http://lanyrd.com/2010/science-online-london/">Science Online London 2010</a> page), and that you can see the conferences of your Twitter contacts. Future versions of Lanyrd will do a lot more (you can see some of the upcoming features at the <a href="https://web.archive.org/web/20120525041249/http://lanyrd.com/2010/dconstruct/">dConstruct 2010</a> page), but the site is already useful and just looks gorgeous.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120525041249im_/http://lanyrd.com/static/img/logo/lanyrd-158x158.png" class="kg-image" alt="Lanyrd Logo"></figure><p>The Lanyrd blog launched yesterday and <a href="https://web.archive.org/web/20120525041249/http://lanyrd.com/blog/2010/welcome/">the first post</a> contains some interesting background information. Lanyrd is a joint project by web developers and newly-weds Simon Willison and Natalie Downe. They launched Lanyrd from Morocco while on an extended honeymoon and don’t plan to be back to England before the end of next year.</p><p>Sites such as <strong><strong>Eventbrite</strong></strong>, <strong><strong>Upcoming</strong></strong> or <strong><strong>Slideshare</strong></strong> have been around for a while. Twitter is becoming an increasingly useful tool for conferences (<a href="https://web.archive.org/web/20120525041249/http://blogs.plos.org/mfenner/2010/09/12/lanyrd-a-new-twitter-mashup-for-conferences/The%20Great%20Science%20Online%20London%20Tweetup">The Great Science Online London Tweetup</a>). And Lanyrd is one of the nicest examples that I have seen so far that integrates Twitter into a service targeted at conference attendees.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Peer review is not a piece of cake]]></title>
            <link>https://blog.martinfenner.org/posts/peer-review-is-not-a-piece-of-cake</link>
            <guid>c06b9473-9515-43c9-b006-f356158e38e2</guid>
            <pubDate>Thu, 09 Sep 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[On Monday Jenny Rohn published a blog post Peer review is no picnic
[https://web.archive.org/web/20120525040917/http://www.guardian.co.uk/science/blog/2010/sep/06/peer-review] 
that concludes:

> So the next time you hear someone asserting that scientists aren’t critical, of
their own work or that of their colleagues, remember that if a finding has made
its way into a reputable journal, it’s most likely despite every last objection
that the researcher and all of his lab-mates could come up with ]]></description>
            <content:encoded><![CDATA[<p>On Monday <strong><strong>Jenny Rohn</strong></strong> published a blog post <a href="https://web.archive.org/web/20120525040917/http://www.guardian.co.uk/science/blog/2010/sep/06/peer-review">Peer review is no picnic</a> that concludes:</p><blockquote><em><em><em><em><em><em><em><em>So the next time you hear someone asserting that scientists aren’t critical, of their own work or that of their colleagues, remember that if a finding has made its way into a reputable journal, it’s most likely despite every last objection that the researcher and all of his lab-mates could come up with – to say nothing of those nasty peer reviewers.</em></em></em></em></em></em></em></em></blockquote><p>As always with Jenny’s blog posts, the discussion is as interesting as the blog post itself. Joe Dunckley, Maxine Clarke, Austin Elliott, David Colquhoun, and others wrote thoughtful comments about peer review (the comments are now closed). Peer review is an important topic that I want to write more about on this blog (I wrote <a href="https://web.archive.org/web/20120525040917/http://blogs.nature.com/mfenner/2009/07/13/the-value-of-peer-review">The Value of Peer Review</a> after attending SciFoo 2009).</p><p>Jenny normally writes at Nature Network (<a href="https://web.archive.org/web/20120525040917/http://blogs.nature.com/ue19877e8/">Mind the Gap</a>), but for this post was invited as guest blogger for the <a href="https://web.archive.org/web/20120525040917/http://www.guardian.co.uk/science/series/blog-festival">Guardian science blogs</a>. They launched just last week and for the <a href="https://web.archive.org/web/20120525040917/http://www.guardian.co.uk/science/series/blog-festival">Blog Festival</a> blog have started to invite guest bloggers such as <a href="https://web.archive.org/web/20120525040917/http://www.guardian.co.uk/science/blog/2010/sep/03/amateur-astronomy-telescope-night-sky">Stephen Curry</a> (Nature Network) and <a href="https://web.archive.org/web/20120525040917/http://www.guardian.co.uk/science/blog/2010/sep/01/psychedelic-drugs-mental-illness">Mo Costandi</a> (scienceblogs.com).</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[BioMed Central drafts position statement on open data]]></title>
            <link>https://blog.martinfenner.org/posts/biomed-central-drafts-position-statement-on-open-data</link>
            <guid>2a9edc27-ab49-4a69-ba22-2a1279c79bf6</guid>
            <pubDate>Wed, 08 Sep 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Last week BioMed Central published a draft position statement on open data (PDF
[https://web.archive.org/web/20120525103921/http://blogs.openaccesscentral.com/blogs/bmcblog/resource/opendatastatementdraft.pdf]
). Iain Hrynaszkiewicz explained the statement
[https://web.archive.org/web/20120525103921/http://blogs.openaccesscentral.com/blogs/bmcblog/entry/join_the_data_debate_draft] 
on the BioMed Central Blog, including the five Ws of open data:

 1. Why make data more open?
 2. What data to make]]></description>
            <content:encoded><![CDATA[<p>Last week <strong><strong>BioMed Central</strong></strong> published a draft position statement on open data (<a href="https://web.archive.org/web/20120525103921/http://blogs.openaccesscentral.com/blogs/bmcblog/resource/opendatastatementdraft.pdf">PDF</a>). <strong><strong>Iain Hrynaszkiewicz</strong></strong> <a href="https://web.archive.org/web/20120525103921/http://blogs.openaccesscentral.com/blogs/bmcblog/entry/join_the_data_debate_draft">explained the statement</a> on the BioMed Central Blog, including the five Ws of open data:</p><ol><li>Why make data more open?</li><li>What data to make more open?</li><li>Where to make data more open?</li><li>When to make data more open?</li><li>How to make data more open?</li></ol><p>Please comment on the statement.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ORCID as unique author identifier: what is it good for and should we worry or be happy?]]></title>
            <link>https://blog.martinfenner.org/posts/orcid-as-unique-author-identifier-what-is-it-good-for-and-should-we-worry-or-be-happy</link>
            <guid>1419da41-5f52-491d-92c9-d2399ff2e1b0</guid>
            <pubDate>Tue, 07 Sep 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[This was the title of the session with Geoff Bilder, Gudmundur Thorisson and
myself at the Science Online London Conference
[https://web.archive.org/web/20120525074318/http://www.scienceonlinelondon.org/programme.php?tab=abstracts#breakout12] 
last Saturday. Geoff first introduced the ORCID initiative, including these
principles:

From Slideshare presentation
[https://web.archive.org/web/20120525074318/http://www.slideshare.net/CrossRef/crossref-contributor-id] 
by Geoff Bilder.

We then talked ]]></description>
            <content:encoded><![CDATA[<p>This was the title of the session with <strong><strong>Geoff Bilder</strong></strong>, <strong><strong>Gudmundur Thorisson</strong></strong> and myself at the <a href="https://web.archive.org/web/20120525074318/http://www.scienceonlinelondon.org/programme.php?tab=abstracts#breakout12">Science Online London Conference</a> last Saturday. Geoff first introduced the ORCID initiative, including these principles:</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120525074318im_/http://blogs.plos.org/mfenner/files/2010/09/bilder.jpg" class="kg-image" alt></figure><p><a href="https://web.archive.org/web/20120525074318/http://www.slideshare.net/CrossRef/crossref-contributor-id"><em><em>From Slideshare presentation</em></em></a><em><em> by Geoff Bilder.</em></em></p><p>We then talked about two use cases for ORCID. I started with the manuscript submission scenario (<a href="https://web.archive.org/web/20120525074318/http://www.slideshare.net/mfenner/orcid-as-unique-author-identifier-what-is-it-good-for-and-should-we-worry-or-be-happy">my Slideshare presentation</a>), and I suggested that a critical mass of journals using ORCID would have these benefits to researchers:</p><ol><li>Reduced submission work</li><li>Automated CVs and publication lists</li><li>Find related works by author</li><li>Semantic meaning of authorship</li></ol><p>Semantic meaning of authorship (“corresponding author” or “analyzed microarray data”) is not a goal for the first ORCID release, but an interesting future perspective.</p><p>Gudmundur followed with a similar presentation about the research dataset submission scenario (<a href="https://web.archive.org/web/20120525074318/http://www.slideshare.net/gthorisson/thorisson-science-online-london-sep2010">his Slideshare presentation</a>).</p><p>The process is similar to journal submission, but interacts with repositories instead of journals. Gudmundur showed how this workflow could integrate the submission of a manuscript and the corresponding research data. He also demonstrated how datasets in repositories associated with a particular author could be discovered with the help of ORCID.</p><p>But most of the session was dedicated to discussion. As open data was one of the main themes of the conference, the meaning of “open” in ORCID was one of the topics. We talked about how the identifier ORCID could be combined with authentication such as the OAuth system. Phil Lord wanted a modification of the journal submission use case, in that the ORCIDs of authors should be provided with the manuscript, and not entered in the journal submission system. And we talked about whether ORCID should also be used for blog posts, Wikipedia submissions, Slideshare presentations, etc. – I argued that not everything needs to be attributed and measured (see my <a href="https://web.archive.org/web/20120525074318/http://blogs.plos.org/mfenner/2010/09/04/unmeasurable-science/">previous blog post</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120525074318im_/http://farm5.static.flickr.com/4096/4953236107_349c7d7b02_m.jpg" class="kg-image" alt="Konferenspåse"><figcaption>Flickr photo by malin_c.</figcaption></figure><p>Today the ORCID initiative reached another important milestone. ORCID is<a href="https://web.archive.org/web/20120525074318/http://orcid.org/node/166"> now an official non-profit organization</a> with a first board of directors. I am looking forward to serve on this board of directors.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Great Science Online London Tweetup]]></title>
            <link>https://blog.martinfenner.org/posts/the-great-science-online-london-tweetup</link>
            <guid>6485959f-2df2-4b84-8292-acb9af557afe</guid>
            <pubDate>Mon, 06 Sep 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[The 3rd Annual Science Online London Conference
[https://web.archive.org/web/20120524054709/http://www.scienceonlinelondon.org/] 
took place last Friday and Saturday. For me it was again a fantastic event. Matt
Brown is collecting all blog posts, pictures, videos, etc. over at this Nature
Network post
[https://web.archive.org/web/20120524054709/http://blogs.nature.com/u6e5b2ce1/2010/09/05/science-online-london-2010-index-of-blog-posts-videos-photos-and-stuff]
. As one of the organizers of the co]]></description>
            <content:encoded><![CDATA[<p>The 3rd Annual <a href="https://web.archive.org/web/20120524054709/http://www.scienceonlinelondon.org/">Science Online London Conference</a> took place last Friday and Saturday. For me it was again a fantastic event. Matt Brown is collecting all blog posts, pictures, videos, etc. over at <a href="https://web.archive.org/web/20120524054709/http://blogs.nature.com/u6e5b2ce1/2010/09/05/science-online-london-2010-index-of-blog-posts-videos-photos-and-stuff">this Nature Network post</a>. As one of the organizers of the conference (mainly helping with the program) I was pretty nervous, and I have to admit that I didn’t sleep well Thursday and Friday. And much better Saturday.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120524054709im_/http://farm5.static.flickr.com/4133/4964420222_41f382e41c.jpg" class="kg-image" alt="IMG_0087"></figure><p><em><em>Flickr photo by Mendeley.com with me on far left.</em></em></p><p>One important reason to go to a conference is of course to meet the members of your community in real life. It was great to talk to many old friends again, and to meet for the first time in person people I knew from online interactions (Alex Knoll, Brian Mossop, David Dobbs, Matt Cockerill, Jen Mellinn, William Gunn and Jim Caryl and many others not mentioned here). Of course I also missed several friends that came to the first two conferences.</p><p>Of the many impressions of the conference I want to focus on one: <strong><strong>Twitter</strong></strong>.</p><p>Twitter has of course become a mainstream tool to follow conferences. Microblogging for the 2008 conference happened mainly on FriendFeed (the much better tool for conference microblogging and described in this <em><em><a href="https://web.archive.org/web/20120524054709/http://dx.doi.org/10.1371/journal.pcbi.1000263">PLoS Comp Biol</a></em></em> paper). Every year has seen an increase in Twitter use, and both <a href="https://web.archive.org/web/20120524054709/http://summarizr.labs.eduserv.org.uk/?hashtag=scio10">ScienceOnline2010</a> (#scio10) in January and <a href="https://web.archive.org/web/20120524054709/http://summarizr.labs.eduserv.org.uk/?hashtag=solo10">last week’s conference</a> produced more than 6000 tweets. Towards the end of the conference on Saturday the #solo10 hashtag was the <a href="https://web.archive.org/web/20120524054709/http://twitter.com/scottkeir/status/22986296735">#2 trend</a> for London, right before #happbirthdaybeyonce. People not attending the conference were starting to drop in the conversation. We easily beat mainstream science conferences such as the American Society of Clinical Oncology Meeting (#asco10) that I <a href="https://web.archive.org/web/20120524054709/http://blogs.nature.com/mfenner/2010/06/15/using-twitter-at-the-asco-conference">attended earlier this year</a> (almost 5000 tweets, but more than 30.000 attendees). Also interesting: more than 700 people used the #solo10 hashtag in their tweets, even though the conference had only 250 attendees (at least 140 of them with Twitter accounts, listed <a href="https://web.archive.org/web/20120524054709/http://twitter.com/LouWoodley/solo10-attendees">here</a>). And some of those not attending were very active twitterers, particularly Bora Zivkovic (@BoraZ).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120524054709im_/http://farm5.static.flickr.com/4113/4963334783_dcb3fb6163.jpg" class="kg-image" alt="#solo10 Word Cloud"><figcaption>Flickr image by Simon Cockell.</figcaption></figure><p>Simon Cockell has <a href="https://web.archive.org/web/20120524054709/http://blog.fuzzierlogic.com/archives/365">created</a> a Wordle with the conference tweets, which gives a good idea of the topics discussed. The word PDF is a good example. <a href="https://web.archive.org/web/20120524054709/http://www.mjrobbins.net/">Martin Robbins</a> said in the <strong><strong>Rebooting Science Journalism</strong></strong> panel discussion that “The PDF is an insult to science…” (rightfully so), and this resulted in a longer discussion in the Twitter backchannel.</p><p>That <a href="https://web.archive.org/web/20120524054709/http://alicerosebell.wordpress.com/2010/09/03/taking-science-journalism-upstream/">same panel discussion</a> also made the best use of Twitter as integral part of a session. <a href="https://web.archive.org/web/20120524054709/https://twitter.com/christineottery">Christine Ottery</a> was monitoring the Twitter feed and provided feedback for the panelists. We used Twitterwalls for most sessions, but the speakers often did not use information.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120524054709im_/http://farm5.static.flickr.com/4090/4963802359_790df7b240.jpg" class="kg-image" alt="IMG_0066"></figure><p><em><em>Flickr photo by Mendeley.com</em></em></p><p>Twitter several times was also mentioned as an important discovery tool, and I agree that it has become of the best tools to discover interesting stuff, blog posts, papers, etc. I couldn’t attend the <strong><strong>Recommendation Tools for Scientists</strong></strong> session (with Kevin Emamy and Jason Hoyt), so I don’t know whether Twitter as recommendation tool was discussed there.</p><p>It would be interesting to do a more detailed analysis of the #solo10 tweets, e.g. by analyzing which sessions were the most tweeted, what were the most popular keywords and links, or by categorizing tweets into different categories. This could make an interesting session at the <a href="https://web.archive.org/web/20120524054709/http://www.scienceonline2010.com/index.php/wiki/2011_Program_Suggestions">ScienceOnline2011</a> Conference next January – of course using some of the cool data visualization tricks that <a href="https://web.archive.org/web/20120524054709/http://www.davidmccandless.com/">David McCandless</a> showed us in his session.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Flipboard: PLoS BLOGs on the iPad]]></title>
            <link>https://blog.martinfenner.org/posts/flipboard-plos-blogs-on-the-ipad</link>
            <guid>7b99f148-56b4-4036-bffa-e685afde2e7c</guid>
            <pubDate>Sun, 05 Sep 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[The iPad owners among the PLoS BLOGs readers can use the free Flipboard
application to read all the great content that is written by my fellow ploggers
PLoGsters (as suggested by David Kroll
[https://web.archive.org/web/20120604164119/http://blogs.plos.org/takeasdirected/2010/09/03/latest-from-the-plogsters]
). In Flipboard simply create a new section and then add content to it by
following the @plosblogs Twitter account.

There are of course many other ways Flipboard can be used to follow scien]]></description>
            <content:encoded><![CDATA[<p>The iPad owners among the PLoS BLOGs readers can use the free Flipboard application to read all the great content that is written by my fellow ploggers PLoGsters (as suggested by <a href="https://web.archive.org/web/20120604164119/http://blogs.plos.org/takeasdirected/2010/09/03/latest-from-the-plogsters">David Kroll</a>). In Flipboard simply create a new section and then add content to it by following the @plosblogs Twitter account.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120604164119im_/http://farm5.static.flickr.com/4103/4959511883_06a51c32ce.jpg" class="kg-image" alt="PLoS Blogs on Flipboard"></figure><p>There are of course many other ways Flipboard can be used to follow science blogs, e.g. by following the @researchblogs Twitter account. More info can be found in <a href="https://web.archive.org/web/20120604164119/http://blogs.nature.com/mfenner/2010/08/01/flipboard-changes-the-way-we-use-twitter">this blog</a> posts I wrote last month.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Unmeasurable Science]]></title>
            <link>https://blog.martinfenner.org/posts/unmeasurable-science</link>
            <guid>200fde29-34bd-4bc2-8681-05df9fe6ade4</guid>
            <pubDate>Sat, 04 Sep 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[On Wednesday PLoS BLOGs launched with a splash. We (both PLoS BLOGs as a whole
and me individually) got a lot of positive feedback and words of encouragement –
so we are off to a good start. As both our community manager Brian Mossop and
myself are currently in London for the Science Online London Conference, we
could celebrate the launch in person. With a good pint of British ale Thursday
evening.

Today I want to talk about something that is sticking in my head since a
conversation a few weeks]]></description>
            <content:encoded><![CDATA[<p>On Wednesday PLoS BLOGs launched with a splash. We (both PLoS BLOGs as a whole and me individually) got a lot of positive feedback and words of encouragement – so we are off to a good start. As both our community manager Brian Mossop and myself are currently in London for the Science Online London Conference, we could celebrate the launch in person. With a good pint of British ale Thursday evening.</p><p>Today I want to talk about something that is sticking in my head since a conversation a few weeks ago with some friends (all esteemed professors in biology or medicine) over another beer. And this has of course been discussed before, both on this blog and elsewhere. Doing science is not only about doing exciting research, and communicating the results to your peers and the public. Measuring scientific output – of funded research projects, a particular researcher, or an institution – seems to be equally important. I would argue that in recent years this has even become the most important aspect of doing science. The successful researcher of today is not necessarily a brilliant mind, skillful experimenter or successful communicator, but a good manager of science. Grants need to be written, collaborations and networks built and maintained, and papers published.</p><p>This is all good and well in the sense that researchers should be held accountable for how they are using their funding, often from public sources. And we want to fund the right projects, i.e. those that have the highest likelihood of achieving something new and exciting. But there are two very big problems with this:</p><h3 id="we-don-t-really-know-how-to-evaluate-science-particularly-in-numbers-that-can-be-used-to-compare-research-projects-">We don’t really know how to evaluate science, particularly in numbers that can be used to compare research projects. </h3><p>This problem is aggravated by the fact that funding (and hiring) decisions are predictions based on past performance. Excellent science by definition is new and groundbreaking, and predicting scientific progress is really hard to do. The past performance of a researcher, the research environment he is working in (colleagues, scientific equipment, etc.), and of course the project outline written down in the proposal are all very helpful. But can we really  predict the next Nature or Cell paper before a project has even started? And the evaluation of scientific output is also extremely difficult. Do you just look at published papers? And if so, how do you evaluate the scientific impact of those papers? Through peer review? By the number of times they were cited? Citation counts have several problems, one of them is the fact that they take a few years to accumulate. The journal a paper is published in? How good an indicator of the impact of the individual paper is that? Should you rather look at download counts or other article-level metrics? We need to think much more about these important issues, as our funding (and hiring) decisions depend on it. I am very happy that I was invited to a workshop by the National Science Foundation this November that will talk about some of these issues. Unique digital author identifiers will play an increasing role in our efforts to tackle the technical aspects of this problem, and I will say something about the ORCID initiative.</p><h3 id="the-evaluation-of-science-is-taking-up-more-and-more-of-our-time-that-is-then-missing-time-for-doing-research-">The evaluation of science is taking up more and more of our time that is then missing time for doing research. </h3><p>Before the first experiment is even started, a project has taken months or sometimes even years of grant writing and grant reviewing. The regulatory requirements are also increasing, and in the case of clinical research involving patients (something I do) can be overwhelming. After a research project is finished, paper writing and reviewing (and writing a report for the funding agency) again takes many months. In the end it might have taken us two years to do the experiments, but five years from beginning to end of the project.</p><p>If we want researchers to do more research – and less grant writing, manuscript writing and peer reviewing (because all this output has to be evaluated by someone), we have to ask funding organizations and institutions to do something about this. There are many possible solutions, and some of them have already been realized:</p><ul><li>Grants can be given for longer periods of time, e.g. 5 years instead of 3 years</li><li>The review of grants could just look at the researcher, and doesn’t try to predict scientific discoveries based on the proposed work</li><li>Smaller grants, e.g. less than $50.000 don’t need to have a concluding report, a link to a paper published with the results should be enough. Similarly we might need fewer progress reports in larger grants.</li><li>Many aspects of grant and paper writing could be made less time-consuming by standardization and automation.</li></ul><p>There is a lot of potential in the last point, and the workflow is broken at many points. Why do researchers have to list their publications in their CV, this information is publicly available? Why does every funding organization want a slightly different format for their grant proposals, can’t we standardize this? Why don’t we have better paper writing tools? Microsoft Word doesn’t really now about the different required sections for a manuscript and is far from perfect for collaborative writing.</p><p>But I have to stop writing now, the second day of the Science Online London Conference is about to begin. I’m wearing my new PLoS BLOGs T-shirt and look forward to another great conference day. I will write about the conference in a separate post in the next fews, but for now let’s just say this conference is even better than in 2009.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Welcome to Gobbledygook]]></title>
            <link>https://blog.martinfenner.org/posts/welcome-to-gobbledygook</link>
            <guid>9a28381f-b575-4b90-88ed-ccc0891c166f</guid>
            <pubDate>Wed, 01 Sep 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Flickr photo by {Away until inspiration comes}.

In his introductory post
[https://web.archive.org/web/20120506093643/http://blogs.plos.org/blog/2010/08/31/the-niche-blog-network-lessons-from-the-past-visions-for-the-future/]
, PLoS community manager Brian Mossop talks about how PLoS Blogs came to life,
and the lessons learned from other blogging networks. For me personally it all
started a few weeks ago with a phone call one Friday evening from Pete Binfield
[https://web.archive.org/web/2012050]]></description>
            <content:encoded><![CDATA[<figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120506093643im_/http://farm3.static.flickr.com/2309/2187485704_a598c84739.jpg" class="kg-image" alt="Welcome"></figure><p><em><em>Flickr photo by {Away until inspiration comes}.</em></em></p><p>In his <a href="https://web.archive.org/web/20120506093643/http://blogs.plos.org/blog/2010/08/31/the-niche-blog-network-lessons-from-the-past-visions-for-the-future/">introductory post</a>, PLoS community manager Brian Mossop talks about how PLoS Blogs came to life, and the lessons learned from other blogging networks. For me personally it all started a few weeks ago with a phone call one Friday evening from <a href="https://web.archive.org/web/20120506093643/http://www.plos.org/about/people/one.php#pbinfield">Pete Binfield</a>, Publisher of PLoS ONE and the PLoS Community Journals.</p><p>Since August 2007 and until today I was blogging over at <a href="https://web.archive.org/web/20120506093643/http://blogs.nature.com/mfenner">Nature Network</a>. The focus of that blog was how the internet is changing scientific publishing. This is obviously a very broad topic. A lot of my blog posts looked at rather technical aspects, including new products and services (the <a href="https://web.archive.org/web/20120506093643/http://blogs.nature.com/mfenner/2010/08/29/elsevier-launches-sciverse-integrates-sciencedirect-scopus-more">last post</a> was about Elsevier’s SciVerse that launched last weekend).</p><p>I will continue to write about the same topics. I particularly like to continue <a href="https://web.archive.org/web/20120506093643/http://blogs.nature.com/mfenner/2010/08/31/bye-bye-nature-network">interviews</a>, and I want to write more recipes (<a href="https://web.archive.org/web/20120506093643/http://blogs.nature.com/mfenner/2009/08/08/recipe-distributing-papers-for-a-journal-club">Recipe: Distributing papers for a journal club</a>). Of course I also want to try different things, e.g. shorter blog posts of interesting stuff I find (something I currently post on Twitter). Please contact me if you have something related to scientific publishing that you want me to write about.</p><p>PLoS Blogs is not the only new science blogging network. <a href="https://web.archive.org/web/20120506093643/http://www.guardian.co.uk/science/blog/2010/aug/31/blogging-digital-media">Guardian Science Blogs</a> launched just yesterday, and <a href="https://web.archive.org/web/20120506093643/http://scientopia.org/blogs/">Scientopia</a> and <a href="https://web.archive.org/web/20120506093643/http://www.science3point0.com/blogs/">Science 3.0</a> in the last few weeks. And <a href="https://web.archive.org/web/20120506093643/http://scienceblogging.org/2010/08/19/some-thoughts-about-science-blog-aggregation/">Scienceblogging.org</a>, an aggregator of science blogs (to keep track of all those great blog posts), was launched by Anton Zuiker, Bora Zivkovic and Dave Munger two weeks ago.</p><p>This is a good week for science blogging for another reason. The <a href="https://web.archive.org/web/20120506093643/http://www.scienceonlinelondon.org/">Science Online London Conference</a> takes place this Friday and Saturday. Most relevant to the launch of PLoS Blogs is the panel discussion Friday afternoon <a href="https://web.archive.org/web/20120506093643/http://www.scienceonlinelondon.org/programme.php?tab=abstracts#panel2">The state of science blogging?</a></p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120506093643im_/http://www.scienceonlinelondon.org/graphics/logo.gif" class="kg-image" alt="Science Online London"></figure><p>There will of course be other exciting sessions. The panel <a href="https://web.archive.org/web/20120506093643/http://www.scienceonlinelondon.org/programme.php?tab=abstracts#panel1">“Rebooting” (aka the future of) science journalism</a> with David Dobbs, Ed Yong, Martin Robbins and Alice Bell should be a lot of fun. The closing panel discussion on Saturday <a href="https://web.archive.org/web/20120506093643/http://www.scienceonlinelondon.org/programme.php?tab=abstracts#panel3">If you build it, will they come?</a> will look at the adoption (or lack thereof) of  Web 2.0 tools for scholarly communication, based on a <a href="https://web.archive.org/web/20120506093643/http://www.rin.ac.uk/our-work/communicating-and-disseminating-research/use-and-relevance-web-20-researchers">recent report</a> by the Research Information Network. And a number of sessions focus on open data, a trending topic I’m personally very interested in. Together with Geoff Bilder and Gudmundur Thorisson I will moderate a session <a href="https://web.archive.org/web/20120506093643/http://www.scienceonlinelondon.org/programme.php?tab=abstracts#breakout11">ORCID as unique author identifier: what is it good for and should we worry or be happy?</a></p><p>Most importantly, Science Online London is a great opportunity to meet many of my fellow science bloggers in person, and this includes Brian Mossop, the PLoS Blogs community manager. Expect more reports from Science Online London in the next few days, for more up to date information please follow me on Twitter (<a href="https://web.archive.org/web/20120506093643/http://twitter.com/mfenner">@mfenner</a>, conference hashtag <a href="https://web.archive.org/web/20120506093643/http://twitter.com/#search?q=%23solo10">#solo10</a>) or try the <a href="https://web.archive.org/web/20120506093643/http://www.science3point0.com/solo10-2/">video stream</a> put together by Graham Steel.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Bye-Bye Nature Network]]></title>
            <link>https://blog.martinfenner.org/posts/bye-bye-nature-network</link>
            <guid>08341060-3e8f-4966-bbe6-fb719735af01</guid>
            <pubDate>Wed, 01 Sep 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[This is my last post for this Nature Network blog. Later today, I will start a
new blog somewhere else – also called Gobbledygook and covering the same topics.

Writing this blog here on Nature Network since August 2007 has been an
incredible experience, something that can't be covered in a single blog post. I
simply want to say thank you to all the people I interacted with over the years
– both online and in person. And a particular thanks for Matt, Corie, Anna and
Lou from Nature Network who m]]></description>
            <content:encoded><![CDATA[<p>This is my last post for this Nature Network blog. Later today, I will start a new blog somewhere else – also called Gobbledygook and covering the same topics.</p><p>Writing this blog here on Nature Network since August 2007 has been an incredible experience, something that can't be covered in a single blog post. I simply want to say thank you to all the people I interacted with over the years – both online and in person. And a particular thanks for Matt, Corie, Anna and Lou from Nature Network who made all this possible.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120525055619im_/http://farm4.static.flickr.com/3619/3378007233_f8d5e5a539.jpg" class="kg-image" alt="bye bye"><figcaption>Flickr photo by skazar.</figcaption></figure><p>There are several of my posts I particularly like – and some I like a little less. Something I really enjoyed doing – and something that I think works very well in the blog format – is interviews. I did around 20 interviews in the last two years (the last two on the <a href="https://web.archive.org/web/20120525055619/http://lindau.nature.com/">Lindau Laureate Meeting</a> blog), and I've listed them all below.</p><ul><li><a href="https://web.archive.org/web/20120525055619/http://blogs.nature.com/mfenner/2008/09/05/interview-with-victor-henning-from-mendeley">Victor Henning about Mendeley</a></li><li><a href="https://web.archive.org/web/20120525055619/http://blogs.nature.com/mfenner/2008/10/03/interview-with-alexander-griekspoor">Alex Griekspoor about Papers</a></li><li><a href="https://web.archive.org/web/20120525055619/http://blogs.nature.com/mfenner/2008/11/07/interview-with-pablo-fernicola">Pablo Fernicola about the Microsoft Word Article Authoring Add-In</a></li><li><a href="https://web.archive.org/web/20120525055619/http://blogs.nature.com/mfenner/2009/01/24/interview-with-moshe-pritsker">Moshe Pritsker about JOVE</a></li><li><a href="https://web.archive.org/web/20120525055619/http://blogs.nature.com/mfenner/2009/01/30/interview-with-kevin-emamy">Kevin Emamy about CiteULike</a></li><li><a href="https://web.archive.org/web/20120525055619/http://blogs.nature.com/mfenner/2009/02/17/interview-with-geoffrey-bilder">Geoff Bilder about CrossRef and author identifiers</a></li><li><a href="https://web.archive.org/web/20120525055619/http://blogs.nature.com/mfenner/2009/02/27/lemon8-xml-interview-with-mj-suhonos">MJ Suhonos about Lemon8-XML</a></li><li><a href="https://web.archive.org/web/20120525055619/http://blogs.nature.com/mfenner/2009/03/04/zotero-interview-with-trevor-owens">Trevor Owens about Zotero</a></li><li><a href="https://web.archive.org/web/20120525055619/http://blogs.nature.com/mfenner/2009/03/25/editorial-manager-interview-with-richard-wynne">Richard Wynne about Editorial Manager</a></li><li><a href="https://web.archive.org/web/20120525055619/http://blogs.nature.com/mfenner/2009/04/28/faculty-of-1000-interview-with-richard-grant">Richard Grant about Faculty of 1000</a></li><li><a href="https://web.archive.org/web/20120525055619/http://blogs.nature.com/mfenner/2009/05/01/extyles-interview-with-elizabeth-blake-and-bruce-rosenblum">Elizabeth Blake and Bruce Rosenblum about eXtyles</a></li><li><a href="https://web.archive.org/web/20120525055619/http://blogs.nature.com/mfenner/2009/05/25/oai-pmh-interview-with-tony-hammond">Tony Hammond about OAI-PMH</a></li><li><a href="https://web.archive.org/web/20120525055619/http://blogs.nature.com/mfenner/2009/08/15/plos-one-interview-with-peter-binfield">Peter Binfield about PLoS ONE and article level metrics</a></li><li><a href="https://web.archive.org/web/20120525055619/http://blogs.nature.com/mfenner/2009/08/20/streamosphere-interview-with-euan-adie">Euan Adie about Streamosphere</a></li><li><a href="https://web.archive.org/web/20120525055619/http://blogs.nature.com/mfenner/2009/10/01/conference-blogging-interview-with-alex-knoll">Alex Knoll about Conference Blogging</a></li><li><a href="https://web.archive.org/web/20120525055619/http://blogs.nature.com/mfenner/2009/11/04/uk-pubmed-central-interview-with-phil-vaughan">Phil Vaughan about UK PubMed Central</a></li><li><a href="https://web.archive.org/web/20120525055619/http://blogs.nature.com/mfenner/2009/11/26/nature-communications-interview-with-lesley-anson">Lesley Anson about Nature Communications</a></li><li><a href="https://web.archive.org/web/20120525055619/http://blogs.nature.com/mfenner/2010/02/15/sciencefeed-interview-with-ijad-madisch">Ijad Madisch about ScienceFeed</a></li><li><a href="https://web.archive.org/web/20120525055619/http://blogs.nature.com/mfenner/2010/04/26/researcherid-interview-with-renny-guida">Renny Guida about Researcher ID</a></li><li><a href="https://web.archive.org/web/20120525055619/http://blogs.nature.com/mfenner/2010/08/03/endnote-interview-with-jason-rollins">Jason Rollins about Endnote</a></li><li><a href="https://web.archive.org/web/20120525055619/http://www.scilogs.eu/en/blog/lindaunobel/2010-07-13/interview-with-edmond-fischer">Nobel Laureate Edmond Fischer</a></li><li><a href="https://web.archive.org/web/20120525055619/http://www.scilogs.eu/en/blog/lindaunobel/2010-07-14/an-interview-francoise-barre-sinoussi">Nobel Laureate Francoise Barré-Sinoussi – together with Lou Woodley</a></li></ul><p>Please stay in touch. And if possible, come around to the <a href="https://web.archive.org/web/20120525055619/http://www.scienceonlinelondon.org/">Science Online London Conference</a> this weekend.</p><p>Update: Gobbledygook has moved to <a href="https://web.archive.org/web/20120525055619/http://blogs.plos.org/mfenner">PLoS Blogs</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Elsevier launches SciVerse, integrates ScienceDirect + Scopus + More]]></title>
            <link>https://blog.martinfenner.org/posts/elsevier-launches-sciverse-integrates-sciencedirect-scopus-more</link>
            <guid>ba707f10-d046-43c5-b2b3-5ec184c1c28d</guid>
            <pubDate>Sun, 29 Aug 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Elsevier on Saturday launched SciVerse
[https://web.archive.org/web/20120524201220/http://info.sciverse.com/], the new
Elsevier platform that combines ScienceDirect (full-text journal articles) and 
Scopus (abstract and citation database of peer-reviewed literature). In 2011 
SciTopics (research summaries) will also be integrated and outside developers
will be able to build SciVerse applications (Elsevier and 3rd party tools that
integrate with ScienceDirect and Scopus). Current users of Scopus ]]></description>
            <content:encoded><![CDATA[<p>Elsevier on Saturday launched <a href="https://web.archive.org/web/20120524201220/http://info.sciverse.com/">SciVerse</a>, the new Elsevier platform that combines <em><em>ScienceDirect</em></em> (full-text journal articles) and <em><em>Scopus</em></em> (abstract and citation database of peer-reviewed literature). In 2011 <em><em>SciTopics</em></em> (research summaries) will also be integrated and outside developers will be able to build SciVerse <em><em>applications</em></em> (Elsevier and 3rd party tools that integrate with ScienceDirect and Scopus). Current users of Scopus and ScienceDirect can continue to use these services or access them from the SciVerse platform.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120524201220im_/http://blogs.plos.org/mfenner/files/2010/11/sciverse.jpg" class="kg-image" alt></figure><p>Both the ScienceDirect and Scopus service are not available this evening, put the new SciVerse functionality should be available in a few hours. As I had no access to SciVerse prior to the launch today, I will need a few days to have a closer look at both the integration of existing products, and the new features. But obviously missing from the announcement is <a href="https://web.archive.org/web/20120524201220/http://www.2collab.com/">2collab</a>, Elsevier's social bookmarking service that integrates with Scopus and ScienceDirect.</p><p>Reference managers a few years ago started to store the PDFs of fulltext articles associated with a citation. It is no surprise to see the same trend from developers of abstracts databases. The integration of Scopus with ScienceDirect almost looks like Elsevier's answer to PubMed/PubMed Central. The recently launched <a href="https://web.archive.org/web/20120524201220/http://ukpmc.ac.uk/">UK PubMed Central</a> is doing a particularly nice job integrating abstracts and fulltext articles.</p><p>The most interesting aspect of SciVerse is the possibility for 3rd party developers to access the service via APIs and to develop both free and commercial applications. We will see whether SciVerse develops into an open platform that works with other publishers, funders and institutions, or whether SciVerse will become a very large data silo. Elsevier's Rafael Sidi <a href="https://web.archive.org/web/20120524201220/http://www.researchinformation.info/features/feature.php?feature_id=255">writes</a> that <em><em>Open data and open APIs offer huge opportunities for research and innovation</em></em>, and earlier this week Mendeley's Jason Hoyt <a href="https://web.archive.org/web/20120524201220/http://www.mendeley.com/blog/open-access/researcher-which-side-of-history/">wrote</a> something similar. Data is also at the heart of the business models of many web-based companies – let's hope that these two interests don't collide.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Supplementary Information: should I stay or should I go?]]></title>
            <link>https://blog.martinfenner.org/posts/supplementary-information-should-i-stay-or-should-i-go</link>
            <guid>e51ac28e-de7c-417b-8198-e4fbec9b5501</guid>
            <pubDate>Fri, 27 Aug 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[On August 11, the Journal of Neuroscience published an Announcement Regarding
Supplemental Material
[https://web.archive.org/web/20120524201522/http://www.jneurosci.org/cgi/content/full/30/32/10599] 
by Editor-in-Chief John Maunsell. In it John Maunsell announces that the journal
in November will stop accepting supplementary material in article submissions.
The announcement has lead to an extensive discussion in the science blogosphere
with a number of relevant posts listed below[^1]:

 * Drugmo]]></description>
            <content:encoded><![CDATA[<p>On August 11, the <em><em>Journal of Neuroscience</em></em> published an <a href="https://web.archive.org/web/20120524201522/http://www.jneurosci.org/cgi/content/full/30/32/10599">Announcement Regarding Supplemental Material</a> by Editor-in-Chief John Maunsell. In it John Maunsell announces that the journal in November will stop accepting supplementary material in article submissions. The announcement has lead to an extensive discussion in the science blogosphere with a number of relevant posts listed below[^1]:</p><ul><li><em>Drugmonkey</em>: <a href="https://web.archive.org/web/20120524201522/http://scientopia.org/blogs/drugmonkey/2010/08/11/yay-j-neuroscience-agrees-with-me-that-supplementary-materials-is-bs-and-ruining-science/">Yay! J. Neuroscience Agrees with Me that "Supplementary Material" is BS and Ruining Science!</a></li><li><em>Juniorprof</em>: <a href="https://web.archive.org/web/20120524201522/http://juniorprof.wordpress.com/2010/08/11/journal-of-neuroscience-is-getting-rid-of-supplemental-data/">Journal of Neuroscience is getting rid of supplemental data</a></li><li><em>Stanford Neuroblog</em>: <a href="https://web.archive.org/web/20120524201522/http://www.stanford.edu/group/neurostudents/cgi-bin/neuroblog/wanted-primary-figures-for-publication-auxiliary-material-need-not-apply/">Wanted: Primary Figures for Publication. Auxiliary Material Need Not Apply.</a></li><li><em>john hawks weblog</em>: <a href="https://web.archive.org/web/20120524201522/http://johnhawks.net/weblog/topics/metascience/supplementary-info-neuroscience-2010.html">Down with supplements</a></li><li><em>Christina’s LIS Rant</em>: <a href="https://web.archive.org/web/20120524201522/http://scientopia.org/blogs/christinaslisrant/2010/08/12/supplemental-materials-or-no/">Supplemental materials or no?</a></li><li><em>Christina’s LIS Rant</em>: <a href="https://web.archive.org/web/20120524201522/http://scientopia.org/blogs/christinaslisrant/2010/08/12/more-questions-about-supplemental-materials/">More questions about supplemental materials</a></li><li><em>Book of Trogool</em>: <a href="https://web.archive.org/web/20120524201522/http://scientopia.org/blogs/bookoftrogool/2010/08/12/disrupting-with-data/">Disrupting with data</a></li><li><em>petermr's blog</em>: <a href="https://web.archive.org/web/20120524201522/http://wwmm.ch.cam.ac.uk/blogs/murrayrust/?p=2540">Supplementary Data must be published somewhere to validate the science</a></li><li><em>Micro World</em>: <a href="https://web.archive.org/web/20120524201522/http://bioenergyrus.blogspot.com/2010/08/where-i-disagree-with-drugmonkey.html">Where I disagree with Drugmonkey …</a></li><li><em>The Scholarly Kitchen</em>: <a href="https://web.archive.org/web/20120524201522/http://scholarlykitchen.sspnet.org/2010/08/16/ending-the-supplemental-data-arms-race/">Ending the Supplemental Data "Arms Race</a>"</li><li><em>Research Remix</em>: <a href="https://web.archive.org/web/20120524201522/http://researchremix.wordpress.com/2010/08/13/supplementary-materials-is-a-stopgap-for-data-archiving/">Supplementary Materials Is A Stopgap For Data Archiving</a></li></ul><p>The main arguments against supplementary information are that it overburdens reviewers (and in turn authors), and it counteracts the concept of a self-contained research report. The main argument for supplementary information is that sometimes essential information can’t be provided within the context of a journal article (e.g. video, large datasets), especially in those journals that still have print editions. Several blog posts emphasized that supplementary information is particularly important to provide the research data with the article. I think that Heather Piwowar’s <a href="https://web.archive.org/web/20120524201522/http://researchremix.wordpress.com/2010/08/13/supplementary-materials-is-a-stopgap-for-data-archiving/">post</a> is the best discussion of the relevant issues.</p><p>I’m obviously two weeks late with this blog posts, but I have been on vacation, and I’m only slowly catching up with all the interesting discussions that have happened while I was away. I think the discussion of supplementary information is very important, because it really is a discussion of our concept of a scientific paper. And this concept is <a href="https://web.archive.org/web/20120524201522/http://blogs.nature.com/mfenner/2009/07/26/how-does-the-article-of-the-future-look-like">changing rapidly</a> for many reasons, including the push to mobile platforms, and the wish by many to publish multimedia files (e.g. 3D structures) and research data with a paper.</p><p><em>Phil Bourne: Beyond the PDF.</em></p><p>It appears to me that we haven’t talked much about supplementary information, and it really has been something everybody was doing out of necessity, without too much thinking about many of the issues, including standard formats, problems for users finding and storing this supplementary information, and copyright issues.</p><p>Most journals have (often similar) instructions regarding supplementary information, e.g.:</p><ul><li><em>Supplementary Information is peer-reviewed material directly relevant to the conclusion of a paper that cannot be included in the printed version for reasons of space or medium. – </em><a href="https://web.archive.org/web/20120524201522/http://www.nature.com/nature/authors/submissions/final/suppinfo.html">Nature</a></li><li><em>Although BMC Medicine does not restrict the length and quantity of data in a paper, there may still be occasions where an author wishes to provide data sets, tables, movie files, or other information as additional information.</em> – <a href="https://web.archive.org/web/20120524201522/http://www.biomedcentral.com/bmcmed/ifora/">BMC Medicine</a></li><li><em>Supplemental Information may include additional control data, validation of methods and reagents, primary data, nonprintable media files, or large data sets. It can also include detailed information regarding Experimental Procedures, including materials (oligonucleotides, plasmids, strains, etc.).</em> – <a href="https://web.archive.org/web/20120524201522/http://www.cell.com/supplemental_information_guide">Cell</a></li><li><em>We strongly encourage authors to include such things as videos, 3-D structures/images, sequence alignments and data sets that are very large, such as those obtained with microarray hybridization experiments. – </em><a href="https://web.archive.org/web/20120524201522/http://www.jbc.org/site/misc/ifora.xhtml#supplemental_data">J Biol Chem</a></li><li><em>We encourage authors to submit essential supporting files and multimedia files along with their manuscripts. All supporting material will be subject to peer review. – </em><a href="https://web.archive.org/web/20120524201522/http://www.plosone.org/static/guidelines.action">PLoS ONE</a></li><li><em>Science does not accept as supporting online material HTML files including JavaScript or other scripting languages or Cascading Style Sheets, PowerPoint presentations, and TeX or LaTeX files.</em> – <a href="https://web.archive.org/web/20120524201522/http://www.sciencemag.org/about/authors/prep/prep_online.dtl#submit">Science</a></li><li><em>Instead of appearing in the printed version of the journal, supporting information is posted on the PNAS Web site at the time of publication. SI is referred to in the text and cannot be altered by authors after acceptance. – </em><a href="https://web.archive.org/web/20120524201522/http://www.pnas.org/site/misc/iforc.shtml#x">PNAS</a></li><li><em>The amount of online-only material should be limited and justified. Online-only material should be original and not previously published. – </em><a href="https://web.archive.org/web/20120524201522/http://jama.ama-assn.org/misc/ifora.dtl#OnlineOnlyMaterial">JAMA</a></li></ul><p>The announcement of the <em><em>Journal of Neuroscience</em></em> will hopefully initiate a broader discussion of the usefulness and best format of supplementary information. The most interesting aspect for me and several of the bloggers discussing the announcement is the publication of the research data associated with a paper. For now supplementary information is often the only place these data can be published, but there are <a href="https://web.archive.org/web/20120524201522/http://researchremix.wordpress.com/2010/08/13/supplementary-materials-is-a-stopgap-for-data-archiving/">many reasons</a> why this is not the best idea in the long run.</p><p>[^1]: This is a perfect example for why we need better systems to track blog posts relating to an article. We have <a href="https://web.archive.org/web/20120524201522/http://blogs.nature.com/">Nature Blogs</a>, <a href="https://web.archive.org/web/20120524201522/http://streamosphere.nature.com/">Streamosphere</a> or <a href="https://web.archive.org/web/20120524201522/http://www.ubervu.com/conversations/www.jneurosci.org/cgi/content/full/30/32/10599">UberVu</a> (and probably others) but they are far from perfect.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Book Review: Cognitive Surplus by Clay Shirky]]></title>
            <link>https://blog.martinfenner.org/posts/book-review-cognitive-surplus-by-clay-shirky</link>
            <guid>81b6732b-b3a6-410a-8c36-4648e1267942</guid>
            <pubDate>Sat, 14 Aug 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[In his new book Cognitive Surplus Clay Shirky argues that in the last 50 years
many of those living in industrialized countries have seen a dramatic increase
in free time, paired with better education and a higher standard of living. But
a very large part of that free time or cognitive surplus is now routinely used
to watch television, a passive activity that makes us a consumer rather than a
participant. In a trend that is also true for other media (eg. music), we have
seen a shift away from pa]]></description>
            <content:encoded><![CDATA[<p>In his new book <em><em>Cognitive Surplus</em></em> Clay Shirky argues that in the last 50 years many of those living in industrialized countries have seen a dramatic increase in free time, paired with better education and a higher standard of living. But a very large part of that free time or cognitive surplus is now routinely used to watch television, a passive activity that makes us a consumer rather than a participant. In a trend that is also true for other media (eg. music), we have seen a shift away from participation (usually on an amateur level) towards consuming professionally produced media content.</p><p>In the last few years we have seen a dramatic change in Internet technologies that now make it extremely easy for everybody to produce content and distribute it on the Internet (e.g. Wikipedia, blogs, photos on Flickr, or YouTube videos) something we usually call Web 2.0 or social media. Clay Shirky contrasts the time that users spend editing <em><em>Wikipedia</em></em> (or other social media activities) with the hours we typically watch television to kill the argument that most of us would have no time for social media.</p><p>The combination of increased free time and social media is creating a lot of interesting collaborative projects, and the book is full of interesting examples. Clay Shirky stresses that social media only enable new uses of the cognitive surplus, it is how they are used that decides whether we create personal, communal, public or civic values. He uses <a href="https://web.archive.org/web/20120524201437/http://www.icanhazcheeseburger.com/">ICanHazCheeseburger</a> (a popular website that shows cute pictures of cats with cute captions) as an example for communal value, the <a href="https://web.archive.org/web/20120524201437/http://www.apache.org/">Apache Webserver</a> Open Source software project as an example of public value, and <a href="https://web.archive.org/web/20120524201437/http://www.ushahidi.com/">Ushahidi.com</a> (a site started to allow citizens to track outbreaks of ethnic violence in Kenya) as an example of civic value.</p><p>Social media in the context of science and medicine are discussed briefly in the book. Clay Shirky describes the <em><em>Invisible College</em></em>, a group formed around 1645 in London by Robert Boyle, Robert Hooke and others that established many of the principles of conducting science that are still valid today (test every hypothesis with experiments, describe experiments detailed enough so that they could be reproduced, etc.). The group formed the core of what a little later became the <em><em>Royal Society</em></em>. Clay Shirky argues that the collaborative nature of how science was conducted by the Invisible College lead to a dramatic increase in our scientific knowledge and contrasts this with how alchemy was performed at the time (reclusive and secretive).</p><p>Clay Shirky also talks about <a href="https://web.archive.org/web/20120524201437/http://www.patientslikeme.com/">PatientsLikeMe.com</a>, a site that allows patients with chronic diseases to share their health information, both for personal advice, but also as participants in clinical trials. Patients Like Me only works because patients are willing to share their personal healthcare information, and that is a cultural shift from the strict privacy that usually surrounds information about your personal health. Patients participating in Patients Like Me obviously think that the benefits from sharing their personal healthcare information outweigh the risks.</p><p>Cognitive Surplus is very entertaining reading, and on almost every page Clay Shirky gives us food for thought that will keep us busy for days or weeks. The book is required reading for everybody who is interested in how social media are changing the way we consume, communicate, and conduct science. After finishing the book, you will no longer think that the scientific article of the future is primarily about integration of video or other multimedia, intelligent navigation, or display on mobile devices. The successful journals of the future will be those that work hardest in facilitating collaboration among scientists, and this requires cultural changes as much as it requires technological changes.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What comes after Google Wave?]]></title>
            <link>https://blog.martinfenner.org/posts/what-comes-after-google-wave</link>
            <guid>66714a9e-90d4-4498-87cd-a96ee1d6a4fe</guid>
            <pubDate>Mon, 09 Aug 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Google announced last week
[https://web.archive.org/web/20120524201336/http://googleblog.blogspot.com/2010/08/update-on-google-wave.html] 
that they will stop further development of Google Wave, and essentially shut
down the service by the end of the year. Some of the more exciting parts of Wave
will be reused in other Google products or – since many parts of Google Wave
have been made available under an Open Source license, in products by other
companies. But the announcement is an admission th]]></description>
            <content:encoded><![CDATA[<p>Google <a href="https://web.archive.org/web/20120524201336/http://googleblog.blogspot.com/2010/08/update-on-google-wave.html">announced last week</a> that they will stop further development of Google Wave, and essentially shut down the service by the end of the year. Some of the more exciting parts of Wave will be reused in other Google products or – since many parts of Google Wave have been made available under an Open Source license, in products by other companies. But the announcement is an admission that the adoption by users and developers had not been what Google had hoped for with this very ambitious project.</p><p>Google Wave was first announced at Google I/O in May 2009, and <a href="https://web.archive.org/web/20120524201336/http://blogs.nature.com/mfenner/2009/07/18/using-google-wave-for-a-week-its-still-great">I had a Google Wave account</a> since July 2009 thanks to being invited to the SciFoo conference (where every attendee got one of those then still rare Wave invites). Like <a href="https://web.archive.org/web/20120524201336/http://cameronneylon.net/blog/the-triumph-of-document-layout-and-the-demise-of-google-wave/">others</a>, I saw the tremendous potential to improve on existing tools for researchers to collaborate. But after 12 months, Google Wave hasn't replaced email as communication tool in at least some of my collaborations. Many people have pointed out the reasons for the lack of user adoption of Google Wave. I want to focus on something else: Google Wave tried to solve a problem that is very relevant to scientists, and we still don't have a solution for that problem.</p><p>Email has become the primary tool for many scientists to collaborate with colleagues both within the same institution, and between different institutions. Despite this popularity, email and the typical workflow around it has several significant shortcomings:</p><p><em><em>Email is not good in tracking longer conversations between more than two people.</em></em> Even though email messages can be grouped together, it quickly becomes difficult to follow the discussion. And it is even more complicated for those joining the discussion later.</p><p><em><em>Email is not good in sharing documents.</em></em> Sending large documents repeatedly back and forth is not only a waste of network bandwidth and limited email storage capacities, but is also not a very productive way to collaboratively work on a longer document, as it quickly becomes difficult to merge the different document versions together.</p><p><em><em>Word processors are not good tools to write scientific documents.</em></em> Traditional word processors such as Microsoft Word or LaTeX care too much about document formatting, and still approach a document primarily as something an individual user edits on a single computer.</p><p>Maybe user uptake for Google Wave was low because the approach to tackle these problems was too radical. Many other, less radical tools try to solve the same problems, including mailing lists, wikis, social bookmarking sites such as CiteULike, collaborative writing tools such as Google Docs, and project management tools such as Basecamp.</p><p>Where do we go from here? Because most of the technology behind Google Wave has been made available, we could continue to use and develop Google Wave and Wave extensions for scientists. But that is a very risky strategy that not many people would follow. We can also wait for new tools that help scientists collaborate, and there interesting products announced every few months. For the time being I will continue trying to convince my colleagues to use some of the existing tools instead of email where appropriate, something that is surprisingly difficult.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Endnote: Interview with Jason Rollins]]></title>
            <link>https://blog.martinfenner.org/posts/endnote-interview-with-jason-rollins</link>
            <guid>5d1eb596-e83c-4e06-989b-8be0b45b67ed</guid>
            <pubDate>Tue, 03 Aug 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Reference management is a frequent topic on this blog. The last few years we
have seen both a large increase in the number of available tools
[https://web.archive.org/web/20120524201927/http://blogs.nature.com/mfenner/2009/03/15/reference-manager-overview]
, but also big changes in how we use reference management software
[https://web.archive.org/web/20120524201927/http://blogs.nature.com/mfenner/2010/05/19/post]
. But for many of us the first reference management software was Endnote.

I first ]]></description>
            <content:encoded><![CDATA[<p>Reference management is a frequent topic on this blog. The last few years we have seen both a large increase in the number of <a href="https://web.archive.org/web/20120524201927/http://blogs.nature.com/mfenner/2009/03/15/reference-manager-overview">available tools</a>, but also big changes in <a href="https://web.archive.org/web/20120524201927/http://blogs.nature.com/mfenner/2010/05/19/post">how we use reference management software</a>. But for many of us the first reference management software was Endnote.</p><p>I first used Endnote as a medical student in 1990 (Endnote Plus at that time, published by Niles Software), and I’m still a regular user. I can’t say that for many other programs (probably also Microsoft Word, Excel and Powerpoint, Adobe Photoshop, SPSS and until recently Freehand). The latest version – Endnote X4 – was just released/is about to be released (Windows in June, Macintosh in August). This is a good opportunity to look at how Endnote has changed over the years and why it is still such a popular application. For this I interviewed <em><em>Jason Rollins</em></em>, who is leading the Endnote development team.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120524201927im_/http://blogs.plos.org/mfenner/files/2010/11/endnote_group.jpg" class="kg-image" alt></figure><p><em><em>Members of the EndNote development team: Jason Rollins, Jiaquan Ma, David Pedrick, Rachel Hubbard, Mayra Aguas, Paul Patanella, Lisa Epps, Lynette Grabow, Howard Harrison, Jennifer Melinn, Bill Colsher, Tilla Edmunds, Gandalf Sollenberger (with the Philadelphia in the background).</em></em></p><h3 id="1-what-is-endnote">1. What is EndNote?</h3><p>Researchers around the world depend on EndNote to simplify collaboration, reference collection, and bibliography formatting. It is these researchers who have molded the EndNote you see today, allowing us to address the pain points as technology evolves. While EndNote originated on the Macintosh and Windows desktops, users now have the added combination of the Web where they can transfer reference groups and share them with colleagues easily. EndNote connects to many parts of the research landscape including online resources for both basic bibliographic information and full text, word processors, <a href="https://web.archive.org/web/20120524201927/http://www.researcherid.com/">ResearcherID</a> for uniquely identifying personal works that will dovetail into the <a href="https://web.archive.org/web/20120524201927/http://www.orcid.org/">ORCID</a> initiative as well as decreasing time to publish when submitting EndNote formatted documents into publishing systems.</p><h3 id="2-how-is-endnote-different-from-other-reference-managers">2. How is EndNote different from other reference managers?</h3><p>EndNote defined reference management software for generations of researchers and continues to offer the highest quality formatting available in any tool. Today some variation of the core functionality – searching, organizing, sharing, and citing of scholarly reference material – is available in many competing tools.</p><p>Two key differences between EndNote and other reference management tools include gathering and formatting references. First, EndNote is far more accurate and flexible when it comes to the variety of publishing styles it supports. EndNote not only provides <a href="https://web.archive.org/web/20120524201927/http://www.endnote.com/support/enstyles.asp">over 4,500 formatting styles</a> but it also allows for the most control and customization of this formatting. Our EndNote team works closely with leading publishers and editors across many academic disciplines to ensure that EndNote offers the formatting control our users need to meet exacting publisher specifications. Most of the power of this formatting is built-in and achieved automatically by EndNote – without the user having to think about it. Of course there are many customization options too; these allow users to easily make changes and tweaks to the way EndNote formats citations, footnotes, and bibliographies.</p><p>Second, EndNote offers the best options for easy import and export of user data. The Online Search, Find Full Text, importing, and Web capture options are all features that make it easy to obtain accurate bibliographic information and manage full text data in EndNote. The “RIS Tagged Data” format has become a de facto standard for hundreds of databases and software tools. Plus, the EndNote XML specification is something that is openly shared with partners and competitors alike in hopes of making it easier for our customers to move their data into and out of any system they may need to use.</p><h3 id="3-what-is-new-in-endnote-x4">3. What is new in EndNote X4?</h3><p>There’s a <a href="https://web.archive.org/web/20120524201927/http://www.endnote.com/enx4wnvid/Whats_New-SD.asp">longer list of new and improved features</a>, but a few of the key features are new PDF functions, better support for collaborative writing and more robust footnote handling.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120524201927im_/http://blogs.plos.org/mfenner/files/2010/11/endnote_screenshot.jpg" class="kg-image" alt></figure><p>EndNote X4 can create records from PDFs using metadata and a DOI, attaching the original file to the newly created reference. Then, PDFs are further incorporated into users’ libraries by indexing the attached files and making them searchable along with the reference data.</p><p>The connection between EndNote and a word processor is more visible by incorporating cited references from documents right into the EndNote interface. This way, users can work with their references in a word processor, and focus on the references in a particular paper while managing things in EndNote. Editing and managing individual citations in Word now combines all the functionality in one place which is a big help for people working on large projects, or collaboratively. The “Traveling Library” sharing feature in Word has also been significantly improved to help people work together. The APA 6th style is fully supported and EndNote X4 offers much more customization and intelligence to the way footnote formatting styles are handled.</p><p>The full list of features, online videos, and a free trial version of EndNote X4 are available at <a href="https://web.archive.org/web/20120524201927/http://www.endnote.com/">www.endnote.com</a>. EndNote version X4 was released in June for Windows with the Macintosh version available by the end of this summer.</p><h3 id="4-how-does-endnote-handle-pdf-management-can-you-share-pdf-files-with-other-users">4. How does EndNote handle PDF management? Can you share PDF files with other users?</h3><p>With version X4, we have added more PDF-related functionality to EndNote. Now, you can import PDF meta-data and search the full text of attached PDF files. You can share EndNote library files that contain PDF attachments easily – or any other file type. EndNote Web does not currently support direct file attachments but this is something we are currently working on.</p><h3 id="5-what-is-endnote-web-how-does-it-relate-to-endnote">5. What is EndNote Web, how does it relate to EndNote?</h3><p>EndNote provides <a href="https://web.archive.org/web/20120524201927/http://www.endnoteweb.com/">EndNote Web</a> to users for easy sharing and collaboration features. Transferring references up to the web provides extra flexibility to work with EndNote references anywhere. There’s a browser plug-in available with EndNote Web, that captures and saves references from web pages and allows users to send references to a desktop or web-based library. The web version has some features that the desktop doesn’t, and vice versa; each is complimentary to the other. EndNote Web is also integrated into the Web of Knowledge database platform and a limited version of EndNote Web is provided to ResearcherID users for managing their personal publication list.</p><h3 id="6-can-i-use-endnote-with-collaborative-writing-tools-such-as-google-docs-or-microsoft-office-live">6. Can I use EndNote with collaborative writing tools such as Google Docs or Microsoft Office Live?</h3><p>While we do not have any custom built plug-ins or similar tools for Google Docs or Microsoft Office Live, as always, it’s easy to drag-and-drop or copy-and-paste in EndNote citations. If you look closely at other products that claim to be compatible, you’ll see that they are offering a simple copy and paste function as well. Based on our research thus far, these writing tools do not yet offer the right APIs to build more robust integration. This is something we hear from customers quite often so we will keep an eye on developments and hope to offer something more once the APIs are available.</p><h3 id="7-how-does-endnote-help-with-reference-management-beyond-writing-manuscripts-e-g-reading-lists-for-students-writing-blog-posts-etc-">7. How does EndNote help with reference management beyond writing manuscripts, e.g reading lists for students, writing blog posts, etc.?</h3><p>You can easily use EndNote to include citations, hyperlinks, or any other reference data into nearly any type of document in almost any format and customize the output for your specific needs. You can share groups of references with both read only and read-write access; this supports sharing reading lists and more involved collaboration. But, reference management has evolved into so much more than just finding things to insert into a document. With the grouping, searching, and file attachment options, EndNote is a way to organize the majority of your research. These features make EndNote robust and flexible at handling a huge volume of items. Plus, you’ve always been able to store files of any type with EndNote, therefore making it possible to utilize much of the organizational capabilities for non-traditional sources like video clips or datasets.</p><h3 id="8-endnote-has-seen-many-updates-since-its-initial-release-more-than-20-years-ago-what-do-you-see-as-some-of-the-most-significant-changes-during-that-time">8. EndNote has seen many updates since its initial release more than 20 years ago. What do you see as some of the most significant changes during that time?</h3><p>EndNote started out for the Mac Plus and DOS; so clearly a lot has changed along with many of the major developments in personal software technology since the late 1980′s. EndNote introduced online searching and direct export to simplify the movement of references from discovery to a personal collection for citing in papers. Cite While You Write is often imitated but never duplicated for ease of use when citing references in Apple Pages, Microsoft Word, and OpenOffice.org Writer. The fundamental problem that EndNote has been solving for our customers has morphed over the years. While keeping up with changes in formatting rules and reference types, which have always been the core strength of EndNote, we have also evolved EndNote into a more complete reference management solution. Where at one time the majority of our users worked alone on a single computer writing a static manuscript, today most customers interact with a global network of collaborators on multiple projects that might be delivered in several formats. Now with EndNote Web and ResearcherID, EndNote users can easily promote their work and connect with others. Another area that has significantly changed over the last twenty years is the means by which users access research material, and as our user’s focus has shifted from basic reference data and description to inclusion of the full text of sources and supporting documentation, EndNote has too. EndNote supports this with OpenURL linking, proxy and open source authentication, browser plug-ins, and other functions.</p><p>Many of the developments we currently have underway will further support online sharing and connectivity and should prove to be some of the most valuable including mobile functionality and more.</p><h3 id="9-what-are-your-responsibilities-at-thomson-reuters">9. What are your responsibilities at Thomson Reuters?</h3><p>My team is responsible for the development of bibliographic management tools. This involves leading the development teams for EndNote and Reference Manager, listening to customer input, and coordinating partnerships.</p><h3 id="10-what-did-you-do-before-working-at-thomson-reuters">10. What did you do before working at Thomson Reuters?</h3><p>I joined the EndNote team in 2001. Before that, I was completing a PhD in Educational Technology from Drexel University and working for two different consulting firms and a small web start-up.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Flipboard changes the way we use Twitter]]></title>
            <link>https://blog.martinfenner.org/posts/flipboard-changes-the-way-we-use-twitter</link>
            <guid>0dbbfb35-918b-42ca-a234-9a1d2f09a0c4</guid>
            <pubDate>Sun, 01 Aug 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Flipboard [https://web.archive.org/web/20120628140034/http://www.flipboard.com/] 
is a personalized social magazine for the iPad. The free application was
released on July 22, and instantly created a lot of buzz. Because of the
overwhelming interest, they had to create a waitlist for the personal features
and I could only sign up for and start using them a few days ago. Flipboard is a
true iPad application, it would not work the same on a laptop computer or mobile
phone.

Flipboard allows you to]]></description>
            <content:encoded><![CDATA[<p><a href="https://web.archive.org/web/20120628140034/http://www.flipboard.com/">Flipboard</a> is a <em><em>personalized social magazine</em></em> for the iPad. The free application was released on July 22, and instantly created a lot of buzz. Because of the overwhelming interest, they had to create a waitlist for the personal features and I could only sign up for and start using them a few days ago. Flipboard is a true iPad application, it would not work the same on a laptop computer or mobile phone.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120628140034im_/http://blogs.plos.org/mfenner/files/2010/11/flipboard3.jpg" class="kg-image" alt></figure><p>Flipboard allows you to set up nine sections for different content. You can select them from a predefined list, but the really interesting part is to create your own sections from Twitter users or Twitter lists:</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120628140034im_/http://blogs.plos.org/mfenner/files/2010/11/flipboard4.jpg" class="kg-image" alt></figure><p>You can also create a section with all the Twitter accounts you follow. Flipboard will display Tweets in a visually pleasing way. Flipboard doesn't display all Tweets in your Twitter timeline, but filters the most popular and interesting tweets.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120628140034im_/http://blogs.plos.org/mfenner/files/2010/11/flipboard1.jpg" class="kg-image" alt></figure><p>But Flipboard does something else. Rather than just displaying tweets, it follows the links provided in your Twitter stream and displays that information (photos, YouTube videos, blog posts, etc.). It can't handle all the links and for example has problems with links to FriendFeed (very common in my Twitter feed).</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120628140034im_/http://blogs.plos.org/mfenner/files/2010/11/flipboard2.jpg" class="kg-image" alt></figure><p>To filter the information, you can also create sections from Twitter lists (or individual users). I had created lists with Science Journals (<a href="https://web.archive.org/web/20120628140034/https://twitter.com/mfenner/science-journals">@mfenner/science-journals</a>) and Nature Network (<a href="https://web.archive.org/web/20120628140034/https://twitter.com/mfenner/nature-network">@mfenner/nature-network</a>) bloggers in the past.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120628140034im_/http://blogs.plos.org/mfenner/files/2010/11/flipboard5.jpg" class="kg-image" alt></figure><p>Twitter lists are great for following meetings (e.g. <a href="https://web.archive.org/web/20120628140034/https://twitter.com/BoraZ/scienceonline2010">@BoraZ/scienceonline2010</a>), but unfortunately Flipboard doesn't yet allow to create sections from Twitter hashtags (e.g. <a href="https://web.archive.org/web/20120628140034/https://twitter.com/#search?q=%23scifoo">#scifoo</a>).</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120628140034im_/http://blogs.plos.org/mfenner/files/2010/11/flipboard6.jpg" class="kg-image" alt></figure><p>When you click on an individual entry, you will see a longer text, and also Tweets referring to that time as well as a link to the fulltext article.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120628140034im_/http://blogs.plos.org/mfenner/files/2010/11/flipboard7.jpg" class="kg-image" alt></figure><p>Flipboard does something similar with your Facebook News Feed. It is important to understand that Flipboard is not an RSS reader, but rather a visually pleasing aggregator from News in your Twitter and Facebook networks. Flipboard doesn't present all the information in these networks, but rather filters the most popular and “interesting” stuff.</p><p>How is Flipboard relevant to reading scholarly papers? It can be used out of the box for journals that tweet their table of contents or at least the most interesting papers. And everybody can create a Twitter account that retweets interesting papers in a particular subject area or from a particular institution (ideally this should be integrated with a bookmarking tool such as CiteULike). Scientific publishers could also work with Flipboard to integrate their content (you see in the example above that the scraping used by flipboard sometimes isn't perfect). Almost all scientific papers have an abstract. The abstract would display well in Flipboard, and could then link to the full text at the journal site.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120628140034im_/http://blogs.plos.org/mfenner/files/2010/11/flipboard8.jpg" class="kg-image" alt></figure><p><em><em>To get started reading science blogs on Flipboard, add a ResearchBlogging.org section by subscribing to the @ResearchBlogs Twitter account.</em></em></p><p>Flipboard is of course also a great tool to follow science blogs. It probably works best if a new Twitter account is set up just for that purpose. This account could announce all new posts from a particular blog, or about a particular topic. Twitter lists could then be used for blogging networks such as <em><em>Nature Network</em></em> or <em><em>Scienceblogs.com</em></em>. Flipboard would also work very well to cover the blogging and tweeting of scientific conferences. With all these activities we will very soon need more than the nine sections that Flipboard supports.</p><p><em><em>Update 08/01/10: Added ResearchBlogging.org screenshot.</em></em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Yet another look at blogging networks]]></title>
            <link>https://blog.martinfenner.org/posts/yet-another-look-at-blogging-networks</link>
            <guid>5c50b44a-98a6-49ee-b8c9-0869006dc525</guid>
            <pubDate>Mon, 26 Jul 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[The last few days we have seen a number of blog posts reflecting on the pros and
cons of science blogging networks. Bora Zivkovic last week announced his
departure from scienceblogs.com, and in his must-read post reflected on the
history of science blogging (A Farewell to Scienceblogs: the Changing Science
Blogging Ecosystem
[https://web.archive.org/web/20120611102940/http://scienceblogs.com/clock/2010/07/scienceblogs_and_me_and_the_ch.php]
). Richard Grant on Saturday wrote down his thoughts On]]></description>
            <content:encoded><![CDATA[<p>The last few days we have seen a number of blog posts reflecting on the pros and cons of science blogging networks. Bora Zivkovic last week announced his departure from scienceblogs.com, and in his must-read post reflected on the history of science blogging (<a href="https://web.archive.org/web/20120611102940/http://scienceblogs.com/clock/2010/07/scienceblogs_and_me_and_the_ch.php">A Farewell to Scienceblogs: the Changing Science Blogging Ecosystem</a>). Richard Grant on Saturday wrote down his thoughts <a href="https://web.archive.org/web/20120611102940/http://blogs.nature.com/rpg/2010/07/24/on-nature-network">On Nature Network</a>. Cameron Neylon draws an interesting parallel between science networks and scientific journals (<a href="https://web.archive.org/web/20120611102940/http://cameronneylon.net/blog/the-nature-of-science-blog-networks/">The Nature of Science Blog Networks</a>). And Katherine Haxton compared her experience blogging on Nature Network vs. blogging on her own blog (<a href="https://web.archive.org/web/20120611102940/http://www.possibilitiesendless.com/?p=270">Science Blogging Networks</a>).</p><p>My thoughts on this: it's complicated. I don't see how blogging networks (or any particular network) can be inherently better or worse than setting up your own blog. It's an individual decision. Right now I like to write as part of a blogging network. But I may change my mind and write for a different network, or on an individually hosted blog. I would like to add three considerations to the discussion.</p><h3 id="consider-institutional-blogs">Consider institutional blogs</h3><p>We shouldn't forget that there is also a third alternative to blogging networks and individually hosted blogs: a blog hosted by your institution. An institutional blog has a different set of advantages and disadvantages and is therefore not the right choice for everybody. But I will soon start an official blog for the cancer center of my medical school (I've gotten the green light from administration and the PR department, the WordPress 3.0 test system is set up, but it probably will be a few weeks before the first post). I have no intentions stopping the Gobbledygook blog – the new blog will have a different focus, and it will be in German. <a href="https://web.archive.org/web/20120611102940/http://network.nature.com/profile/rpg">Richard</a> have written institutional blogs for years, but this will be new for me – and will also be the first official blog at our university.</p><h3 id="we-need-a-better-aggregator-for-science-blogs">We need a better aggregator for science blogs</h3><p>Most of us probably have subscribed to a good number of science blogs with an RSS reader. The best way to find interesting new posts is probably through links in blogs we read regularly. In addition, we stumble upon interesting posts via links in blog networks, blog rolls, <em><em>Twitter</em></em>, <em><em>FriendFeed</em></em>, or shared items in <em><em>Google Reader</em></em>.</p><p>In addition, we also need a good aggregator of science blogs. I have to admit that I have stopped using <em><em>Technorati</em></em> a while ago. <a href="https://web.archive.org/web/20120611102940/http://blogs.nature.com/">Nature.com Blogs</a> was a good start, and <a href="https://web.archive.org/web/20120611102940/http://www.researchblogging.org/">ResearchBlogging</a> does a good job of covering blog posts about peer-reviewed literature. But we probably can do much better. Whether the best strategy would be to put more resources into one of the existing services (see also Lou's <a href="https://web.archive.org/web/20120611102940/http://blogs.nature.com/rpg/2010/07/24/on-nature-network#comment-61864">comment</a> from earlier today), or whether to start building something new, I don't know. But there is a lot of potential for building a better discovery service that at the same time will become an interesting archive of what the science blogosphere is talking about. <a href="https://web.archive.org/web/20120611102940/http://blogs.nature.com/mfenner/2009/08/20/streamosphere-interview-with-euan-adie">Streamosphere</a> is already a good step in the right direction.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120611102940im_/http://farm4.static.flickr.com/3538/3849275338_d77a382643.jpg" class="kg-image" alt><figcaption>Flickr Photo by Eva Amsen. Taken during Science Online London 2009 at the Royal Institution.</figcaption></figure><h3 id="personal-interactions-are-important">Personal interactions are important</h3><p>It is usually much more rewarding to interact with someone online whom you have also met in person. This is true not only for scientific collaborations, but of course also for social media. This is obviously much easier if you live in places like London, but even Hannover (where I live) has a really nice <a href="https://web.archive.org/web/20120611102940/http://blogs.nature.com/mfenner/2010/05/11/action-points">Science 2.0 community</a>. And this is why science blogging conferences such as <a href="https://web.archive.org/web/20120611102940/http://www.scienceonline2010.com/index.php/wiki/">ScienceOnline2010</a> or <a href="https://web.archive.org/web/20120611102940/http://www.scienceonlinelondon.org/">Science Online London 2010</a> are so important. Science Online London is only six weeks away (September 3-4 at the British Library), and I'm really excited not only about the program, but also because of all the people I will for the first time or again meet in person.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Discuss Papers with JournalFire]]></title>
            <link>https://blog.martinfenner.org/posts/discuss-papers-with-journalfire</link>
            <guid>d9c6d26a-caba-48f7-8734-d4b82e1ebb34</guid>
            <pubDate>Mon, 05 Jul 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[About a year ago I wrote a blog post about how to use Web 2.0 tools for a
journal club (Recipe: Distributing papers for a journal club
[https://web.archive.org/web/20120611103044/http://blogs.nature.com/mfenner/2009/08/08/recipe-distributing-papers-for-a-journal-club]
). Although reference management tools such as CiteULike and Mendeley can be
used for journal clubs, discussion features are often more of an afterthought.
At the time I therefore recommended FriendFeed. In February of this year,
S]]></description>
            <content:encoded><![CDATA[<p>About a year ago I wrote a blog post about how to use Web 2.0 tools for a journal club (<a href="https://web.archive.org/web/20120611103044/http://blogs.nature.com/mfenner/2009/08/08/recipe-distributing-papers-for-a-journal-club">Recipe: Distributing papers for a journal club</a>). Although reference management tools such as <em><em>CiteULike</em></em> and <em><em>Mendeley</em></em> can be used for journal clubs, discussion features are often more of an afterthought. At the time I therefore recommended FriendFeed. In February of this year, ScienceFeed was released (<a href="https://web.archive.org/web/20120611103044/http://blogs.nature.com/mfenner/2010/02/15/sciencefeed-interview-with-ijad-madisch">ScienceFeed: Interview with Ijad Madisch</a>). It is very similar to FriendFeed, but is smarter about finding and storing references to scientific literature. Unfortunately ScienceFeed integrates very tightly with ResearchGate. You have to sign up for both services to use the reference management features.</p><p>Last week a new web-based tool specifically for Journal Clubs was released. <a href="https://web.archive.org/web/20120611103044/http://journalfire.com/">JournalFire</a> was created by <a href="https://web.archive.org/web/20120611103044/http://journalfire.com/johnmdelacruz">John Delacruz</a>, <a href="https://web.archive.org/web/20120611103044/http://journalfire.com/riccardoschmid">Riccardo Schmid</a> and <a href="https://web.archive.org/web/20120611103044/http://journalfire.com/timhill">Tim Hill</a>. JournalFire is free for public discussion groups, but costs money for private groups with more than 4 members.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611103044im_/http://blogs.plos.org/mfenner/files/2010/11/journalfire.jpg" class="kg-image" alt></figure><p>Feel free to participate in the discussion about this article <a href="https://web.archive.org/web/20120611103044/http://journalfire.com/node/70269107">here</a>. I've picked a paper that is not only freely available and nicely written, but also in a very hot field.</p><h3 id="what-i-like">What I like</h3><p>JournalFire really focuses on discussion around a paper. Journal Clubs are used to organize the discussion, similar to groups in other tools. The interface is uncluttered, and almost everything is available as RSS feed.</p><h3 id="what-i-would-like">What I would like</h3><p>JournalFire was just released publicly, so it obviously doesn't have all the features of a more mature product. More importantly, JournalFire covers a fairly specific reference management niche, it is therefore important that data exchange with other reference management tools is as easy as possible.<br>JournalFire has a bookmarklet similar to what other reference managers offer, and you can export one or more references in Endnote or BibTex format. <a href="https://web.archive.org/web/20120611103044/http://ocoins.info/">COinS</a> support should be an obvious addition, and an API would allow tighter integration with other tools, e.g. to add Journal Club features to your CiteULike, Mendeley or Zotero library.</p><p>The Journal Club concept of JournalFire fills a very interesting niche. But in the end, social tools for scientists are about critical mass. It will be interesting to see whether JournalFire can attract enough users in this very crowded market.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Innovations in Reference Management]]></title>
            <link>https://blog.martinfenner.org/posts/innovations-in-reference-management</link>
            <guid>ac2f0e96-1a36-4d9d-a508-cd893cbbb08e</guid>
            <pubDate>Tue, 22 Jun 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Earlier today I attended the fabulous Innovations in Reference Management
[https://web.archive.org/web/20120611100639/http://www.open.ac.uk/telstar/event] 
workshop in Birmingham organized by Owen Stephens
[https://web.archive.org/web/20120611100639/http://www.twitter.com/ostephens] 
from Open University. Owen has written some blog posts
[https://web.archive.org/web/20120611100639/http://www.open.ac.uk/blogs/telstar/tag/irm10/] 
summarizing the sessions almost in real time. You also find a good ]]></description>
            <content:encoded><![CDATA[<p>Earlier today I attended the fabulous <a href="https://web.archive.org/web/20120611100639/http://www.open.ac.uk/telstar/event">Innovations in Reference Management</a> workshop in Birmingham organized by <a href="https://web.archive.org/web/20120611100639/http://www.twitter.com/ostephens">Owen Stephens</a> from Open University. Owen has written <a href="https://web.archive.org/web/20120611100639/http://www.open.ac.uk/blogs/telstar/tag/irm10/">some blog posts</a> summarizing the sessions almost in real time. You also find a good number of interesting tweets under the hashtag <a href="https://web.archive.org/web/20120611100639/http://twitter.com/#search?q=%23irm10">#irm10</a>. My presentation was called <em><em>Trends in Reference Management</em></em>:</p><p>I started my presentation by giving a brief historical perspective on how reference management has evolved during the past 25 years. It seems as if many users today use reference managers the same way as users did in 1992 (when <a href="https://web.archive.org/web/20120611100639/http://dx.doi.org/10.1021/ci00010a601">this review of Endnote</a> was written): a database of (mostly) journal articles installed on your desktop computer that can fetch references directly from online databases (in 1992 we had Endlink and the Z39.50 protocol for that) and that inserts bibliographies into Microsoft Word. It is odd that many seemingly old problems still exist today:</p><ul><li>Digital Object Identifiers (<a href="https://web.archive.org/web/20120611100639/http://www.doi.org/">DOIs</a>) were introduced in 2000, and many journals not only are using them, but also exist only as electronic versions. Nevertheless most citation styles still use volumes, issues and page numbers instead of the DOI. And some journals and online databases (e.g. PubMed) still don’t display the DOI prominently, but rather use their own numbering scheme.</li><li>Most reference managers allow sharing of reference in private or public groups. We have collaborative writing tools (Google Docs, Zoho Writer, Adobe Buzzword, and now even <a href="https://web.archive.org/web/20120611100639/http://workspace.officelive.com/">Microsoft Office Live Workspace</a>) since at least 2006, but I think that no reference manager supports them beyond a simple copy and past function.</li><li>Almost all journal articles can be downloaded as PDF files, but users are still confused what they are allowed to do with these PDFs. Many publishers don’t even allow redistribution of these PDFs within an institution (e.g. for a journal club), something that is probably often ignored. These problems obviously don’t exist for Open Access publications, but it again gets complicated with those publishers that use a <a href="https://web.archive.org/web/20120611100639/http://creativecommons.org/licenses/by-nc/3.0/">Creative Commons Noncommercial license</a>. And to the best of my knowledge, no reference manager is providing copyright information for its references.</li></ul><h3 id="mobile-applications-for-reference-management">Mobile Applications for Reference Management</h3><p>In the next part of my presentation I briefly talked about some of the mobile applications handling scientific journal articles (<a href="https://web.archive.org/web/20120611100639/http://www.nature.com/mobileapps/">Nature</a>, <a href="https://web.archive.org/web/20120611100639/http://speakingofmedicine.plos.org/2010/03/22/introducing-the-plos-medicine-iphone-application/">PLoS Medicine</a>, <a href="https://web.archive.org/web/20120611100639/http://blogs.nejm.org/now/index.php/introducing-the-nejm-this-week-iphone-app/2010/06/18/">NEJM</a> and <a href="https://web.archive.org/web/20120611100639/http://info.scopus.com/mobile">Scopus</a> for iPhone and <a href="https://web.archive.org/web/20120611100639/http://itunes.apple.com/us/app/plos-reader/id370880976">PLoS Reader</a> and <a href="https://web.archive.org/web/20120611100639/http://mekentosj.com/papers/ipad/">Papers for iPad</a>) that have been released in the last few months. The small screen of mobile phones makes it difficult to read scientific papers (especially the tables and figures). The best use case is probably the scanning of journal table of contents or search alerts with bookmarking of interesting papers for later reading. In other words, a perfect job for a RSS reader synchronized with the desktop.</p><p>The iPad is a very different story, as for me it is a very good device for reading a scientific paper – better than a desktop computer and even better than a printout if you don’t have a color printer. And <a href="https://web.archive.org/web/20120611100639/http://mekentosj.com/papers/ipad/">Papers for iPad</a> is the perfect software for this use case, especially if you also own Papers for Mac and can simply transfer all your PDF files complete with reference information to the iPad.</p><h3 id="orcid">ORCID</h3><p>I then moved on to give a brief introduction to the <a href="https://web.archive.org/web/20120611100639/http://www.orcid.org/">ORCID</a> initiative (most of my slides were kindly provided my <a href="https://web.archive.org/web/20120611100639/http://www.slideshare.net/CrossRef/crossref-contributor-id">Geoff Bilder</a> and <a href="https://web.archive.org/web/20120611100639/http://www.slideshare.net/hratner/introduction-to-orcid-stm-spring-2010">Howard Ratner</a>). Open Researcher &amp; Contributor ID aims to provide a universal unique identifier for researchers. Past approaches have not gained universal acceptance because they were restricted to a particular discipline (e.g. <a href="https://web.archive.org/web/20120611100639/http://www.repec.org/">RePEec</a>), country (e.g. <a href="https://web.archive.org/web/20120611100639/http://www.slideshare.net/amandahill/names-project-update">Names Project</a>) or were started by a single private company (e.g. <a href="https://web.archive.org/web/20120611100639/http://info.scopus.com/scopus-in-detail/tools/authoridentifier/">Scopus Author Identifier</a>). ORCID is a broad initiative with already more than 100 <a href="https://web.archive.org/web/20120611100639/http://www.orcid.org/gallery.php">participating organizations</a>. ORCID <a href="https://web.archive.org/web/20120611100639/http://dx.doi.org/10.1038/462825a">was announced</a> in December 2009, and the initiative is currently working hard to first incorporate as a non-profit organization and then launch a first public prototype (based on the <a href="https://web.archive.org/web/20120611100639/http://www.researcherid.com:80/Home.action">ResearcherID</a> software). Both milestones are planned for later this year.</p><p>Reference managers will of course benefit from a widely accepted author identifier, e.g. by making it much easier to find all publications from a particular author. ORCID can also be used to track other scientific contributions, e.g. peer review, blog posts, presentations or primary research datasets. Reference managers could also track (some of) these contributions by researchers.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Twitter at the ASCO Conference]]></title>
            <link>https://blog.martinfenner.org/posts/using-twitter-at-the-asco-conference</link>
            <guid>a4c021fb-4e5f-4ce2-9a43-8911b0e81b2f</guid>
            <pubDate>Tue, 15 Jun 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Last week I attended the annual meeting of the American Society of Clinical
Oncology [https://web.archive.org/web/20120630144308/http://www.asco.org/] 
(ASCO) in Chicago. For my work it is the most important scientific meeting of
the year, and it is also by far the biggest with more than 30.000 participants.

Chicago last Thursday. Flickr image by ifmuth.Blogging is a great way to report from conferences
[https://web.archive.org/web/20120630144308/http://blogs.nature.com/mfenner/2010/01/17/scien]]></description>
            <content:encoded><![CDATA[<p>Last week I attended the annual meeting of the <a href="https://web.archive.org/web/20120630144308/http://www.asco.org/">American Society of Clinical Oncology</a> (ASCO) in Chicago. For my work it is the most important scientific meeting of the year, and it is also by far the biggest with more than 30.000 participants.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120630144308im_/http://farm5.static.flickr.com/4010/4691440146_bcaea61ee0.jpg" class="kg-image" alt><figcaption>Chicago last Thursday. Flickr image by ifmuth.</figcaption></figure><p>Blogging is a great way to <a href="https://web.archive.org/web/20120630144308/http://blogs.nature.com/mfenner/2010/01/17/scienceonline2010-i-wish-i-was-there">report from conferences</a>, and for me <em><em>FriendFeed</em></em> is the best microblogging tool to do that. Its use at the 2008 ISMB conference was nicely described in a <em><em>PLoS Comp Biol</em></em> paper (<a href="https://web.archive.org/web/20120630144308/http://dx.doi.org/10.1371/journal.pcbi.1000263">doi:10.1371/journal.pcbi.1000263</a>) by Neil Saunders et al. Unfortunately it seems as if the use of FriendFeed is declining at every online science conference I go to, as everybody seems to be switching to <em><em>Twitter</em></em>. At the <a href="https://web.archive.org/web/20120630144308/http://bibcamp.wordpress.com/">BibCamp</a> that we organized in May we didn't even bother trying FriendFeed (or similar microbloging tools such as Google Buzz, ScienceFeed, <a href="https://web.archive.org/web/20120630144308/http://hotpotato.com/">Hot Potato</a>, <a href="https://web.archive.org/web/20120630144308/http://www.snapgroups.com/">Snapgroups</a> or Google Wave).</p><p>My main problem with using Twitter at conferences is that it is difficult to connect all tweets talking about a particular session together. General hashtags like <em><em>#asco10</em></em> are not a solution for conferences with several thousand tweets. The ASCO conference organizers asked us to use two <a href="https://web.archive.org/web/20120630144308/http://www.asco.org/ascov2/MyASCO/Twitter">special hash tags</a> for some sessions, but it seemed as if almost nobody was using them. I would not be surprised if someone invents (or has already done so) a nice service that connects tweets to conference sessions.</p><p>Another shortcoming of Twitter is the lack of automatic long-term archiving. Therefore <a href="https://web.archive.org/web/20120630144308/http://www.twitter.com/RMEOncology">@RMEOncology</a> created a Twapper Keeper that was <a href="https://web.archive.org/web/20120630144308/http://twapperkeeper.com/hashtag/asco10">used to archive</a> the about 4500 tweets with the <em><em>#asco10</em></em> hashtag. You can do interesting things with such an archive, Cornelius Puschmann did an <a href="https://web.archive.org/web/20120630144308/http://wisspub.net/2010/05/11/bibcamp-auswertung/">analysis</a> of the Twitter activity at our recent <a href="https://web.archive.org/web/20120630144308/http://blogs.nature.com/mfenner/2010/05/11/action-points">BibCamp</a> using the R statistical software.</p><p>On the second conference day we were invited by the ASCO organizers to a tweetup. We were twice as many people compared to the tweetup at last year's ASCO (including one familiar face, <a href="https://web.archive.org/web/20120630144308/http://www.twitter.com/MaverickNY">@MaverickNY</a>). We had an interesting discussion for about an hour. The most important message for me: we were encouraged to freely tweet about the sessions, restricted only by common sense and not a particular policy. A large meeting like ASCO with a lot of press coverage and videotaping of most sessions obviously has it easier to allow widespread Twitter use than a smaller meeting like the Cold Spring Harbor Biology of Genomes meeting. The discussions at that meeting in May were nicely covered by <a href="https://web.archive.org/web/20120630144308/http://scienceblogs.com/geneticfuture/2010/05/what_a_difference_a_year_makes.php">Daniel MacArthur</a> and <a href="https://web.archive.org/web/20120630144308/http://embargowatch.wordpress.com/2010/05/24/can-you-tweet-from-cold-spring-harbor-laboratory-meetings/">Ivan Oransky</a>.</p><p>ASCO was also the first conference that I attended with an iPad. We had free WiFi (that worked in most places) and the iPad was great to write Tweets, look through the PDF of the sessions and posters I wanted to see (created back home), and to keep in touch via email and Skype. The only caveat: adding links to Tweets was difficult without multitasking – although this should become easier with the soon to be released iOS 4.</p><p>I did not like all Tweets about the conference. Twitter was obviously also used as a marketing tool. Although there is nothing wrong with that, some pharma companies felt it necessary to tweet exactly the same message again and again. I would have like more tweets reporting key findings from the sessions and/or adding comments or interesting links (e.g. with background information). But I'm optimistic that Twitter coverage at ASCO11 will be even better, and that by ASCO12 Twitter will be as mainstream and popular as email was at ASCO10.</p><p><em><em>Starting tomorrow, I will also write for the <a href="https://web.archive.org/web/20120630144308/http://lindau.nature.com/">Lindau Nobel Laureate Meeting</a> blog (the meeting itself will start June 27). The Nature Network blog has <a href="https://web.archive.org/web/20120630144308/http://blogs.nature.com/u6e5b2ce1/2010/06/15/nature-at-the-lindau-nobel-laureates-meeting-2010">more info</a> about the event.</em></em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[On Privacy]]></title>
            <link>https://blog.martinfenner.org/posts/on-privacy</link>
            <guid>dd9ce9fc-9988-4812-ae2a-7ef91df6ea27</guid>
            <pubDate>Mon, 24 May 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Social media and privacy have a complicated relationship, as using social media
implies giving up at least some privacy. And the value of a social networking
site is directly related not only to the number of users, but also the extend of
personal data that the site has collected. All this is of course not unique to
social networking tools for scientists, but conducting science using online
tools is different from connecting to friends or people with similar personal
interests with these tools. ]]></description>
            <content:encoded><![CDATA[<p>Social media and privacy have a complicated relationship, as using social media implies giving up at least some privacy. And the value of a social networking site is directly related not only to the number of users, but also the extend of personal data that the site has collected. All this is of course not unique to social networking tools for scientists, but conducting science using online tools is different from connecting to friends or people with similar personal interests with these tools. One area where privacy issues are particularly prominent is the creation of unique identifiers for scientists, and here I'm involved in the <a href="https://web.archive.org/web/20120611031846/http://www.orcid.org/">ORCID</a> initiative. We need a discussion about required privacy standards, and about the limitations of privacy that we accept in order to increase our scientific productivity.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120611031846im_/http://farm3.static.flickr.com/2052/2404940312_e759c4030d.jpg" class="kg-image" alt><figcaption>Flickr image by rpongsaj.</figcaption></figure><p>I've collected a couple of thoughts below. They are of course only starting points for a discussion, and I welcome any comments and suggestions.</p><h3 id="1-the-service-allows-anonymous-access">1. The service allows anonymous access</h3><p>Social networking sites for scientists can be quite useful for anonymous users, e.g. for reading of blog or forum posts. Users should not be required to sign up for a service just for reading a public message or looking at the public part of a user profile. Many social networking sites unfortunately require a user account these activities, and they often nag users to “complete their personal profiles”, i.e. to provide as much personal information as possible.</p><h3 id="2-the-service-has-a-privacy-policy">2. The service has a privacy policy</h3><p>I've randomly checked five social networking sites for scientists, and they all provide a privacy policy:</p><ul><li><a href="https://web.archive.org/web/20120611031846/http://www.academia.edu/privacy">Academia.edu</a></li><li><a href="https://web.archive.org/web/20120611031846/http://www.citeulike.org/privacy">CiteULike</a></li><li><a href="https://web.archive.org/web/20120611031846/http://www.mendeley.com/privacy/">Mendeley</a></li><li><a href="https://web.archive.org/web/20120611031846/http://network.nature.com/site/privacy">Nature Network</a></li><li><a href="https://web.archive.org/web/20120611031846/http://www.researchgate.net/application.PrivacyPolicy.html">ResearchGate</a></li></ul><p>The privacy policy should make clear what user data the service is collecting, whether the service is providing these user data to third parties, and should provide an email address for further privacy questions.</p><h3 id="3-personal-data-are-owned-by-the-user">3. Personal data are owned by the user</h3><p>All personal data that a user uploads should be owned by the user, and the privacy policy should make this clear. This implies that a user should be allowed to cancel an account and delete all personal information (surprisingly difficult with most social networking sites), and that the user decides which part of the personal data is shared with others, and with whom.</p><p>To give users control over their privacy settings, these settings should default to not being public for everything beyond the most basic information, and they should not change over time (<a href="https://web.archive.org/web/20120611031846/http://mattmckeon.com/facebook-privacy/">The Evolution of privacy on Facebook</a>).</p><p>Obviously there is a large grey area. If I comment on a blog post by someone else, who owns this comment, and who should be allowed to delete this comment – me, the author of the blog post, and/or the provider of the social networking site? And who owns my usage data of a service, how often I logged in, my activity in the service, etc.?</p><h3 id="4-the-service-protects-the-user-data">4. The service protects the user data</h3><p>The social networking site should make all efforts necessary to protect the personal data of a user. There are examples where this has gone wrong, and this includes examples involving scientists (<a href="https://web.archive.org/web/20120611031846/http://www.nature.com/news/2009/090423/full/news.2009.398.html">Fake Facebook pages spin web of deceit</a>).</p><p><em><em>Update 5/25/10: I didn't see it when I wrote the post, but last week the Electronic Frontier Foundation published a <a href="https://web.archive.org/web/20120611031846/http://www.eff.org/deeplinks/2010/05/bill-privacy-rights-social-network-users">Bill of Privacy Rights for Social Network Users</a> (thanks to Renny Guida for the link).</em></em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What is a reference manager?]]></title>
            <link>https://blog.martinfenner.org/posts/what-is-a-reference-manager</link>
            <guid>c1996550-cd4b-4ce7-aa88-58a5f7d5dc54</guid>
            <pubDate>Wed, 19 May 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Next month I will participate in the Innovations in Reference Management
[https://web.archive.org/web/20120611032215/http://www.open.ac.uk/telstar/event/programmet] 
event organized by Owen Stevens
[https://web.archive.org/web/20120611032215/http://twitter.com/ostephens] from
the Open University. My own presentation has the title Trends in Reference
Management, where I will try to focus on three emerging areas:

 * Use of mobile devices for reference management (e.g. Nature.com iPhone App
   [ht]]></description>
            <content:encoded><![CDATA[<p>Next month I will participate in the <a href="https://web.archive.org/web/20120611032215/http://www.open.ac.uk/telstar/event/programmet">Innovations in Reference Management</a> event organized by <a href="https://web.archive.org/web/20120611032215/http://twitter.com/ostephens">Owen Stevens</a> from the Open University. My own presentation has the title <em><em>Trends in Reference Management</em></em>, where I will try to focus on three emerging areas:</p><ul><li>Use of mobile devices for reference management (e.g. <a href="https://web.archive.org/web/20120611032215/http://blogs.nature.com/mfenner/2010/02/08/nature-com-iphone-app-in-pictures">Nature.com iPhone App</a>, <a href="https://web.archive.org/web/20120611032215/http://mekentosj.com/papers/ipad/">Papers for iPad</a>, <a href="https://web.archive.org/web/20120611032215/http://info.scopus.com/mobile/">Scopus Alerts for iPhone</a>)</li><li>Social networks as discovery tools (using CiteULike, Twitter, FriendFeed, ResearchBlogging.org, etc. for finding relevant references instead of keyword searches in databases)</li><li>Integration of unique author identifiers (specifically <a href="https://web.archive.org/web/20120611032215/http://blogs.nature.com/mfenner/2010/01/03/orcid-or-how-to-build-a-unique-identifier-for-scientists-in-10-easy-steps">ORCID</a>) into reference management tools. This will allow many interesting uses of reference lists (e.g. for automated researcher profiles or evaluation)</li></ul><p>Reference management has obviously come a long way from the desktop applications <a href="https://web.archive.org/web/20120611032215/http://dx.doi.org/10.1021/ci00010a601">created 25 years ago</a>. In 2010 it is difficult to give a good description of the typical functions of reference managers. My 8 answers are below.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611032215im_/http://blogs.plos.org/mfenner/files/2010/11/refman1.jpg" class="kg-image" alt="refman1.jpg"></figure><p>At their core all reference managers are databases that store references. They should allow the user to create or import all required reference types, find duplicate records, and connect references by the same author or published in the same journal. I don't think any reference manager completely supports even this small feature set (most of them don't do author disambiguation). And we rarely use reference managers in a much broader sense, e.g. for storing important blog posts or references to research datasets (as <a href="https://web.archive.org/web/20120611032215/http://bit.ly/do6Ujj">suggested by Cameron Neylon</a>).</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611032215im_/http://blogs.plos.org/mfenner/files/2010/11/refman2.jpg" class="kg-image" alt="refman2.jpg"></figure><p>Reference managers should put references into manuscripts, allowing a variety of citation styles. Only a few reference managers allow the user to edit citation styles, an important feature if an obscure style is required. Surprisingly, the functionality for reference management is usually not built into most word processors, but rather provided by the reference management software.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611032215im_/http://blogs.plos.org/mfenner/files/2010/11/refman3.jpg" class="kg-image" alt="refman3.jpg"></figure><p>All reference managers can import references directly from bibliographic databases, either by direct database connections, or via bookmarklets.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611032215im_/http://blogs.plos.org/mfenner/files/2010/11/refman4.jpg" class="kg-image" alt="refman4.jpg"></figure><p>Some reference managers (most notably <a href="https://web.archive.org/web/20120611032215/http://mekentosj.com/papers/">Papers</a>) not only manage references, but also organize the full-text PDF files associated with them. There are many good reasons to keep the assets (such as scientific papers) and their metadata (the references) connected, e.g. to allow full-text searches in your reference manager. Scientific journals have started to embed reference metadata in full-text PDF files (<a href="https://web.archive.org/web/20120611032215/http://blogs.nature.com/wp/nascent/2008/12/xmp_labelling_for_nature.html">XMP Labelling for Nature</a>), making it easier to connect reference and PDF file.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611032215im_/http://blogs.plos.org/mfenner/files/2010/11/refman5.jpg" class="kg-image" alt="refman5.jpg"></figure><p>Many reference managers now offer a web-based version. This allows using the same reference database with more than one computer and sharing of references with others. Some reference managers (e.g. CiteULike, RefWorks) only provide a web-based version.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611032215im_/http://blogs.plos.org/mfenner/files/2010/11/refman6.jpg" class="kg-image" alt="refman6.jpg"></figure><p>Many bibliographic databases (e.g. PubMed or Scopus) now allow user accounts with storing of search strategies, interesting references, etc. Does that make them reference managers?</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611032215im_/http://blogs.plos.org/mfenner/files/2010/11/refman7.jpg" class="kg-image" alt="refman7.jpg"></figure><p>The flow of reference information has become much more complicated than simply from online database to reference manager to manuscript. <a href="https://web.archive.org/web/20120611032215/http://blogs.nature.com/mfenner/2010/05/03/bibapp-mashups-for-universities">Institutional bibliographies</a> are a good example.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611032215im_/http://blogs.plos.org/mfenner/files/2010/11/refman8.jpg" class="kg-image" alt="refman8.jpg"></figure><p>More and more we find interesting references not via keyword searches in databases, but rather through our social networks. This could be via <a href="https://web.archive.org/web/20120611032215/http://blog.citeulike.org/?p=136">CiteULike recommendations</a>, blog posts discussing a paper on ResearchBlogging.org, or one of your contacts talking about an interesting paper on Twitter, FriendFeed or Facebook. Current reference managers don't provide good functionality to handle this user behavior.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Action points]]></title>
            <link>https://blog.martinfenner.org/posts/action-points</link>
            <guid>50a2b54f-1a82-4156-9e2a-fce5383a5cb1</guid>
            <pubDate>Tue, 11 May 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Last weekend was BibCamp Hannover, a “BarCamp for librarians and other hackers”.
If you understand German, you can read about the sessions, discusions and people
in the Blog
[https://web.archive.org/web/20120611033833/http://bibcamp.wordpress.com/], Wiki
[https://web.archive.org/web/20120611033833/http://bibcamp.pbworks.com/browse/#view=ViewFolder&param=BibCamp%202010]
, and FriendFeed Room
[https://web.archive.org/web/20120611033833/http://friendfeed.com/bibcamp]. And
Steffi Suhr wrote a nice p]]></description>
            <content:encoded><![CDATA[<p>Last weekend was <em><em>BibCamp Hannover</em></em>, a “BarCamp for librarians and other hackers”. If you understand German, you can read about the sessions, discusions and people in the <a href="https://web.archive.org/web/20120611033833/http://bibcamp.wordpress.com/">Blog</a>, <a href="https://web.archive.org/web/20120611033833/http://bibcamp.pbworks.com/browse/#view=ViewFolder&amp;param=BibCamp%202010">Wiki</a>, and <a href="https://web.archive.org/web/20120611033833/http://friendfeed.com/bibcamp">FriendFeed Room</a>. And Steffi Suhr wrote a nice post about <a href="https://web.archive.org/web/20120611033833/http://blogs.nature.com/stuffysour/2010/05/09/the-most-beautiful-library-in-the-world">The most beautiful library in the world</a> in her <em><em>Nature Network</em></em> blog.</p><p>The BarCamp format worked very well for us – we used traditional pen and paper for session planning:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120611033833im_/http://edlf.files.wordpress.com/2010/05/p1060059.jpg" class="kg-image" alt><figcaption>Me, helping with session planning last Friday. Picture by <a href="https://web.archive.org/web/20120611033833/http://edlf.wordpress.com/2010/05/07/bibcamp-2010-tag-1/">Martin Kramer</a> .</figcaption></figure><p>I had suggested a session about <a href="https://web.archive.org/web/20120611033833/http://blogs.nature.com/mfenner/2010/05/03/bibapp-mashups-for-universities">Institutional Bibliographies</a>. We combined this with two related suggestions and had a very interesting session about the communication of scientists with librarians. One interesting theme in the discussion was the notion that the scientific workflow seems to be broken at some specific steps, for example:</p><h3 id="deposition-of-post-prints-in-institutional-repositories">Deposition of post-prints in institutional repositories</h3><p>Most journals allow authors to make the final version of an accepted manuscript (i.e. after peer review) <a href="https://web.archive.org/web/20120611033833/http://www.sherpa.ac.uk/romeoinfo.html#prepostprints">publicly available</a> via an institutional repository. Most librarians would be happy to help their authors with this step, but unfortunately have no good tools to track the papers published at their institution, and never see the final version of accepted manuscript (many journals don't allow publisher-generated PDFs in repositories).</p><h3 id="re-submission-of-rejected-manuscripts">Re-submission of rejected manuscripts</h3><p>Rejected manuscripts are usually resubmitted to a different journal (most manuscripts will eventually be published <a href="https://web.archive.org/web/20120611033833/http://dx.doi.org/10.1097/01.ede.0000254668.63378.32">somewhere</a>). Unfortunately the next journal will most likely use a different manuscript format, different citation styles and a different manuscript submission system. Some journals provide a <a href="https://web.archive.org/web/20120611033833/http://www.nature.com/authors/author_services/transfer_manuscripts.html">manuscript transfer service</a>, but the comments made in the peer review are usually not available to the editors and reviewers of the next journal.</p><h3 id="connecting-scientists-to-publications">Connecting scientists to publications</h3><p>This is a topic that I have written about before (<a href="https://web.archive.org/web/20120611033833/http://blogs.nature.com/mfenner/2010/01/03/orcid-or-how-to-build-a-unique-identifier-for-scientists-in-10-easy-steps">ORCID or how to build a unique identifier for scientists in 10 easy steps</a>). Until we have unique author identifiers, it is difficult or impossible to reliably find the papers published by a particular person (a good number of papers by <em><em>Fenner M</em></em> in <em><em>PubMed</em></em> are not written by me).</p><h3 id="distributing-papers-for-a-journal-club">Distributing papers for a journal club</h3><p>Another topic I have written about before (<a href="https://web.archive.org/web/20120611033833/http://blogs.nature.com/mfenner/2009/08/08/recipe-distributing-papers-for-a-journal-club">Recipe: Distributing papers for a journal club</a>). The problem is not only that email is really bad for sending PDF files to a group of people, but that most journals don't allow redistribution of their content, even within an institution with an institutional subscription.</p><h3 id="citation-counts">Citation counts</h3><p>The number of times a paper is cited is often used as a proxy to the importance of the science in that paper. Citation counts (e.g. in the form of Impact Factor or H-Index) are often used to evaluate researchers. There are many problems with this approach, because citation counts are influenced by many other factors (e.g. time, popularity of the subject area, self-citations). But the biggest problem is the fact that there is no general agreement on how to count citations, and no database that makes this information freely available.</p><p>It might make sense to make a list of these broken steps, and then estimate the effort that would be required to fix each of them. Some broken steps are more important, and some fixes easier than others, so this exercise would give a good list of action points.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[BibApp: Mashups for Universities]]></title>
            <link>https://blog.martinfenner.org/posts/bibapp-mashups-for-universities</link>
            <guid>a1c4e777-f7b3-4b9e-bab4-4eae11aa25f8</guid>
            <pubDate>Mon, 03 May 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Friday afternoon discussions can be dangerous. Last Friday I talked with Heinz
Pampel
[https://web.archive.org/web/20120611032822/http://network.nature.com/people/UC0402CE6/profile] 
from the Helmholtz Open Access Initiative
[https://web.archive.org/web/20120611032822/http://oa.helmholtz.de/index.php?id=137] 
via email about the upcoming BibCamp Hannover
[https://web.archive.org/web/20120611032822/http://bibcamp.wordpress.com/] (a
barcamp for librarians and others interested in online science th]]></description>
            <content:encoded><![CDATA[<p>Friday afternoon discussions can be dangerous. Last Friday I talked with <a href="https://web.archive.org/web/20120611032822/http://network.nature.com/people/UC0402CE6/profile">Heinz Pampel</a> from the <a href="https://web.archive.org/web/20120611032822/http://oa.helmholtz.de/index.php?id=137">Helmholtz Open Access Initiative</a> via email about the upcoming <a href="https://web.archive.org/web/20120611032822/http://bibcamp.wordpress.com/">BibCamp Hannover</a> (a barcamp for librarians and others interested in online science that will take place this weekend). I said that I was looking for a solution to make the list of publications from our institution more accessible – we currently store them in a regularly updated <a href="https://web.archive.org/web/20120611032822/http://www.refworks.com/refshare/?site=047931198224000000%2F">RefWorks database</a>. Heinz not only told me that there is actually a name for this kind of tool (Current Research Information System or <a href="https://web.archive.org/web/20120611032822/http://www.cris2010.org/">CIRS</a>), but also gave me some good links. <a href="https://web.archive.org/web/20120611032822/http://espace.library.uq.edu.au/eserv/UQ:152805/Simon_Porter.pdf">This talk</a> by Simon Porter from the University of Melbourne is for example a very good introduction to the topic.</p><p>Heinz also suggested <a href="https://web.archive.org/web/20120611032822/http://bibapp.org/">BibApp</a>, a nice web-based tool for exactly this purpose written in the <em><em>Ruby on Rails</em></em> programming language. Because I am a <a href="https://web.archive.org/web/20120611032822/http://www.mh-hannover.de/studien/en/">very familiar with Ruby on Rails</a>, I took a closer look.</p><p><a href="https://web.archive.org/web/20120611032822/http://vimeo.com/2104723">Bibapp – Find Campus Experts</a> from <a href="https://web.archive.org/web/20120611032822/http://vimeo.com/user885968">Eric Larson</a> on <a href="https://web.archive.org/web/20120611032822/http://vimeo.com/">Vimeo</a>.</p><p>In the end I spent a good deal of this weekend setting up <em><em>BibApp</em></em>, importing all publications from our institution since 2008 via <em><em>Refworks</em></em>, starting to add researchers and research groups and adjusting <em><em>BibApp</em></em> to our needs (mainly changing the layout to our institution style and starting to translate the templates into German). <em><em>BibApp</em></em> is already a fully working system and is telling me some interesting things. This includes many interesting papers from our institution that I didn't know about, but also that two researchers each have published more than 150 papers in two years, and that my colleagues have published 6 papers in <a href="https://web.archive.org/web/20120611032822/http://www.timeshighereducation.co.uk/story.asp?storycode=411168">Medical Hypotheses</a> since 2008. The technical aspects of setting up <em><em>BibApp</em></em> are almost solved (it helps that I run two other <em><em>Ruby on Rails</em></em> applications at my institution), so now I have to convince our library and administration that it's worth having (and maintaining) such a CRIS tool.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611032822im_/http://farm5.static.flickr.com/4060/4572784486_52fff91a71_d.jpg" class="kg-image" alt></figure><p><a href="https://web.archive.org/web/20120611032822/http://blogs.nature.com/mfenner/2010/04/17/improving-the-conduct-of-science">Two weeks ago I wrote about the upper part of the figure</a> (for a <em><em>NSF</em></em> workshop that got postponed to September because of the Volcano ash). As far as I can see, there is no standard between funding agencies for grant reporting, and both DataCite and ORCID are fairly new initiatives.</p><p>The CRIS can be used to facilitate access to institutional repositories or primary research datasets. <em><em>BibApp</em></em> supports the <a href="https://web.archive.org/web/20120611032822/http://www.ariadne.ac.uk/issue54/allinson-et-al/">SWORD</a> protocol for article deposition, and it automatically checks all papers against the <a href="https://web.archive.org/web/20120611032822/http://www.sherpa.ac.uk/romeo/">ROMEO</a> database of publisher copyright policies. A CRIS is a great discovery tool and it can be further enhanced by integration with social networking tools (e.g. with the <a href="https://web.archive.org/web/20120611032822/http://blogs.nature.com/u6e5b2ce1/2010/04/28/coming-soon-all-new-nature-network">new Nature Network</a> or the <a href="https://web.archive.org/web/20120611032822/http://www.mendeley.com/blog/press-release/announcing-mendeley-open-api/">Mendeley API</a> both announced a few days ago).</p><p>The main interest of administrations if of course evaluation of research output. A CRIS can be used to do exactly that, and it has two advantages: a) it can automate some of the processes that are currently done manually by researchers (regularly collecting and reporting information about grants and publications), and b) it is a great platform to develop new tools for the evaluation of scientific output. My institution currently evaluates using the <em><em>Impact Factor</em></em> of the published papers, and takes first and last authorship (and female authors) into consideration. A CRIS would allow an institution to use similar tools as the <a href="https://web.archive.org/web/20120611032822/http://www.plosone.org/static/almInfo.action">PLoS Article-Level Metrics</a> (usage data, citation data, usage by social networking tools) instead of the <em><em>Impact Factor</em></em>, and several projects are already trying some of that, e.g. the German <a href="https://web.archive.org/web/20120611032822/http://www.dini.de/projekte/oa-statistik/english/">Open Access Statistics</a> project.</p><p>But the best thing about CRIS tools in general, and <em><em>BibApp</em></em> in particular, is that they add a lot of value for relatively little effort. Universities don't want to (and can't) compete with large institutions or companies such as scientific publishers. Because they basically just integrate the data that are already available in an intelligent way (Mashup in Web 2.0 language), they require a reasonable effort to maintain. This will be particularly true once the <a href="https://web.archive.org/web/20120611032822/http://www.orcid.org/">ORCID</a> system of unique author identifiers is in place, because author disambiguation is currently one of the most time-consuming aspects (even though <em><em>BibApp</em></em> is pretty smart about this). I'm looking forward to adapt <em><em>BibApp</em></em> to the <em><em>ORCID</em></em> prototype system that is planned for this summer.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ResearcherID: Interview with Renny Guida]]></title>
            <link>https://blog.martinfenner.org/posts/researcherid-interview-with-renny-guida</link>
            <guid>c624cf86-0cf0-4224-8dba-b8a0bf2a3820</guid>
            <pubDate>Mon, 26 Apr 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Open Researcher and Contributor ID or ORCID
[https://web.archive.org/web/20120611094015/http://blogs.nature.com/mfenner/2010/01/03/orcid-or-how-to-build-a-unique-identifier-for-scientists-in-10-easy-steps] 
is a community effort to standardize researcher identification. The initiative
was first announced last December, and is supported by a growing number
[https://web.archive.org/web/20120611094015/http://www.orcid.org/gallery.php] of
publishers, scholarly societies and academic institutions. Th]]></description>
            <content:encoded><![CDATA[<p><em><em>Open Researcher and Contributor ID</em></em> or <a href="https://web.archive.org/web/20120611094015/http://blogs.nature.com/mfenner/2010/01/03/orcid-or-how-to-build-a-unique-identifier-for-scientists-in-10-easy-steps">ORCID</a> is a community effort to standardize researcher identification. The initiative was first announced last December, and is supported by a <a href="https://web.archive.org/web/20120611094015/http://www.orcid.org/gallery.php">growing number</a> of publishers, scholarly societies and academic institutions. <a href="https://web.archive.org/web/20120611094015/http://science.thomsonreuters.com/">Thomson Reuters</a> is not only one of the founding members of the initiative, but will also provide the <a href="https://web.archive.org/web/20120611094015/http://www.researcherid.com/">ResearcherID</a> technology as a starting point for building the <em><em>ORCID</em></em> platform.</p><p>I asked <em><em>Renny Guida</em></em> from Thomson Reuters a few questions about his thoughts on unique author identifiers in general, and <em><em>ResearcherID</em></em> and <em><em>ORCID</em></em> in particular.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611094015im_/http://blogs.plos.org/mfenner/files/2010/11/researcherid.jpg" class="kg-image" alt></figure><p><em><em>Members of the ResearcherID team (from left to right): Renny Guida, Ellen Rotenberg, Chiu Chung and Deepak Chaturvedi.</em></em></p><h3 id="1-what-is-researcherid">1. What is ResearcherID?</h3><p><em><em>ResearcherID</em></em> is a freely available system that helps the research community overcome the issues of individual name ambiguity and attribution. The system is available to everyone and allows a researcher to establish, maintain and control a biographic and bibliographic profile.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611094015im_/http://images.isiknowledge.com/WOK48B3/images/RID/rid_logo.gif" class="kg-image" alt></figure><p>The system is the result of research that found that individual identity touches many parts of scholarly communications and that researcher’s and academic institutions were ready to take some ownership of the issue to ensure proper attribution of scholarship.</p><p>The Registry is composed of biographical and bibliographic components which are accessible at the <a href="https://web.archive.org/web/20120611094015/http://www.researcherid.com/">researcherid.com</a> web site. A web services API makes it possible for an institution or organization to create and use this information in their systems.</p><p>The biographical component of the registry creates a unique identifier and persistent web location for each user. The system is designed to allow a researcher to control privacy and access to the information in their profile, even if the information was uploaded by the researcher’s institution. Researchers can enter and manage information about themselves, their work and their affiliations (<a href="https://web.archive.org/web/20120611094015/http://www.researcherid.com/rid/C-3246-2009">example record</a>).</p><p>The bibliographic component of <em><em>ResearcherID</em></em> allows researchers to connect articles, patents and other scholarly items to their profile. Items can be loaded from <a href="https://web.archive.org/web/20120611094015/http://www.endnote.com/">EndNote</a>, the <a href="https://web.archive.org/web/20120611094015/http://www.isiwebofknowledge.com/">Web of Science</a> or in a text format into one of three lists.</p><p>As I mentioned, a web service makes it possible for organizations to create and manage both biographic and bibliographic information for their faculty and researchers. Upload, download and search services make this possible. For example, David Palmer at the University of Hong Kong uses ResearcherID to create <a href="https://web.archive.org/web/20120611094015/http://hub.hku.hk/rp/rp00023">identifiers for HKU researchers</a>.</p><h3 id="2-what-is-orcid-and-how-does-it-relate-to-researcherid"><em><em>2. What is ORCID, and how does it relate to ResearcherID?</em></em></h3><p>Open Researcher Contributor ID (ORCID) is an initiative to better understand how the issue of individual name ambiguity can be solved. Currently a cross section of the research community is discussing ways to overcome the issue with the hopes of establishing an open registry that will be used by the global research community.</p><p>One of the lessons learned from Thomson Reuters’ development of <em><em>ResearcherID</em></em> is that issues of name ambiguity and attribution are problems bigger than any one organization can address. And, having multiple solutions to the problem will not meet the needs of researchers or the scholarly community. Based on <em><em>ResearcherID’s</em></em> innovation and success in addressing these issues, Thomson Reuters plans to offer <em><em>ORCID</em></em> the technology and infrastructure for managing the biographic registry.</p><p>The initiative has grown from early 2009 conversations between <em><em>Thomson Reuters</em></em> and <em><em>Nature Publishing Group</em></em> into a global collaboration of academic institutions, research centers, funding agencies, scholarly societies and publishers. To date, everyone involved in the effort is volunteering their time and work is focused on the business and technical implications of creating an independent organization to manage an identity registry system. More information on the initiative can be found at the <a href="https://web.archive.org/web/20120611094015/http://www.orcid.org/">ORCID</a> web site.</p><h3 id="3-there-are-numerous-other-initiatives-for-unique-researcher-identifiers-what-is-unique-about-researcherid">3.There are numerous other initiatives for unique researcher identifiers. What is unique about ResearcherID?</h3><p>What is unique about <em><em>ResearcherID</em></em> is that it is a global initiative and not focused on any one organization, consortia, country or application. The system also ensures privacy and allows researchers to control their information. The <em><em>ResearcherID</em></em> is portable and can be included in authored works and used as a dynamic link on the researcher’s website.</p><h3 id="4-how-does-researcherid-integrate-with-other-thomson-reuters-products-and-outside-tools">4. How does ResearcherID integrate with other Thomson Reuters products and outside tools?</h3><p>The system was designed to allow all users, independent of any Thomson Reuters subscriptions, to benefit from the application. All users have the ability to establish an identifier, create and manage a biographical profile, and manage a list of scholarly works.</p><p>The <em><em>ResearcherID</em></em> system is closely coupled with EndNote and by May 2010, all ResearcherID users, independent of any Thomson Reuters subscriptions, will have access to the <a href="https://web.archive.org/web/20120611094015/http://www.endnoteweb.com/">web version of EndNote</a>.</p><p>For users having access to the <em><em>Web of Knowledge</em></em> there are a number of additional integration points:</p><ul><li>Researchers can claim items from the <em><em>Web of Science</em></em> by either performing a search or using the <a href="https://web.archive.org/web/20120611094015/http://images.isiknowledge.com/WOK46/help/WOS/h_da_sets.html">Distinct Author Sets</a> feature that automatically clusters articles believed to be authored by the same person.</li><li>Researchers can claim items from any <em><em>Web of Knowledge</em></em> database.</li><li>The ResearcherID system uses the <em><em>Web of Science</em></em> to automatically update the ResearcherID system with the times cited count.</li><li><em><em>Web of Science</em></em> times cited information is used to generate personal citation reports and the citation visualizations (currently being tested in <a href="https://web.archive.org/web/20120611094015/http://wokinfo.com/researcherid/ridlabs/">ResearcherID labs</a> environment) to visualize collaboration and citation networks.</li></ul><h3 id="5-do-you-plan-to-continue-with-researcherid-once-orcid-is-fully-working">5. Do you plan to continue with ResearcherID once ORCID is fully working?</h3><p>Our plans are to adapt and enhance <em><em>ResearcherID</em></em> to support <em><em>ORCID</em></em> and allow users access to personal citation functionality and services on our <em><em>Web of Knowledge</em></em> platform. Thomson Reuters is one of the catalysts of the <em><em>ORCID</em></em> initiative and is working with our colleagues to help <em><em>ORCID</em></em> succeed. We will fully support <em><em>ORCID</em></em> from the enhanced <em><em>ResearcherID</em></em> platform and other Thomson Reuters applications.</p><h3 id="6-what-would-be-the-benefits-of-using-a-unique-author-identifier-for-a-researcher">6. What would be the benefits of using a unique author identifier for a researcher?</h3><p>The unique author identifier not only addresses the issues of name ambiguity and attribution but benefits the scholarly communication process by maintaining the link between the researchers and their scholarship. This link is critical given the proliferation of information available on the Web and the demand to find information in less time and with greater accuracy.</p><p>The real benefit comes when all involved in the scholarly communications can leveraged the identifier to improve the process. Easier tenure review, better assessment of research output, more intelligent discovery tools, enhanced peer review systems and tools to maintain scholarly networks are some of the benefits of using a unique author identifier.</p><h3 id="7-where-do-you-see-the-biggest-obstacles-in-launching-a-universally-accepted-unique-identifier-for-researchers-technical-or-social">7. Where do you see the biggest obstacles in launching a universally accepted unique identifier for researchers? Technical or social?</h3><p>I see two large obstacles. The first, and I believe most important, is the researchers perception of security and ownership. The system must allow researchers to control and manage their data.</p><p>The second biggest obstacle is the sustainability of an organization that would provide the identity service. <em><em>ORCID</em></em> is proving that the community can work together to address the technicalities of a global identity system. And, while there are significant technical, privacy and other challenges, the initiative is making great progress. However, such a service requires resources and the system must be supported financially by the community.</p><h3 id="8-what-is-your-thinking-with-regards-to-automatically-assigning-unique-author-identifiers-to-papers-vs-authors-claiming-their-publications">8. What is your thinking with regards to automatically assigning unique author identifiers to papers vs. authors claiming their publications?</h3><p>I see these as being two complementary and important services. Algorithmic clustering of papers is a common feature in many applications including the <em><em>Web of Science</em></em> and other bibliographic databases. For the most part, these clustering systems do a good job identifying a high percentage of papers for a particular author. However, because none of the industry’s algorithms are perfect, a claiming system that enables the researcher to associate all of their papers is an ideal complement and helps to complete a career profile of a researcher’s works.</p><h3 id="9-what-are-your-responsibilities-at-thomson-reuters">9. What are your responsibilities at Thomson Reuters?</h3><p>My title is Director of eResearch Services. Some of my responsibilities include individual identity initiatives (ResearcherID and the Web of Science Distinct Author Sets clustering system), Web of Knowledge web services, inbound and outbound linking, current awareness and alerting systems, and <a href="https://web.archive.org/web/20120611094015/http://www.projectcounter.org/about.html">COUNTER</a> compliant usage reporting systems.</p><h3 id="10-what-did-you-do-before-working-at-thomson-reuters">10. What did you do before working at Thomson Reuters?</h3><p>Prior to joining Thomson Reuters in 1997 I worked for the Datapro division of McGraw Hill in product development and for Xerox selling document management systems.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Standards for the Conduct of Science in the Information Age]]></title>
            <link>https://blog.martinfenner.org/posts/standards-for-the-conduct-of-science-in-the-information-age</link>
            <guid>7729b858-f9b7-4af1-a98d-045309f98995</guid>
            <pubDate>Sat, 17 Apr 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Assuming our airports are again open next weekend, I will be attending a meeting
organized by the NSF
[https://web.archive.org/web/20120611093752/http://www.nsf.gov/] (National
Science Foundation) and EuroHORCS
[https://web.archive.org/web/20120611093752/http://www.eurohorcs.org/E/Pages/home.aspx] 
(European Heads of Research Councils) on Changing the Conduct of Science in the
Information Age in Washington on April 26. We have been asked to submit a one
page white paper in advance of the meeting]]></description>
            <content:encoded><![CDATA[<p>Assuming our airports are again open next weekend, I will be attending a meeting organized by the <a href="https://web.archive.org/web/20120611093752/http://www.nsf.gov/">NSF</a> (National Science Foundation) and <a href="https://web.archive.org/web/20120611093752/http://www.eurohorcs.org/E/Pages/home.aspx">EuroHORCS</a> (European Heads of Research Councils) on <em><em>Changing the Conduct of Science in the Information Age</em></em> in Washington on April 26. We have been asked to submit a one page white paper in advance of the meeting.<sup><a href="https://web.archive.org/web/20120611093752/http://blogs.plos.org/mfenner/2010/04/17/improving_the_conduct_of_science/#fn1">1</a></sup> I decided to focus on the importance of standards, obviously leaving out many other important technological and social aspects. But defining and adhering to standards will enable or enhance a number of very interesting ways to conduct and report science in the information age.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120611093752im_/http://farm4.static.flickr.com/3569/3429599200_2f54dbd5cd_d.jpg" class="kg-image" alt><figcaption>Flickr photo by Joye~.</figcaption></figure><p>Improving the conduct of science through digital technology requires standards for linking to and formatting scholarly resources. These standards should be coordinated by independent organizations that are not restricted to geographic areas or particular research domains.</p><h3 id="data-access">Data access</h3><p>Digital Object Identifiers (<a href="https://web.archive.org/web/20120611093752/http://www.doi.org/">DOIs</a>) are the primary system to link to digital content. The International <a href="https://web.archive.org/web/20120611093752/http://www.datacite.org/">DataCite</a> initiative is the DOI registration agency for scientific primary data. Although there are many uses of DOIs for primary research data (e.g. <a href="https://web.archive.org/web/20120611093752/http://www.pangaea.de/">PANGAEA</a>, earth system research), many systems still use different identifiers.<br>Research funders and journals working in specific domains should collaborate on standards and best practices for primary research datasets, and journal publishers should encourage or even require linking to research datasets from publications. Successful examples include <a href="https://web.archive.org/web/20120611093752/http://www.ncbi.nlm.nih.gov/genbank/">GenBank</a> (genetic sequences) and <a href="https://web.archive.org/web/20120611093752/http://www.mged.org/Workgroups/MIAME/miame.html">MIAME</a> (microarray gene expression).</p><h3 id="knowledge-access">Knowledge access</h3><p>DOIs have become the standard identifier for electronic scholarly publications and are managed by the <a href="https://web.archive.org/web/20120611093752/http://www.crossref.org/">CrossRef</a> registration agency. Journal articles, databases and websites linking to scholarly publications should use DOIs whenever possible instead of internal identifiers such as the PubMed ID or direct links to publisher webpages. Publishers should implement citation styles that use the DOI instead of volume, issue and page numbers.<br>The <a href="https://web.archive.org/web/20120611093752/http://dtd.nlm.nih.gov/">NLM DTD</a> is the standard format used by PubMed Central and many scholarly publishers to produce content for reading in the HTML, PDF or ePub formats. The <a href="https://web.archive.org/web/20120611093752/http://www.microsoft.com/mscorp/tc/scholarly_communication.mspx">Article Authoring Add-in for Microsoft Office Word</a> and <a href="https://web.archive.org/web/20120611093752/http://pkp.sfu.ca/lemon8">Lemon8-XML</a> allow researchers to produce content in the NLM DTD format. The workflow of writing, reviewing and publishing scientific papers should be based completely on the NLM DTD and tools for collaborative writing, journal submission and peer review should be build around that format.</p><h3 id="attribution">Attribution</h3><p>The recently announced<a href="https://web.archive.org/web/20120611093752/http://blogs.plos.org/mfenner/2010/04/17/improving_the_conduct_of_science/#fn2">2</a> Open Researcher and Contributor ID (<a href="https://web.archive.org/web/20120611093752/http://www.orcid.org/">ORCID</a>) is one of many initiatives for a unique researcher identifier, but has probably the broadest support among institutions, publishers and research organizations. ORCID will be managed by an independent non-profit organization, and will allow the exchange of profiles with other researcher identifier systems such as those used by <a href="https://web.archive.org/web/20120611093752/http://www.scopus.com/">Scopus</a>, <a href="https://web.archive.org/web/20120611093752/http://repec.org/">RePEc</a>, or <a href="https://web.archive.org/web/20120611093752/https://twiki.cern.ch/twiki/bin/view/Inspire/WebHome">Inspire</a>.<br>The information in the author profile may be initially provided by an institution, society or publisher, but should eventually be claimed by the individual researcher because of privacy concerns and because automated author disambiguation is never 100% accurate. Attribution should include all aspects of scholarly activity, including curation of primary research datasets and peer review.<br>The Public Library of Science (PLoS) <a href="https://web.archive.org/web/20120611093752/http://article-level-metrics.plos.org/">article-level metrics</a> make available comprehensive information (citations, downloads, social bookmarks, comments, etc.) with every published article. This system should be linked to author identifiers and developed into a standard for scholarly resources. Other scholarly publishers and databases for primary research data should then adopt these metrics.</p><p>fn1. Cameron Neylon's draft white paper is <a href="https://web.archive.org/web/20120611093752/http://cameronneylon.net/blog/draft-white-paper-researcher-identifiers/">here</a>.</p><p>fn2. Interestingly both DataCite and ORCID were first announced December 1, 2009 at two independent events in London (press releases <a href="https://web.archive.org/web/20120611093752/http://www.tib-hannover.de/en/the-tib/news/news/id/133/">here</a> and <a href="https://web.archive.org/web/20120611093752/http://orcid.org/media/pdf/ORCID_Announcement.pdf">here</a>).</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Editorial Peer Reviewers’ Recommendations at a General Medical Journal: Are They Reliable and Do Editors Care?]]></title>
            <link>https://blog.martinfenner.org/posts/editorial-peer-reviewers-recommendations-at-a-general-medical-journal-are-they-reliable-and-do-editors-care</link>
            <guid>820b0e9d-518b-47e3-ba24-862575e0de9e</guid>
            <pubDate>Mon, 12 Apr 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Peer review is central to how we evaluate science and therefore how journal
papers, grants and jobs are awarded. Peer review is done in many different ways,
and has dramatically changed in the last 25 years. But the purpose of peer
review is still to improve the quality of research by providing feedback, and to
evaluate the quality of research. The evaluation serves as a filter both for
limited resources (e.g. grants or jobs; publication in a journal is no longer a
limited resource), and for oth]]></description>
            <content:encoded><![CDATA[<figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611094155im_/http://www.researchblogging.org/public/citation_icons/rb2_large_gray.png" class="kg-image" alt="ResearchBlogging.org"></figure><p>Peer review is central to how we evaluate science and therefore how journal papers, grants and jobs are awarded. Peer review is done in many different ways, and has dramatically changed in the last 25 years. But the purpose of peer review is still to improve the quality of research by providing feedback, and to evaluate the quality of research. The evaluation serves as a filter both for limited resources (e.g. grants or jobs; publication in a journal is no longer a limited resource), and for other researchers to focus on the most relevant work in their field.</p><p>It is therefore surprising that relatively little research on peer review itself has been done. Most discussions focus on the shortcomings of peer review, and the arguments are often based on personal experience and/or interests. Good research on peer review can help to improve the peer review process. Last week such a paper was published in <em><em>PLoS ONE.</em></em></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120611094155im_/http://farm4.static.flickr.com/3093/3157621454_902378aa2f_d.jpg" class="kg-image" alt><figcaption>Flickr image by Gideon Burton.</figcaption></figure><p><strong><strong>Richard Kravitz</strong></strong> and colleagues looked at the recommendations of peer reviewers, and how they influenced the editorial decision to publish or reject a paper. The study looked at 6213 manuscripts received 2004-2008 at the <em><em>Journal of General Internal Medicine (JGIM)</em></em> where four of the authors were either current or former editors in chief.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611094155im_/http://www.wiley.com/bw/content/BPL_Images/Journal_Banners/JGI/JGI_large.jpg" class="kg-image" alt></figure><p>At <em><em>JGIM</em></em> submitted manuscripts were first screened by an editor in chief and a deputy editor. Most manuscripts were rejected at this step, 2264 manuscripts (36%) were sent out for peer review. 2916 reviewers wrote a total of 5581 reviews (1-4 per manuscript) which included comments and a recommendation. Eventually 43% of the manuscripts were accepted for publication.</p><p>Overall, there was agreement between all reviewers in just over half of the manuscripts (54.6% ), furthermore editors did not follow these recommendations in another 10% of manuscripts:</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611094155im_/http://www.plosone.org/article/fetchObjectAttachment.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0010072.t001&amp;representation=PNG_M" class="kg-image" alt></figure><p><strong><strong>Table 1. Likelihood of Initial Decision to Reject in Relation to Reviewer Agreement.</strong></strong></p><p>The inter-reviewer agreement was slightly higher than what would have been expected by chance, and was lower than the agreement between recommendations for several manuscripts by the same reviewer. In contrast, there was little correlation between editorial decisions for different manuscripts handled by the same editor.</p><p>The authors write in the discussion:</p><p><em><em>If reviewers cannot regularly agree on whether to recommend rejection or further consideration, the marginal contribution of such summative recommendations may be small, and worse, they may distract from reviewers' primary contribution, which is to improve the reporting – and ultimately the performance – of science.</em></em></p><p>The paper authors consider the following to improve reviewer recommendations: using more reviewers per manuscript, providing better training for reviewers, or recommendations could be dropped altogether and reviewers asked to focus instead on evaluating the strengths and weaknesses of manuscripts. Some journals are obviously using this latter approach.</p><p>Several studies <a href="https://web.archive.org/web/20120611094155/http://dx.doi.org/10.1097/01.ede.0000254668.63378.32">have shown</a> that most rejected manuscripts will eventually be published somewhere else. One important reason is that publication space in journals is no longer a scarcity as it was before electronic publishing became widespread. This means that the ultimate decision whether or not something will be published in a peer-reviewed journal rests with the authors and not the editors or reviewers. Reviewers should keep this in mind.</p><h3 id="references">References</h3><p>Kravitz, R., Franks, P., Feldman, M., Gerrity, M., Byrne, C., &amp; Tierney, W. (2010). Editorial Peer Reviewers' Recommendations at a General Medical Journal: Are They Reliable and Do Editors Care? <em>PLoS ONE, 5</em> (4) https://doi.org/<a href="https://web.archive.org/web/20120611094155/http://dx.doi.org/10.1371/journal.pone.0010072">10.1371/journal.pone.0010072</a></p><p><strong><strong>Further reading:</strong></strong><br>* <a href="https://web.archive.org/web/20120611094155/http://www.nasw.org/users/mslong/2010/2010_04/PeerReview.htm">Questioning the Value of Recommendations in Peer Review</a> (Michael Long)<br>* <a href="https://web.archive.org/web/20120611094155/http://bit.ly/9YUs0q">Scrap peer review and beware of top journals</a> (Richard Smith)<br>* <a href="https://web.archive.org/web/20120611094155/http://cameronneylon.net/blog/peer-review-what-is-it-good-for/">Peer review: What is it good for?</a> (Cameron Neylon)<br>* <a href="https://web.archive.org/web/20120611094155/http://backreaction.blogspot.com/2010/04/peer-review-vi.html">Peer Review VI</a> (Sabine Hossenfelder)<br>* <a href="https://web.archive.org/web/20120611094155/http://blogs.nature.com/mfenner/2009/07/13/the-value-of-peer-review">The value of peer review</a> (me)</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Lst wk I had g8t fun @ #hcsmeucamp]]></title>
            <link>https://blog.martinfenner.org/posts/lst-wk-i-had-g8t-fun-hcsmeucamp</link>
            <guid>2025b1d0-ea26-4c6a-93ad-172d0d5b2b59</guid>
            <pubDate>Mon, 05 Apr 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Last Wednesday I attended the Health Care Social Media Camp
[https://web.archive.org/web/20120611095039/http://www.hcsmeucamp.com/] in
Berlin. Healthcare Social Media in Europe – #hcsmeu for short – is a community
of healthcare twitterers and social media users from Europe, started in August
2009 by Silja Chouquet
[https://web.archive.org/web/20120611095039/http://www.whydotpharma.com/about/] 
and Andrew Spong [https://web.archive.org/web/20120611095039/http://stwem.com/].
I am interested in #hc]]></description>
            <content:encoded><![CDATA[<p>Last Wednesday I attended the <a href="https://web.archive.org/web/20120611095039/http://www.hcsmeucamp.com/">Health Care Social Media Camp</a> in Berlin. Healthcare Social Media in Europe – <em><em>#hcsmeu</em></em> for short – is a community of healthcare twitterers and social media users from Europe, started in August 2009 by <a href="https://web.archive.org/web/20120611095039/http://www.whydotpharma.com/about/">Silja Chouquet</a> and <a href="https://web.archive.org/web/20120611095039/http://stwem.com/">Andrew Spong</a>. I am interested in <em><em>#hcsmeu</em></em> not only because I am a physician treating cancer patients, but also because there is a lot of overlap to many other things that interest me in doing and communicating science online, e.g. <a href="https://web.archive.org/web/20120611095039/http://blogs.nature.com/mfenner/2010/03/15/chances-and-problems-of-doing-science-online">doing clinical research with online tools</a>, <a href="https://web.archive.org/web/20120611095039/http://blogs.nature.com/mfenner/2010/03/22/cancer-and-the-media">reporting about cancer in the media</a>, or <a href="https://web.archive.org/web/20120611095039/http://blogs.nature.com/mfenner/2009/01/16/scienceonline09-providing-public-health-and-medical-information-to-all">better access to public health and medical information</a>.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611095039im_/http://farm5.static.flickr.com/4034/4482350267_c1105c6103.jpg" class="kg-image" alt></figure><p><em><em>Me listening to Andrew starting off the meeting. #hcsmeucamp Twitter feed in background. Flickr photo by dikomci.</em></em></p><p>The meeting was a small gathering of about 45 people in the <a href="https://web.archive.org/web/20120611095039/http://blogs.nature.com/mfenner/2009/07/10/i-was-at-scibarcamp-palo-alto">unconference</a> format, so we started with collecting questions we wanted to discuss during the day. For each of the four sessions (patient perspective, health care professional perspective, pharma perspective and <em><em>#hcsmeu</em></em> mission and goals) we first split up in 3-4 smaller groups to talk about some of the questions, and in the second half summarized and discussed this as the whole group. Some participants demonstrated excellent skills with the whiteboard.</p><p><em><em>How do we get scientists interested in social media?</em></em> is something we have talked about many times, e.g. <a href="https://web.archive.org/web/20120611095039/http://scholarlykitchen.sspnet.org/2009/10/19/scientists-still-not-joining-social-networks/">Scientists Still Not Joining Social Networks</a>. Healthcare professionals are also reluctant to use social media, and we come up with some interesting suggestions to do something about this:</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611095039im_/http://farm5.static.flickr.com/4017/4483005134_dfea7e66ba_d.jpg" class="kg-image" alt></figure><p><em><em>Flickr photo by dikomci.</em></em></p><p>We also talked about the dangers of getting involved with social media. The doctor-patient communication is something that is difficult to translate from personal communication to social media (lawyers and some other professions have similar problems), but otherwise the dangers look familiar to many scientists:</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611095039im_/http://farm5.static.flickr.com/4027/4483004296_b2ba2d7482_d.jpg" class="kg-image" alt></figure><p><em><em>Flickr photo by dikomci.</em></em></p><p>The last part of the meeting was about <em><em>#hcsmeu</em></em>, and what next steps to take. <em><em>#hcsmeu</em></em> is very focused on moving the agenda forward, and that spirit penetrated the whole meeting. After a very productive discussion we come up with some good action items:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120611095039im_/http://farm3.static.flickr.com/2715/4482357507_121d7b3c86.jpg" class="kg-image" alt><figcaption>Flickr photo by dikomci.</figcaption></figure><p><em><em>#hcsmeu</em></em> is very much built around Twitter, including regular Tweetups every Friday, and <a href="https://web.archive.org/web/20120611095039/http://twitter.com/#search?q=%23hcsmeucamp">heavy use</a> during the meeting. I am a big <a href="https://web.archive.org/web/20120611095039/http://www.twitter.com/mfenner">Twitter fan</a>, but have <a href="https://web.archive.org/web/20120611095039/http://blogs.nature.com/mfenner/2010/01/17/scienceonline2010-i-wish-i-was-there">argued before</a> that Twitter is not the best tool for conference microblogging, as it is difficult to have connected discussions around a topic, and archieving is complicated. And pictures, documents and links are better collected in places like <em><em>Flickr</em></em>, <em><em>Slideshare</em></em> and <em><em>delicious</em></em>. <a href="https://web.archive.org/web/20120611095039/http://mfenner.posterous.com/best-practices-for-hcsmeu-social-media-use">My suggestions</a> for the <em><em>#hcsmeu</em></em> knowledge hub therefore include more use of other social media tools, plus <em><em>FriendFeed</em></em> or a similar aggregation/microblogging tool to connect all the social media in one place. <em><em>Facebook</em></em> is actually a perfect tool to do all this, but unfortunately is still perceived by many as a tool for <em><em>private</em></em> social networking. I met many very smart and friendly people during the meeting and hope to hear more about <em><em>#hcsmeu</em></em> at <em><em>Science Online London 2010</em></em> in September or the <a href="https://web.archive.org/web/20120611095039/http://www.medicine20congress.com/ocs/index.php/med/med2010">Medicine 2.0 conference</a> in November.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Let’s make science metrics more scientific]]></title>
            <link>https://blog.martinfenner.org/posts/lets-make-science-metrics-more-scientific</link>
            <guid>31ea8d5d-6433-4205-b08d-e7107a0b2d8b</guid>
            <pubDate>Mon, 29 Mar 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[In the March 25 edition of Nature, Julia Lane
[https://web.archive.org/web/20120611101936/http://client.norc.org/jole/SOLEweb/JLHome.html]
, Program Director of the Science of Science and Innovation Policy Program
[https://web.archive.org/web/20120611101936/http://www.nsf.gov/funding/pgm_summ.jsp?pims_id=501084&org=NSF&from_org=NSF] 
at the National Science Foundation, wrote an interesting opinion piece about the
assessment of scientific performance. She argues that the current systems of
measur]]></description>
            <content:encoded><![CDATA[<figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611101936im_/http://www.researchblogging.org/public/citation_icons/rb2_large_gray.png" class="kg-image" alt="ResearchBlogging.org"></figure><p>In the March 25 edition of <em><em>Nature</em></em>, <a href="https://web.archive.org/web/20120611101936/http://client.norc.org/jole/SOLEweb/JLHome.html">Julia Lane</a>, Program Director of the <a href="https://web.archive.org/web/20120611101936/http://www.nsf.gov/funding/pgm_summ.jsp?pims_id=501084&amp;org=NSF&amp;from_org=NSF">Science of Science and Innovation Policy Program</a> at the National Science Foundation, wrote an interesting opinion piece about the assessment of scientific performance. She argues that the current systems of measurement are inadequate, as they have several inherent problems and do not capture the full spectrum of scientific activities. Good scientific metrics are difficult, but without them we risk making the wrong decisions about funding and academic positions.</p><p>Julia Lane suggests that we develop and use standard identifiers both for researchers and their scientific output (examples given include the <a href="https://web.archive.org/web/20120611101936/http://www.doi.org/">DOI</a> for publications and <a href="https://web.archive.org/web/20120611101936/http://www.orcid.org/">ORCID</a> as unique author identifier), that we develop standards for reporting scientific achievements (e.g. using the <a href="https://web.archive.org/web/20120611101936/http://www.nsf.gov/bfa/dias/policy/rppr/">Research Performance Progress Report</a> format), and that we open up and connect the various tools and databases that collect scientific output. She cites the <a href="https://web.archive.org/web/20120611101936/http://lattes.cnpq.br/english/index.htm">Lattes</a> database for Brasilian researchers as a successful example for systematically collecting scientific output. Another example given is the ongoing <a href="https://web.archive.org/web/20120611101936/http://nrc59.nas.edu/star_info2.cfm">STAR METRICS</a> project which measures the impact of federally funded research on economic, scientific and social outcomes.</p><p>The article emphasizes that is not enough to think about how to best collect and report scientific output, but that it is equally important to understand what these data mean and how to use them, and this may differ from field to field. Knowledge creation is complex and measuring this can not be reduced to counting scientific papers and the number of times they are cited. Social scientists and economists should be involved in this step. Julia Lane suggests an international platform supported by funding agencies in which ideas and potential solutions for science metrics can be discussed.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120611101936im_/http://farm4.static.flickr.com/3406/3592693331_ae8822f91d_d.jpg" class="kg-image" alt><figcaption>Flickr photo by jepoirrier.</figcaption></figure><p>The article contains a lot of food for thought and has already collected some insightful <a href="https://web.archive.org/web/20120611101936/http://www.nature.com/nature/journal/v464/n7288/full/464488a.html#comments">comments</a>. In perfect timing, <em><em>Nature</em></em> this week not only made <a href="https://web.archive.org/web/20120611101936/http://www.nature.com/press_releases/naturenews.html">Nature News available without a subscription</a>, but also <a href="https://web.archive.org/web/20120611101936/http://dx.doi.org/10.1038/464466a">added commenting to all their articles</a>. I would like to add some thoughts on topics that were not covered because of space constraints and different perspective.</p><h3 id="what-are-the-standard-identifiers-for-research-output"><strong><strong>What are the standard identifiers for research output?</strong></strong></h3><p>Using standard identifiers for research output is an essential first step, and the standard identifier for scientific papers is the DOI. So why is it that PubMed (the most important database for biomedical articles, published by the U.S. National Institutes of Health) still uses their own PMID and doesn't display the DOI in their abstract and summary views? And where is the DOI in abstracts, full-text HTML or PDF of articles published in the New England Journal of Medicine, to take just one popular medical journal as an example? Both PubMed and the NEJM obviously use the DOI, but why do they make it so difficult for others?</p><p>The unique author identifier <a href="https://web.archive.org/web/20120611101936/http://www.orcid.org/">ORCID</a> was mentioned in the article (disclaimer: I am a member of the ORCID technical working group). There are many other initiatives for uniquely identifying researchers, most of them older than <strong><strong>ORCID</strong></strong> which was started in November 2009. But is very important that we can agree on a single author identifier that is supported by researchers, institutions, journals and funding organizations. <strong><strong>ORCID</strong></strong> already has support from a growing list of <a href="https://web.archive.org/web/20120611101936/http://www.orcid.org/gallery.php">ORCID members</a> and is our best chance for a widely supported and open unique author identifier. But this list of <strong><strong>ORCID</strong></strong> members is very short on funding organizations (with notable exceptions such as the <strong><strong>Wellcome Trust</strong></strong> and <strong><strong>EMBO</strong></strong>). What is holding them back, and that includes the <strong><strong>National Science Foundation</strong></strong> (where Julia Lane works) and the U.S. <strong><strong>National Institutes of Health (NIH)</strong></strong>?</p><p>Persistent identifiers are essential to attribute, cite and share primary research data sets. We have a long tradition for this with <a href="https://web.archive.org/web/20120611101936/http://www.ncbi.nlm.nih.gov/Genbank/">sequence data</a>, and there is growing demand in other research areas, especially when huge amounts of data are collected (one example is <a href="https://web.archive.org/web/20120611101936/http://www.pangaea.de/about/">PANGEA</a> for earth system research). <a href="https://web.archive.org/web/20120611101936/http://www.datacite.org/">DataCite</a> is a new initiative that aims to improve the scholarly infrastructure around datasets, and to <em><em>increase acceptance of research data as legitimate, citable contributions to the scientific record</em></em>.</p><p>With the focus on research papers, we forget that we do not have standard identifiers for many aspects of scientific activity, including</p><ul><li>research grants</li><li>principal investigator in clinical trials</li><li>scientific prizes and awards</li><li>invited lectures</li><li>curation of scientific databases</li><li>mentoring of students</li></ul><h3 id="how-do-we-measure-scientific-output">How do we measure scientific output?</h3><p>Citations are the traditional way to measure the impact of a scientific paper. Some of the problems with this approach are well-known and were for example highlighted in a 2007 editorial in the <em><em>Journal of Cell Biology</em></em> (<a href="https://web.archive.org/web/20120611101936/http://dx.doi.org/10.1083/jcb.200711140">Show me the data</a>). We need a metric that is open and not proprietary, and that measures the citations of an individual paper and not the journal as a whole. We should also not forget that the number of citations can't be compared between different fields.</p><p>A 2009 analysis by the <a href="https://web.archive.org/web/20120611101936/http://www.mesur.org/">MESUR</a> project indicates that scientific impact of a paper can not be measured by any single indicator (<a href="https://web.archive.org/web/20120611101936/http://dx.doi.org/10.1371/journal.pone.0006022">A Principal Component Analysis of 39 Scientific Impact Measures</a>). Alternatives to citations are usage statistics such as HTML page views and PDF downloads, popularity in social bookmarking sites, coverage in blog posts, and comments to articles. The <em><em>PLoS</em></em> <a href="https://web.archive.org/web/20120611101936/http://article-level-metrics.plos.org/">article level metrics</a> introduced in September 2009 combine these different metrics, and make the data openly available.</p><p>How best to measure the other aspects of scientific output is largely unknown. It is possible to count the number of research grants or the total amount of money awarded, but should we simply count the number of submitted research datasets, invited lectures, science blog posts, etc., or do we need some quality indicator similar to citations?</p><h3 id="why-do-we-need-all-this"><strong><strong>Why do we need all this?</strong></strong></h3><p>Julia Lane emphasizes that we need science metrics to make the right decisions about funding and academic positions. And I fully agree with her that we need more research by social scientists and economists to better understand what these data mean and how best to use them. There is a lot of anecdotal evidence that suggests that science metrics alone may be poor indicators of future scientific achievements, simply because there are too many confounding factors. Maybe we also need to find a better term, as metric implies that scientific output can be reduced to <a href="https://web.archive.org/web/20120611101936/http://friendfeed.com/jcbradley/18e76233/binfield-article-level-metrics-should-be">one or more numbers</a>.</p><p>Another important motivation for improving science metrics, and not mentioned in the article, is to reduce the burden on researchers and administrators in evaluating research. The proportion of time spent doing research vs. time spent applying for funding, submitting manuscripts, filling out evaluation forms, doing peer review, etc. has become ridiculous for many active scientists. Initiatives such as the standardized <strong><strong>Research Performance Progress Report</strong></strong> format mentioned in the paper or automated tools to created a publication list or CV can reduce this burden. Funding organizations are also trying to reduce the burden of evaluating research , e.g. by increasing the time of funding from 3 to 5 years, reducing the number of papers that can be listed in grant applications (<a href="https://web.archive.org/web/20120611101936/http://blogs.nature.com/mfenner/2010/03/01/german-research-foundation-says-that-numbers-arent-everything">German Research Foundation says that numbers aren't everything</a>), or funding <a href="https://web.archive.org/web/20120611101936/http://www.wellcome.ac.uk/Funding/investigator-awards/Implementation/index.htm">investigators and not projects</a>.</p><p>Science metrics are not only important for evaluating scientific output, they are also great discovery tools, and this may indeed be their more important use. Traditional ways of discovering science (e.g. keyword searches in bibliographic databases) are increasingly superseded by <a href="https://web.archive.org/web/20120611101936/http://blogs.nature.com/mfenner/2010/02/22/there-is-still-so-much-to-learn-in-reference-management">non-traditional approaches</a> that use social networking tools for awareness, evaluations and popularity measurements of research findings.</p><h2 id="references">References</h2><p>Lane, J. (2010). Let's make science metrics more scientific <em>Nature, 464</em> (7288), 488-489 https://doi.org/<a href="https://web.archive.org/web/20120611101936/http://dx.doi.org/10.1038/464488a">10.1038/464488a</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Cancer and the media]]></title>
            <link>https://blog.martinfenner.org/posts/cancer-and-the-media</link>
            <guid>0513379f-da11-4b9d-a519-bbef072c5188</guid>
            <pubDate>Mon, 22 Mar 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Last Tuesday the Archives of Internal Medicine released a study that analyzed
the news reporting about cancer in 8 large-readership newspapers and 5 national
magazines in the United States. The authors identified 2228 cancer-focused
articles published between 2005-2007 and did a more detailed analysis on a
random sample of 436 (20%) articles.

20% of articles discussed cancer in general, 35% focused on breast cancer, and
15% focused on prostate cancer. 32% of the articles focused on survival and]]></description>
            <content:encoded><![CDATA[<figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611100654im_/http://www.researchblogging.org/public/citation_icons/rb2_large_gray.png" class="kg-image" alt="ResearchBlogging.org"></figure><p>Last Tuesday the <em><em>Archives of Internal Medicine</em></em> released a study that analyzed the news reporting about cancer in 8 large-readership newspapers and 5 national magazines in the United States. The authors identified 2228 cancer-focused articles published between 2005-2007 and did a more detailed analysis on a random sample of 436 (20%) articles.</p><p>20% of articles discussed cancer in general, 35% focused on breast cancer, and 15% focused on prostate cancer. 32% of the articles focused on survival and 8% focused on death and dying. 57% articles discussed aggressive treatments, but only two articles exclusively discussed end-of-life palliative care. Only 13% of articles reported that aggressive treatment might fail to cure or extend life, and only 30% of articles mentioned that cancer treatments can result in (sometimes serious) adverse events.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120611100654im_/http://farm1.static.flickr.com/4/7942175_28dd6be677_o_d.jpg" class="kg-image" alt><figcaption>Flickr image by Andraia.</figcaption></figure><p>Cancer is the second most common cause of death in the United States and therefore cancer news coverage is relevant to many people. One important finding of the study is the relative under-reporting of death and dying and palliative care, despite the well-documented benefits for patients and their families. The <strong><strong>Pallimed</strong></strong> blog <a href="https://web.archive.org/web/20120611100654/http://www.pallimed.org/2010/03/cancer-reporting-in-media-guess-what.html">discusses this</a> in more detail. The article was also discussed at <a href="https://web.archive.org/web/20120611100654/http://www.scientificblogging.com/news_articles/media_exaggerates_progress_cancer_research">Scientific Blogging</a> and at <a href="https://web.archive.org/web/20120611100654/http://blog.syracuse.com/cny/2010/03/media_paint_overly_optimistic_view_of_cancer_medical_study_says.html">syracuse.com</a>.</p><p>I am not surprised by these findings, as they seem to reflect the expectations of most cancer patients and their families towards treatment. In my personal experience as a doctor treating cancer patients, most patients, relatives and their treating physicians (including myself) are overly optimistic about the potential benefits of an aggressive cancer treatment (especially if part of a clinical trial), and talk much less about the possibility of the treatment not working, side effects, or death and dying. The scientific literature <a href="https://web.archive.org/web/20120611100654/http://dx.doi.org/10.1200/JCO.2008.17.2221">supports this personal experience</a>.</p><p>The study raises a number of additional questions:</p><ul><li>What scientific information was used as background information for the news reports? Conference reports vs. published papers, case reports vs. large randomized trials, research in animal models vs. clinical research? Was a source for the research provided in the news reports?</li><li>What is the cancer news coverage by science/medical bloggers? Is there a similar bias towards aggressive treatment approaches and an under-reporting of treatment failures and adverse events?</li><li>Are there geographical differences (U.S. vs. Europe, urban vs. rural areas) in cancer news reporting and changes over time?</li><li>How are other areas of science covered in the media, e.g. other common diseases such as Alzheimer’s disease or malaria, climate research or other reasearch areas with large public interest, or basic science research?</li></ul><p><em><em>Thanks to <strong><strong>Ivan Oransky</strong></strong> and his <strong><strong>Embargo Watch</strong></strong> blog to <a href="https://web.archive.org/web/20120611100654/http://embargowatch.wordpress.com/2010/03/19/are-these-embargo-breaks/">alert</a> me to this paper.</em></em></p><h2 id="references">References</h2><p>Fishman J, Ten Have T, &amp; Casarett D (2010). Cancer and the Media: How Does the News Report on Treatment and Outcomes? <em>Arch Int Med</em> https://doi.org/<a href="https://web.archive.org/web/20120611100654/http://dx.doi.org/10.1001/archinternmed.2010.11">10.1001/archinternmed.2010.11</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Chances and problems of doing science online]]></title>
            <link>https://blog.martinfenner.org/posts/chances-and-problems-of-doing-science-online</link>
            <guid>e425d457-1cdd-4cec-a9e3-dc6f2cd5f782</guid>
            <pubDate>Mon, 15 Mar 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Last month (shortly after ScienceOnline2010
[https://web.archive.org/web/20120611095611/http://go2.wordpress.com/?id=725X1342&site=scholarlykitchen.wordpress.com&url=http%3A%2F%2Fscienceonline2010.com%2Findex.php%2Fwiki]
) David Crotty wrote in a blog post Science and Web 2.0: Talking About Science
vs. Doing Science
[https://web.archive.org/web/20120611095611/http://scholarlykitchen.sspnet.org/2010/02/08/science-and-web-2-0-talking-about-science-versus-doing-science/]
:

> Nearly all of the more]]></description>
            <content:encoded><![CDATA[<p>Last month (shortly after <a href="https://web.archive.org/web/20120611095611/http://go2.wordpress.com/?id=725X1342&amp;site=scholarlykitchen.wordpress.com&amp;url=http%3A%2F%2Fscienceonline2010.com%2Findex.php%2Fwiki">ScienceOnline2010</a>) <em><em>David Crotty</em></em> wrote in a blog post <a href="https://web.archive.org/web/20120611095611/http://scholarlykitchen.sspnet.org/2010/02/08/science-and-web-2-0-talking-about-science-versus-doing-science/">Science and Web 2.0: Talking About Science vs. Doing Science</a>:</p><blockquote><em><em>Nearly all of the more visible attempts (of science and Web 2.0) so far have focused on talking about science, rather than tools for actually doing science.</em></em></blockquote><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120611095611im_/http://farm4.static.flickr.com/3661/3655816702_769a47f34d_d.jpg" class="kg-image" alt><figcaption>Flickr picture from Ivan Walsh.</figcaption></figure><p>The blog post is required reading for everybody interested in science and Web 2.0 and has attracted a lot of thoughtful comments (on the blog and <a href="https://web.archive.org/web/20120611095611/http://friendfeed.com/the-life-scientists/9bdff1e4/science-and-web-2-0-talking-about-vs-doing">on FriendFeed</a>). In another discussion <em><em>Thomas S</em>ö<em>derquist</em></em> from the <a href="https://web.archive.org/web/20120611095611/http://www.corporeality.net/museion/">Medical Museion in Copenhagen</a> reminded me that there are limitations of what can be done online. My blog focusses on talking about science rather than doing science, but this post is about doing science online. My research focusses on clinical cancer research, and in this field the advantages and limitations of doing science online are obviously different from other subject areas (bioinformatics for example obviously looks very different). It probably makes sense to ask yourself the following questions:</p><ul><li>Are your research data collected in (or easily converted into) digital form?</li><li>Are standard data formats and standard tools (preferably as open source software) available?</li><li>Do you regularly collaborate with scientists in other locations?</li><li>Is information about ongoing research projects publicly available?</li><li>Do journals have policies regarding the publication of your primary research data?</li><li>Are there objections to make the research data freely available?</li></ul><h3 id="are-your-research-data-collected-in-or-easily-converted-into-digital-form">Are your research data collected in (or easily converted into) digital form</h3><p>Electronic medical records (<a href="https://web.archive.org/web/20120611095611/http://en.wikipedia.org/wiki/Electronic_health_record">EMR</a>) have the potential to <a href="https://web.archive.org/web/20120611095611/http://www.nytimes.com/2009/03/01/business/01unbox.html">improve patient care and reduce costs</a>. But for now, often only some clinical information (particularly lab and radiology results) is available electronically and paper-based patient records are still commonly used. And both electronic and paper-based records have to be adapted to be useful for clinical research, e.g. by allowing a detailed documentation of adverse events.</p><p>The raw clinical data of a patient in a trial (called <em><em>source data</em></em> in clinical research) are entered into a case-report form (<a href="https://web.archive.org/web/20120611095611/http://en.wikipedia.org/wiki/Case_report_form">CRF</a>). The purpose of this two-step process is to make sure that all required data are collected and that they are entered correctly. Many clinical trials now use electronic CRFs or electronic data capture (EDC). But these tools are still surprisingly difficult to use and more expensive than paper-based solutions, so that many trials stick to paper CRFs and enter the data into a computer at a later stage. It also doesn't help that the EDC market is very fragmented, so that institutions have to learn to use several different tools.</p><h3 id="are-standard-data-formats-and-standard-tools-preferably-as-open-source-software-available">Are standard data formats and standard tools (preferably as open source software) available?</h3><p>Clinical Data Interchange Standard (<a href="https://web.archive.org/web/20120611095611/http://www.cdisc.org/">CDISC</a>) is the standard format for clinical research data. <a href="https://web.archive.org/web/20120611095611/http://www.openclinica.org/">OpenClinica</a> and <a href="https://web.archive.org/web/20120611095611/https://cabig.nci.nih.gov/workspaces/CTMS/?pid=primary.2006-10-24.9768040952&amp;sid=ctmsws&amp;status=True">Clinical Trials Management System of CaBIG</a> are two examples of Open Source tools for clinical research.</p><h3 id="do-you-regularly-collaborate-with-scientists-in-other-locations">Do you regularly collaborate with scientists in other locations?</h3><p>Most clinical trials are multi-center trials that are conducted in different locations, often even in different countries or continents. The coordination of the different trial locations uses email and web conferencing, but often relies more on human resources than on modern Web 2.0 tools.</p><h3 id="is-information-about-ongoing-research-projects-publicly-available">Is information about ongoing research projects publicly available?</h3><p>Clinical research is one of only a few research areas where information about (almost) all ongoing research project is publicly available. Clinical trial registries serve two purposes. They make it much easier for patients and their treating physicians to find relevant clinical trials. And they allow clinical researchers to understand what clinical research is going on in their field, and to avoid publication bias. <a href="https://web.archive.org/web/20120611095611/http://clinicaltrials.gov/">ClinicalTrials.gov</a> at the US National Institutes of Health is the largest clinical trial registry. For reasons that are difficult to understand, the European Clinical Trials database (<a href="https://web.archive.org/web/20120611095611/https://eudract.emea.europa.eu/">EudraCT</a>) is not available to the public, but work is in progress to <a href="https://web.archive.org/web/20120611095611/http://www.ecpc-online.org/component/docman/doc_download/68-ecpc-comments-on-qconsultation-for-data-fields-from-eudract-made-available-on-eudrapharmq.html?ItemId=127">change</a> that.</p><h3 id="do-journals-have-policies-regarding-the-publication-of-your-primary-research-data">Do journals have policies regarding the publication of your primary research data?</h3><p>An article in the <em><em>BMJ</em></em> last month by Iain Hrynaszkiewicz and colleagues<sup><a href="https://web.archive.org/web/20120611095611/http://blogs.plos.org/mfenner/2010/03/15/chances_and_problems_of_doing_science_online/#fn1">1</a></sup> tries to provide guidance on how to provide raw clinical data for publication. The main focus of the paper is patient privacy. Publication of raw clinical data either as dataset or as part of a research paper is still very uncommon. The meta-analysis of individual patient data<sup><a href="https://web.archive.org/web/20120611095611/http://blogs.plos.org/mfenner/2010/03/15/chances_and_problems_of_doing_science_online/#fn2">2</a></sup> requires the raw clinical data of several clinical trials, and because of the required effort is probably underused.</p><h3 id="are-there-objections-to-make-the-research-data-freely-available">Are there objections to make the research data freely available?</h3><p>Patient privacy is a major concern when publishing raw clinical data, and it's therefore critical to remove all identifying information from the dataset. This not only includes <em><em>direct identifiers</em></em> such as patient names, birthdates, unique identifying numbers or facial photographs, but also <em><em>indirect identifers</em></em> such as place of treatment, rare disease or treatment, occupation or place of work, etc. It is the consensus of the authors of the <em><em>BMJ</em></em> paper that datasets with three or more indirect identifiers should be evaluated for the risk that individuals might be identifiable before they are made available,</p><p>In contrast to <a href="https://web.archive.org/web/20120611095611/http://usefulchem.wikispaces.com/">Open Notebook Science</a>, it is impossible to make the results of a clinical trial publicly available before the trial is completed. The statistical design of the clinical trial is based on the number of patients needed to show a significant difference – looking at interim data could influence patient recruitment. Double-blind designs (where neither patient nor treating physician now which treatment arm the patient is in) are based on the same principle.</p><p>In clinical research there is often more at stake than the well-being of patients and the careers of the scientists involved. <em><em>Pfizer</em></em> and <em><em>Roche</em></em> last week each lost $1 billion in <a href="https://web.archive.org/web/20120611095611/http://www.marketwatch.com/story/abbott-pfizer-lead-drug-stocks-lower-2010-03-12">stock market value</a> after they both announced <a href="https://web.archive.org/web/20120611095611/http://www.pharmastrategyblog.com/2010/03/more-phase-iii-cancer-trials-flop-pfizer-sutent-and-roche-avastin.html">negative results of large phase III cancer trials</a>. Drug companies therefore have a great interest in whether and when research findings are published, and that includes the raw clinical data. The selective publication of positive research findings is called <a href="https://web.archive.org/web/20120611095611/http://blogs.nature.com/mfenner/2009/11/18/publication-bias-in-clinical-trials">publication bias</a> and the <a href="https://web.archive.org/web/20120611095611/http://blogs.nature.com/mfenner/2008/08/02/fdaaa-push-to-open-data-in-clinical-medicine">mandatory reporting of clinical trial results in ClinicalTrials.gov</a> was introduced by the FDA to reduce publication bias.</p><h3 id="summary">Summary</h3><p>Online tools can help with doing clinical research and there is probably a lot of untapped potential. From the perspective of an individual researcher or a small research group, it probably makes the most sense to develop and/or use tools that solve specific problems. I use the online project management tool <a href="https://web.archive.org/web/20120611095611/http://basecamphq.com/">Basecamp</a> to coordinate one clinical research project. And I have created a web-based <a href="https://web.archive.org/web/20120611095611/http://www.mh-hannover.de/studien/">clinical trials registry</a> for our university hospital. The internet version of this registry helps patients and referring physicians to find clinical trials at our institution. The intranet version helps us manage our clinical trials, e.g. by keeping all required documents in one place, keeping track of the patients registered in clinical trials, and serious adverse event reporting. And I don't see why clinical researchers can't adopt the <a href="https://web.archive.org/web/20120611095611/http://pantonprinciples.org/">Panton Principles</a> – which endorse that data related to published science should be explicitly placed in the public domain – whenever possible.</p><h2 id="references">References</h2><p><em><em>Hrynaszkiewicz et al</em></em>. Preparing raw clinical data for publication: guidance for journal editors, authors, and peer reviewers. <em><em>BMJ</em></em> 2010 <a href="https://web.archive.org/web/20120611095611/http://dx.doi.org/10.1136/bmj.c181">https://doi.org/10.1136/bmj.c181</a></p><p><em><em>Simmonds et al</em></em>. Meta-analysis of individual patient data from randomized trials: a review of methods used in practice. <em><em>Clin Trials</em></em> 2005 <a href="https://web.archive.org/web/20120611095611/http://dx.doi.org/10.1191/1740774505cn087oa">https://doi.org/10.1191/1740774505cn087oa</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How do researchers use online journals?]]></title>
            <link>https://blog.martinfenner.org/posts/how-do-researchers-use-online-journals</link>
            <guid>758226a3-b239-4343-a16e-b1b68e520fd6</guid>
            <pubDate>Mon, 08 Mar 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Last Monday I was listening to a very interesting presentation by Ian Rowlands
[https://web.archive.org/web/20120611102310/http://www.ucl.ac.uk/infostudies/ian-rowlands/]
, reader in scholarly communication in the Department of Information Studies
[https://web.archive.org/web/20120611102310/http://www.infostudies.ucl.ac.uk/] 
at University College London. He and his colleagues are interested in how
researchers find and use information, and how this has changed with the
internet, especially for t]]></description>
            <content:encoded><![CDATA[<figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611102310im_/http://www.researchblogging.org/public/citation_icons/rb2_large_gray.png" class="kg-image" alt="ResearchBlogging.org"></figure><p>Last Monday I was listening to a very interesting presentation by <a href="https://web.archive.org/web/20120611102310/http://www.ucl.ac.uk/infostudies/ian-rowlands/">Ian Rowlands</a>, reader in scholarly communication in the <a href="https://web.archive.org/web/20120611102310/http://www.infostudies.ucl.ac.uk/">Department of Information Studies</a> at University College London. He and his colleagues are interested in how researchers find and use information, and how this has changed with the internet, especially for the <a href="https://web.archive.org/web/20120611102310/http://www.jisc.ac.uk/whatwedo/programmes/resourcediscovery/googlegen.aspx">Google Generation</a> (people born after 1993). If you want to be part in this research (and have some fun), you can take part in the <a href="https://web.archive.org/web/20120611102310/http://www.bbc.co.uk/labuk/experiments/webbehaviour">BBC Web Behaviour Test</a>. The test will help you discover which species of web animal you are (I'm a <strong><strong>fox</strong></strong>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120611102310im_/http://farm1.static.flickr.com/14/17071467_11820b826c_d.jpg" class="kg-image" alt><figcaption>Flickr photo by Craig Anderson.</figcaption></figure><p>In another project, funded by the Research Information Network (<a href="https://web.archive.org/web/20120611102310/http://www.rin.ac.uk/">RIN</a>), Ian and his colleagues are studying how researchers are using electronic journals. The findings of the first part of the project were presented and discussed in a <a href="https://web.archive.org/web/20120611102310/http://www.rin.ac.uk/news/events/e-journals-revolution-how-use-scholarly-journals-shaping-research">workshop last July</a>. The presentations are available as PDF download, and as <a href="https://web.archive.org/web/20120611102310/http://www.rin.ac.uk/resources/rin-publications/podcasts/e-journals-revolution-podcast">podcast</a> with interviews of the speakers. The findings were summarized in a paper also published last July: <strong><strong>Online use and information seeking behaviour: institutional and subject comparisons of UK researchers</strong></strong>.</p><p>In the paper, the use of <a href="https://web.archive.org/web/20120611102310/http://www.oxfordjournals.org/">Oxford Journals</a> by 10 major UK research institutions was analyzed in the fields of life sciences, economics and history, using the server logs for the full year 2007. Some of the key findings of the study include:</p><h3 id="one-third-of-users-access-oxford-journals-outside-business-hours">One third of users access Oxford Journals outside business hours</h3><p>9.7% of uses happened on a Saturday/Sunday and 30.1% between 6 PM and 9 AM. This means that about one third of users accessed Oxford Journals outside typical business hours, either working late or from home (the study didn't distinguish between these two). These numbers indicate that remote access (from home, but probably also when traveling) is important for many users. This is obviously not an issue for Open Access journals, but institutions need to provide practical solutions (<a href="https://web.archive.org/web/20120611102310/http://en.wikipedia.org/wiki/Virtual_private_network">VPN</a>, etc.) for subscription journals. From personal experience this remote access is still overly complicated. And these numbers also mean that librarians will not be available for support questions one third of the time.</p><h3 id="around-40-of-sessions-originated-from-a-google-search">Around 40% of sessions originated from a Google Search</h3><p>In 2004 Oxford Journals opened up to Google for indexing. I didn't expect this important role that Google seems to play in finding scholarly papers, and I would be very interested in feedback from blog readers. Only 4% of sessions originated from Google Scholar (22% in economics). These results probably explain why Google Scholar hasn't seen that much development since it was launched. The search function at Oxford Journals was rarely used.</p><p>43% of users of history journals, but only 16% of users of life sciences journals used navigational tools (table of contents, etc.) provided by the journal. This statistic obviously doesn't look at users getting the table of contents via email or RSS, but it again shows that access via search now probably is more common than via browsing.<sup><a href="https://web.archive.org/web/20120611102310/http://blogs.plos.org/mfenner/2010/03/08/evaluating_usage_patterns_of_online_journals/#fn1">1</a></sup></p><h3 id="most-users-spend-little-time-on-journal-webpages-but-return-often">Most users spend little time on journal webpages, but return often</h3><p>The average number of articles viewed per session was 1.1, and the average session time was just over 4 minutes. Users rather return often, usually via a search. These numbers indicate that journal webpages are not a place where users spend a lot of time. Unless journals change this (e.g. by more active involvement of users via comments and other social networking features, etc.), they probably can't expect to generate significant revenue from online advertising. The internet has not only dramatically changed the role of <a href="https://web.archive.org/web/20120611102310/http://blogs.nature.com/mfenner/2010/01/24/scientists-and-librarians-friend-or-foe">libraries</a>, but also for journals, as users are mostly interested in single articles, rather than the journal as a whole.</p><h3 id="the-median-age-of-articles-was-48-months-life-sciences-73-months-economics-and-90-months-history-">The median age of articles was 48 months (life sciences), 73 months (economics), and 90 months (history)</h3><p>In the life sciences only 25% of the articles were no more than 16 months old, but another 25% were over 104 months old. I would have expected that the median age of articles would be much lower in the life sciences (it was two years in a similar study with ScienceDirect<a href="https://web.archive.org/web/20120611102310/http://blogs.plos.org/mfenner/2010/03/08/evaluating_usage_patterns_of_online_journals/#fn2">2</a>). It seems as if most papers are not accessed when they are published (in the first few months after publication), but rather as the result of a search strategy, e.g. when writing a paper.</p><h3 id="life-sciences-users-rarely-read-abstracts-on-publisher-platforms">Life sciences users rarely read abstracts on publisher platforms</h3><p>This should not come as a surprise, as life sciences users typically read abstracts in specialized databases, particularly <strong><strong>PubMed</strong></strong>. But maybe Journal publishers should stop displaying papers in an abstract view, saving users and themselves some effort. <em><em>PLoS</em></em> journals don't have an abstract view, but the <em><em>Biomed Central</em></em> journals (which are also Open Access) do. Subscription journals (including <em><em>Nature</em></em>) typically display the abstract instead of full-text to users without subscription access, so there is also no need for a separate abstract view for them.</p><p>The number of PDF views was higher than the number of full-text HTML views (178,152 vs. 106,582). This difference was much more pronounced in economics and history journals, probably indicating that here most papers were printed out and <a href="https://web.archive.org/web/20120611102310/http://blogs.nature.com/mfenner/2010/01/10/how-do-you-read-papers-2010-will-be-different">not read on the computer</a>.</p><h2 id="references">References</h2><p>Nicholas, D., Clark, D., Rowlands, I., &amp; Jamali, H. (2009). Online use and information seeking behaviour: institutional and subject comparisons of UK researchers <em>Journal of Information Science, 35</em> (6), 660-676 https://doi.org/<a href="https://web.archive.org/web/20120611102310/http://dx.doi.org/10.1177/0165551509338341">10.1177/0165551509338341</a></p><p><sup>1</sup> My July 2008 blog post <a href="https://web.archive.org/web/20120611102310/http://blogs.nature.com/mfenner/2008/07/19/do-online-journals-narrow-science-and-scholarship">Do online journals narrow science and scholarship?</a> discussed potential consequences.</p><p><sup>2</sup> CIBER, Evaluating the usage and impact of e-journals in the UK. Working paper 5. Available at <a href="https://web.archive.org/web/20120611102310/http://www.ucl.ac.uk/infostudies/research/ciber/">http://www.ucl.ac.uk/infostudies/research/ciber/</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[German Research Foundation says that numbers aren’t everything]]></title>
            <link>https://blog.martinfenner.org/posts/german-research-foundation-says-that-numbers-arent-everything</link>
            <guid>8ccbebf5-9550-4104-8f1d-e08558126d2f</guid>
            <pubDate>Mon, 01 Mar 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Last Tuesday the German Research Foundation (DFG) announced
[https://web.archive.org/web/20120611102053/http://www.dfg.de/en/service/press/press_releases/2010/pressemitteilung_nr_07/index.html] 
changes to the grant application process, going in effect in July. Researchers
are no longer allowed to list all their publications in their grant proposals.
The number of publications is limited to five per researcher and to two per year
of planned funding (e.g. 6 papers for a 3-year grant). Publication]]></description>
            <content:encoded><![CDATA[<p>Last Tuesday the <em><em>German Research Foundation</em></em> (DFG) <a href="https://web.archive.org/web/20120611102053/http://www.dfg.de/en/service/press/press_releases/2010/pressemitteilung_nr_07/index.html">announced</a> changes to the grant application process, going in effect in July. Researchers are no longer allowed to list all their publications in their grant proposals. The number of publications is limited to five per researcher and to two per year of planned funding (e.g. 6 papers for a 3-year grant). Publications submitted but not yet accepted for publication will no longer be allowed.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120611102053im_/http://farm1.static.flickr.com/37/76463757_24a1858d2e_m_d.jpg" class="kg-image" alt><figcaption>Flickr image by CarbonNYC.</figcaption></figure><p>Some of the reasoning behind this change was explained in the press conference where the policy change was announced. The DFG wants to put more emphasis on quality instead of quantity, in other words counteract the trend to publish several small pieces of incremental research findings (the <a href="https://web.archive.org/web/20120611102053/http://scienceblogs.com/drugmonkey/2009/01/repost_thoughts_on_the_least_p.php">least publishable unit</a> or <em><em>LPU</em></em>). The DFG didn't say so, but this might also reduce the practice of “honorary coauthorship” with some researchers being coauthors of 20 or even 50 papers per year. And the DFG is not happy with the increasing use of the Journal Impact Factor and other metrics as a token measure for the quality of research output. And as a reaction to problems with publication lists in <a href="https://web.archive.org/web/20120611102053/http://www.spiegel.de/unispiegel/studium/0,1518,622474,00.html">G&amp;Atilde;¶ttingen</a> they want to stop the practice of including unpublished work in reference lists for grant applications.</p><p>These changes will decrease the administrative workload of the applicant, reviewer and the DFG. With much shorter reference lists in grant applications, reviewers will have it much easier to take a closer look at the research output of the applicant, instead of relying on an unfortunate proxy such as the <em><em>Journal Impact Factor</em></em>. Researchers seeking funding from the DFG will now probably be more likely to write fewer but more substantial papers. And research that doesn't have the potential for a substantial paper, but is nevertheless worth publishing, can be quickly published in a reasonable journal instead of going through several rounds of submissions to a number of journals.</p><p>But how do you select your five best publications (assuming you have written more than five)? Choices include:</p><ul><li>publication date, e.g. a list of the five most recent publications</li><li>Journal Impact Factor</li><li>citation counts, page views, downloads or other article-level metrics</li><li>personal preference</li></ul><p>Using my personal preference (and not too much thought), I picked four papers and one correspondence:</p><ul><li><em><em>Shioda T, Fenner MH, Isselbacher KJ</em></em> Msg1, a novel melanocyte-specific gene, encodes a nuclear protein and is associated with pigmentation. <em><em>PNAS</em></em> 1996 <a href="https://web.archive.org/web/20120611102053/http://www.ncbi.nlm.nih.gov/pmc/articles/PMC37985/?tool=pubmed">PubMed Central</a><br>The first paper from my postdoctoral research project. We identified and cloned a new gene thought to be involved in cancer metastasis, using a technology called differential display to compare the gene expression profile of two melanoma cell lines. This was before the mouse and human genomes were sequenced, and before microarrays became available. What took us two years of work 15 years ago can now probably be done in a few weeks.</li><li><em><em>Sado T, Fenner MH, Tan SS Tam P, Shioda T, Li E</em></em> X Inactivation in the Mouse Embryo Deficient for Dnmt1: Distinct Effect of Hypomethylation on Imprinted and Random X Inactivation. <em><em>Dev Biol</em></em> 2000 <a href="https://web.archive.org/web/20120611102053/http://dx.doi.org/10.1006/dbio.2000.9823">doi:10.1006/dbio.2000.9823</a><br>I spent most of my time as a post-doc generating a knockout mouse for the gene identified in the previous paper. As the knockout mouse had no obvious phenotype, it took another post-doc (the first author) to finish the project.</li><li><em><em>Krege S et al.</em></em> European consensus conference on diagnosis and treatment of germ cell cancer: a report of the second meeting of the European Germ Cell Cancer Consensus group (EGCCCG): part I. <em><em>Eur Urol</em></em> 2008 <a href="https://web.archive.org/web/20120611102053/http://dx.doi.org/10.1016/j.eururo.2007.12.024">doi:10.1016/j.eururo.2007.12.024</a><br>This paper summarizes the conclusions of a consensus conference on the diagnosis and treatment of testicular cancer, and is the best review on the subject. I am one of over 80 coauthors, something I haven't done before or since. The journal published this as two papers because of length. This would have been a perfect paper for an Open Access journal, I hope I can convince the coauthors to do so when we update this in 2011.</li><li><em><em>Fenner MH, Beutel G, Gruenwald V.</em></em> Targeted therapies for patients with germ cell tumors. <em><em>Expert Opin Investig Drugs</em></em> 2008 <a href="https://web.archive.org/web/20120611102053/http://dx.doi.org/10.1517/13543784.17.4.511">doi:10.1517/13543784.17.4.511</a><br>Testicular cancer is one of the few chemotherapy success stories, as most patients with advanced metastatic disease can be cured. Targeted therapies have become important treatment options in many cancers. This is the first review to look at the evidence for the use of targeted therapies in testicular cancer.</li><li><em><em>Fenner MH.</em></em> Duplication: stop favouring applicant with longest list. <em><em>Nature</em></em> 2008 <a href="https://web.archive.org/web/20120611102053/http://dx.doi.org/10.1038/452029a">doi:10.1038/452029a</a><br>This is a <em><em>Nature</em></em> correspondence, included here only to show that comments made in a Nature Network forum can end up in <em><em>Nature</em></em>. And because it is relevant to this blog post, as I suggested to <em><em>ask applicants to select their best three, five or ten papers</em></em> instead of giving grants or jobs to those with the longest publication list.</li></ul><p>The Wellcome Trust last year <a href="https://web.archive.org/web/20120611102053/http://www.wellcome.ac.uk/Funding/investigator-awards/Implementation/index.htm">announced</a> a different change to they grant application process. Starting later this year, they will stop accepting proposals for project grants, and rather evaluate the reaseach output of the scientist asking for funding (<em><em>Investigator Awards</em></em>). They argue that researchers that alrady have shown excellence in the past shoudn't be burdened with the administrative overhead and restrictions of writing a detailed project proposal every three years.</p><p>It will be interesting to see how institutions and other research funders in Germany (e.g. <a href="https://web.archive.org/web/20120611102053/http://www.helmholtz.de/en/">Helmholtz</a> or <a href="https://web.archive.org/web/20120611102053/http://www.wgl.de/">Leibniz</a>) or elsewhere react to this DFG policy change. I would be happy if this is a step towards more reasonable publication policies. And I hope that the upcoming unique author identifier <a href="https://web.archive.org/web/20120611102053/http://www.orcid.org/">ORCID</a> will not be used for even more complicated bibliometric calculations, but rather as a tool for researchers to showcase their most interesting work.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[There is still so much to learn in reference management]]></title>
            <link>https://blog.martinfenner.org/posts/there-is-still-so-much-to-learn-in-reference-management</link>
            <guid>0557f48b-f4a1-42c6-8e17-474a37b6e9d6</guid>
            <pubDate>Mon, 22 Feb 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Last week Lambert Heller
[https://web.archive.org/web/20120611033209/http://wikify.org/] and myself did a
two-day workshop Reference Management in Times of Web 2.0 for a group of German
librarians. We introduced and tested the following five programs:

 * RefWorks
   [https://web.archive.org/web/20120611033209/http://www.refworks.com/]
 * Zotero [https://web.archive.org/web/20120611033209/http://www.zotero.org/]
 * CiteULike
   [https://web.archive.org/web/20120611033209/http://www.citeulike.org]]></description>
            <content:encoded><![CDATA[<p>Last week <a href="https://web.archive.org/web/20120611033209/http://wikify.org/">Lambert Heller</a> and myself did a two-day workshop <em><em>Reference Management in Times of Web 2.0</em></em> for a group of German librarians. We introduced and tested the following five programs:</p><ul><li><a href="https://web.archive.org/web/20120611033209/http://www.refworks.com/">RefWorks</a></li><li><a href="https://web.archive.org/web/20120611033209/http://www.zotero.org/">Zotero</a></li><li><a href="https://web.archive.org/web/20120611033209/http://www.citeulike.org/">CiteULike</a></li><li><a href="https://web.archive.org/web/20120611033209/http://www.mendeley.com/">Mendeley</a></li><li><a href="https://web.archive.org/web/20120611033209/http://www.endnote.com/">Endnote</a></li></ul><p>The goal of the workshop was to introduce the participants to the Web 2.0 aspects of these reference managers. We briefly talked about <a href="https://web.archive.org/web/20120611033209/http://www.mekentosj.com/papers">Papers</a> and <a href="https://web.archive.org/web/20120611033209/http://www.citavi.com/">Citavi</a>, but neither of them offers any Web 2.0 functionality. The goal of the workshop was <em><em>not</em></em> to pick the best reference manager. With the exception of <em><em>CiteULike</em></em> (which is more of a social bookmarking service and can't be used to directly put references into manuscripts), all of them are probably good choices for most users. For some of the minor differences, please check my reference manager chart that I have updated for the workshop (PDF <a href="https://web.archive.org/web/20120611033209/http://www.slideshare.net/mfenner/reference-manager-overview-2-1-3250074">here</a>):</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611033209im_/http://blogs.plos.org/mfenner/files/2010/11/Reference%20Manager%20Overview%202.3.jpg" class="kg-image" alt></figure><p>We had used FriendFeed for the slides, links and comments in a <a href="https://web.archive.org/web/20120611033209/http://friendfeed.com/bibman2">similar workshop last July</a>. This time we picked <a href="https://web.archive.org/web/20120611033209/http://www.sciencefeed.com/">ScienceFeed</a>, both because ScienceFeed can be used for reference management, and to test the service that launched just three days earlier. The ScienceFeed group can be found <a href="https://web.archive.org/web/20120611033209/http://www.sciencefeed.com/zblit2">here</a>, but is in German. FriendFeed and ScienceFeed are not only great for conference microblogging, but are also excellent teaching tools, especially in a workshop where every participant has an internet-connected computer. We also had a few people listening in and putting up comments.</p><p>The workshop did help me understand what could become one of the most important features of reference managers. (I would exclude <em><em>Endnote</em></em>, because it doesn't allow public groups or sharing of fulltext files). Libraries used to be places where you could find, store and read literature. A library would hold a subset of all the available literature, but still far more texts than an individual could keep at his home. A library serves as an intermediary that helps the user get access to the literature he is interested in.</p><p>A reference manager that stores all references and the associated fulltext PDF files in an accessible (public or password-protected) place can fullfill exactly the same role. It is not necessary that an individual user stores every reference and fulltext paper on his own computer. And he doesn't have to find all references for himself. Librarians could help with this, e.g. by not only handling a users search request, but also filing the associated PDF files in a group folder. Other group folders would have the table of contents of your favorite journals (e.g. <a href="https://web.archive.org/web/20120611033209/http://www.citeulike.org/journals">CiteULike Journals</a>). We used to go to the library for exactly these things. And now we do this all on our own, often not asking for help from our local library.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120611033209im_/http://farm4.static.flickr.com/3047/2587796561_36fea74569.jpg" class="kg-image" alt><figcaption>Flickr photo by haydnseek.</figcaption></figure><p>In the last session I talked about non-traditional ways to find scientific literature. Traditional would mean one of the following search strategies, summarized by Duncan Hull et al.<sup><a href="https://web.archive.org/web/20120611033209/http://blogs.plos.org/mfenner/2010/02/22/there_is_still_so_much_to_learn_in_reference_management/#fn1">1</a></sup>:</p><ul><li><em><em>Search</em></em> – Search bibliographic databases</li><li><em><em>Browse</em></em> – Scan tables of contents</li><li>Recommend – Recommendations by colleagues</li></ul><p><em><em>Twitter</em></em> is just a modern tool for strategies <em><em>#2</em></em> (check the Twitter list <a href="https://web.archive.org/web/20120611033209/http://twitter.com/mfenner/science-journals">@mfenner/science-journals</a> for some science journals using Twitter to announce interesting articles) and <em><em>#3</em></em> (papers recommended by friends you talk to via Twitter).</p><p>The non-traditional approach basically lets other people do the work for you. Some examples include:</p><ul><li>Experts pick noteworthy papers in your field – <a href="https://web.archive.org/web/20120611033209/http://www.f1000.com/">Faculty of 1000</a> and <a href="https://web.archive.org/web/20120611033209/http://www.researchblogging.org/">Research Blogging</a>.</li><li>You follow what people with similar interests are reading – <em><em>CiteULike</em></em> and <em><em>Mendeley</em></em></li><li>Recommendations based on what is in your library – <a href="https://web.archive.org/web/20120611033209/http://blog.citeulike.org/?p=136">CiteULike recommendations</a></li><li>Most popular articles in your research field of interest – <em><em>CiteULike</em></em> and <em><em>Mendeley</em></em>. The <em><em>PLoS</em></em> <a href="https://web.archive.org/web/20120611033209/http://www.plos.org/cms/node/485">article-level metrics</a> have the potential to do the same.</li></ul><h3 id="references">References</h3><p><em><em>Hull D, Pettifer SR, Kell DB.</em></em> Defrosting the digital library: bibliographic tools for the next generation web. <em><em>PLoS Computational Biology</em></em>. 2008 https://doi.org/<a href="https://web.archive.org/web/20120611033209/http://dx.doi.org/10.1371/journal.pcbi.1000204">10.1371/journal.pcbi.1000204</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ScienceFeed: Interview with Ijad Madisch]]></title>
            <link>https://blog.martinfenner.org/posts/sciencefeed-interview-with-ijad-madisch</link>
            <guid>3b5cf96a-aa68-4622-b5f6-72ea58c94d54</guid>
            <pubDate>Mon, 15 Feb 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Microblogging is blogging of short text messages, photos or other media and is
best exemplified by Twitter. Twitter use has grown tremendously in 2009, and
this also includes many scientists.1
[https://web.archive.org/web/20120611032951/http://blogs.plos.org/mfenner/2010/02/15/sciencefeed_interview_with_ijad_madisch/#fn1] 
 FriendFeed is a another microblogging tool that not only allows sending of
short text messages, but connects them together in groups and discussions
threads similar to what y]]></description>
            <content:encoded><![CDATA[<p>Microblogging is blogging of short text messages, photos or other media and is best exemplified by <em><em>Twitter</em></em>. Twitter use has grown tremendously in 2009, and this also includes many scientists.<sup><a href="https://web.archive.org/web/20120611032951/http://blogs.plos.org/mfenner/2010/02/15/sciencefeed_interview_with_ijad_madisch/#fn1">1</a></sup> <em><em>FriendFeed</em></em> is a another microblogging tool that not only allows sending of short text messages, but connects them together in groups and discussions threads similar to what you can do in online forums. FriendFeed, especially <a href="https://web.archive.org/web/20120611032951/http://friendfeed.com/the-life-scientists">The Life Scientists</a> group has been a popular place for many scientists for the last 18 months or so. <a href="https://web.archive.org/web/20120611032951/http://network.nature.com/people/U42E63119/profile">Cameron Neylon</a> wrote a good introduction to the service back in June 2008: <a href="https://web.archive.org/web/20120611032951/http://blog.openwetware.org/scienceintheopen/2008/06/12/friendfeed-for-scientists-what-why-and-how/">FriendFeed for scientists: what, why, and how?</a>. FriendFeed is a great tool for conference blogging, and the ISBM 2008 conference was probably the first scientific conference where it was used extensively, resulting in a <em><em>PLoS Computational Biology</em></em> paper.<sup><a href="https://web.archive.org/web/20120611032951/http://blogs.plos.org/mfenner/2010/02/15/sciencefeed_interview_with_ijad_madisch/#fn2">2</a></sup> FriendFeed is also often used to comment on blog posts, and here it is competing for attention with comments that are put directly on a blog. FriendFeed is a good example for a generic Web 2.0 tool that is much more useful to scientists than many Web 2.0 tools targeted specifically at scientists (the <a href="https://web.archive.org/web/20120611032951/http://fcw.com/Articles/2009/10/26/NIH-grant-creates-Facebook-for-Scientists.aspx">Facebooks for scientists</a>).</p><p>FriendFeed <a href="https://web.archive.org/web/20120611032951/http://www.facebook.com/press/releases.php?p=116581">was acquired by Facebook</a> in August 2009, and users started to worry about the long-term future of FriendFeed. An Open Source version of the FriendFeed web server was recently released as <a href="https://web.archive.org/web/20120611032951/http://bret.appspot.com/entry/tornado-web-server">Tornado</a> (source code on <a href="https://web.archive.org/web/20120611032951/http://github.com/facebook/tornado">GitHub</a>). Although other services (including <em><em>Facebook</em></em>) offer similar functionality, no service has (yet) emerged as an alternative popular with scientists. FriendFeed use seemed to be declining at the two recent <em><em>Science Online London 2009</em></em> and <em><em>ScienceOnline2010</em></em> conferences, as more and more people were using Twitter.</p><p>Last Tuesday <a href="https://web.archive.org/web/20120611032951/http://www.google.com/buzz">Google Buzz</a> was released. <em><em>Buzz</em></em> is also a microblogging service, tightly integrated with <em><em>Google Mail</em></em> and <em><em>Google Reader</em></em>. If offers many of the same features as <em><em>FriendFeed</em></em>, and because it integrates with Google Mail, it has a large number of potential users from the start – including a large number of people involved in the Science Bloggosphere. <em><em>Buzz</em></em> will certainly get some of the features that are still missing, e.g. an easy way to import content from other sources, including a bookmarklet. And <em><em>Buzz</em></em> works very well on iPhone and Android phones and there also uses location information – e.g. all the <em><em>Buzz</em></em> discussions near you. But at the moment many people wonder how best to integrate <em><em>Buzz</em></em> with <em><em>FriendFeed</em></em> and <em><em>Twitter</em></em>, and all the other online tools they use – it doesn't make sense to read the same content again and again on all these services.</p><p>In this context it is very interesting to see <a href="https://web.archive.org/web/20120611032951/http://www.sciencefeed.com/">ScienceFeed</a> launching as a new microblogging service this week. <em><em>ScienceFeed</em></em> in many ways is similar to <em><em>FriendFeed</em></em>, but tries to add features of particular interest to scientists. I spoke with <em><em>Ijad Madisch</em></em> about ScienceFeed.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611032951im_/http://gallery.me.com/mfenner/100079/ijad_madisch/web.jpg" class="kg-image" alt></figure><h3 id="1-what-is-sciencefeed">1. What is ScienceFeed?</h3><p>ScienceFeed is science as it happens, communicated through a microblogging platform. Conceptualized and designed by scientists, it is a bridge between online scientific networking platforms, scientific databases, and the wider online science community. The ScienceFeed platform allows users to post microblogs, sometimes just a few sentences, on scientific headlines, new findings, controversy, conferences and ideas related to science. Community members can follow the feeds of fellow members and comment on topics in which they are interested, allowing real-time communication and transfer of ideas.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611032951im_/http://gallery.me.com/mfenner/100079/sciencefeed_screenshot/web.jpg" class="kg-image" alt></figure><p>ScienceFeed is an interactive and dynamic platform – like science itself. Here, scientists, journalists, librarians, students, and those with an interest in science, will be able to communicate in a way that has no borders. Individuals from all over the world are able to participate and observe, helping to make science accessible to all. Integral to the concept of Science 2.0 is having online resources that are archivable and searchable – ScienceFeed will do just this. Science is not limited to the laboratory: it happens through interactions of communities. ScienceFeed is excited to build such a community.</p><h3 id="2-how-is-sciencefeed-different-from-friendfeed">2. How is ScienceFeed different from FriendFeed?</h3><p>In basic functionality, ScienceFeed isn't much different to <a href="https://web.archive.org/web/20120611032951/http://friendfeed.com/the-life-scientists">FriendFeed</a>. However, I think with the help of the community we will develop and add applications to the platform that could make it very efficient for scientific communication. There are two differences from FriendFeed which we have already implemented: 1) Specific scientific publications can easily be searched for and then entered as a linked-in reference within a feed, and, 2) Groups can be marked as an event (e.g. a conference). My vision is to have event streams in ScienceFeed, which then can be visualized and presented in a much better way. However, the most important part is that we listen to the feedback of the community and develop specific applications based on their ideas and feedback.</p><h3 id="3-what-special-features-does-sciencefeed-provide-for-conference-microblogging">3. What special features does ScienceFeed provide for conference microblogging</h3><p>An important feature from ScienceFeed is that groups can be marked as a specific event, such as a conference. Administrators of these groups will be able to import hashtags from Twitter, so all tweets will be aggregated and displayed within this group. A possibility for future growth is the integration of an entire conference program (sessions, panels, etc.) into the group, which then can be commented on by group members. It is important to us that the scientific community has an input into the development of this feature so that we can build a stronger, more efficient platform based on the needs of our users.</p><h3 id="4-can-you-import-references-into-sciencefeed-only-via-your-reference-database-or-also-via-citeulike-or-other-bookmarking-service">4. Can you import references into ScienceFeed only via your reference database, or also via CiteULike or other bookmarking service?</h3><p>Martin, thank you so much for this great idea. Based on your feedback we worked hard to make this happen before launch. Yes, now ScienceFeed can import from other bookmarking services such as <a href="https://web.archive.org/web/20120611032951/http://www.citeulike.org/">CiteUlike</a> or <a href="https://web.archive.org/web/20120611032951/http://www.connotea.org/">Connotea</a>. Furthermore ScienceFeed supports <a href="https://web.archive.org/web/20120611032951/http://www.oclc.org/productworks/coins.htm">CoiNS</a>, which identifies automatically based on a weblink whether or not bibliographic data is in the specified URL.</p><h3 id="5-how-is-sciencefeed-different-from-twitter">5. How is ScienceFeed different from Twitter?</h3><p>There are several differences, but the largest are that in ScienceFeed there is no character limitation and groups can be tagged as a specific event – facilitating real-time, online communication about the event.</p><h3 id="6-what-is-the-advantage-of-having-a-social-networking-tool-specifically-for-scientists">6. What is the advantage of having a social networking tool specifically for scientists?</h3><p>I think the most important part is the non-dilution of information in an environment where the platform and focus is specifically on science. Consider the following: You can find a biomedical scientific paper by searching in Google, but you could also use PubMed, which has a high probability of faster and better results. It is the same as within <a href="https://web.archive.org/web/20120611032951/https://www.researchgate.net/">ResearchGATE</a>: You have large groups (Methods, Immunology, Neuroscience, Philosophy, etc.) with a very focused population, which again makes your search more directed and efficient with better results.</p><h3 id="7-what-is-the-relationship-between-sciencefeed-and-researchgate">7. What is the relationship between ScienceFeed and ResearchGATE?</h3><p>ScienceFeed will be a scientific microblogging platform completely autonomous from ResearchGATE, because I think they target the same group, but with various usage patterns.</p><p>The publication reference tool used for inserting papers into ScienceFeed accesses the custom-built database of ResearchGATE. This database now has a public API which makes it possible for everyone to connect to the ResearchGATE literature database. I think that microarticles which are pretty successful in Researchgate (published in our ResearchBLOG) could be a part of ScienceFeed as well. I see ScienceFeed as a platform which will be useful to various scientific platforms as <a href="https://web.archive.org/web/20120611032951/http://www.mendeley.com/">Mendeley</a>, <a href="https://web.archive.org/web/20120611032951/http://www.academia.edu/">Academia</a>, <a href="https://web.archive.org/web/20120611032951/https://www.researchgate.net/">ResearchGATE</a>, etc. It could be a platform that helps connect all these different platforms.</p><h3 id="8-will-there-be-a-publicly-available-api-for-sciencefeed">8. Will there be a publicly available API for ScienceFeed?</h3><p>Yes, there will be an API.</p><h3 id="9-what-are-your-responsibilities-in-sciencefeed">9. What are your responsibilities in ScienceFeed?</h3><p>I am, as in ResearchGATE, one of the co-founders and a kind of CEO. I want to build a team of innovative and forward-thinking individuals to help develop ideas and work conceptually on the future directions of ScienceFeed.</p><h3 id="10-what-did-you-do-before-working-on-sciencefeed">10. What did you do before working on ScienceFeed?</h3><p>I am a co-founder and CEO of ResearchGATE and I am also working at Massachusetts General Hospital, Harvard Medical School, in Boston as a researcher. Before ResearchGATE I studied Medicine and Computer Science and completed my doctoral thesis in Virology, while working for some time in Gastroenterology as a medical doctor.</p><h3 id="11-could-you-provide-contact-information-for-people-that-have-further-questions-about-sciencefeed">11. Could you provide contact information for people that have further questions about ScienceFeed?</h3><p>I can be contacted anytime at: <a href="https://web.archive.org/web/20120611032951/mailto:ijad.madisch@sciencefeed.com">ijad.madisch@sciencefeed.com</a>.</p><h2 id="references">References</h2><p><em><em>Bonetta L.</em></em> Should You Be Tweeting? <em><em>Cell</em></em> 2009 <a href="https://web.archive.org/web/20120611032951/http://dx.doi.org/10.1016/j.cell.2009.10.017">https://doi.org/10.1016/j.cell.2009.10.017</a></p><p><em><em>Saunders N et al.</em></em> Microblogging the ISMB: A New Approach to Conference Reporting. <em><em>PLoS Comput Biol</em></em> 2009 https://doi.org/<a href="https://web.archive.org/web/20120611032951/http://dx.doi.org/10.1371/journal.pcbi.1000263">10.1371/journal.pcbi.1000263</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Happy third birthday Nature Network!]]></title>
            <link>https://blog.martinfenner.org/posts/happy-third-birthday-nature-network</link>
            <guid>dfa99dbd-48e6-4cff-82d6-8b219ff92563</guid>
            <pubDate>Sun, 14 Feb 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Nature Network turns 3 years old today
[https://web.archive.org/web/20120611031820/http://blogs.nature.com/nautilus/2008/02/happy_birthday_nature_network.html]
, and it has been a very interesting ride. I wasn't around when Nature Network
started, but posted by first Gobbledygook blog post (the blog had a different
name back then) in August 2007. We passed the 50.000 comments milestone
[https://web.archive.org/web/20120611031820/http://network.nature.com/people/U6E5B2CE1/blog/2010/01/26/nature-n]]></description>
            <content:encoded><![CDATA[<p>Nature Network turns <a href="https://web.archive.org/web/20120611031820/http://blogs.nature.com/nautilus/2008/02/happy_birthday_nature_network.html">3 years old today</a>, and it has been a very interesting ride. I wasn't around when Nature Network started, but posted by first Gobbledygook blog post (the blog had a different name back then) in August 2007. We passed the <a href="https://web.archive.org/web/20120611031820/http://network.nature.com/people/U6E5B2CE1/blog/2010/01/26/nature-network-blogs-receives-50-000th-comment">50.000 comments milestone</a> just a few weeks ago. And we were told that big changes to the blogging platform underneath are imminent.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120611031820im_/http://farm4.static.flickr.com/3524/3276406889_c13486bbfb_o_d.jpg" class="kg-image" alt><figcaption>Flickr image by Graham Steel.</figcaption></figure><p>I have had many, many positive experiences in these 2 1/2 years. I learned a lot about science publishing and met a large number of very nice and very clever people both online and offline. I wrote about 160 blog posts and an uncounted number of comments during that time, and writing blog posts is still a lot of fun and something I like doing on a regular basis (I decided a while ago to aim for one blog post per week). I am also excited about the upcoming <a href="https://web.archive.org/web/20120611031820/http://www.scienceonlinelondon.org/">Science Online London 2010</a> meeting, although the exact date and location have not yet been set.</p><p>Happy birthday.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Nature.com iPhone app in pictures]]></title>
            <link>https://blog.martinfenner.org/posts/nature-com-iphone-app-in-pictures</link>
            <guid>c0e5d8f7-58bf-4365-a8a6-a6b3045d7b9a</guid>
            <pubDate>Mon, 08 Feb 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Just four weeks ago I wrote a blog post titled How do you read papers? 2010
will
be different
[https://web.archive.org/web/20120611032102/http://network.nature.com/people/mfenner/blog/2010/01/10/how-do-you-read-papers-2010-will-be-different]
. Not only have we since seen the announcement of the Apple iPad
[https://web.archive.org/web/20120611032102/http://www.apple.com/ipad/], but
last Monday the free Nature.com iPhone app was launched
[https://web.archive.org/web/20120611032102/http://www.natur]]></description>
            <content:encoded><![CDATA[<p>Just four weeks ago I wrote a blog post titled <a href="https://web.archive.org/web/20120611032102/http://network.nature.com/people/mfenner/blog/2010/01/10/how-do-you-read-papers-2010-will-be-different">How do you read papers? 2010 will be different</a>. Not only have we since seen the announcement of the Apple <a href="https://web.archive.org/web/20120611032102/http://www.apple.com/ipad/">iPad</a>, but last Monday the free Nature.com iPhone app was <a href="https://web.archive.org/web/20120611032102/http://www.nature.com/press_releases/iphone.html">launched</a>. The application gives access to the full text of all <em><em>Nature</em></em> and <em><em>Nature News</em></em> content (through until 30 April 2010, how access is handled afterwards hasn't been announced yet). A version for the <em><em>Android</em></em> platform was promised for April, and the app will work with the just-announced <em><em>iPad</em></em>. I included a few screenshots for those without an iPhone or iPod Touch. A free Nature.com personal account is needed to use the app.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611032102im_/http://gallery.me.com/mfenner/100079/IMG_0626/web.jpg" class="kg-image" alt></figure><p>The iPhone app doesn't use HTML or PDF but rather <a href="https://web.archive.org/web/20120611032102/http://blogs.nature.com/wp/nascent/2010/02/new_naturecom_iphone_app.html">the ePub format</a>. The Nature.com website will soon offer downloads in ePub format (an example article is <a href="https://web.archive.org/web/20120611032102/http://blogs.nature.com/wp/nascent/article.epub">here</a>). <a href="https://web.archive.org/web/20120611032102/http://www.adobe.com/products/digitaleditions/">Adobe Digital Editions</a> and <a href="https://web.archive.org/web/20120611032102/http://www.lexcycle.com/">Stanza</a> are examples of ePub readers. In contrast to PDF, ePub adapts to the screen size and is therefore a much better format for the iPhone.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611032102im_/http://gallery.me.com/mfenner/100079/IMG_0627/web.jpg" class="kg-image" alt></figure><p>References are links in the text, clicking on them opens a new window.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611032102im_/http://gallery.me.com/mfenner/100079/IMG_0628/web.jpg" class="kg-image" alt></figure><p>Figures are also links in the text that open in a new window. The figures can be saved to the iPhone Photos application.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611032102im_/http://gallery.me.com/mfenner/100079/IMG_0630/web.jpg" class="kg-image" alt></figure><p>The iPhone app also gives access to the full text of <em><em>Nature News</em></em>. In contrast to <em><em>Nature</em></em> papers, images are rendered within the text.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611032102im_/http://gallery.me.com/mfenner/100079/IMG_0640/web.jpg" class="kg-image" alt></figure><p><em><em>Nature</em></em> and <em><em>Nature News</em></em> content, as well as PubMed search results can be saved for later reading. This content (or rather the <em><em>DOI</em></em>) is also available from the new <a href="https://web.archive.org/web/20120611032102/http://www.nature.com/mobileapps/web/bookmarks">Nature.com mobile apps</a> page. Because the <em><em>Nature.com mobile apps</em></em> page stores only the <em><em>DOI</em></em>, a <em><em>Nature</em></em> subscription is required to access the full-text article from there. From the <em><em>Nature.com mobile apps</em></em> page you can also export the citation in <em><em>RIS</em></em> format.</p><p>The Nature.com iPhone app also searches both Nature.com and PubMed. Regular searches can be saved.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611032102im_/http://gallery.me.com/mfenner/100079/IMG_0633/web.jpg" class="kg-image" alt></figure><p>PubMed searches will retrieve abstracts, with a link to the full-text article via the <em><em>DOI</em></em>.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611032102im_/http://gallery.me.com/mfenner/100079/IMG_0635/web.jpg" class="kg-image" alt></figure><p><a href="https://web.archive.org/web/20120611032102/http://mekentosj.com/papers/iphone/">Papers for iPhone</a> is another app that allows PubMed searches. You can also search for the latest <em><em>Nature</em></em> content, but the fulltext content is available only with a subscription and only as HTML or PDF.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611032102im_/http://gallery.me.com/mfenner/100079/IMG_0643/web.jpg" class="kg-image" alt></figure><p>Support for <a href="https://web.archive.org/web/20120611032102/http://www.web-books.com/Publishing/epub.htm">ePub</a> is the most exiting feature for me, as it opens the door for many interesting mobile applications. I hope that more scientific journals will start to use the format (Hindawi was one of the first publishers <a href="https://web.archive.org/web/20120611032102/http://www.hindawi.com/epub.html">to support ePub</a>), and that we then start to see mobile apps for more than a single journal.</p><p>Bug reports, suggestions and feature requests can be sent to <a href="https://web.archive.org/web/20120611032102/mailto:mobile%40nature.com">mobile@nature.com</a>. Or add your comments to Henry Gee's <a href="https://web.archive.org/web/20120611032102/http://network.nature.com/people/henrygee/blog/2010/02/01/nature-on-your-iphone">Nature On Your iPhone</a> post.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A digital preservation primer for scientists]]></title>
            <link>https://blog.martinfenner.org/posts/a-digital-preservation-primer-for-scientists</link>
            <guid>44e07384-ddcd-43bd-9a80-65eb2dfbc683</guid>
            <pubDate>Mon, 01 Feb 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[This weeks's blog post
[https://web.archive.org/web/20120611032500/http://www.corporeality.net/museion/2010/02/01/a-digital-preservation-primer-for-scientists/] 
is a guest post on the Biomedicine on Display blog – I was kindly invited by
Thomas Soderqvist from the Medical Museum of the University of Copenhagen.

> The first email was sent in 1964, but that first email has been lost forever. - 
Lucy Nowell
[https://web.archive.org/web/20100405101403/http://www.esf.org/ext-ceo-news-singleview/art]]></description>
            <content:encoded><![CDATA[<p><em>This weeks's <a href="https://web.archive.org/web/20120611032500/http://www.corporeality.net/museion/2010/02/01/a-digital-preservation-primer-for-scientists/">blog post</a> is a guest post on the Biomedicine on Display blog – I was kindly invited by Thomas Soderqvist from the Medical Museum of the University of Copenhagen.</em></p><blockquote>The first email was sent in 1964, but that first email has been lost forever. - <a href="https://web.archive.org/web/20100405101403/http://www.esf.org/ext-ceo-news-singleview/article/digital-preservation-alliance-set-to-tackle-sciences-new-frontier-368.html">Lucy Nowell</a></blockquote><p>As we have moved to digital formats both for primary research data and scientific publications, digital preservation has become critical to secure permanent access to scientific information. Digital preservation turned out to be much more difficult than creating digital content, as preservation requires long-term thinking about many issues including file formats, storage solutions and funding. Digital preservation turned out to be too big for individual libraries, publishers or research disciplines, and large collaborative efforts were started in the last five years.</p><h3 id="alliance-for-permanent-access">Alliance for Permanent Access</h3><p>The <a href="https://web.archive.org/web/20100405101403/http://www.alliancepermanentaccess.eu/">Alliance for Permanent Access</a> is a European strategic framework for digital preservation of scientific information. The alliance coordinates the efforts of different funders, research support organizations and major European research laboratories (e.g. <a href="https://web.archive.org/web/20100405101403/http://public.web.cern.ch/">CERN</a> or <a href="https://web.archive.org/web/20100405101403/http://www.esa.int/esaCP/index.html">ESA</a>).</p><h3 id="sustainable-digital-data-preservation-and-access-network-partners-datanet-">Sustainable Digital Data Preservation and Access Network Partners (DataNet)</h3><p><a href="https://web.archive.org/web/20100405101403/http://www.nsf.gov/funding/pgm_summ.jsp?pims_id=503141">Sustainable Digital Data Preservation and Access Network Partners</a> is a digital preservation project by the <a href="https://web.archive.org/web/20100405101403/http://www.nsf.gov/">National Science Foundation</a>. The deadline for proposals was May 2009, and $100 million will be awarded over the next five years. Wow.</p><h3 id="portico">Portico</h3><p><a href="https://web.archive.org/web/20100405101403/http://www.portico.org/">Portico</a> is a not-for-profit digital preservation service for scholarly content. Portico was launched in 2005 with initial support by <a href="https://web.archive.org/web/20100405101403/http://www.jstor.org/">JSTOR</a>, <a href="https://web.archive.org/web/20100405101403/http://www.ithaka.org/">Ithaka</a>, the <a href="https://web.archive.org/web/20100405101403/http://www.loc.gov/">Library of Congress</a>, and the <a href="https://web.archive.org/web/20100405101403/http://www.mellon.org/">Andrew W. Mellon Foundation</a>. The Portico archive currently contains close to 15 million papers and is archiving journal content for many publishers and libraries for a <a href="https://web.archive.org/web/20100405101403/http://www.portico.org/publishers/pub_contribution.html">fee</a>. Portico steps in (a so-called trigger event) when a publisher</p><ul><li>stops operations</li><li>ceases to publish a title</li><li>no longer offers back issues</li><li>has a castastrophic failure of the delivery platform</li></ul><h3 id="file-formats">File formats</h3><p><a href="https://web.archive.org/web/20100405101403/http://www.pdfa.org/">PDF/A</a> was approved as an ISO standard for long-term archiving of electronic documents in 2005. Before PDF/A, many organizations (including our institution) used the raster graphics format TIFF. The major advantage of the PDF format is the handling text and vector graphics in addition to raster images, allowing full-text search and smaller file sizes. Because the PDFformat is constantly changing, PDF/A was based on a specific PDF version (1.4) with the following specifications:</p><ul><li>self-contained, no external images or fonts</li><li>no sound or movies</li><li>metadata in the <a href="https://web.archive.org/web/20100405101403/http://blogs.nature.com/wp/nascent/2008/12/xmp_labelling_for_nature.html">XMP</a> format</li><li>no password protection</li></ul><p>Most scientific papers are now produced in XML, usually using the <a href="https://web.archive.org/web/20100405101403/http://dtd.nlm.nih.gov/">NLM DTD</a>. The <a href="https://web.archive.org/web/20100405101403/http://dtd.nlm.nih.gov/archiving/">Archiving and Interchange Tag Set</a> is a flavor of the NLM DTD that is intented for archiving.</p><h3 id="storage-solutions">Storage solutions</h3><p>Hard disks, tape and optical media are possible storage solutions. Tape is the ideal solution for long-term storage of research papers, but the digital preservation of research data in many areas (e.g. sequencing, high-energy physics) can’t be done with tape because of the exponential growth of these data. Hard disk storage has another problem: the <a href="https://web.archive.org/web/20100405101403/http://www.genomeweb.com/sequencing/wellcome-trust-sanger-institute-upgrade-sequencing-capacity-unveils-annual-revie">energy requirements of data centers</a>.</p><p>We live in a digital world, and this of course includes how we do and communicate science. It is surprising that we have barely started to think about digital preservation.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Scientists and librarians: friend or foe?]]></title>
            <link>https://blog.martinfenner.org/posts/scientists-and-librarians-friend-or-foe</link>
            <guid>a063e781-5b0a-492b-89d4-27e96cfe29d9</guid>
            <pubDate>Sun, 24 Jan 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[Following the ScienceOnline2010 conference, librarian Dorothea Salo wrote on
her
blog
[https://web.archive.org/web/20120611111844/http://scienceblogs.com/bookoftrogool/2010/01/science_online_2010_scientists.php]
:

> This disconnect is the number one threat to science librarianship today –
perhaps to all academic librarianship. How can science libraries persist when
scientists haven't the least notion that libraries or librarians are relevant to
their work?
These are serious questions, and of co]]></description>
            <content:encoded><![CDATA[<p>Following the <em><em>ScienceOnlin</em>e<em>2010</em></em> conference, librarian Dorothea Salo <a href="https://web.archive.org/web/20120611111844/http://scienceblogs.com/bookoftrogool/2010/01/science_online_2010_scientists.php">wrote on her blog</a>:</p><blockquote>This disconnect is the number one threat to science librarianship today – perhaps to all academic librarianship. How can science libraries persist when scientists haven't the least notion that libraries or librarians are relevant to their work?</blockquote><p>These are serious questions, and of course I don't have the answers. But I would like to add my thoughts from a researcher perspective. The role of libraries in providing teaching material for students (textbooks, etc.) is another story that I will not touch today.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120611111844im_/http://farm1.static.flickr.com/34/101350718_fa52fcef35_d.jpg" class="kg-image" alt><figcaption>Flickr image by Radioher</figcaption></figure><p>I'm old enough to remember the time (maybe 15 years ago) before literature searches were possible via the internet and scientific papers were available in electronic form. I had to go to the library to use Medline or Current Contents in printed form (and later on CD-ROM), or to flip through the newest issues of the most interesting journals. I would photocopy the papers I would then read at home, and then file away for later use.</p><p><em><em>PubMed</em></em>, <em><em>Scopus</em></em>, <em><em>Web of Science</em></em>, and <em><em>Google Scholar</em></em> now allow me to search the literature from my desk at work (or from home). Most researchers (including myself) probably don't have the skills for sophisticated searches, but these tools <a href="https://web.archive.org/web/20120611111844/http://network.nature.com/people/U2929A0EA/blog/2008/03/22/i-am-not-yelling-not-out-loud">more or less</a> get the job done. Electronic publishing means that I can obtain a paper directly from the journal (as long as I access the journal from a computer in the university network), and licensing is handled (almost) transparently by the library.</p><p>These developments have dramatically reduced the time researchers spend at a library. This is good, as this saves them a lot time. But interactions between researchers and librarians have also been dramatically reduced. Publishers and other companies (e.g. <em><em>Thomson Reuters</em></em>, <em><em>Mendeley</em></em>, <em><em>Faculty of 1000</em></em>, to name just a few) have used the opportunities to adapt their offerings to the internet (e.g. electronic-only journals), and to create new products that weren't possible (or even thinkable) before. Although libraries have in many ways adapted to the internet as well, they probably haven't seized the opportunity to the same degree. The homepage of my university library is a place I visit much less often than <em><em>PubMed</em></em> or the pages of my favorite journals. Some ideas of how this could be changed are listed below. Most of them are not new, but maybe at least some libraries haven't gone all the way to make their pages an attractive destination for the researchers of their institution.<sup><a href="https://web.archive.org/web/20120611111844/http://blogs.plos.org/mfenner/2010/01/24/scientists_and_librarians_friend_or_foe/#fn1">1</a></sup></p><h3 id="provide-and-support-online-reference-manager">Provide and support online reference manager</h3><p>Institutions should support at least one online reference manager, possible options include (in no particular order) Zotero, Mendeley, Endnote Web, Refworks and CiteULike. RefWorks and Endnote Web are commercial products and require a private (Endnote) and/or institutional license. The institution should either pick a free reference manager as their primary choice, or buy an institutional license in order to allow every researcher and student to use these tools without additional cost.<a href="https://web.archive.org/web/20120611111844/http://blogs.plos.org/mfenner/2010/01/24/scientists_and_librarians_friend_or_foe/#fn2">2</a></p><p>As we can't expect everybody to use the same reference manager, libraries have to help with several products. The easiest way to do so is via an online forum (see next paragraph), as this is more efficient than one-to-one support and allows experienced users to help out. Online reference managers provide additional features (e.g. they can be used from different computers, allow shared folders for groups of users) and should therefore be preferred over standalone applications.</p><h3 id="online-user-training-and-support">Online user training and support</h3><p>User support is obviously one of the central functions of science libraries. This has two aspects: helping with a specific problem (e.g. finding scientific literature), but also training users to do this on their own. The required skills include use of <em><em>PubMed</em></em> and other databases, and reference managers such as <em><em>Endnote</em></em> or <em><em>Zotero</em></em>. Skills in <a href="https://web.archive.org/web/20120611111844/http://en.wikipedia.org/wiki/Evidence-based_medicine">evidence-based medicine</a> are critical to find and appreciate the appropriate medical literature, but in my experience many physicians and medical students would benefit from additional training in this area.</p><p>Introductory classes, help in person or a phone call are sometimes the best way to do this, but often users require quick help for a specific situation that can best done with online tools. Appropriate tools include email, online forum, <em><em>Twitter</em></em>, <a href="https://web.archive.org/web/20120611111844/http://www.yammer.com/">Yammer</a> (a microblogging tool similar to Twitter but for institutions), <em><em>SlideShare</em></em>, <em><em>FaceBook</em></em> (and <em><em>StudiVZ</em></em> in Germany), <em><em>FriendFeed</em></em>, and Wikis. Every institution should make a decision about the services they plan to support, with emphasis on tools that are easy to use.</p><h3 id="institutional-bibliographies">Institutional bibliographies</h3><p>A regularly updated listing of all publications of an institution is not only a valuable PR service, but is often also required by administrations to evaluate research output. Librarians are often involved in this, but there is probably a lot of untapped potential. As <a href="https://web.archive.org/web/20120611111844/http://network.nature.com/people/mfenner/blog/2010/01/03/orcid-or-how-to-build-a-unique-identifier-for-scientists-in-10-easy-steps">unique identifiers for researchers</a> become more widespread, there really no longer is a need for researchers to compile publication lists themselves.<sup><a href="https://web.archive.org/web/20120611111844/http://blogs.plos.org/mfenner/2010/01/24/scientists_and_librarians_friend_or_foe/#fn3">3</a></sup></p><h3 id="article-deposition-in-institutional-repositories">Article deposition in institutional repositories</h3><p>Most journals allow researchers to post their accepted papers in institutional repositories of their institution. But because this requires technical skills and extra time, many researchers aren't particularly eager to make use of them. Institutional bibliographies can obviously be nicely integrated with institutional repositories, thus reducing redundant work.</p><h3 id="help-authors-with-article-submissions">Help authors with article submissions</h3><p>Article processing charges for authors are often handled by their libraries, and sometimes libraries have <a href="https://web.archive.org/web/20120611111844/http://www.biomedcentral.com/info/libraries/membership">membership deals</a> with publishers that give authors a discount. But researchers often are left alone with the article submission process. Most authors submit at most a handful of papers each year, and they have to deal not only with different article formats between journals (most notably different reference styles), but also different article submission systems (e.g. <a href="https://web.archive.org/web/20120611111844/http://network.nature.com/people/mfenner/blog/2009/03/25/editorial-manager-interview-with-richard-wynne">Editorial Manager</a>, <a href="https://web.archive.org/web/20120611111844/http://www.ejpress.com/index.shtml">eJournal Press</a>, <a href="https://web.archive.org/web/20120611111844/http://scholarone.com/products/manuscript/">Manuscript Central</a> or <a href="https://web.archive.org/web/20120611111844/http://benchpress.highwire.org/">BenchPress</a>). The total number of papers submitted by an institution is much larger, and thus at least some recurring problems could be avoided or at least the time required reduced with centralized support from the library.</p><h3 id="help-with-web-2-0-tools-for-scientists">Help with Web 2.0 tools for scientists</h3><p>Libraries don't have to reinvent all the Web 2.0 tools for scientists that are already out there, but they are a good place to help interested researchers get started with some of them (e.g. <em><em>ResearchGate</em></em>, <em><em>Nature Network</em></em>, or <em><em>Academia.edu</em></em>). Ideally, these tools could be integrated into the library webpages via an API.</p><p>fn1. I would like to be proven wrong by great examples of libraries gone Web 2.0.</p><p>fn2. My university picked <em><em>RefWorks</em></em> as their primary reference manager.</p><p>fn3. <em><em>Scopus</em></em> is already pretty good in this.</p><p><em>Three events from last week inspired me to write this: a blog post by (and short Twitter conversation with) Dorothea Salo (<a href="https://web.archive.org/web/20120611111844/http://scienceblogs.com/bookoftrogool/2010/01/science">Science Online 2010: Scientists and librarians</a>online_2010_scientists.php), a meeting with the other organizers of <a href="https://web.archive.org/web/20120611111844/http://bibcamp.wordpress.com/">BibCamp Hannover</a> (“a BarCamp for librarians and other hackers” in May 2010), and a discussion via email with <a href="https://web.archive.org/web/20120611111844/http://network.nature.com/people/obst/profile">Oliver Obst</a> from the Medical Library, University of Münster.</em></p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611111844im_/http://www.linkwithin.com/pixel.png" class="kg-image" alt="Related Posts Plugin for WordPress, Blogger..."></figure>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ScienceOnline2010 – I wish I was there]]></title>
            <link>https://blog.martinfenner.org/posts/scienceonline2010-i-wish-i-was-there</link>
            <guid>e745cc7a-8959-4fca-bfd2-db92b7693abd</guid>
            <pubDate>Sun, 17 Jan 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[ScienceOnline2010
[https://web.archive.org/web/20120611111833/http://www.scienceonline2010.com/] 
just finished a few hours ago, and from what everyone was saying it was yet
another wonderful meeting. I attended last year and moderated a session called 
Providing public health and medical information to all
[https://web.archive.org/web/20120611111833/http://network.nature.com/people/mfenner/blog/2009/01/16/scienceonline09-providing-public-health-and-medical-information-to-all]
, but unfortunatel]]></description>
            <content:encoded><![CDATA[<p><a href="https://web.archive.org/web/20120611111833/http://www.scienceonline2010.com/">ScienceOnline2010</a> just finished a few hours ago, and from what everyone was saying it was yet another wonderful meeting. I attended last year and moderated a session called <a href="https://web.archive.org/web/20120611111833/http://network.nature.com/people/mfenner/blog/2009/01/16/scienceonline09-providing-public-health-and-medical-information-to-all">Providing public health and medical information to all</a>, but unfortunately could not come this year. News about <em><em>ScienceOnline2010</em></em> are <a href="https://web.archive.org/web/20120611111833/http://www.scienceonline2010.com/index.php/wiki/BlogMedia_Coverage/">all over the place</a>, including <a href="https://web.archive.org/web/20120611111833/http://network.nature.com/people/henrygee/blog/2010/01/16/the-beowulf-effect">from</a> our own Henry Gee.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120611111833im_/http://farm3.static.flickr.com/2735/4077980037_193311abe6_d.jpg" class="kg-image" alt><figcaption>Flickr picture by missbakersflickr.</figcaption></figure><p><a href="https://web.archive.org/web/20120611111833/http://network.nature.com/people/steelgraham/profile">Graham Steel</a> has more experience attending ScienceOnline remotely (<a href="https://web.archive.org/web/20120611111833/http://mcblawg.blogspot.com/2009/01/science-online-09-how-was-it-via.html">Science Online '09 – How was it…. via the internet?</a>), but I did my best to follow the meeting via <a href="https://web.archive.org/web/20120611111833/http://twitter.com/#search?q=%23scio10">Twitter</a>, <a href="https://web.archive.org/web/20120611111833/http://friendfeed.com/scienceonline2010">FriendFeed</a>, <a href="https://web.archive.org/web/20120611111833/http://www.flickr.com/search/?q=scio10">Flickr</a>, <a href="https://web.archive.org/web/20120611111833/http://www.youtube.com/results?search_query=%23scio10">YouTube</a> and of course <a href="https://web.archive.org/web/20120611111833/http://www.scienceonline2010.com/index.php/wiki/BlogMedia_Coverage/">blogs and media</a>. There was even a <a href="https://web.archive.org/web/20120611111833/http://scienceblogs.com/scienceonline/2010/01/scienceonline2010_iphone_app.php">ScienceOnline2010 iPhone app</a>. Another ScienceOnline2010 trend was the widespread use of <a href="https://web.archive.org/web/20120611111833/http://www.theflip.com/">Flip</a> for short videos such as this one:</p><p>The overall online coverage far exceeds what I see at my usual science meetings, but following the sessions live (including the ability to ask questions) is far more difficult. For that you need video and microblogging (or the two combined in <em><em>Second Life</em></em>). Video of sessions from two rooms was streamed live at <a href="https://web.archive.org/web/20120611111833/http://www.ustream.tv/channel/scienceonline2010">Ustream</a> and <em><em>Second Life</em></em>, but Ustream didn't always work for me (didn't try Second Life, all sessions were recorded and will appear on <em><em>YouTube</em></em> next week).</p><p>For me and many others <em><em>FriendFeed</em></em> is the perfect microblogging tool for conferences (<a href="https://web.archive.org/web/20120611111833/http://dx.doi.org/10.1371/journal.pcbi.1000263">Microblogging the ISMB: A New Approach to Conference Reporting</a>). But the use of <em><em>FriendFeed</em></em> in Science 2.0 conferences seems on the decline during the last 12 months because of <em><em>Twitter</em></em>, and at <em><em>ScienceOnline2010</em></em> it was no longer possible to follow sessions using <em><em>FriendFeed</em></em>. <em><em>Twitter</em></em> is wonderful for many things, but makes it very difficult to create a connected discussion around a particular topic such as a conference session. But the <em><em>Twitter</em></em> board was cool:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20120611111833im_/http://farm5.static.flickr.com/4042/4278414875_2673a584b7_d.jpg" class="kg-image" alt><figcaption>Flickr picture by SignalShare.</figcaption></figure><p>After <em><em>ScienceOnline2010</em></em> is before <a href="https://web.archive.org/web/20120611111833/http://www.scienceonlinelondon.org/">Science Online London 2010</a>. The <a href="https://web.archive.org/web/20120611111833/http://blog.f1000.com/2010/01/15/on-the-run&amp;acirc;&amp;#128;&amp;#148;15jan10/">first planning meeting</a> took place last Friday. We hope to announce the location and date in the coming weeks, but we are aiming for a bigger (with enough room for at least 250 people) and longer (two days) meeting with enough parallel sessions to cover all the topics that we care about.</p><p>One personal goal I have for <em><em>Science Online London 2010</em></em> is to provide an even better experience for those unable to attend in person. <em><em>Second Life</em></em> worked <a href="https://web.archive.org/web/20120611111833/http://network.nature.com/people/joannascott/blog/2009/09/07/science-online-london-links">pretty well</a> for us last year, and we even had one speaker <a href="https://web.archive.org/web/20120611111833/http://seedmagazine.com/content/article/telepresent_at_the_future/">giving his presentation</a> this way. But maybe we can do better. Livestreaming of good quality audio is probably the most important thing, the video quality seems less critical. Alternatively, we can synchronize the audio with the slides from the presentation, something <em><em>SlideShare</em></em> calls <a href="https://web.archive.org/web/20120611111833/http://www.slideshare.net/faqs/slidecast">SlideCast</a>. We might also find better ways to use <em><em>Twitter</em></em> for microblogging, e.g. by using separate hashtags for every session and a <em><em>Twitter</em></em> board in the sessions.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How do you read papers? 2010 will be different]]></title>
            <link>https://blog.martinfenner.org/posts/how-do-you-read-papers-2010-will-be-different</link>
            <guid>f402c338-3acb-43df-912c-fbc32cb950b2</guid>
            <pubDate>Sun, 10 Jan 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[In November 2008 I wrote a blog post called How do you read papers?
[https://web.archive.org/web/20120611111551/http://network.nature.com/people/mfenner/blog/2008/11/02/how-do-you-read-papers] 
The blog post was actually about different strategies to find interesting
papers, e.g. browsing journal tables of content (TOC), different search
strategies, filtering by papers others read, or filtering by experts (e.g. 
Faculty of 1000). A paper by Duncan Hull
[https://web.archive.org/web/20120611111551]]></description>
            <content:encoded><![CDATA[<p>In November 2008 I wrote a blog post called <a href="https://web.archive.org/web/20120611111551/http://network.nature.com/people/mfenner/blog/2008/11/02/how-do-you-read-papers">How do you read papers?</a> The blog post was actually about different strategies to <em><em>find</em></em> interesting papers, e.g. browsing journal tables of content (TOC), different search strategies, filtering by papers others read, or filtering by experts (e.g. <em><em>Faculty of 1000</em></em>). A paper by <a href="https://web.archive.org/web/20120611111551/http://network.nature.com/people/duncan/profile">Duncan Hull</a> et al. published around that time in <em><em>PLoS Computational Biology</em></em> (<a href="https://web.archive.org/web/20120611111551/http://dx.doi.org/10.1371/journal.pcbi.1000204">Defrosting the Digital Library: Bibliographic Tools for the Next Generation Web</a>) also talked about finding strategies and the best tools for this.</p><p>In this blog post I want to talk about the actual reading of scientific papers that you found with one of the strategies mentioned above. There are some interesting recent developments, and I think we will see some significant changes in how we read papers in 2010.</p><h3 id="printed-journal">Printed journal</h3><p>Holding the printed journal in your hands is probably still the most satisfying reading experience because of professional typesetting and color reproduction. But unless you have a personal journal subscription, it is not convenient as you would have to go to the library to read the paper. Plus, many journals no longer produce a printed version, or the library has only an electronic subscription.</p><h3 id="photocopy">Photocopy</h3><p>This used to be the most common way to read papers 20 years ago. But the quality of photocopies is usually worse than a printout of an electronic version, and photocopies are far more inconvenient to obtain. Reading photocopied papers will only be necessary for the small number of journals that produce no electronic version, or for older papers.<sup><a href="https://web.archive.org/web/20120611111551/http://blogs.plos.org/mfenner/2010/01/10/how_do_you_read_papers_2010_will_be_different/#fn1">1</a></sup></p><h3 id="pdf-printout">PDF printout</h3><p>This is the way most people read scientific papers today, unless they just want to look up small parts of it. Quality color printers have become affordable, and the reading experience is similar to the printed journal with the added convenience of electronic distribution. Most people use PDF printouts for reading, and later discard the paper copy, sometimes even the same day. This is not only more expensive than reading on an electronic device, but also not very friendly to the environment.<sup><a href="https://web.archive.org/web/20120611111551/http://blogs.plos.org/mfenner/2010/01/10/how_do_you_read_papers_2010_will_be_different/#fn2">2</a></sup></p><h3 id="reading-pdfs-on-a-computer">Reading PDFs on a computer</h3><p>This approach appears to be very common for reading just parts of a paper, e.g. to look up experimental details, a figure or reference. Most people now store papers as PDF (hopefully with an intelligent program such as <a href="https://web.archive.org/web/20120611111551/http://mekentosj.com/papers/">Papers</a>) and not the PDF printouts. Looking at the PDF on the screen is therefore often the first step, and then the decision is made whether or not to print out the paper to read it in more detail. Reading PDFs on screen is possible, but not really convenient for longer texts. Screen sizes are often too small for the PDF format (A4 or US letter). Many people don't like the eye strain from looking at a screen for longer periods of time, although this is probably more relevant for reading books rather than reading a scientific paper.</p><p><a href="https://web.archive.org/web/20120611111551/http://getutopia.com/documents/overview">Utopia</a> is a PDF viewer launched in December by the University of Manchester that enhances PDF files of the <em><em>Semantic Biochemical Journal</em></em> with interactive content and live linking to web resources. <a href="https://web.archive.org/web/20120611111551/http://duncan.hull.name/2009/12/11/utopia/">Read more about Utopia</a> at <em><em>Duncan Hull</em></em>'s blog.</p><h3 id="reading-on-a-mobile-device">Reading on a mobile device</h3><p>Many mobile devices such as the iPhone can open PDFs and <a href="https://web.archive.org/web/20120611111551/http://mekentosj.com/papers/iphone/">Papers for iPhone</a> makes this process convenient. But PDFs in an A4 or US letter format are almost impossible to read on a small screen.</p><p>The <a href="https://web.archive.org/web/20120611111551/http://river-valley.tv/epub-is-the-only-format-we-need/">ePub format</a> is more suitable for smaller screens found on mobile devices. ePub is usually used for e-Books, but the Open Access publisher Hindawi <a href="https://web.archive.org/web/20120611111551/http://www.hindawi.com/epub.html">since 2008 provides papers also in that format</a>. Although ePub is more suitable than PDF for mobile devices, it doesn't solve the problem that figures and tables are simply difficult to show on a small screen. Mobile devices are probably great for reading journal table of contents or the abstract of a paper (and an RSS reader such as <a href="https://web.archive.org/web/20120611111551/http://www.newsgator.com/individuals/netnewswireiphone/default.aspx">NetNewsWire for iPhone</a> is perfect for this), but not fulltext papers.</p><h3 id="reading-on-an-e-reader">Reading on an e-Reader</h3><p>The screen of an e-Reader uses <a href="https://web.archive.org/web/20120611111551/http://river-valley.tv/electronic-paper-technology/">electronic ink</a> which not only means a much longer battery life, but also a very pleasing reading experience, including reading in direct sunlight. Electronic ink is black &amp; white, which is not a problem for fiction books, but limits the use for scholarly papers (and scientific textbooks). Ideally an e-Reader should have a 10″ screen, similar in size to A4 or US letter paper.</p><p>The <a href="https://web.archive.org/web/20120611111551/http://www.techcrunch.com/2009/05/07/how-big-can-the-kindle-get/">Kindle DX</a> from Amazon was announced in May 2009 and is currently the most popular e-Reader. The Kindle uses its own file format, but a <a href="https://web.archive.org/web/20120611111551/http://www.amazon.com/gp/help/customer/display.html?ie=UTF8&amp;nodeId=200324680">recent software update</a> now allows the Kindle to open PDF files. Some journals (e.g. the <em><em>New England Journal of Medicine</em></em>) offer Kindle subscriptions, but that <a href="https://web.archive.org/web/20120611111551/http://patrickmd.net/blog/2009/05/22/cancelled-kindle-subscription-to-nejm/">doesn't include early release articles</a> and no subscriber access to the journal website. A <a href="https://web.archive.org/web/20120611111551/http://www.nytimes.com/2010/01/09/technology/personaltech/09reader.html">number of similar devices</a> were demonstrated this week at the Consumer Electronics Show in Las Vegas. The Kindle is primarily an e-Book reader, and <em><em>David Crotty</em></em> over on the Scholarly Kitchen blog <a href="https://web.archive.org/web/20120611111551/http://scholarlykitchen.sspnet.org/2010/01/06/doing-the-kindle-math-does-amazons-opacity-conceal-a-shameful-truth">is skeptical</a> about dedicated e-readers, because he thinks that market is just too small.</p><h3 id="apple-islate">Apple iSlate</h3><p>Unless you have been on a remote island for the last three months, you will know that Apple will announce a <a href="https://web.archive.org/web/20120611111551/http://daringfireball.net/2009/12/the_tablet">tablet computer</a> later this month. There is wide speculation about the technical details, but it looks like this will be a very interesting device for reading scholarly papers. In contrast to e-Readers it will not use electronic ink. This means a shorter battery life, but allows color documents and many other more traditional computer uses. Based on the iPod and iPhone experience, many people think that the iSlate will change the way we use tablet computers. One of them is <em><em>Kent Anderson</em></em> (<a href="https://web.archive.org/web/20120611111551/http://scholarlykitchen.sspnet.org/2009/10/02/game-over-man-has-the-disruption-of-publishing-already-occurred/">Game Over, Man – Has the Disruption of Publishing Already Occurred?</a>).</p><p>Sports Illustrated did a very nice <a href="https://web.archive.org/web/20120611111551/http://sportsillustrated.cnn.com/2009/magazine/12/02/tablet/index.html">demo of a fictional tablet computer</a> in December, and it is obvious that many of these concepts can also be applied to scholarly publishing.</p><h3 id="reading-web-pages">Reading Web Pages</h3><p>Most examples mentioned above try to reproduce the experience of reading something printed on paper on an electronic device. An alternative approach would move beyond the traditional format of a paper and rather takes advantage of the electronic medium. And it looks like the web technologies HTML5 and Flash are best suited for this. Cell Press was <a href="https://web.archive.org/web/20120611111551/http://network.nature.com/people/mfenner/blog/2009/07/26/how-does-the-article-of-the-future-look-like">experimenting with this approach</a> in 2009, and officially launched their <a href="https://web.archive.org/web/20120611111551/http://beta.cell.com/index.php/2010/01/cell-launches-article-of-the-future-format/">Article of the Future</a> with the first 2010 issue of Cell (all papers will be available without subscription for 60 days, you can provide feedback <a href="https://web.archive.org/web/20120611111551/http://beta.cell.com/index.php/feedback-cell-press-new-article-format/">here</a>). The basic idea of the Article of the Future is to break away from the concept of reading a paper from beginning to end, and to make navigation between the different parts of a paper much easier.</p><p>Whereas the <em><em>Article of the Future</em></em> tries to make navigation with a paper easier, the <em><em>PLoS</em></em> <a href="https://web.archive.org/web/20120611111551/http://network.nature.com/people/mfenner/blog/2009/08/15/plos-one-interview-with-peter-binfield">article-level metrics</a> help with navigating to related content: citations, blog posts, reader comments, etc. The <em><em>Notes</em></em> feature lets registered users highlight text for specific comments – very much what you would do on a printed paper (but with the added benefit that everybody can see this note).</p><p>I'm most excited about projects that enhance the scientific paper instead of recreating an exact electronic version of the traditional paper. And HTML is a more promising format than PDF for these approaches. <em><em>Michael Clarke</em></em> (with whom I had the pleasure to do a session at SciFoo 2009) reminded us that <em><em>Tim Berners-Lee</em></em> invented the WWW in 1991 to facilitate scientific communication (with HTML and navigation both within and between documents as central concepts), but papers and journals have changed surprisingly little in the last 18 years (<a href="https://web.archive.org/web/20120611111551/http://scholarlykitchen.sspnet.org/2010/01/04/why-hasnt-scientific-publishing-been-disrupted-already/">Why Hasn't Scientific Publishing Been Disrupted Already?</a>).</p><p>fn1. Many journals are scanning their older papers and make them available in electronic form, e.g. <em><em>Nature</em></em>. The first issue of <em><em>Nature</em></em> from 1869 can be seen <a href="https://web.archive.org/web/20120611111551/http://www.nature.com/nature/about/first/index.html">here</a>.</p><p>fn2. We all know that computers haven't brought us the paperless office, but that we all use more paper than 10 years ago.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ORCID or how to build a unique identifier for scientists in 10 easy steps]]></title>
            <link>https://blog.martinfenner.org/posts/orcid-or-how-to-build-a-unique-identifier-for-scientists-in-10-easy-steps</link>
            <guid>1baed5cd-c206-4ef4-a163-5ed02945766d</guid>
            <pubDate>Sun, 03 Jan 2010 00:00:00 GMT</pubDate>
            <description><![CDATA[ORCID [https://web.archive.org/web/20151003022831/http://www.orcid.org/] stands
for Open Researcher and Contributor ID and was announced
[https://web.archive.org/web/20151003022831/http://science.thomsonreuters.com/orcid/media/pdf/ORCID_Announcement.pdf] 
in early December. This blog post tries to summarize some of the problems that
have to be solved to develop a unique identifier for scientists.

1. Identify the problem
The Researcher Identification Primer
[https://web.archive.org/web/201510030]]></description>
            <content:encoded><![CDATA[<p><a href="https://web.archive.org/web/20151003022831/http://www.orcid.org/">ORCID</a> stands for <em><em>Open Researcher and Contributor ID</em></em> and was <a href="https://web.archive.org/web/20151003022831/http://science.thomsonreuters.com/orcid/media/pdf/ORCID_Announcement.pdf">announced</a> in early December. This blog post tries to summarize some of the problems that have to be solved to develop a unique identifier for scientists.</p><h3 id="1-identify-the-problem">1. Identify the problem</h3><p>The <a href="https://web.archive.org/web/20151003022831/http://www.gen2phen.org/researcher-identification-primer">Researcher Identification Primer</a> by the Gen2Phen Knowledge Center lists some of the problems that a unique identifier for scientists tries to solve, including</p><ul><li>Disambiguation of author names in the scientific literature and establishing/validating relationships between authors and publications.</li><li>A solid foundation for permitting and tracking online scientific contributions, such as database submissions, scientific blogging, and community curation efforts.</li><li>Knowledge discovery applications using some or all of the above components.</li></ul><p>The Gen2Phen <a href="https://web.archive.org/web/20151003022831/http://www.gen2phen.org/event/irbw2009-workshop-may-13-14-toronto">workshop</a> co-organized by <a href="https://web.archive.org/web/20151003022831/http://network.nature.com/people/U7739E111/profile">Gudmundur Thorisson</a> in May 2009 discussed these issues in much more detail. One of several articles talking about the problem of disambiguation of author names (especially Asian author names) appeared in <em><em>Nature News</em></em> in <a href="https://web.archive.org/web/20151003022831/http://dx.doi.org/10.1038/451766a">February 2008</a>. A December 2009 <em><em>Nature</em></em> <a href="https://web.archive.org/web/20151003022831/http://dx.doi.org/10.1038/462825a">editorial</a> emphasized that a unique identifier for researchers will be especially valuable to track scientific contributions that are not related to authoring a paper. Phil Bourne and J. Lynne Fink also wrote about this in PLoS Computational Biology in December 2008: <a href="https://web.archive.org/web/20151003022831/http://dx.doi.org/10.1371/journal.pcbi.1000247">I Am Not a Scientist, I Am a Number</a>. A number of tools have tried to solve this problem, but it is not possible to link the researcher identities in the many systems.</p><h3 id="2-define-what-you-want-to-accomplish">2. Define what you want to accomplish</h3><p><a href="https://web.archive.org/web/20151003022831/http://network.nature.com/people/gbilder/profile">Geoff Bilder</a> gave a very good introduction to the problem at <a href="https://web.archive.org/web/20151003022831/http://www.scienceonlinelondon.org/">Science Online London</a> in August and <a href="https://web.archive.org/web/20151003022831/http://www.stm-assoc.org/event_presentations.php?event_id=19">STM Innovations</a> in December. Both talks were similar, but the latter is available as <a href="https://web.archive.org/web/20151003022831/http://river-valley.tv/crossref-contributor-id/">video</a> and <a href="https://web.archive.org/web/20151003022831/http://www.stm-assoc.org/2009_12_04_Innovations_Bilder_CrossRef_Contributor_ID.pdf">PDF</a>. He emphasized that ORCID is about <em><em>Knowledge Discovery</em></em> and not <em><em>Access Control</em></em>, and explained the terminology for <em><em>subject</em></em>, <em><em>identifier</em></em>, <em><em>profile</em></em>, <em><em>persona</em></em> and <em><em>credential</em></em>. <em><em>Access Control</em></em> is a related problem that is sometimes mixed in, but there is no requirement that a unique researcher identifier also has to provide secure access via whatever mechanism (<a href="https://web.archive.org/web/20151003022831/http://openid.net/">Open ID</a> is one solution to that problem).</p><h3 id="3-win-support-of-stakeholders">3. Win support of stakeholders</h3><p>Founding members of the ORCID initiative can be <a href="https://web.archive.org/web/20151003022831/http://science.thomsonreuters.com/orcid/gallery.html">found on the ORCID homepage</a> and include publishers, funders, universities, organizations and software companies. A number of important stakeholders are already part of the initiative, support by more funders (besides the Wellcome Trust) and software companies (particularly those that build reference managers or social networking sites for scientists) would be great. Probably the biggest name not on the list is the U.S. National Library of Medicine that runs the <em><em>PubMed</em></em> database of biomedical literature (the ORCID members Wellcome Trust and British Library are involved in UK PubMed Central).</p><h3 id="4-make-decisions-about-the-general-design-of-the-system">4. Make decisions about the general design of the system</h3><p>Some of the design decisions obviously are not set in stone at this stage. One continuing discussion is <em><em>centralized vs. federated</em></em>, and it looks like ORCID will be a centralized system similar to the DOI. Geoff Bilder has some <a href="https://web.archive.org/web/20151003022831/http://network.nature.com/people/mfenner/blog/2009/02/17/interview-with-geoffrey-bilder">good arguments</a> for a centralized system. Another recurring theme is <em><em>how much control an individual researcher has</em></em> over his ORCID record. Although external assertion from publishers or funders will certainly be part of ORCID, the individual researcher will have an important role, not only because of privacy concerns, but also because this is the easiest way to fix errors that even the best automated algorithms for author assignment will produce. And it looks as if ORCID will be an extensible system that will for example allow publishers or social networking sites to add functionality they require. The discussion at the STM Innovations meeting in early December touches some of these issues and is <a href="https://web.archive.org/web/20151003022831/http://river-valley.tv/how-researcherid-will-resolve-name-ambiguity-in-the-scholarly-ecosystem/">recorded as video</a> (after the talk by David Kochalko).</p><h3 id="5-pick-a-name">5. Pick a name</h3><p>The name <em><em>Open Researcher and Contributor ID</em></em> (ORCID) is obviously a combination of <a href="https://web.archive.org/web/20151003022831/http://www.researcherid.com/">ResearcherID</a> (Thomson Reuters) and <a href="https://web.archive.org/web/20151003022831/http://www.stm-assoc.org/2009_12_04_Innovations_Bilder_CrossRef_Contributor_ID.pdf">Contributor ID</a> (CrossRef). I would have preferred a simpler name, but I guess we have to get used to ORCID.</p><h3 id="6-build-on-available-tools">6. Build on available tools</h3><p>ORCID will be based on the ResearcherID software from Thomson Reuters. From what I’ve seen, the <a href="https://web.archive.org/web/20151003022831/http://openid.net/">Open ID</a> system will not be a central part of ORCID. But ORCID certainly will be designed to work together with Open ID and other authentication mechanisms. I don’t know what Elsevier and the <a href="https://web.archive.org/web/20151003022831/http://www.stm-assoc.org/2009_12_04_Innovations_Weertman_Taking_the_guesswork_out_of_author_searching.pdf">Scopus Author ID</a> will contribute to ORCID.</p><h3 id="7-form-an-independent-organization">7. Form an independent organization</h3><p>In order to be adopted widely, ORCID must be run by an independent organization, and not by a single publisher, software company, research organization or funder. With the experience of <a href="https://web.archive.org/web/20151003022831/http://network.nature.com/people/mfenner/blog/2009/02/17/interview-with-geoffrey-bilder">running the DOI system</a> to identify digital objects such as scientific papers, CrossRef would be one obvious candidate, but the ORCID founding members have yet to decide on that.</p><h3 id="8-secure-financing">8. Secure financing</h3><p>Starting and maintaining ORCID will obviously cost money. In my little <a href="https://web.archive.org/web/20151003022831/http://network.nature.com/people/mfenner/blog/2009/04/26/a-few-questions-about-author-identifiers-the-answers">survey about author identifiers back in April 2009</a>, the opinions were split about who should pay for this. Journal publishers and database maintainers (referring to such databases as PubMed, Scopus or Web of Science) were the two most common answers. ORCID will make it easier for funding agencies to evaluate scientists and they might therefore also contribute to the system. Individual researchers hopefully will not have to pay for any of this, but their input in time is obviously required.</p><h3 id="9-promote-orcid">9. Promote ORCID</h3><p>A <em><em>Nature</em></em> <a href="https://web.archive.org/web/20151003022831/http://dx.doi.org/10.1038/462825a">editorial</a> in December was a good start to promote ORCID to a wider audience. A unique identifier for scientists will only become accepted if widely used. That’s why it is important that publishers and funders quickly adopt this service. Software companies that build interesting tools around ORCID are also critical, e.g. integration of ORCID into manuscript submission systems (including the use of ORCID for the peer reviewers) and social networking sites (including of course Nature Network). My experience with the DOI for papers (e.g. the <a href="https://web.archive.org/web/20151003022831/http://network.nature.com/people/mfenner/blog/2009/10/11/thoughts-on-the-pubmed-redesign">limited support in PubMed</a>) tells me that adoption of ORCID will be a long process.</p><h3 id="10-involve-individual-researchers">10. Involve individual researchers</h3><p>Individual researchers currently have no way to get directly involved in ORCID. But some level of involvement is critical for an author identifier to work. The best place is currently probably the <a href="https://web.archive.org/web/20151003022831/http://www.linkedin.com/groups?gid=1807278">LinkedIn Group</a> Unique Identifiers for Researchers started by <a href="https://web.archive.org/web/20151003022831/http://network.nature.com/people/U42E63119/profile">Cameron Neylon</a>. But I hope we soon see ORCID discussions on Nature Network and other social networking sitess. The best place on Nature Network to discuss ORCID is currently probably the <a href="https://web.archive.org/web/20151003022831/http://network.nature.com/groups/socialnotworking/forum/topics">Scientific Researchers and Web 2.0: Social Not Working?</a> Forum.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Science and Sustainability]]></title>
            <link>https://blog.martinfenner.org/posts/science-and-sustainability</link>
            <guid>90bda30c-9224-4ef5-8a62-b067c1ad9600</guid>
            <pubDate>Sun, 27 Dec 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[Sustainability in science is nothing new. The term sustainability science was
probably first used in 2001 (see Wikipedia entry
[https://web.archive.org/web/20151002031738/http://en.wikipedia.org/wiki/Sustainability_science]
), and the title of this blog post was already used by a 2002 editorial
[https://web.archive.org/web/20151002031738/http://dx.doi.org/10.1126/science.297.5583.897] 
in Science. There are both journals (Sustainability: Science, Practice & Policy
[https://web.archive.org/web/20]]></description>
            <content:encoded><![CDATA[<p>Sustainability in science is nothing new. The term sustainability science was probably first used in 2001 (see <a href="https://web.archive.org/web/20151002031738/http://en.wikipedia.org/wiki/Sustainability_science">Wikipedia entry</a>), and the title of this blog post was already used by a 2002 <a href="https://web.archive.org/web/20151002031738/http://dx.doi.org/10.1126/science.297.5583.897">editorial</a> in <em><em>Science</em></em>. There are both journals (<a href="https://web.archive.org/web/20151002031738/http://ejournal.nbii.org/">Sustainability: Science, Practice &amp; Policy</a>, <a href="https://web.archive.org/web/20151002031738/http://www.springerlink.com/content/120154/">Sustainability Science</a>) and conferences (e.g. <a href="https://web.archive.org/web/20151002031738/http://www.nzsses.auckland.ac.nz/conference/index.htm">here</a> and <a href="https://web.archive.org/web/20151002031738/http://www.vie.unu.edu/article/963">here</a>) about this topic and you can get a <a href="https://web.archive.org/web/20151002031738/http://sustsci.aaas.org/files/University%20Survey%20V2.pdf">degree</a> in sustainability science. The term sustainability is usually used in the context of climate change and the conservation of natural resources.</p><p>Here I want to use sustainability in a broader sense, using the original definition: <em><em>able to be maintained at a certain rate or level</em></em>.<sup><a href="https://web.archive.org/web/20151002031738/http://blogs.plos.org/mfenner/2009/12/27/science_and_sustainability/#fn1">1</a></sup> Examples where the way we currently do science will probably no longer be sustainable in the future include grant applications that have a chance of success as low as 1% (<a href="https://web.archive.org/web/20151002031738/http://blogs.nature.com/peer-to-peer/2009/08/the_wait_continues_for_nih_cha.html">the wait continues for NIH Challenge Grant applicants</a>), the ever-increasing costs for access to scholarly publications (the <a href="https://web.archive.org/web/20151002031738/http://www.sennoma.net/main/archives/2009/06/what_happened_to_serials_price.php">serials crisis</a>), or the exponential growth of drug development costs without any increase in approval for new drugs in the last 60 years (<a href="https://web.archive.org/web/20151002031738/http://pipeline.corante.com/archives/2009/12/09/drug_companies_since_1950.php">Drug Companies Since 1950</a>).</p><p>Sustainability in science requires the individual researcher to think about his responsibility, i.e. to go beyond research that is personally interesting and is paid for by someone. I do think that increasing sustainability in science is a worthy goal, and I picked six examples.</p><h3 id="make-access-to-research-results-affordable">Make access to research results affordable</h3><p>For those not working at an academic institution, many subscription-based journals now make their papers available after a 6-12 month embargo period. Immediate full access to an individual paper in these journals can cost anywhere between $10 and $30 ($31.50 Cell, $15.00 Science, $32.00 Nature, $31.50 The Lancet, $10 New England Journal of Medicine). As you can guess from the wide range for these journals alone, these prizes are probably not calculated to cover actual costs. Deep Dyve <a href="https://web.archive.org/web/20151002031738/http://blog.deepdyve.com/2009/11/03/a-new-market-opportunity/">launched a rental service for scientific content</a> in October. They charge $0.99 per article rental, but currently include only a limited number of journals.</p><p>Most researchers have access to subscription-based journals through their institutions. My university library currently has a <a href="https://web.archive.org/web/20151002031738/http://dx.doi.org/10.3205/mbi000161">budget</a> of about 900,000 € per year for just over 2000 researchers and 2600 students. Even with this money, our institution <a href="https://web.archive.org/web/20151002031738/http://network.nature.com/people/mfenner/blog/2009/10/18/open-access-week-a-researchers-perspective">can't afford subscriptions to all journals</a> import for my work. And subscription costs are increasing much faster than library budgets. <a href="https://web.archive.org/web/20151002031738/http://network.nature.com/people/mfenner/blog/2009/12/13/on-giving-a-talk-about-open-access-in-my-department">Open Access</a> obviously is one answer to this dilemma, but <a href="https://web.archive.org/web/20151002031738/http://dx.doi.org/10.1038/nmat2497">may not work for all journals</a>. Most of us would probably be happy to pay for journal subscriptions if subscription costs simply remained reasonable. Because a handful of publishers own a large part of scientific journals, this will only happen if someone representing a large group of universities and research institutions sits at the negotiation table.</p><h3 id="reduce-the-bureaucracy-in-science-funding">Reduce the bureaucracy in science funding</h3><p>We are currently spending too much time trying to obtain research funding compared to the time actually doing research. This is in part because the chances of obtaining a grant are often fairly low and grant applications have to be submitted many times, and because the duration of some grants is to short, e.g. only 2-3 years (sometimes meaning you have to start writing on the extension grant after the first year). Providing funding to excellent researchers for longer periods of time instead of funding projects is one approach to improve this situation. The Howard Hughes Medical Institute has done this for many years with <a href="https://web.archive.org/web/20151002031738/http://www.hhmi.org/research/investigators/">HHMI investigators</a> and the Wellcome Trust last month announced a similar approach with <a href="https://web.archive.org/web/20151002031738/http://www.wellcome.ac.uk/News/Media-office/Press-releases/2009/WTX057403.htm">Wellcome Trust Investigator Awards</a>.</p><h3 id="communicate-and-use-research-results">Communicate and use research results</h3><p>A lot of research has practical value, but this practical value has to be explored. One nice example from my area of expertise is an international consortium to improve the outcome of a specific form of acute leukemia in the developing world. The first results <a href="https://web.archive.org/web/20151002031738/http://www.hematology.org/Publications/ASH-News-Daily/2009/4636.aspx">were reported</a> at the ASH meeting earlier this month, one of only a handful of abstracts to be picked for the plenary session. But use of research results goes beyond translational research, using them for science education (both in schools and universities) is equally important.</p><p>Science blogging could have an important role in communicating research, and <a href="https://web.archive.org/web/20151002031738/http://phylogenomics.blogspot.com/2009/12/story-behind-nature-paper-on-phylogeny.html">this blog post</a> from a few days ago is a wonderful example of how a blog post can enhance a <em><em>Nature</em></em> <a href="https://web.archive.org/web/20151002031738/http://dx.doi.org/10.1038/nature08656">paper</a>. It would be great if more journals would follow the <em><em>PLoS</em></em> journals in linking to blogs posts about a paper,<sup><a href="https://web.archive.org/web/20151002031738/http://blogs.plos.org/mfenner/2009/12/27/science_and_sustainability/#fn2">2</a></sup> and journals should help their authors to blog, e.g. by asking for a blog post (that could be hosted by the journal) on paper acceptance. Conference blogging is another area where science blogging would be a very welcome addition. It was nice to see <a href="https://web.archive.org/web/20151002031738/http://network.nature.com/people/mfenner/blog/2009/09/17/german-genetics-society-meeting-2009-introduction">official conference blogging</a> at the German Genetics Conference this year, and I hope to see more of that.</p><h3 id="develop-and-promote-technologies-that-make-scholarly-research-more-efficient">Develop and promote technologies that make scholarly research more efficient</h3><p>Obviously I have written a lot about this topic on this blog, e.g. about the <a href="https://web.archive.org/web/20151002031738/http://network.nature.com/people/mfenner/blog/2009/07/26/how-does-the-article-of-the-future-look-like">article of the future</a>, <a href="https://web.archive.org/web/20151002031738/http://network.nature.com/people/mfenner/blog/2009/08/01/bibliographic-management-meets-web-2-0">reference management</a>, <a href="https://web.archive.org/web/20151002031738/http://network.nature.com/people/mfenner/blog/2009/03/25/editorial-manager-interview-with-richard-wynne">paper submission</a>, <a href="https://web.archive.org/web/20151002031738/http://network.nature.com/people/mfenner/blog/2009/05/01/extyles-interview-with-elizabeth-blake-and-bruce-rosenblum">validation, formatting and exporting of scholarly content</a>, or <a href="https://web.archive.org/web/20151002031738/http://network.nature.com/people/mfenner/blog/2009/02/17/interview-with-geoffrey-bilder">researcher identifiers</a>. But I believe that there is still a lot more that can be done, and I expect to see one or more disruptive technologies in the future. Time will tell if <a href="https://web.archive.org/web/20151002031738/http://network.nature.com/people/mfenner/blog/2009/07/18/using-google-wave-for-a-week-its-still-great">Google Wave</a> is one of them, the Open Researcher Contributor Identification Initiative (<a href="https://web.archive.org/web/20151002031738/http://www.orcid.org/">ORCID</a>) announced earlier this month certainly is a very big step forward.</p><h3 id="preserve-research-data">Preserve research data</h3><p>Providing and preserving the research data behind a project is becoming increasingly important, and simply writing up a paper <a href="https://web.archive.org/web/20151002031738/http://www.cotch.net/blog/20091223_1558">is no longer enough</a>.<sup><a href="https://web.archive.org/web/20151002031738/http://blogs.plos.org/mfenner/2009/12/27/science_and_sustainability/#fn3">3</a></sup> In many areas we lack the infrastructure (nomenclature, databases, etc.) and resources for this, especially for long-term preservation. One ambitious project is <a href="https://web.archive.org/web/20151002031738/http://www.elixir-europe.org/page.php">Elixir</a>, which is trying to develop an infrastructure for biological information in Europe. The <a href="https://web.archive.org/web/20151002031738/https://cabig.nci.nih.gov/">CaBIG</a> project at the U.S. National Cancer Institute is trying to do something similar for cancer research. Examples for digital preservation projects can be found at the <a href="https://web.archive.org/web/20151002031738/http://www.bl.uk/aboutus/stratpolprog/ccare/introduction/digital/">British Library</a> and the German <a href="https://web.archive.org/web/20151002031738/http://www.langzeitarchivierung.de/eng/index.htm">Nestor</a> project.</p><h3 id="involve-people-outside-of-universities-and-institutions-in-research">Involve people outside of universities and institutions in research</h3><p>Many areas of research would profit from this approach. A prominent example of citizen science is <a href="https://web.archive.org/web/20151002031738/http://www.galaxyzoo.org/">Galaxy Zoo</a>, where more than 150,000 people are helping with the classification of astronomy images. Involving people is especially in medical research. <a href="https://web.archive.org/web/20151002031738/http://www.pdonlineresearch.org/">PD Online Research</a> is a wonderful community site about research on Parkinson disease that launched in June 2009. <a href="https://web.archive.org/web/20151002031738/http://scienceroll.com/personalized-medicine/">Personalized genetics</a> can give the patient or healthy individual a more active role in healthcare decisions.</p><p>Whether sustainability will ultimately play a greater role in science will ultimately depend on those paying for research. If universities, institutions and funders continue to look mainly at goals that are both too short-term and only an indirect measure of scientific progress (e.g. the Impact Factor of a journal that a paper is published in), and don't honor activities such as data annotation, public outreach, etc., this will be very difficult. I wish you a great start into the new year.</p><p>The German word for sustainability is <em><em>Nachhaltigkeit</em></em>.</p><p>And please, please use the <a href="https://web.archive.org/web/20151002031738/http://www.doi.org/">DOI</a> for that.</p><p>This blog post was one important inspiration for this post, as was the <a href="https://web.archive.org/web/20151002031738/http://www.nature.com/scifoo/index.html">SciFoo</a> meeting in July, and many small things in between, including of course the <a href="https://web.archive.org/web/20151002031738/http://network.nature.com/people/mfenner/blog/2009/08/23/thoughts-on-the-science-online-london-conference">Science Online London</a> meeting.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Increased cancer risk following computed tomography scans]]></title>
            <link>https://blog.martinfenner.org/posts/increased-cancer-risk-following-computed-tomography-scans</link>
            <guid>dc51e60a-ec1d-4635-80fd-1b29d76a7079</guid>
            <pubDate>Sun, 20 Dec 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[Two papers (this
[https://web.archive.org/web/20150922174153/http://dx.doi.org/10.1001/archinternmed.2009.427] 
and this
[https://web.archive.org/web/20150922174153/http://http//dx.doi.org/10.1001/archinternmed.2009.440]
) and an editorial
[https://web.archive.org/web/20150922174153/http://dx.doi.org/10.1001/archinternmed.2009.453] 
in the latest issue of Archives of Internal Medicine examine the cancer risks
associated with the use of computed tomography (CT) examinations.1
[https://web.archive]]></description>
            <content:encoded><![CDATA[<figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20150922174153im_/http://www.researchblogging.org/public/citation_icons/rb2_large_gray.png" class="kg-image" alt="ResearchBlogging.org"></figure><p>Two papers (<a href="https://web.archive.org/web/20150922174153/http://dx.doi.org/10.1001/archinternmed.2009.427">this</a> and <a href="https://web.archive.org/web/20150922174153/http://http//dx.doi.org/10.1001/archinternmed.2009.440">this</a>) and an <a href="https://web.archive.org/web/20150922174153/http://dx.doi.org/10.1001/archinternmed.2009.453">editorial</a> in the latest issue of <em><em>Archives of Internal Medicine</em></em> examine the cancer risks associated with the use of computed tomography (CT) examinations.<sup><a href="https://web.archive.org/web/20150922174153/http://blogs.plos.org/mfenner/2009/12/20/increased_cancer_risk_following_computed_tomography_scans/#fn1">1</a></sup></p><p>Ionizing radiation increases the risk for developing cancer. There is direct evidence from atomic bomb survivors in Japan in 1945 and from nuclear accidents such as the one in Chernobyl in 1986. There are no studies directly demonstrating an increased cancer risk from the diagnostic use of X-rays (either conventional radiographs or computed tomography), as this would require long-term follow-up of a very large number of people. This risk can only be estimated, based on the assumption that there is no lower radiation threshold dose for cancer risk and that a linear correlation exists between dose and cancer risk. Similarly, we know very little about the actual radiation doses that patients receive during a diagnostic CT scan.</p><p>The paper by <strong><strong>Rebecca Smith-Bindman</strong></strong> and her coworkers looked at the radiation doses associated with the 11 most common computed tomography examinations in four hospitals in the San Francisco Bay Area. Radiation doses to individual patients were not measured directly, but were estimated using the commonly used “effective dose”. Median effective doses for routine computed tomography of the head, chest and abdomen-pelvis were 2, 8 and 16 mSv. The highest median effective doses (31 mSv) were used for multiphase CTs of the abdomen-pelvis (range 6-90 mSv). The 8 mSv dose for a routine chest CT is equivalent to 119 conventional chest X-rays. Interestingly, there was wide variation of effective doses used both between hospitals and within the same institution (with a mean 13-fold variation between highest and lowest dose for each CT study type).</p><p>Based on these doses, the authors then estimated the increased risk for radiation-induced cancer following a CT examination. This risk is higher in younger people and in women. Women have a higher risk to develop lung cancer following radiation exposure and they have the added risk of developing breast cancer. The authors estimated that it would for example require approximately 620 CT scans for ruling out pulmonary embolism in 40-year-old women to induce one cancer.</p><p>The second paper by <strong><strong>Amy Berrington de Gonzalez</strong></strong> and coworkers tried to calculate the total number of CT scans performed in the US in 2007, and estimated the increased risk for cancer related to these CT scans based on examination type, age and sex. They used a similar model to calculate the cancer risk as the first paper. Overall, about 72 million CT scans were performed in the US in 2007. The authors estimated that approximately 29.000 additional cancers will eventually develop, equivalent to approximately 2% of all cancers diagnosed annually in the US.</p><p>CT scans have dramatically improved patient care, and the conclusions of these two papers should not preclude their use in patients where the potential benefits clearly outweigh the risks mentioned above. But in order to reduce the cancer risk associated with CT scans, the authors of the two papers and the editorial suggest to optimize and standardize the CT scan procedure (remember the 13-fold difference in dose for the same procedure) and to decrease the number of unnecessary examinations. The latter is most relevant when CT scans are performed as screening procedures.</p><p>More blog posts discussing these papers can be found on <a href="https://web.archive.org/web/20150922174153/http://blogs.nature.com/stories/2049">Nature Blogs</a> and <a href="https://web.archive.org/web/20150922174153/http://www.cancer.org/aspx/Blog/Comments.aspx?id=336">Dr. Len's Cancer Blog</a>.</p><p><sup>1</sup> All three papers are available as full-text without a subscription. And none of the papers displays it's DOI, which makes linking to them unneccessarily difficult.</p><h3 id="references">References</h3><p>Smith-Bindman, R., Lipson, J., Marcus, R., Kim, K., Mahesh, M., Gould, R., Berrington de Gonzalez, A., &amp; Miglioretti, D. (2009). Radiation Dose Associated With Common Computed Tomography Examinations and the Associated Lifetime Attributable Risk of Cancer <em>Archives of Internal Medicine, 169</em> (22), 2078-2086 https://doi.org/<a href="https://doi.org/10.1001/archinternmed.2009.427">10.1001/archinternmed.2009.427</a></p><p>Berrington de Gonzalez, A., Mahesh, M., Kim, K., Bhargavan, M., Lewis, R., Mettler, F., &amp; Land, C. (2009). Projected Cancer Risks From Computed Tomographic Scans Performed in the United States in 2007 <em>Archives of Internal Medicine, 169</em> (22), 2071-2077 https://doi.org/<a href="https://doi.org/10.1001/archinternmed.2009.440">10.1001/archinternmed.2009.440</a></p><p>Redberg, R. (2009). Cancer Risks and Radiation Exposure From Computed Tomographic Scans: How Can We Be Sure That the Benefits Outweigh the Risks? <em>Archives of Internal Medicine, 169</em> (22), 2049-2050 https://doi.org/<a href="https://web.archive.org/web/20150922174153/http://dx.doi.org/10.1001/archinternmed.2009.453">10.1001/archinternmed.2009.453</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[On giving a talk about Open Access in my department]]></title>
            <link>https://blog.martinfenner.org/posts/on-giving-a-talk-about-open-access-in-my-department</link>
            <guid>a3abb5a7-b85c-43f0-a971-2393b4ee6707</guid>
            <pubDate>Sun, 13 Dec 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[Earlier this month I gave this talk in my department. It is basically a summary
of two blog posts that I wrote in October during Open Access Week (Open Access
Week: a researcher's perspective part I
[https://web.archive.org/web/20150908065053/http://network.nature.com/people/mfenner/blog/2009/10/18/open-access-week-a-researchers-perspective] 
and part II
[https://web.archive.org/web/20150908065053/http://network.nature.com/people/mfenner/blog/2009/10/23/open-access-week-a-researchers-perspective]]></description>
            <content:encoded><![CDATA[<p>Earlier this month I gave this talk in my department. It is basically a summary of two blog posts that I wrote in October during <strong><strong>Open Access Week</strong></strong> (Open Access Week: a researcher's perspective <a href="https://web.archive.org/web/20150908065053/http://network.nature.com/people/mfenner/blog/2009/10/18/open-access-week-a-researchers-perspective">part I</a> and <a href="https://web.archive.org/web/20150908065053/http://network.nature.com/people/mfenner/blog/2009/10/23/open-access-week-a-researchers-perspective-part-ii">part II</a>), and I had given a similar talk in November in an <a href="https://web.archive.org/web/20150908065053/http://oa.helmholtz.de/index.php?id=254">Open Access workshop</a> organized by the Helmholtz Association. But because this time my audience (researchers and clinicians in a university hospital) was less knowledgeable about Open Access, I added a few introductory slides in the beginning.</p><p>The discussion is usually the most interesting part, and this topic certainly has a lot of material for discussion. Interestingly, we talked mainly about the problem of <strong><strong>copyright</strong></strong>. Even though anybody who has ever submitted a paper to a (non-Open Access) journal has signed a copyright transfer agreement, the implications of this were not really clear to most people in the audience. Reuse of a figure or table in an academic seminar usually falls under <strong><strong>fair use</strong></strong>, but many journals still require a (free) permission.<sup><a href="https://web.archive.org/web/20150908065053/http://blogs.plos.org/mfenner/2009/12/13/on_giving_a_talk_about_open_access_in_my_department/#fn1">1</a></sup> And using the same figure in a medical conference can cost several hundred dollars, and it doesn't really matter that you are one of the authors of the paper (slides 15-17 in the presentation). Some of my colleagues have run into issues with copyright, usually when the talks of a conference were later redistributed on a CD or website.</p><p>Unfortunately there wasn't enough time to discuss some of the other issues raised in the talk, e.g.</p><ul><li>Why can't our Medical School Library afford an institutional subscription for <em><em>Cell</em></em>?</li><li>Why is there no <strong><strong>institutional repository</strong></strong> at our university?</li><li>Why is it very unlikely that we will have a mandate for Open Access in Germany in the near future?</li><li>Why has the <strong><strong>Impact Factor</strong></strong> become so important in Medicine?</li></ul><p>The seminar was also interesting in that this was one of the rare occasions where I talked publicly in my department about some of the topics that I regularly write about on this blog. I always felt that most of my colleagues don't really care about these topics, and that they probably think I should rather spend my time working on the next paper or grant. I haven't gotten much feedback after the talk, but maybe I should reconsider that position.</p><p><sup>1</sup> Many journals use <a href="https://web.archive.org/web/20150908065053/http://www.copyright.com/">Copyright.com</a>, which makes this process straightforward.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Nature Communications: Interview with Lesley Anson]]></title>
            <link>https://blog.martinfenner.org/posts/nature-communications-interview-with-lesley-anson</link>
            <guid>809705dd-5dcc-4382-9542-63d04d6a93c6</guid>
            <pubDate>Thu, 26 Nov 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[Nature Communications
[https://web.archive.org/web/20170507030644/http://www.nature.com/ncomms/about_journal.html] 
is a new journal that will launch in Spring 2010. The journal will publish
papers in all areas of the physical, chemical and biological sciences and is
open for submissions.

The Nature Publishing Group publishes one fully open access journal (Molecular
Systems Biology
[https://web.archive.org/web/20170507030644/http://www.nature.com/msb/index.html]
) and more than ten journals
[ht]]></description>
            <content:encoded><![CDATA[<p><a href="https://web.archive.org/web/20170507030644/http://www.nature.com/ncomms/about_journal.html">Nature Communications</a> is a new journal that will launch in Spring 2010. The journal will publish papers in all areas of the physical, chemical and biological sciences and is open for submissions.</p><p>The Nature Publishing Group publishes one fully open access journal (<a href="https://web.archive.org/web/20170507030644/http://www.nature.com/msb/index.html">Molecular Systems Biology</a>) and <a href="https://web.archive.org/web/20170507030644/http://www.nature.com/press_releases/greengold.html">more than ten journals</a> that offer an open access option for authors (including <a href="https://web.archive.org/web/20170507030644/http://www.nature.com/emboj/index.html">EMBO Journal</a>). <em>Nature Communications</em> will be the first Nature journal with an open access option for authors. <em>Nature Communications</em> papers where the author has not opted for open access will be available through an institutional subscription, or by purchasing an individual article.</p><p>There are many good arguments for open access, but from a journal perspective the publishing model must make business sense. Most open access journals use the author-pays model, and <a href="https://web.archive.org/web/20170507030644/http://dx.doi.org/10.1038/nmat2497">an editorial in the August 2009 issue</a> of Nature Materials talked about some of the difficulties of this publishing model for the Nature journals – the costs that are currently spread among many subscribers would be prohibitely high for an author-pays option. Some high-profile open access journals have article publication charges that are probably not covering all costs (e.g. <a href="https://web.archive.org/web/20170507030644/http://www.plosbiology.org/home.action">PLoS Biology</a>) or use a different business model that doesn't require article publication charges (<a href="https://web.archive.org/web/20170507030644/http://www.bmj.com/">BMJ</a>). <em>Nature Communications</em> will have an <a href="https://web.archive.org/web/20170507030644/http://www.nature.com/ncomms/open_access/index.html">article publication charge of $5000</a>, which is higher than what most journals charge. I interviewed Lesley Anson, the Chief Editor of <em>Nature Communications</em>, to learn more about the journal.</p><h3 id="1-can-you-describe-nature-communications-for-me">1. Can you describe Nature Communications for me?</h3><p><em>Nature Communications</em> is the latest journal in NPG's portfolio. It is an online-only multidisciplinary journal publishing original research papers in all areas of the biological, chemical and physical sciences. The published research will be of the quality associated with Nature-branded journals, but won't necessarily have the high impact or broad appeal of papers published in Nature and the Nature research journals. In other words, we expect that papers published in <em>Nature Communications</em> will be of interest and importance to specialists within each field.</p><p>All research papers will be in Article format, regardless of their length, and will undergo rigorous, yet efficient, peer review and be published rapidly online. Authors of primary research papers can choose to make their published article available via subscribed access, or open access through the payment of a publication fee. <em>Nature Communications</em> will also publish occasional Reviews and Editorials.</p><h3 id="2-what-will-you-be-doing-differently-from-other-nature-journals">2. What will you be doing differently from other Nature journals?</h3><p>There are a number of differences between <em>Nature Communications</em> and other Nature-branded journals. For example, all other Nature titles publish print issues with regular news and comment sections and are available only by subscribed access.</p><p>Like other Nature-branded journals, <em>Nature Communications</em> has an independent team of editors who are responsible for maintaining the quality of the published research through rigorous peer review. However, <em>Nature Communications</em> has streamlined the editorial process – by limiting presubmission enquiries, appeals and the number of rounds of review – in order to secure rapid decisions for authors. The journal has also undertaken to publish research papers within 28 days of acceptance.</p><p>Another distinctive feature of <em>Nature Communications</em> is its Editorial Advisory Panel – to be announced shortly – which will consist of recognized experts from all areas of science. Their collective expertise will support the editorial team in ensuring that every field is represented in the journal.</p><h3 id="3-is-nature-communications-still-a-journal-in-the-traditional-sense-the-journal-is-online-only-has-no-news-and-views-and-the-articles-will-be-so-specialized-that-most-people-will-probably-find-papers-by-a-database-search-rather-than-by-looking-at-the-table-of-contents-">3. Is Nature Communications still a journal in the traditional sense? The journal is online only, has no news and views, and the articles will be so specialized that most people will probably find papers by a database search rather than by looking at the table of contents.</h3><p>I suppose it depends on your definition of a traditional journal. One could argue that we are going back to the roots of learned journals by focussing predominantly on primary research. In addition, like the traditional Nature-branded journals, <em>Nature Communications</em> has a defined scope and publishes only high-quality research.</p><p>We appreciate, though, that readers are accustomed to browsing journals by issue, therefore we are implementing technology to make the online browsing experience both intuitive and effective. There will be a number of ways for authors to find papers of interest to them, including personalization options and an extensive browse by subject category.</p><h3 id="4-why-did-you-decide-to-have-a-hybrid-model-of-both-open-access-and-subscribed-access">4. Why did you decide to have a hybrid model of both open access and subscribed access?</h3><p>We consider ourselves fortunate in being able to offer authors the choice of publishing with open access as well as subscribed access. Increasing support by funders for open access publication has made hybrid business models more viable for publishers. Furthermore, <em>Nature Communications</em> focus on primary research is particularly suited to a hybrid model because the cost of commissioning, editing and producing secondary content is minimized. These factors, and the high rejection rates on Nature and the Nature research journals, make open access charges for any other Nature-branded journal prohibitively high in the current market.</p><h3 id="5-with-this-hybrid-model-in-place-it-will-be-interesting-to-closely-watch-how-open-access-and-subscribed-access-articles-are-accessed-over-time-">5. With this hybrid model in place, it will be interesting to closely watch how open access and subscribed access articles are accessed over time.</h3><p>Yes, it will be interesting to monitor the average view rates for open-access versus subscribed access papers and we will be doing this following launch.</p><h3 id="6-does-an-author-decide-about-open-vs-subscribed-access-before-or-after-a-paper-is-accepted-for-publication">6. Does an author decide about open vs. subscribed access before or after a paper is accepted for publication?</h3><p>Authors won't have to make a final decision about access to their paper until the point at which their paper is accepted. Where authors do indicate a preference one way or the other during the editorial process, the reviewers will be blind to that choice.</p><h3 id="7-what-percentage-of-articles-do-you-expect-to-be-open-access">7. What percentage of articles do you expect to be open access?</h3><p>We can't predict what the open-access take-up will be like, and we are therefore prepared for open access uptake varying from as little as 0% to as much as 100%. <em>Nature Communications</em> business model works at both extremes and all values in between.</p><h3 id="8-how-does-the-transfer-of-a-manuscript-rejected-at-another-nature-journal-work">8. How does the transfer of a manuscript rejected at another Nature Journal work?</h3><p>The process of transferring a manuscript to <em>Nature Communications</em> is exactly the same as the transfer mechanism between the existing Nature-branded journals. Control rests entirely with the author, so transfers are only made at the authors request and with the understanding that the reviewers' reports from the previous journal will be transferred to <em>Nature Communications</em>.</p><p>Importantly, because all Nature journals are editorially independent, authors rejected from another Nature journal can choose to submit their paper to <em>Nature Communications</em> as a new submission. In that case, any submission or peer review details will remain confidential to the journal from which the manuscript was rejected.</p><h3 id="9-what-are-your-responsibilities-at-nature-communications">9. What are your responsibilities at Nature Communications?</h3><p>I am the Chief Editor of <em>Nature Communications</em> , which means I am responsible for the editorial content of the journal. I have a team of talented editors to help in the task of selecting suitable manuscripts for publication: a biologist, a physicist and a chemist. Their profiles are <a href="https://web.archive.org/web/20170507030644/http://www.nature.com/ncomms/authors/about_eds/index.html">available on our website</a>.</p><h3 id="10-what-did-you-do-before-starting-to-work-at-nature-communications">10. What did you do before starting to work at Nature Communications?</h3><p>Before taking on the task of launching Nature Communications, I spent more than ten years as a manuscript editor at Nature. During that time, I handled a number of different areas in the cellular and molecular sciences, and was also responsible for the editorial content of Nature's <a href="https://web.archive.org/web/20170507030644/http://www.nature.com/nature/supplements/insights/index.html">Insight Programme</a>. Before joining Nature I trained as a biophysicist at the University of Bristol and University College London.</p><h3 id="11-what-are-the-best-places-to-find-out-more-about-nature-communications">11. What are the best places to find out more about Nature Communications?</h3><p>For more information about <em>Nature Communications</em>, including the journal's Aims and Scope, biographies of the editors and how to submit, please visit <a href="https://web.archive.org/web/20170507030644/http://www.nature.com/naturecommunications">our website</a>. Any questions can be directed to our <a href="https://web.archive.org/web/20170507030644/http://network.nature.com/groups/naturecommunications/forum/topics">dedicated forum</a> on Nature Network, or sent directly to the editorial team by e-mailing <a href="https://web.archive.org/web/20170507030644/mailto:natcomms%40nature.com">natcomms@nature.com</a>. We look forward to hearing from you!</p><p><em>For further information, please also read the <a href="https://web.archive.org/web/20170507030644/http://blogs.nature.com/nautilus/2009/10/nature">announcement</a></em> on the <em>Nautilus</em> blog and the <a href="https://web.archive.org/web/20170507030644/http://blog.openwetware.org/scienceintheopen/2009/11/16/nature-communications-qa/">Nature Communications Q&amp;A</a> with Grace Baynes on the <em>Science in the open</em> blog._</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Publication bias in clinical trials]]></title>
            <link>https://blog.martinfenner.org/posts/publication-bias-in-clinical-trials</link>
            <guid>1a8a7ffd-caa8-440e-b161-bf839dc12c35</guid>
            <pubDate>Wed, 18 Nov 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[Last week the New England Journal of Medicine (NEJM) published a paper on
selective outcome reporting in clinical trials (Vedula et al. 2009
[https://web.archive.org/web/20151003094331/http://dx.doi.org/10.1056/NEJMsa0906126]
). The primary and secondary outcome(s) of a clinical trial could for example be
survival in cancer patients or rate of heart attacks and other cardiovascular
events in patients taking cholesterol-lowering drugs. These outcomes are defined
in the study protocol before the f]]></description>
            <content:encoded><![CDATA[<p>Last week the <em><em>New England Journal of Medicine</em></em> (<em><em>NEJM</em></em>) published a paper on selective outcome reporting in clinical trials (<a href="https://web.archive.org/web/20151003094331/http://dx.doi.org/10.1056/NEJMsa0906126">Vedula et al. 2009</a>). The primary and secondary outcome(s) of a clinical trial could for example be survival in cancer patients or rate of heart attacks and other cardiovascular events in patients taking cholesterol-lowering drugs. These outcomes are defined in the study protocol before the first patient is treated, and whether or not the primary outcome is reached (using statistical testing) defines whether a trial was positive or negative. The study protocol is approved by an institutional review board (IRB) and can only be changed later (and that includes changes in the protocol-defined outcomes) if again approved by the IRB.</p><p>The study in the <em><em>NEJM</em></em> examined the outcomes of 20 clinical trials for the drug gabapentin using internal documents of the drug companies Pfizer and Parke-Davis, and compared them to the outcomes reported for those trials that were published as peer-reviewed papers. The internal company documents were obtained as the result of litigation in which the company admitted guilt for off-label marketing (i.e. marketing for uses that were not approved by the Food and Drug Administration) of gabapentin.</p><p>12 of the 20 trials were published as peer-reviewed papers. In 8 of these 12 papers the primary outcome differed from the primary outcome defined in the protocol. New primary outcomes were introduced in 6 papers, and protocol-defined primary outcomes were not reported in 5 papers. Trials with primary outcomes that were not significant were either not reported as full paper or were reported with a changed primary outcome.</p><p>Selective outcome reporting has also been reported by other authors and is not limited to trials funded by drug companies (Chan 2004). Selective outcome reporting is a major problem because it a) distorts our scientific knowledge and b) is unethical as it involves research on human subjects. This could for example lead to repeated studies of a clearly ineffective or harmful drug or intervention. The distortion of our scientific knowledge by selective outcome reporting in scientific journals is also a concern for research in other areas. Confirmation of important research findings by independent groups is important, but a lot of research is probably repeated simply because negative results were not published. Some of these reporting biases could be avoided by making more unpublished research data available, either by <a href="https://web.archive.org/web/20151003094331/http://usefulchem.blogspot.com/">Open Notebook Science</a> or by publishing “unexciting negative” findings in peer-review journals or preprint archives such as <a href="https://web.archive.org/web/20151003094331/http://precedings.nature.com/">Nature Precedings</a>.</p><p>Clinical medicine tries to solve the problem of publication bias by making public registration of clinical trials mandatory before patient enrollment. This registration is required by most major medical journals since 2004 (De Angelis 2004). U.S. legislation also requires clinical trial registration and since 2008 (<a href="https://web.archive.org/web/20151003094331/http://network.nature.com/people/mfenner/blog/2008/08/02/fdaaa-push-to-open-data-in-clinical-medicine">FDAAA: Push to open data in clinical medicine</a>) this includes outcome reporting within 12 months after data for the last subject were received in the publicly available <a href="https://web.archive.org/web/20151003094331/http://www.clinicaltrials.gov/">Clinicaltrials.gov</a> database. These efforts should ensure ethical standards of clinical research, and help to avoid the kinds of biases reported in the paper by Vedula et al.</p><p>Clinical trial registries obviously also serve other purposes, as they help interested patients and their relatives and treating physicians to find clinical trials they want to participate in, and they help researchers and clinicians to do a systematic overview of the ongoing research in a particular field. Just as there are centralized databases (e.g. <a href="https://web.archive.org/web/20151003094331/http://www.ncbi.nlm.nih.gov/pmc/">PubMed Central</a> and the <a href="https://web.archive.org/web/20151003094331/http://www.controlled-trials.com/mrct/">Current Controlled Trials</a> metaRegister of controlled trials) and institutional repositories for full-text papers, there are also both central (e.g. <a href="https://web.archive.org/web/20151003094331/http://clinicaltrials.gov/">Clinicaltrials.gov</a> – Both PubMed Central and Clinicaltrials.gov are efforts by the U.S. <a href="https://web.archive.org/web/20151003094331/http://www.nih.gov/">National Institutes of Health</a>) and institutional clinical trial registries. But in contrast to institutional repositories, there are no standard software tools available that an institution can use. I am involved in building a <a href="https://web.archive.org/web/20151003094331/http://www.mh-hannover.de/studien">clinical trial registry for our institution</a>, and the effort is both technically demanding, and socially challenging. The information in a clinical trial registry is constantly changing and we need to keep the efforts required by the individual researchers at a mininum. We also have to walk a fine line of what information can be made publicly available, and here we follow the standards provided by both Clinicaltrials.gov and the <a href="https://web.archive.org/web/20151003094331/http://www.who.int/ictrp/en/">WHO</a>.</p><p><a href="https://web.archive.org/web/20151003094331/https://eudract.emea.europa.eu/">EudraCT</a>, the mandatory European clinical trials registry is regretably not available to the public.</p><h3 id="references">References</h3><p>Vedula, S., Bero, L., Scherer, R., &amp; Dickersin, K. (2009). Outcome Reporting in Industry-Sponsored Trials of Gabapentin for Off-Label Use <em>New England Journal of Medicine, 361</em> (20), 1963-1971 https://doi.org/<a href="https://web.archive.org/web/20151003094331/http://dx.doi.org/10.1056/NEJMsa0906126">10.1056/NEJMsa0906126</a></p><p>Chan, A.-W. (2004). Outcome reporting bias in randomized trials funded by the Canadian Institutes of Health Research. Canadian Medical Association Journal, 171(7), 735–740. <a href="https://doi.org/10.1503/cmaj.1041086">https://doi.org/10.1503/cmaj.1041086</a></p><p>De Angelis, C., Drazen, J. M., Frizelle, F. A., Haug, C., Hoey, J., Horton, R., … Weyden, M. B. V. D. (2004). Clinical Trial Registration: A Statement from the International Committee of Medical Journal Editors. New England Journal of Medicine, 351(12), 1250–1251. <a href="https://doi.org/10.1056/nejme048225">https://doi.org/10.1056/nejme048225</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Moving article-level metrics forward]]></title>
            <link>https://blog.martinfenner.org/posts/moving-article-level-metrics-forward</link>
            <guid>a04ebbf0-f3e5-4a86-bfb0-f6b560d9e664</guid>
            <pubDate>Wed, 11 Nov 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[In September PLoS started to show usage data (downloads, citations, but also use
of social bookmarking services and blog posts) for all their published papers (
article-level metrics at PLoS: addition of usage data
[https://web.archive.org/web/20120611103840/http://www.plos.org/cms/node/485]). 
PLoS is not the first publisher to do that, but certainly the largest to date.
Two Nature Network bloggers wrote about these changes back in June (The
Scientist: On article-level metrics and other animals]]></description>
            <content:encoded><![CDATA[<p>In September <em><em>PLoS</em></em> started to show usage data (downloads, citations, but also use of social bookmarking services and blog posts) for all their published papers (<a href="https://web.archive.org/web/20120611103840/http://www.plos.org/cms/node/485">article-level metrics at PLoS: addition of usage data</a>). <em><em>PLoS</em></em> is not the first publisher to do that, but certainly the largest to date. Two Nature Network bloggers wrote about these changes back in June (The Scientist: <a href="https://web.archive.org/web/20120611103840/http://network.nature.com/people/rpg/blog/2009/06/22/on-article-level-metrics-and-other-animals">On article-level metrics and other animals</a>) and August (Gobbledygook: <a href="https://web.archive.org/web/20120611103840/http://network.nature.com/people/mfenner/blog/2009/08/15/plos-one-interview-with-peter-binfield">PLoS One: Interview with Peter Binfield</a>), and a number of blogs commented on this new feature, including:</p><ul><li>A Blog around the Clock: <a href="https://web.archive.org/web/20120611103840/http://scienceblogs.com/clock/2009/09/article-level_metrics_at_plos_1.php">Article-Level Metrics at PLoS – Download Data</a></li><li>The Scholarly Kitchen: <a href="https://web.archive.org/web/20120611103840/http://scholarlykitchen.sspnet.org/2009/09/22/plos-releases-article-level-usage-data/">PLoS Releases Article-level Metrics</a></li><li>BMJ Group Blogs: <a href="https://web.archive.org/web/20120611103840/http://blogs.bmj.com/bmj/2009/11/02/richard-smith-the-beginning-of-the-end-for-impact-factors-and-journals/">Richard Smith: The beginning of the end for impact factors and journals</a></li><li>Blue Lab Coats: <a href="https://web.archive.org/web/20120611103840/http://bluelabcoats.wordpress.com/2009/09/18/article-level-metrics-debut-at-plos/">Article Level Metrics Debut at PLOS</a></li></ul><p>There are a number of reasons why article-level metrics are a good idea, and I hope that many other journal publishers will follow. But in this blog post I want to talk about some of the shortcomings of the current implementation of article-level metrics.</p><h3 id="article-level-metrics-should-be-combined-from-different-places">Article-level metrics should be combined from different places</h3><p>Full-text articles live in more than one place. Obviously at the journal publisher's website, but in many cases also in one or more institutional repositories and at <a href="https://web.archive.org/web/20120611103840/http://www.ncbi.nlm.nih.gov/pmc/">PubMed Central</a> (or similar places for papers not published in the life sciences). Which of these places produces the most reliable article-level metrics or should the HTML views, PDF downloads, etc. from all these places be combined? The decentralized nature of institutional repositories makes it especially difficult to combine usage statistics from them, but there are <a href="https://web.archive.org/web/20120611103840/http://www.dini.de/projekte/oa-statistik/english/">projects</a> that try to tackle this problem. A unique identifier is required to combine the usage data from these different sources, and we have the <a href="https://web.archive.org/web/20120611103840/http://www.doi.org/">DOI</a> for that. <em><em>PubMed Central</em></em> and similar large repositories could not only start to provide their own usage data, but also combine them with the usage data from those journal publishers that already provide them.</p><h3 id="article-level-metrics-need-author-identifiers"><em><em>Article-level metrics need author identifiers</em></em></h3><p>Evaluating the “impact” of a researcher is one obvious use for article-level metrics. In order to be able to do that for more than a handful of researchers, we need unique author identifiers. This year we have had many discussions about author identifiers (including <a href="https://web.archive.org/web/20120611103840/http://network.nature.com/people/mfenner/blog/2009/02/17/interview-with-geoffrey-bilder">this blog</a> and at the <a href="https://web.archive.org/web/20120611103840/http://network.nature.com/people/mfenner/blog/2009/08/23/thoughts-on-the-science-online-london-conference">Science Online London Conference</a>), and I hope that in 2010 we will finally see an evolving standard that is picked up by journal publishers. It would be in the interest of PLoS to combine their article-level metrics with an author identifier as soon as possible, most likely the proposed CrossRef ContributorID, rather than the Elsevier Scopus Author Identifier or the Thomson Reuters Researcher ID.</p><h3 id="article-level-metrics-should-enhance-literature-searches">Article-level metrics should enhance literature searches</h3><p>We all know how Google became the most popular search engine (<a href="https://web.archive.org/web/20120611103840/http://en.wikipedia.org/wiki/PageRank">Pagerank</a>). And article usage data would be a tremendous boost for scientific literature databases such as PubMed. A literature search should sort the results by usage data (e.g. a combination of number of citations, HTML views and PDF downloads) rand not the rather boring publication date, author or journal name. Normally I would think that <em><em>Google Scholar</em></em> would be the first place to implement such a functionality, but I haven't seen much innovation from Google Scholar lately.</p><h3 id="article-level-metrics-should-not-only-be-numbers">Article-level metrics should not only be numbers</h3><p>As we don't want to reduce a paper to simple numbers, it is important to provide more than HTML views and PDF download counts. Citations counts are useful numbers, but linking to the citing papers is even more interesting. Similarly we want to see links to <em><em>Faculty of 1000</em></em> recommendations and blog posts aggregated at <em><em>ResearchBlogging.org</em></em>. If we extend this further, we should probably start to think about a better name for article-level metrics. And I hope we never start to call this ALM.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[UK PubMed Central: Interview with Phil Vaughan]]></title>
            <link>https://blog.martinfenner.org/posts/uk-pubmed-central-interview-with-phil-vaughan</link>
            <guid>ac3cc70d-1574-41f3-8565-46197d2fade6</guid>
            <pubDate>Wed, 04 Nov 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[PubMed Central
[https://web.archive.org/web/20120611110953/http://www.ncbi.nlm.nih.gov/pmc/] 
was launched in February 2000 by the U.S. National Institutes of Health (NIH) as
a free digital archive of journal articles. Just as PubMed, PubMed Central
covers research in the life sciences, but not other areas of research, e.g.
engineering, physical sciences or astronomy.

Some journal articles are available as full text as soon as they are published,
and most journals provide free access to full te]]></description>
            <content:encoded><![CDATA[<figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611110953im_/http://ukpmc.ac.uk/ppmc-localhtml/images/ukpmc_145.jpg" class="kg-image" alt></figure><p><a href="https://web.archive.org/web/20120611110953/http://www.ncbi.nlm.nih.gov/pmc/">PubMed Central</a> was launched in February 2000 by the U.S. National Institutes of Health (NIH) as a free digital archive of journal articles. Just as PubMed, PubMed Central covers research in the life sciences, but not other areas of research, e.g. engineering, physical sciences or astronomy.</p><p>Some journal articles are available as full text as soon as they are published, and most journals provide free access to full text articles within a year of publication. Some journals only provide the full text of some articles, including research funded by the NIH under the <a href="https://web.archive.org/web/20120611110953/http://publicaccess.nih.gov/policy.htm">NIH Public Access Policy</a>. The majority of fulltext articles in PubMed Central are not Open Access, but are protected by copyright. These articles are often made available under a license that allows redistribution and reuse.</p><p>All articles are deposited using the <a href="https://web.archive.org/web/20120611110953/http://dtd.nlm.nih.gov/">NLM-DTD</a> XML format, which is a standard text format suitable for text mining and long-term archiving. Most articles are deposited directly by the journals, so that authors do not have to get involved in the technical aspects of article deposition.</p><p>PubMed Central is a centralized archive of full text papers and not simply an interface to search these articles at the websites of the participating journals. Neither is PubMed Central an interface to search the various institutional repositories at universities and institutions. The NIH thinks that this centralized approach makes it easier to develop additional functionality, including the integration with other databases (e.g. the protein or nucleotide databases) hosted at the NIH.</p><p>UK PubMed Central was launched in 2007 as the first PubMed Central outside the United States (<a href="https://web.archive.org/web/20120611110953/http://pubmedcentralcanada.ca/">PubMed Central Canada</a>, the second international PubMed Central, launched this week). In January 2010 UK PubMed Central will launch a number of new services, and I used the opportunity to ask UK PubMed Central Programme Manager Philip Vaughan a few questions.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611110953im_/http://gallery.me.com/mfenner/100079/philip_vaughan/web.jpg" class="kg-image" alt></figure><h3 id="1-what-is-uk-pubmed-central">1. What is UK PubMed Central?</h3><p><a href="https://web.archive.org/web/20120611110953/http://ukpmc.ac.uk/">UK PubMed Central</a> offers free access to 1.6 million full text journal articles in the fields of Life Sciences, Biomedicine and Health. Our content is free to read, print off and download. There is no registration process required: users can simply visit our site and start searching!It has been developed in consultation with the UK biomedical and health research community, and is about to launch some exciting new services in January 2010. Usage of our service has been growing steadily; we average 300,000 downloads a month currently.</p><h3 id="2-why-do-we-need-more-than-one-pubmed-central-and-how-are-they-connected">2. Why do we need more than one PubMed Central, and how are they connected</h3><p>UKPMC gives the UK research community access to all the content of <a href="https://web.archive.org/web/20120611110953/http://www.ncbi.nlm.nih.gov/pmc/">PMC</a>, but with added value specifically for the UK. For example, it offers Grant Reporting features, whereby users can search for current grants from all of our 8 Funders, can link the publications they produce to the grants they originated from, and view the impact of their work through access to citations. UK researchers can also <a href="https://web.archive.org/web/20120611110953/https://ukmss.mimas.ac.uk/ukmss">deposit their manuscripts directly</a> into UKPMC.We will also be accessing additional content not in PMC; around 475,000 extra articles, reviews, guidelines and theses. Through the involvement of our funders, we are capturing around 90% of recently published journal articles from biomedical research conducted in the UK.We are therefore increasing the visibility of UK research in the field.</p><p>Our colleagues at NCBI in the US have been very supportive of our activities: they have been keen for the PMC “project” to expand beyond the US. A Canadian version (<a href="https://web.archive.org/web/20120611110953/http://pubmedcentralcanada.ca/">PMC Canada</a>) launched this week.</p><h3 id="3-why-does-a-search-use-pubmed-central-and-not-the-uk-version">3. Why does a search use PubMed Central and not the UK version?</h3><p>The current UKPMC site searches PMC as it “mirrors” its content. But this about to change; from January UKPMC will be a “stand alone” service with its own up to date archive of content, as well as access to all the content of PubMed and PubMed Central.</p><h3 id="4-what-is-the-relationship-between-uk-pubmed-central-and-institutional-repositories">4. What is the relationship between UK PubMed Central and institutional repositories?</h3><p>UKPMC is a subject repository and as such receives content from researchers at Higher Education institutions across the UK. Deposition in UKPMC is mandatory if a researcher has a grant from any of our 8 Funders: <a href="https://web.archive.org/web/20120611110953/http://www.wellcome.ac.uk/">Wellcome Trust</a>, <a href="https://web.archive.org/web/20120611110953/http://www.cancerresearchuk.org/">Cancer Research UK</a>, <a href="https://web.archive.org/web/20120611110953/http://www.mrc.ac.uk/">Medical Research Council</a>, <a href="https://web.archive.org/web/20120611110953/http://www.bhf.org.uk/">British Heart Foundation</a>, <a href="https://web.archive.org/web/20120611110953/http://www.arc.org.uk/">Arthritis Research Campaign</a>, <a href="https://web.archive.org/web/20120611110953/http://www.bbsrc.ac.uk/">Biotechnology and Biosciences Research Council</a>, <a href="https://web.archive.org/web/20120611110953/http://www.nihr.ac.uk/">National Institute for Health Research</a> and the <a href="https://web.archive.org/web/20120611110953/http://www.sehd.scot.nhs.uk/cso/">Chief Scientists Office</a>. Consequently we do not need to harvest content from other repositories. However, institutional repositories are welcome to harvest our content; we have an OAI-PMH interface to enable this. On the other hand, there are no plans to develop of institutional repositories from within UK PubMed Central.</p><h3 id="5-what-is-the-material-that-can-t-be-put-into-uk-pubmed-central">5. What is the material that can't be put into UK PubMed Central?</h3><p>If the material is not peer-reviewed research published in a recognized journal by a known publisher, it would not be deposited into UK PubMed Central. One aspect of our development programme has been to identify what further content we could provide links to, for example published research theses and NHS clinical guidelines. But, we have rigid quality control procedures in place even to provide links to material. That said, we do remain open minded, in terms of what material we may consider linking to in the future. And who knows what might arise in the future. For example, there does seem to be an appetite to include images.</p><h3 id="6-can-i-submit-my-accepted-manuscript-if-it-was-not-funded-by-one-of-the-uk-pubmed-central-funders">6. Can I submit my accepted manuscript if it was not funded by one of the UK PubMed Central funders?</h3><p>Unfortunately not at this time. We are hopeful that other UK research councils and funders of life sciences research will come on board in due course. The more funders that come on board, the stronger our service will become and the stronger our message on the importance of OA.</p><h3 id="7-what-is-the-uk-pubmed-central-oai-service">7. What is the UK PubMed Central OAI service?</h3><p>The UK PubMed Central OAI service, (<a href="https://web.archive.org/web/20120611110953/http://ukpmc.ac.uk/ppmc-localhtml/oai_service.html">UKPMC-OAI</a>) provides access to metadata of all items in the UKPMC archive, as well as to the full text of a subset of these items.</p><h3 id="8-what-are-your-responsibilities-at-uk-pubmed-central">8. What are your responsibilities at UK PubMed Central?</h3><p>I am the Programme Manager. I am responsible for the current service which has been in existence since January 2007. I also co-ordinate all our current development activities, which are undertaken by ourselves at the <a href="https://web.archive.org/web/20120611110953/http://www.bl.uk/">British Library</a>, by the University of Manchester (<a href="https://web.archive.org/web/20120611110953/http://mimas.ac.uk/">MIMAS</a>), the National Centre for Text Mining (<a href="https://web.archive.org/web/20120611110953/http://www.nactem.ac.uk/">NacTEM</a> – based at the University of Manchester) and the European Bioinformatics Institute (<a href="https://web.archive.org/web/20120611110953/http://www.ebi.ac.uk/">EBI</a>) based at Hinxton in Cambridgeshire.</p><h3 id="9-what-did-you-do-before-starting-to-work-on-uk-pubmed-central">9. What did you do before starting to work on UK PubMed Central?</h3><p>I originally trained as a librarian. Since then I have worked mainly in health and medical information work, both in Higher Education and in the National Health Service. My most recent post was for <a href="https://web.archive.org/web/20120611110953/http://www.jisc.ac.uk/">JISC</a> (Joint Information Systems Committee) as a Programme Manager. I was responsible for a portfolio of development projects in the digital library sphere. I joined UKPMC at the <a href="https://web.archive.org/web/20120611110953/http://www.bl.uk/">British Library</a> in May 2008.</p><h3 id="10-do-you-want-to-talk-about-future-plans-for-uk-pubmed-central">10. Do you want to talk about future plans for UK PubMed Central?</h3><p>Yes please! We are soon to launch some exciting new initiatives in January 2010. Our website has been completely redesigned with a new more intuitive interface, and we will be expanding our content to include all content from PubMed (c. 18m references), patents, theses and <a href="https://web.archive.org/web/20120611110953/http://www.library.nhs.uk/GUIDELINESFINDER/">NHS Clinical Guidelines</a>. Consequently our users will be able to access a vast collection of relevant content through one portal.</p><p>Our search engine will be utilising innovative new techniques such as text mining to retrieve more contextually relevant information, link to other relevant databases and provide a richer search experience for the user. We are also expanding our Grant Reporting functions to allow Grantees and Funders to assess the impact and value of their funded research, and increase their visibility.</p><p>Our Funders are keen to expand the programme, possibly towards a future <em><em>European PubMed Central</em></em>.We are hopeful that as part of this process some European Funders may be joining the programme shortly. Our Funders hope to make an announcement regarding this shortly.</p><p>We are holding a Showcase event on January 12th at the British Library in London to highlight these new developments and the future potential of the service.</p><h3 id="11-could-you-provide-contact-information-for-people-that-have-further-questions-about-pubmed-central">11. Could you provide contact information for people that have further questions about PubMed Central?</h3><p>By all means, people can email us at <a href="https://web.archive.org/web/20120611110953/mailto:ukpmc%40bl.uk">ukpmc@bl.uk</a> and my email is <a href="https://web.archive.org/web/20120611110953/http://bl.uk/">Philip.Vaughan@bl.uk</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Open Access Week: a researcher’s perspective part II]]></title>
            <link>https://blog.martinfenner.org/posts/open-access-week-a-researchers-perspective-part-ii</link>
            <guid>2a65bf06-9dd2-4fb7-86fd-6135b09f8787</guid>
            <pubDate>Fri, 23 Oct 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[This week (October 19-23) is Open Access Week
[https://web.archive.org/web/20120611105548/http://www.openaccessweek.org/] – a
good opportunity to think and write about this topic. On Monday I wrote in a 
blog post
[https://web.archive.org/web/20120611105548/http://network.nature.com/people/mfenner/blog/2009/10/18/open-access-week-a-researchers-perspective]
:

> Open Access can be looked at from many different angles, including the
researcher, the science library, the institution, the funding org]]></description>
            <content:encoded><![CDATA[<p>This week (October 19-23) is <a href="https://web.archive.org/web/20120611105548/http://www.openaccessweek.org/">Open Access Week</a> – a good opportunity to think and write about this topic. On Monday I wrote in a <a href="https://web.archive.org/web/20120611105548/http://network.nature.com/people/mfenner/blog/2009/10/18/open-access-week-a-researchers-perspective">blog post</a>:</p><blockquote>Open Access can be looked at from many different angles, including the researcher, the science library, the institution, the funding organization, the journal, the science journalist, and the general public. Most arguments for or against Open Access depend on that angle. As a researcher, I am most interested in whether Open Access will make my work easier. Again, a researcher can look at Open Access from different roles: reader, author, reviewer, editor.</blockquote><p>In that blog post I then wrote about the role of the researcher as a reader. Now I want to look at the perspective of the researcher as an author.</p><h3 id="choice-of-journal">Choice of journal</h3><p>The decision of where to publish a manuscript for most researchers probably works something like “find the best journal where I can publish my work with the least amount of trouble”. <em><em>Best journal</em></em> usually is a subjective decision, but probably correlates with the <a href="https://web.archive.org/web/20120611105548/http://thomsonreuters.com/products_services/science/free/essays/impact_factor/">Impact Factor</a> of a journal. As the average quality of manuscripts is higher in a <em><em>better journal</em></em>, this will help to value your research in the eyes of granting agencies and job search committees. <em><em>Better journal</em></em> often means higher rejection rates and/or higher numbers of readers. Both factors – and journals that publish a relatively small number of papers – favor a subscription business model<sup><a href="https://web.archive.org/web/20120611105548/http://blogs.plos.org/mfenner/2009/10/23/open_access_week_a_researchers_perspective_part_ii/#fn1">1</a></sup>.</p><h3 id="publication-cost">Publication cost</h3><p>Most Open Access journals use an author-pays model to pay for publication costs. Funding agencies or institutions may pick up these costs, but authors may be left with costs of $2500 or more.</p><h3 id="institutional-repositories">Institutional Repositories</h3><p>Self-archiving in institutional repositories (green access) is a great way to make your publication freely available if the paper is published in a journal that is not Open Access. Unfortunately this often requires extra efforts by the researcher, and the publication will be more difficult to find in the repository than in the journal. This creates little incentive for a researcher to get involved in self-archiving.</p><h3 id="citation-advantage">Citation advantage</h3><p>The effect of free access to the scientific literature on article downloads and citations is difficult to measure. Some<sup><a href="https://web.archive.org/web/20120611105548/http://blogs.plos.org/mfenner/2009/10/23/open_access_week_a_researchers_perspective_part_ii/#fn2">2</a></sup>, but not all studies<sup><a href="https://web.archive.org/web/20120611105548/http://blogs.plos.org/mfenner/2009/10/23/open_access_week_a_researchers_perspective_part_ii/#fn3">3</a></sup> show higher citation rates for articles that are freely available and this citation advantage might be modest<sup><a href="https://web.archive.org/web/20120611105548/http://blogs.plos.org/mfenner/2009/10/23/open_access_week_a_researchers_perspective_part_ii/#fn4">4</a></sup>. Citations are generated by other researchers who have access to your paper, and therefore I'm not surprised if there is not much of a difference between papers in Open Access journals and popular journals that are subscribed my many institutions.</p><h3 id="better-access">Better Access</h3><p>Researchers in poorer countries will have easier access to papers published in Open Access journals, although many subscription journals wave access fees through initiatives such as <a href="https://web.archive.org/web/20120611105548/http://www.who.int/hinari/en/">Hinari</a>. Open Access makes it easier for journalists, high school students, patient advocacy groups and many more people to read your papers. This is obviously of great value to these groups, but I haven't seen many examples where the paper author directly benefitted from this.</p><h3 id="social-responsibility">Social responsibility</h3><p>The argument that publicly funded research should be available to everybody at time of publication can be a motivation for many scientists, but I would be careful to turn this into an obligation. Different countries have different traditions, but in my home country Germany the independence of research and researchers (including the decision where to publish) has become a constitutional right after the atrocities committed in the name of “science” in Nazi Germany. All major German research organizations support Open Access, but in contrast to other countries there is no Open Access mandate.</p><h3 id="summary">Summary</h3><p>Publishing in an Open Access journal has surprisingly little benefits for the author of a paper, and often means additional costs. Unless we want to mandate Open Access publishing from authors because it benefits the other stakeholders (which at least in Germany would be difficult), we should make publishing in an Open Access journal more attractive to authors. It looks like PLoS ONE is doing exactly that, as 400 manuscripts published per month testify.</p><h3 id="references">References</h3><p>Science in the open. (2009). Nature Materials, 8(8), 611–611. <a href="https://doi.org/10.1038/nmat2497">https://doi.org/10.1038/nmat2497</a></p><p>Eysenbach, G. (2006). Citation Advantage of Open Access Articles. PLoS Biology, 4(5), e157. <a href="https://doi.org/10.1371/journal.pbio.0040157">https://doi.org/10.1371/journal.pbio.0040157</a></p><p>Davis, P. M., Lewenstein, B. V., Simon, D. H., Booth, J. G., &amp; Connolly, M. J. L. (2008). Open access publishing, article downloads, and citations: randomised controlled trial. BMJ, 337(jul31 1), a568–a568. <a href="https://doi.org/10.1136/bmj.a568">https://doi.org/10.1136/bmj.a568</a></p><p>Evans, J. A., &amp; Reimer, J. (2009). Open Access and Global Participation in Science. Science, 323(5917), 1025–1025. https://doi.org/10.1126/science.1154562</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Open Access Week: a researcher’s perspective]]></title>
            <link>https://blog.martinfenner.org/posts/open-access-week-a-researchers-perspective</link>
            <guid>e35ac35e-168e-47ca-8130-53d160c37d8d</guid>
            <pubDate>Sun, 18 Oct 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[This week (October 19-23) is Open Access Week
[https://web.archive.org/web/20120611105124/http://www.openaccessweek.org/]:

> Open Access Week is an opportunity to broaden awareness and understanding of
Open Access to research, including access policies from all types of research
funders, within the international higher education community and the general
public.
The following video from SPARC
[https://web.archive.org/web/20120611105124/http://www.arl.org/sparc/] (the
Scholarly Publishing & Acad]]></description>
            <content:encoded><![CDATA[<p>This week (October 19-23) is <a href="https://web.archive.org/web/20120611105124/http://www.openaccessweek.org/">Open Access Week</a>:</p><blockquote>Open Access Week is an opportunity to broaden awareness and understanding of Open Access to research, including access policies from all types of research funders, within the international higher education community and the general public.</blockquote><p>The following video from <a href="https://web.archive.org/web/20120611105124/http://www.arl.org/sparc/">SPARC</a> (the Scholarly Publishing &amp; Academic Resources Coalition) is a good introduction:</p><p>Open Access can be looked at from many different angles, including the <em><em>researcher</em></em>, the <em><em>science library</em></em>, the <em><em>institution</em></em>, the <em><em>funding organization</em></em>, the <em><em>journal</em></em>, the <em><em>science journalist</em></em>, and the <em><em>general public</em></em>. Most arguments for or against Open Access depend on that angle. As a researcher, I am most interested in whether Open Access will make my work easier. Again, a researcher can look at Open Access from different roles:</p><ul><li>Reader</li><li>Author</li><li>Reviewer</li><li>Editor</li></ul><p>The role as a reviewer or editor for an open access paper should be essentially the same as for a paper with subscription-based access. The journal <em><em>Nature Communications</em></em> that launches in April 2010 with a hybrid publishing model of open access and subscription-based access will for example have reviewers and editors <a href="https://web.archive.org/web/20120611105124/http://www.nature.com/ncomms/open_access/pdf/open-access-faqs.pdf">blinded to the author's choice</a>.</p><p>In this blog post I will look at Open Access from the perspective of the researcher as a reader.</p><h3 id="access">Access</h3><p>As a researcher in a German university I am privileged to have institutional access to most journal articles that I need for my work. I use the program <a href="https://web.archive.org/web/20120611105124/http://mekentosj.com/papers/">Papers</a> as my main reference manager. Papers allows me to order my currently 1715 references (and PDFs of fulltext paper to most of them) by journal. Among the 20 journals with the most papers in my library, my institution doesn't have access to three of them:</p><ul><li><em><em>Cell</em></em> (don't ask)</li><li><em><em>Lancet Oncology</em></em></li><li><em><em>Nature Reviews Clinical Oncology</em></em></li></ul><p>Obviously three important journals for someone doing clinical cancer research. I could ask my institution to start subscribing to these journals, start a personal subscription (I had a personal subscription to <em><em>Nature Clinical Practice Oncology</em></em> for two years before it was renamed to <em><em>Nature Reviews Clinical Oncology</em></em>) which would set me back 150-200 â‚¬ per journal, or I could pay for an individual article (either through my library or directly from the journal). All this requires extra time and money, worth only if I think a paper/journal is really important.</p><p><em><em>PLoS Medicine</em></em> is the only open access journal among the 20 most popular journals in my Papers library (The <em><em>BMJ</em></em> has free access to its research articles and is the 24th most popular). Unfortunately there are only a few Open Access journals publishing papers that are relevant to my work.</p><p>As many others I do work from home in the evening or on the weekend, or while travelling. I am lucky that I can access my university network through <a href="https://web.archive.org/web/20120611105124/http://en.wikipedia.org/wiki/Virtual_private_network">VPN</a> and therefore can get fulltext access to journal articles (one of the most important VPN uses for me). But some researchers might not be so lucky, or spend precious extra time setting up and using VPN.</p><p>Researchers that work in a poorer country, or for a smaller university or small biotech startup will have much larger problems. Medical doctors in community hospitals or private practice may not have easy access to any of the relevant journals, and they might depend on reprints given to them by colleagues or representatives from drug companies.</p><h3 id="sharing">Sharing</h3><p>If several people work on a research project, they also want to share the relevant literature in the field. Most subscription-based journals retain the copyright to the paper and don't allow storing in a retrieval system or transmitting of papers without permission. This could mean that you can't email the PDF of a paper to a colleague even if you are the author or his institution also has a subscription. And this could also mean that you can't use a reference manager such as <a href="https://web.archive.org/web/20120611105124/http://www.refworks.com/">Refworks</a> or <a href="https://web.archive.org/web/20120611105124/http://www.mendeley.com/">Mendeley</a> to not only share references with your lab colleagues, but also the full-text PDF files. Strictly following the copyright can make something as common as a <em><em>journal club</em></em> a complicated affair.</p><h3 id="permissions">Permissions</h3><p>As most subscription-based journals retain the copyright to the paper, you have to ask for permissions when reusing tables or figures. Most often this is the case when giving a lecture on a topic. For longer lectures this could mean a large number of required permissions, and the permissions might be granted just for a single occasion. Journals might not care much about using a single figure in a departmental seminar, but it definitely becomes an issue when the lecture is distributed electronically, e.g. as free <a href="https://web.archive.org/web/20120611105124/http://ocw.mit.edu/OcwWeb/web/home/home/index.htm">OpenCourseWare</a> publication of teaching material. Some journals provide Powerpoint slides for the tables and figures and explicitly permit the educational noncommercial use. In my experience most researchers aren't aware that they are using copyrighted material in their slides, and I rarely see the required copyright attributions.</p><h3 id="added-services">Added services</h3><p>This category has great potential, but is currently not yet that relevant in my daily work. Open Access to fulltext articles allows things that aren't possible or much more complicated with subscription-based access. This includes fulltext searches (to find information not in the title, abstract or keywords), semantically enhanced articles, and article-level metrics (<a href="https://web.archive.org/web/20120611105124/http://network.nature.com/people/mfenner/blog/2009/08/15/plos-one-interview-with-peter-binfield">recently introduced by PLoS</a>).</p><h3 id="summary">Summary</h3><p>Researchers at large research institutions often have institutional access to most relevant papers. They are often not aware of the restrictions imposed upon them by the copyright of papers retained by most subscription-based journals. Open Access papers not only are freely accessible, but allow the uncomplicated redistribution and reuse for research and teaching, as well as innovative ways to find interesting research.</p><p>The perspective of the researcher as a paper author is stuff for another blog post…</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Thoughts on the PubMed Redesign]]></title>
            <link>https://blog.martinfenner.org/posts/thoughts-on-the-pubmed-redesign</link>
            <guid>666d847f-0867-4a8e-8d20-d1080235b2b9</guid>
            <pubDate>Sun, 11 Oct 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[It was Anna Kushnir
[https://web.archive.org/web/20120611025631/http://network.nature.com/people/U2929A0EA/profile] 
who started it all. Frustrated with the limitations of PubMed when finishing her
PhD thesis, she wrote a blog post in March 2008 (I Am Not Yelling. Not Out Loud.
[https://web.archive.org/web/20120611025631/http://network.nature.com/people/U2929A0EA/blog/2008/03/22/i-am-not-yelling-not-out-loud]
) about her experience. The blog post created quite a stir in the blogosphere,
especial]]></description>
            <content:encoded><![CDATA[<p>It was <a href="https://web.archive.org/web/20120611025631/http://network.nature.com/people/U2929A0EA/profile">Anna Kushnir</a> who started it all. Frustrated with the limitations of PubMed when finishing her PhD thesis, she wrote a blog post in March 2008 (<a href="https://web.archive.org/web/20120611025631/http://network.nature.com/people/U2929A0EA/blog/2008/03/22/i-am-not-yelling-not-out-loud">I Am Not Yelling. Not Out Loud.</a>) about her experience. The blog post created quite a stir in the blogosphere, especially among <a href="https://web.archive.org/web/20120611025631/http://mbanks.typepad.com/my_weblog/2008/04/the-anna-kushni.html">science librarians</a>. At the heart of the controversy was Anna's complaint that PubMed is too complicated to use, and that some science librarians felt PubMed simply is complicated and that users such as Anna should take better advantage of the resources available to better use PubMed. David Lipman, director of the <a href="https://web.archive.org/web/20120611025631/http://www.ncbi.nlm.nih.gov/">NCBI</a> and responsible for PubMed, said:</p><blockquote>Although the current engine works well for some users and some queries, I understand Anna's frustration and we are in the midst of a number of changes that will make PubMed work better for her and many other users.</blockquote><p>In May 2009 a PubMed redesign <a href="https://web.archive.org/web/20120611025631/http://www.nlm.nih.gov/pubs/techbull/mj09/ppt/sunrise_gillikin/sunrise_2009_gillikin.html">was shown</a> at the 2009 Annual Meeting of the Medical Library Association (MLA) in Honolulu, and the presentation explains a lot of the ideas behind the redesign. On September 30 the redesigned PubMed <a href="https://web.archive.org/web/20120611025631/http://www.nlm.nih.gov/pubs/techbull/so09/so09_pm_redesign.html">was unveiled to the public</a>, and as early as next week it will become the default PubMed web interface.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611025631im_/http://www.ncbi.nlm.nih.gov/portal/portalrc.fcgi/35308/img/35053" class="kg-image" alt></figure><p>Several science bloggers have already written about the PubMed redesign, including a <a href="https://web.archive.org/web/20120611025631/http://laikaspoetnik.wordpress.com/2009/10/01/pubmed&amp;Acirc;&amp;reg;-redesign-is-here-to-try/">very detailed blog post</a> by Jacqueline Limpens.</p><h3 id="what-is-pubmed">What is PubMed?</h3><p>MEDLINE is a database of more than 19 million citations for biomedical articles, hosted by the U.S. National Library of Medicine. <a href="https://web.archive.org/web/20120611025631/http://www.pubmed.gov/">PubMed</a> is the freely available Web interface to that database. Not only is the content of PubMed available from other databases (e.g. <a href="https://web.archive.org/web/20120611025631/http://www.scopus.com/">Scopus</a> or <a href="https://web.archive.org/web/20120611025631/http://thomsonreuters.com/products_services/science/science_products/scholarly_research_analysis/research_discovery/web_of_science">Web of Science</a>), but PubMed can be searched not only via the Web interface, but also from within other applications, e.g. a reference manager such as <a href="https://web.archive.org/web/20120611025631/http://www.endnote.com/">Endnote</a> or <a href="https://web.archive.org/web/20120611025631/http://mekentosj.com/papers">Papers</a>. And PubMed doesn't cover all scientific journals, many disciplines (e.g. physics, social sciences) aren't included at all. In other words, the Pubmed web interface is not the only way to find biomedical articles, and in fact will not find literature not related to the life sciences. But the PubMed web interface is probably by far the most popular way to search for biomedical literature.</p><h3 id="what-is-the-target-audience">What is the target audience?</h3><p>The PubMed website is intended for at least 5 different audiences:</p><ul><li>science librarian</li><li>researcher in the life sciences</li><li>clinician</li><li>patient or patient relative</li><li>teacher, high school student, journalist and anybody else interested in life sciences research</li></ul><p>Before PubMed <a href="https://web.archive.org/web/20120611025631/http://www.nlm.nih.gov/archive//20050113/news/press_releases/free_medline.html">was announced in June 1997 by the U.S. vice president Al Gore</a> as free web-based access to the MEDLINE database, most users were librarians, plus of a small group of academics with paid access (remember <a href="https://web.archive.org/web/20120611025631/http://www.annals.org/cgi/content/abstract/105/2/321">Grateful Med</a> ?). Now we have a number of target audiences with different experience in literature search strategies and different intentions:</p><ul><li>librarian vs. academic vs. the general public</li><li>basic life sciences research vs. clinical research</li></ul><p>As PubMed is the most popular but not the only interface to the MEDLINE database, the primary target audience will not be a librarian, but someone with less experience in searching the biomedical literature (and less time). I would lump academics together with the general public here, and think that the typical PubMed search should be as simple as the typical Google search. Everything much more complicated than a simple input box should be moved to an <em><em>advanced search options</em></em> page, or should be done via a different interface to the MEDLINE database.</p><p>Searching for clinical literature is very different from searching for basic science research. Here a search is often done to help in the decision making for a particular patient, and <a href="https://web.archive.org/web/20120611025631/http://en.wikipedia.org/wiki/Evidence-based_medicine">evidence-based medicine</a> is used to find the most relevant scientific literature (with meta-analyses and randomized controlled trials providing the best evidence).</p><p>Searching for basic science literature has very different goals. It is either finding the needle in the haystack, e.g. you want to find all the published literature on the <a href="https://web.archive.org/web/20120611025631/http://dx.doi.org/10.1126/science.1176325">C3PO</a> gene, or you want to find a review article as an overview over a particular field. But basic science review articles don't have the rigourous tools available to evaluate clinical research mentioned above, and here review articles only differ in the personal perspective of the reviewers and completeness and actuality of the primary literature that was covered. Searching for basic science literature should also be tightly integrated with the other databases at the NCBI. This is done via the <a href="https://web.archive.org/web/20120611025631/http://www.ncbi.nlm.nih.gov/sites/gquery">Entrez Search Page</a>, so that a search for <em><em>C3PO</em></em> also links to the organisms it was described in, e.g. this one:</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611025631im_/http://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Tribolium.castaneum.jpg/91px-Tribolium.castaneum.jpg" class="kg-image" alt></figure><p>I don't think that a search interface to MEDLINE can be good at both the clinical and basic science literature. The current PubMed is much closer to the latter, so I think that the primary target audience for PubMed is the <em><em>academic or general public interest in basic life sciences research</em></em>. A good search interface for the clinical literature would be something very different and has to include both databases of evidence-based evaluations (particularly from the <a href="https://web.archive.org/web/20120611025631/http://www.cochrane.org/">Cochrane Collaboration</a>) and from ongoing and completed clinical trials (particularly <a href="https://web.archive.org/web/20120611025631/http://clinicaltrials.gov/">Clinicaltrials.gov</a>, just like PubMed also hosted at the NIH).</p><h3 id="where-is-web-2-0">Where is Web 2.0?</h3><p>A PubMed redesign in 2009 can't be complete without looking at what Web 2.0 has to offer. This means that users should be offered a personal PubMed account that links to their libraries for fulltext articles, stores common searches, creates RSS outputs, allows sharing of search results with other users, publishes a link to an interesting article on Twitter, and possibly other enhancements (e.g. a public profile page of all your PubMed articles, but that wouldn't work without <a href="https://web.archive.org/web/20120611025631/http://themindwobbles.wordpress.com/2009/08/22/breakout-3-author-identity-creating-a-new-kind-of-reputation-online/">author identifiers</a>). This also means a clean design, use of Javascript/AJAX for the user interface, a version for mobile users (particularly iPhone), and frequent small updates instead of a big desgin change every 2-3 years.</p><h3 id="and-how-is-the-redesigned-pubmed">And how is the redesigned PubMed?</h3><p>After this rather long introduction, what do I like about the PubMed Redesign?</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611025631im_/http://gallery.me.com/mfenner/100079/pubmed/web.jpg" class="kg-image" alt></figure><h3 id="like">Like</h3><ul><li>Clean, uncluttered design</li><li>Saving a search as RSS is faster and more obvious. I hope that this will make many more people use RSS than email alerts for their regular searches (<a href="https://web.archive.org/web/20120611025631/http://network.nature.com/people/mfenner/blog/2009/06/21/recipe-receiving-journal-table-of-contents-automatically">why I like RSS</a>)</li><li>Auto suggest: some of the most popular PubMed searches will be displayed based on the terms entered</li><li>Some use of Javascript/AJAX</li></ul><h3 id="dislike">Dislike</h3><ul><li>The <a href="https://web.archive.org/web/20120611025631/http://www.doi.org/">DOI</a> (the best unique identifier for a paper and the easiest way to link to the full-text article) is still not displayed in the standard abstract view (you find the DOI in the Medline and XML views)</li><li>Small design flaw: no easy way to go back from advanced search to basic search</li><li>Layout is now different from MyNCBI and the other NCBI databases (maybe this is work in progress)</li><li>(As far as I know) no version for mobile users</li><li>No send to CiteULike/Connotea/Twitter/FriendFeed, etc. buttons (popular with many journals)</li></ul><p>The redesign will make it easier for inexperienced users to do quick searches (as mentioned above, probably the target audience). Experienced librarians might like the redesign less, as advanced searches have not become easier. But overall the changes are minor. My biggest complaint is the lack of DOI integration. A wasted opportunity. And – as mentioned above – I think we need a different MEDLINE interface for searching the clinical medicine literature.</p><p>How do you like the redesign? Jacqueline Limpens is <a href="https://web.archive.org/web/20120611025631/http://laikaspoetnik.wordpress.com/2009/10/01/pubmed&amp;Acirc;&amp;reg;-redesign-is-here-to-try/">doing a poll on her blog</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Conference Blogging: Interview with Alex Knoll]]></title>
            <link>https://blog.martinfenner.org/posts/conference-blogging-interview-with-alex-knoll</link>
            <guid>cf6d3950-b99d-47b3-9aec-f1173c5950b6</guid>
            <pubDate>Thu, 01 Oct 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[Blogging is a great way to report from a scientific conference
[https://web.archive.org/web/20120611105344/http://www.nature.com/news/2009/090624/full/4591050a.html]
. This could be done either with regular blog posts written in the evening or
after the conference, and/or live-blogging using tools such as Friendfeed or
Twitter. One or more blogging scientists can not only add a unique perspective
to the reports about a conference, but for smaller conferences blogging might be
the only way to lea]]></description>
            <content:encoded><![CDATA[<p>Blogging is <a href="https://web.archive.org/web/20120611105344/http://www.nature.com/news/2009/090624/full/4591050a.html">a great way to report from a scientific conference</a>. This could be done either with regular blog posts written in the evening or after the conference, and/or live-blogging using tools such as Friendfeed or Twitter. One or more blogging scientists can not only add a unique perspective to the reports about a conference, but for smaller conferences blogging might be the only way to learn more about a conference you were unable to attend in person.</p><p>Conference blogging (particularly live-blogging) basically requires four things:</p><ul><li>a wireless network,</li><li>a computer or mobile phone with a full battery,</li><li>a hashtag (and other tools to find the conference blog posts), and</li><li>a blogging policy by the conference organizers.</li></ul><p>Wireless networks are now commonplace, but enough battery power (or power outlets that conference participants can use) can be difficult. A hashtag such as <em><em>#solo09</em></em> for <a href="https://web.archive.org/web/20120611105344/http://www.scienceonlinelondon.org/">Science Online London</a> is essential for live-blogging using Twitter.</p><p>The big problem is the blogging policy, or rather that there usually is a policy only for traditional media, but not for blogging. The blogging from the Cold Spring Harbor <em><em>Biology of Genomes</em></em> meeting in May by Daniel MacArthur <a href="https://web.archive.org/web/20120611105344/http://scienceblogs.com/geneticfuture/2009/06/on_the_challenges_of_conferenc.php">started a very helpful discussion about blogging policies</a>. It is impossible to write anything specific about a conference – and that's the stuff that is most interesting – without a permission from the conference organizer and speaker. This is best done before the conference has started. A July <em><em>Nature</em></em> <a href="https://web.archive.org/web/20120611105344/http://www.nature.com/nature/journal/v460/n7252/full/460152a.html">editorial</a> argues that an opt-out policy, where everything can be blogged about unless the speaker or poster presenter specifically says so, is a reasonable alternative.</p><p>The organizers of the <a href="https://web.archive.org/web/20120611105344/http://www.genetics2009.de/">Annual Meeting of the German Genetics Society</a> that took place two weeks ago in Cologne did this right. Not only did they invite <a href="https://web.archive.org/web/20120611105344/http://network.nature.com/people/alexander-knoll/profile">Alex Knoll</a> to become the official conference blogger, but they also put up a prominent link to his blog posts on the conference homepage, and they asked every speaker before the conference whether Alex would be allowed to blog about their talks. Because his blog on scienceblogs.de (<a href="https://web.archive.org/web/20120611105344/http://www.scienceblogs.de/alles-was-lebt">Alles was lebt</a>) is in German, he decided to put up his blog posts here. I've asked him a few questions about this experience.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611105344im_/http://gallery.me.com/mfenner/100079/alex_knoll/web.jpg" class="kg-image" alt></figure><h3 id="1-did-you-have-fun-being-the-official-blogger-for-the-german-genetics-society-meeting">1. Did you have fun being the official blogger for the <a href="https://web.archive.org/web/20120611105344/http://www.genetics2009.de/">German Genetics Society Meeting</a>?</h3><p>This conference blogging job was a first for me in many ways. I usually blog in German, so I wasn't sure if I would be able to bring more than my dry, scientific English. I also knew beforehand that there would be no theme, that the meeting was a general one. I would have at least to give the impression of having understood the basics of the talks. There would be no flitting about from talk to talk, I wanted to get whole sessions without interruption.</p><p>But on the other hand, I also got to know lots of people, many more than I would have as a lowly PhD student. I attended a conference I almost certainly would not have without the invitation to come and blog.</p><p>And, as any other (science)blogger will tell you, blogging is a labour of love (don't stab me in the back now!). So yes, I had a great time!</p><h3 id="2-blogging-about-the-conference-must-have-been-a-lot-of-work-">2. Blogging about the conference must have been a lot of work!</h3><p>About as much as I expected. I was frantically typing away at my little netbook keyboard during the talks to take notes, and used any spare time to put together the posts. So I did not have as easy a time as regular conference attendees. No problem, I came to do a job!</p><h3 id="3-did-you-meet-any-other-science-bloggers-at-the-conference">3. Did you meet any other science bloggers at the conference?</h3><p>As far as I'm aware, I was the only blogger attending, and also the only one tweeting from the sessions (no worries, no unpublished data got out that route).</p><h3 id="4-what-was-the-feedback-from-the-speakers-what-was-your-experience-getting-permissions-from-speakers-to-blog-about-their-sessions">4. What was the feedback from the speakers? What was your experience getting permissions from speakers to blog about their sessions?</h3><p>I got the whole range. From the really open “Go ahead! Write what you want, put it online. I'll talk about some unpublished stuff as well, but I don't mind” to some who are not interested in getting their work out into a blog at all. Great news for the conference blogging crowd: the balance was tipped more to the pro side! Most of the speakers came out somewhere in between those two sides, probably being a bit cautious about that whole strange blogging stuff. But I got mostly positive feedback from them, and I believe the next blogger will have an easier time when blogging about their talks!</p><h3 id="5-what-tips-would-you-give-a-conference-organizer-who-wants-to-promote-blogging">5. What tips would you give a conference organizer who wants to promote blogging?</h3><p>They should make clear from the start if blogging about the talks is generally OK. That doesn't mean all of the speakers have to allow blogging about their talk, but an official position will help everyone involved. You also don't need to have an <em><em>official</em></em> blogger, but especially at smaller meetings asking someone to blog beforehand is probably the only chance to get a blogger there at all.</p><p>I also have advice for speakers: Start your talk by telling your audience if blogging about it is OK! If a part of your talk is unpublished, tell them that as well. Or put an icon on your slides to indicate which is good to blog about, for example as Daniel MacArthur from the Genetic Future blog <a href="https://web.archive.org/web/20120611105344/http://scienceblogs.com/geneticfuture/2009/07/conference_blogging_icons_for.php">has proposed</a>. If bloggers know beforehand if and what part of the talk is good to go, they will be more willing to take notes in earnest!</p><p>Now that my guest posting here at Martin's blog comes to an end, I would like to leave you with one of the last impressions, a rather lucky shot of Cologne Cathedral I took while leaving. Many thanks to Martin for hosting this conference blog!</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20120611105344im_/http://farm4.static.flickr.com/3492/3969315341_4f8c590711.jpg" class="kg-image" alt></figure>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Thoughts on the Science Online London Conference]]></title>
            <link>https://blog.martinfenner.org/posts/thoughts-on-the-science-online-london-conference</link>
            <guid>b4848bb9-eaac-42eb-a5a1-f93dc22db6a9</guid>
            <pubDate>Sun, 23 Aug 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[We did it. Yesterday was Science Online London
[https://web.archive.org/web/20151002164342/http://www.scienceonlinelondon.org/]
, a conference about the online communication of science that took place at the 
Royal Institution
[https://web.archive.org/web/20151002164342/http://www.rigb.org/]. I hope that
everybody that attended had a great time. You can see a lot of conference
coverage at Twitter (hashtag #solo09) and in the FriendFeed Science Online
London group
[https://web.archive.org/web/201]]></description>
            <content:encoded><![CDATA[<p>We did it. Yesterday was <a href="https://web.archive.org/web/20151002164342/http://www.scienceonlinelondon.org/">Science Online London</a>, a conference about the online communication of science that took place at the <a href="https://web.archive.org/web/20151002164342/http://www.rigb.org/">Royal Institution</a>. I hope that everybody that attended had a great time. You can see a lot of conference coverage at Twitter (hashtag #solo09) and in the <a href="https://web.archive.org/web/20151002164342/http://www.friendfeed.com/solondon">FriendFeed Science Online London group</a>. And don't forget the <a href="https://web.archive.org/web/20151002164342/http://www.flickr.com/groups/solo09/">Flick group</a> with pictures such as this one by Jacqueline Spoetnik:</p><p>Several blog posts have already been written, most notably by Allyson Lister who posted their detailed reports on her <a href="https://web.archive.org/web/20151002164342/http://themindwobbles.wordpress.com/">the mind wobbles</a> blog literally minutes after the sessions had ended. Nico Adams has also already posted blog posts about a number of session on his blog <a href="https://web.archive.org/web/20151002164342/http://wwmm.ch.cam.ac.uk/blogs/adams/">Staudinger's Semantic Molecules</a>.</p><p><em><em>London Pub and Science Tour (Matt Brown)</em></em><br>* <a href="https://web.archive.org/web/20151002164342/http://wwmm.ch.cam.ac.uk/blogs/adams/?p=294">Science Online London 2009! – The Prequel</a> (Staudinger's Semantic Molecules)<br>* <a href="https://web.archive.org/web/20151002164342/http://network.nature.com/hubs/london/blog/2009/08/23/science-online-london-2009-the-beer-and-stuff">Science Online London: The Beer and Stuff</a> (Nature Network London Blog)</p><p><em><em>FringeFrivolous Preconference Event (Jenny Rohn)</em></em><br>* <a href="https://web.archive.org/web/20151002164342/http://wwmm.ch.cam.ac.uk/blogs/murrayrust/?p=2220">Galaxy Zoo at Mendeley</a> (petermr's blog)<br>* <a href="https://web.archive.org/web/20151002164342/http://www.mendeley.com/blog/academic-life/fringe-frivolous-and-science-online-london-2009-pictures/">Fringe Frivolous and Science Online 2009 Pictures</a> (Mendeley Blog)<br>* <a href="https://web.archive.org/web/20151002164342/http://network.nature.com/people/stuffysour/blog/2009/08/24/its-a-control-thing-dummy">It's a control thing, dummy</a> (Science behind the scenes)<br>* <a href="https://web.archive.org/web/20151002164342/http://network.nature.com/people/eva/blog/2009/08/24/lolcats-and-labrats">LOLcats and labrats</a> (Expression Patterns)<br>* <a href="https://web.archive.org/web/20151002164342/http://network.nature.com/people/UE19877E8/blog/2009/08/24/in-which-i-rest-on-my-laurels">In which I rest on my laurels</a> (Mind the Gap)<br>* <a href="https://web.archive.org/web/20151002164342/http://network.nature.com/people/rpg/blog/2009/08/26/on-the-roof">On the roof</a> (The Scientist)</p><p><em><em>Legal and Ethical Aspects of Science Blogging (Petra Boynton, David Allen Green (“Jack of Kent”))</em></em><br>* <a href="https://web.archive.org/web/20151002164342/http://jackofkent.blogspot.com/2009/08/amongst-science-bloggers.html">Among the science bloggers</a> (Jack of Kent)</p><p><em><em>Blogging for impact (Dave Munger, Daniel MacArthur)</em></em><br>* <a href="https://web.archive.org/web/20151002164342/http://themindwobbles.wordpress.com/2009/08/22/blogging-for-impact-science-online-london-2009/">Blogging for Impact</a> (the mind wobbles)<br>* <a href="https://web.archive.org/web/20151002164342/http://wwmm.ch.cam.ac.uk/blogs/adams/?p=312">Science In the Open 2009 London: Blogging for Impact</a> (Staudinger's Semantic Molecules)<br>* <a href="https://web.archive.org/web/20151002164342/http://seedmagazine.com/content/article/telepresent_at_the_future/">Present at the Future</a> (Seed Magazine)</p><p><em><em>What is a scientific paper? (Lee-Ann Coleman, Katharine Barnes, Enrico Balli, Theo Bloom)</em></em><br>* <a href="https://web.archive.org/web/20151002164342/http://themindwobbles.wordpress.com/2009/08/22/breakout-1-what-is-a-scientific-paper/">Breakout 1: What is a scientific paper?</a> (the mind wobbles)<br>* <a href="https://web.archive.org/web/20151002164342/http://wwmm.ch.cam.ac.uk/blogs/adams/?p=317">Notes from Science Online: What is a Scientific Paper?</a> (Staudinger's Semantic Molecules)<br>* <a href="https://web.archive.org/web/20151002164342/http://blog.openwetware.org/scienceintheopen/2009/08/23/the-future-of-the-paperdoes-it-have-one-and-the-answer-is-yes/">The Future of the Paper…Does it have one?</a> (Science in the Open)<br>* <a href="https://web.archive.org/web/20151002164342/http://www.sciencebase.com/science-blog/what-is-a-scientific-paper-solo09.html">What is a scientific paper?</a> (Sciencebase)<br>* <a href="https://web.archive.org/web/20151002164342/http://cotch.net/blog/20090825_0007">What is the scientific paper? 1: Observations</a> (Cotch.net)</p><p><em><em>Breakout 2: Online communication of science by institutions and organizations (Ed Yong, Henry Scowcroft, Paolo Viscardi, Simon Frantz)</em></em><br>* <a href="https://web.archive.org/web/20151002164342/http://wwmm.ch.cam.ac.uk/blogs/adams/?p=315">Notes from Science Online 2009: How the Web enables anyone to be a Scientist</a> (Staudinger's Semantic Molecules)</p><p><em><em>Cat herding: The challenges and rewards of managing online scientific communities (Arikia Millikan, Corie Lok, Ijad Madisch)</em></em><br>* <a href="https://web.archive.org/web/20151002164342/http://themindwobbles.wordpress.com/2009/08/22/cat-herding-the-challenges-and-rewards/">Cat herding: the challenges and rewards of managing online science communities</a> (the mind wobbles)<br>* <a href="https://web.archive.org/web/20151002164342/http://wwmm.ch.cam.ac.uk/blogs/adams/?p=316">Notes From Science Online â€“ Cat Herding</a> (Staudinger's Semantic Molecules)</p><p><em><em>Breakout 3: Author identity</em>:<em> Creating a new kind of reputation online (Duncan Hull, Geoffrey Bilder, Michael Habib, Reynold Guida)</em></em><br>* <a href="https://web.archive.org/web/20151002164342/http://themindwobbles.wordpress.com/2009/08/22/breakout-3-author-identity-creating-a-new-kind-of-reputation-online/">Breakout 3: author identity – creating a new kind of reputation online</a> (the mind wobbles)</p><p><em><em>Breakout 4: Citizen science</em>: <em>How the web enables anyone to be a scientist (Arfon Smith, Mike Peel)</em></em></p><p><em><em>Real-time statistics in science (Victor Henning, Richard Grant, Virginia Barbour)</em></em><br>* <a href="https://web.archive.org/web/20151002164342/http://themindwobbles.wordpress.com/2009/08/22/real-time-statistics-in-science/">Real-time statistics in science</a> (the mind wobbles)<br>* <a href="https://web.archive.org/web/20151002164342/http://wwmm.ch.cam.ac.uk/blogs/adams/?p=314">Notes from Science Online â€“ Real time statistics and new impact metrics in science</a> (Staudinger's Semantic Molecules)</p><p><em><em>Google Wave: Just another ripple or science communication tsunami? (Cameron Neylon, Chris Thorpe, Ian Mulvany)</em></em><br>* <a href="https://web.archive.org/web/20151002164342/http://themindwobbles.wordpress.com/2009/08/22/google-wave-just-another-ripple-or-science-communication-tsunami/">Google Wave: just another ripple or science communication tsunami</a> (the mind wobbles)<br>* <a href="https://web.archive.org/web/20151002164342/http://blog.openwetware.org/scienceintheopen/2009/08/23/reflecting-on-a-wave-the-demo-at-science-online-london-2009/">Reflecting on a Wave: The demo at Science Online London 2009</a> (Science in the Open)<br>* <a href="https://web.archive.org/web/20151002164342/http://blogs.nature.com/wp/nascent/2009/08/riding_a_wave_of_science.html">Riding a Wave of Science</a> (Nascent)<br>* <a href="https://web.archive.org/web/20151002164342/http://scienceblogs.com/highlyallochthonous/2009/08/surfing_the_google_wave.php">Surfing the Google Wave</a> (Highly Allochtonous)</p><p><em><em>Far out: Speculations on science communication 50 years from now (John Gilbey)</em></em><br>* <a href="https://web.archive.org/web/20151002164342/http://themindwobbles.wordpress.com/2009/08/22/far-out-speculations-on-science-communication-50-years-from-now/">Far out: speculation on science communication 50 years from now</a> (the mind wobbles)<br>* <a href="https://web.archive.org/web/20151002164342/http://wwmm.ch.cam.ac.uk/blogs/adams/?p=329">Notes on Science Online – Science Communication 50 years from now</a> (Staudinger's Semantic Molecules)</p><p><em><em>General Posts</em></em><br>* <a href="https://web.archive.org/web/20151002164342/http://www.possibilitiesendless.com/?p=52">Science Online London 2009</a> (Endless Possibilities)<br>* <a href="https://web.archive.org/web/20151002164342/http://colinsbeautypages.co.uk/science-online-london-2009/">Science Online London 2009</a> (Colin's Beauty Pages)<br>* <a href="https://web.archive.org/web/20151002164342/http://network.nature.com/people/eva/blog/2009/08/23/coffee-break">Coffee Break</a> (Expression Pattern)<br>* <a href="https://web.archive.org/web/20151002164342/http://www.ethicalpalaeontologist.com/2009/08/science-online-aftermath.html">Science Online – Aftermath</a> (The Ethical Palaeontologist)<br>* <a href="https://web.archive.org/web/20151002164342/http://cotch.net/blog/20090823_2239">Science Online, London '09</a> (Cotch.net)<br>* <a href="https://web.archive.org/web/20151002164342/http://scienceblogs.com/highlyallochthonous/2009/08/science_online_-">Science Online – the London Edition</a><em><em>the</em></em>london_ed.php (Highly Allochthonous)<br>* <a href="https://web.archive.org/web/20151002164342/http://carmenego.wordpress.com/2009/08/23/science-schmooz-a-thon/">Science Schmooz-a-thon</a> (Carmen Gets Around)<br>* <a href="https://web.archive.org/web/20151002164342/http://www.possibilitiesendless.com/?p=54">Online science communication – a comparison</a> (Endless Possibilities)<br>* <a href="https://web.archive.org/web/20151002164342/http://www.russet.org.uk/blog/2009/08/science-online-london-2009/">Science Online London 2009</a> (Exercise in Irrelevance)<br>* <a href="https://web.archive.org/web/20151002164342/http://www.axiope.com/community/viewtopic.php?f=8&amp;t=1021&amp;start=0">Science Online London 2009 compared to 2008</a> (eCAT community)<br>* <a href="https://web.archive.org/web/20151002164342/http://themindwobbles.wordpress.com/2009/08/24/science-online-london-09-thoughts-rather-than-transcript/">Science Online London 09: Thoughts, not Transcript</a> (the mind boggles)<br>* <a href="https://web.archive.org/web/20151002164342/http://duncan.hull.name/2009/08/24/solo09/">I bet you think this blog is about you, donâ€™t you?</a> (O'Really?)<br>* <a href="https://web.archive.org/web/20151002164342/http://network.nature.com/people/scurry/blog/2009/08/23/what-a-difference-a-year-makes">What a difference a year makes</a> (Reciprocal space)<br>* <a href="https://web.archive.org/web/20151002164342/http://network.nature.com/people/etchevers/blog/2009/08/26/idle-vacation-thoughts-on-science-online-london-2009">Idle vacation thoughts on Science Online London 2009</a> (A Developing Passion)<br>* <a href="https://web.archive.org/web/20151002164342/http://ukwebfocus.wordpress.com/2009/08/26/the-back-channels-for-the-science-online-2009-conference/">The Back Channels of the Science Online 2009 Conference</a> (UK Web Focus)<br>* <a href="https://web.archive.org/web/20151002164342/http://scienceoftheinvisible.blogspot.com/2009/08/unpacking-solo09.html">Unpacking Solo09</a> (Science of the Invisible)</p><p>As one of the conference organizers I'm obviously biased as to how to judge the success of the conference. But I think that organizing a conference about the online communication of science is as much about how to do things, as it is about finding the right session topics. And as Science Online London is only in its second year (and the sister <a href="https://web.archive.org/web/20151002164342/http://www.scienceonline09.com/">ScienceOnline</a> in North Carolina in its third year), there are still a lot of things we can do better.</p><h3 id="what-i-liked">What I liked</h3><p><em><em>No parallel sessions</em></em><br>Although we did have two slots with two parallel sessions each, all the other sessions were in the Faraday Theatre. I find that I always miss some great sessions in conferences with many parallel sessions. More importantly this format makes sure that everybody went to the same conference and discusses the same things. I like this because it builds a sense of community around the conference.</p><p><em><em>No unconference sessions</em></em><br>Deciding on the session topics and speakers at the beginning of the conference is a concept that worked well for similar events in the past (e.g. <a href="https://web.archive.org/web/20151002164342/http://network.nature.com/people/mfenner/blog/2009/07/10/i-was-at-scibarcamp-palo-alto">SciBarCamp Palo Alto</a>, and in part <a href="https://web.archive.org/web/20151002164342/http://www.nature.com/natureconferences/sciblog2008/index.html">Science Blogging London 2008</a>). We decided to not use this format, which not only made the day of the conference much less stressful for the organizers, but also allowed us to have invite some interesting speakers that otherwise might not have come. This format also made it easier to have different topics and speakers from last year's conference (only 3 out of the 29 speakers also spoke last year).</p><p><em><em>Live streaming of audio and video</em></em><br>We did stream audio and video of almost all sessions to <a href="https://web.archive.org/web/20151002164342/http://www.scienceonlinelondon.org/second-life.php">Second Life</a>. Exceptions were the session on legal and ethical aspects of blogging (which we decided not to record because of the risk that some statements could be taken as legal advice), and the breakout sessions (where we could only record audio). One speaker (Dave Munger) even gave his presentation through Second Life.</p><p>Because the Second Life users could give feedback through Second Life, Twitter or FriendFeed, this was really a virtual conference. Live video streaming (using Second Life or other technologies) should really become the norm for this kind of conference.</p><p><em><em>A perfect location</em></em><br>The Royal Institution was really a perfect place for the conference. You could argue about the pink color of the seats, but the <a href="https://web.archive.org/web/20151002164342/http://www.rigb.org/contentControl?action=displayContent&amp;id=00000001054">Faraday Theatre</a> has just the right size and a great of history of communicating science.</p><h3 id="what-i-didn-t-like">What I didn't like</h3><p><em><em>Not enough time in the sessions</em></em><br>We decided to go with 45 minute long sessions, and this turned out to be too short. I would have liked to have a 30 minute discussion in most sessions (some sessions worked well with shorter discussion time, e.g. <em><em>Legal and Ethical Aspects of Science Blogging</em></em>). Although there will always be session that require a different format, next time I would force speakers to limit their introduction to 15 minutes. And I would try to make the sessions 60 minutes long.</p><p><em><em>Not enough time between sessions</em></em><br>The best part of a conference is often the discussions we have between sessions. For this we should have scheduled more time, e.g. by making the coffee breaks 45 minutes long and by having a conference dinner at the beginning of the conference. But the <a href="https://web.archive.org/web/20151002164342/http://www.scienceonlinelondon.org/blog/2009/07/20/announcing-social-events-for-thursday-friday/">social events on Thursday and Friday</a> and the <a href="https://web.archive.org/web/20151002164342/http://network.nature.com/people/UE19877E8/blog/2009/08/06/in-which-we-change-venue-fringefrivolous-blogging-unconference-2009">FringeFrivolous</a> unconference on Friday were not only interesting events, but also great for networking. Doing a conference on a single day is just too short and I would make a repeat conference a two day event. With more time I would also really like to do a brief introduction of every participant (Ã  la <a href="https://web.archive.org/web/20151002164342/http://www.nature.com/scifoo/index.html">SciFoo</a>).</p><p><em><em>Do more moderation</em></em><br>Conferences have a tendency of always having the same people saying the same things. And discussions are sometimes more about people saying something that is dear to their heart rather than asking a question or trying to understand the other side. More moderation could help here to make the discussions more unexpected and productive.</p><p><em><em>That iPhone that just didn't stop ringing</em></em><br>Unfortunately I just couldn't pay full attention to the last part of the discussion about real-time statistics. Despite <a href="https://web.archive.org/web/20151002164342/https://twitter.com/#search?q=%23solo09%20phone">many pleas on Twitter</a>, that iPhone must have ben ringing for at least 5 minutes.</p><p><em><em>This blog will go on summer vacation between August 27-September 20. The author will be in the Southwestern United States and hopes to have little or no internet coverage. During that time fellow German science blogger <a href="https://web.archive.org/web/20151002164342/http://network.nature.com/people/alexander-knoll/profile">Alexander Knoll</a> will write a few guest posts from the Annual Conference of the German <a href="https://web.archive.org/web/20151002164342/http://www.gfgenetik.de/eng/tagungen/tagungen.php">Gesellschaft </a></em><a href="https://web.archive.org/web/20151002164342/http://www.gfgenetik.de/eng/tagungen/tagungen.php">f</a>or <em><a href="https://web.archive.org/web/20151002164342/http://www.gfgenetik.de/eng/tagungen/tagungen.php">Geneti</a></em><a href="https://web.archive.org/web/20151002164342/http://www.gfgenetik.de/eng/tagungen/tagungen.php">c</a>s<em> September 16-19.</em></em></p><p><em><em>Update 08/26/09: included more blog posts about Science Online London</em></em></p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20151002164342im_/http://www.linkwithin.com/pixel.png" class="kg-image" alt="Related Posts Plugin for WordPress, Blogger..."></figure>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Streamosphere: Interview with Euan Adie]]></title>
            <link>https://blog.martinfenner.org/posts/streamosphere-interview-with-euan-adie</link>
            <guid>ddde66f5-d51b-42af-88f8-011bc02fd615</guid>
            <pubDate>Thu, 20 Aug 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[Euan Adie
[https://web.archive.org/web/20151003123050/http://network.nature.com/people/euan/profile] 
in June announced the Streamosphere
[https://web.archive.org/web/20151003123050/http://streamosphere.nature.com/] 
service on the Nascent blog: Welcome to the Streamosphere
[https://web.archive.org/web/20151003123050/http://blogs.nature.com/wp/nascent/2009/06/welcome_to_the_streamosphere.html]
. His simple explanation of what Streamosphere does: 

> Streamosphere lets you track scientific discus]]></description>
            <content:encoded><![CDATA[<p><a href="https://web.archive.org/web/20151003123050/http://network.nature.com/people/euan/profile">Euan Adie</a> in June announced the <a href="https://web.archive.org/web/20151003123050/http://streamosphere.nature.com/">Streamosphere</a> service on the Nascent blog: <a href="https://web.archive.org/web/20151003123050/http://blogs.nature.com/wp/nascent/2009/06/welcome_to_the_streamosphere.html">Welcome to the Streamosphere</a>. His simple explanation of what Streamosphere does: </p><blockquote><em><em>Streamosphere lets you track scientific discussion on the web, in real time.</em></em> </blockquote><p>The Nascent blog post explains Streamosphere in more detail, <a href="https://web.archive.org/web/20151003123050/http://blogs.nature.com/wp/nascent/2009/07/streamosphere_update.html">in another post from July</a> Euan talks about some updates to the service. I've asked Euan a few questions to learn more about Streamosphere.</p><h3 id="1-can-you-explain-what-streamosphere-is-and-does">1. Can you explain what Streamosphere is and does?</h3><p>Streamosphere is an aggregator of scientific activity on the web. It tracks what scientists are talking about on Twitter, wikis, blogs, social bookmarking services, forums and other web 2.0 services.</p><h3 id="2-why-do-we-need-such-a-tool">2. Why do we need such a tool?</h3><p>I think scientific attention (any time you read an abstract, cite a paper or search for a particular gene in a database you're giving it attention) is valuable to aggregate. Traditionally scientists and publishers have measured attention with citation counts and to a lesser extent downloads but there's lots of other online activity – like blogging, commenting and bookmarking – that could be used in new metrics.</p><p>At the moment the relevant activity is</p><ul><li><em><em>distributed</em></em> people could be talking about a paper I'm interested in on Friendfeed, blogs, forums…</li><li><em><em>noisy</em></em> there are tens of thousands of research scientists on Twitter but most of the time they're posting pictures of lolcats</li><li><em><em>lacking provenance</em></em> it's not easy to know whose opinions to trust</li></ul><p>Streamosphere attempts to tackle these issues by aggregating activity from lots of different sites, removing spam, providing filters and disambiguating users on different services. If I'm interested in what other scientists think about a particular paper then I can search for it on Streamosphere. If I want to keep track of everything related to a particular field then I can do that too.</p><h3 id="3-what-is-the-difference-between-streamosphere-postgenomic-and-nature-com-blogs">3. What is the difference between Streamosphere, Postgenomic and Nature.com Blogs?</h3><p><a href="https://web.archive.org/web/20151003123050/http://www.postgenomic.com/">Postgenomic</a> and <a href="https://web.archive.org/web/20151003123050/http://blogs.nature.com/">Nature.com Blogs</a> only aggregate and analyze blog posts. Streamosphere gets a feed from Nature.com Blogs but covers other services too.</p><h3 id="4-what-is-the-difference-to-other-aggregators-such-as-friendfeed">4. What is the difference to other aggregators such as FriendFeed?</h3><p>The main difference is that you can't interact with content on Streamosphere – if you want to comment on a blog post or paper you have to visit it and leave your comment there.</p><p>Another would be that you don't follow people on Streamosphere, you'd follow topics.</p><h3 id="5-how-do-you-decide-whether-something-or-someone-is-related-to-science">5. How do you decide whether something or someone is related to science?</h3><p>If you bookmark or discuss things with <a href="https://web.archive.org/web/20151003123050/http://www.doi.org/">DOIs</a>, <a href="https://web.archive.org/web/20151003123050/http://arxiv.org/help/faq/whynostamp">arxiv IDs</a> or <a href="https://web.archive.org/web/20151003123050/http://blogs.library.ucla.edu/biomedical/2008/09/02/convert-a-pubmed-id-to-a-pubmedcentral-id/">PubMed IDs</a> then you're added to the database. At this point any of your activity that doesn't look scholarly doesn't get aggregated.</p><p>After that it's down to manual checks and whether or not you're following / being followed by other people in Streamosphere.</p><h3 id="6-can-we-follow-streamosphere-via-rss-or-twitter">6. Can we follow Streamosphere via RSS or Twitter?</h3><p>Not yet! I'm hoping to open up the data in Streamosphere via a public API that can output RSS and other formats but don't want to do it before the system is stable. There are also questions surrounding how some messages can be republished. I'm not sure if Streamosphere's API output can include the contents of any tweets, for example, as to receive streaming updates from Twitter you need to sign an agreement saying that you won't distribute them outside of your application (can't remember the exact wording).</p><p>The best way to track updates about Streamosphere is to watch <a href="https://web.archive.org/web/20151003123050/http://blogs.nature.com/wp/nascent/">Nascent</a>.</p><h3 id="7-what-are-your-responsibilities-at-nature-com">7. What are your responsibilities at Nature.com?</h3><p>I'm a product manager in the web publishing group. Generally speaking the areas I'm responsible for are aggregation, the Nature.com Blogs homepage and mobile platforms.</p><h3 id="8-what-did-you-do-before-working-for-nature-com">8. What did you do before working for Nature.com?</h3><p>I was a bioinformatician at the University of Edinburgh in the medical genetics unit, working on candidate disease gene prioritization.</p><p>Before that I'd co-founded and worked on a start-up that tried to extract entities and sentiment from forum posts. We brought in very little revenue for eighteen months, ran out of funding and went bust in 2001. It was an education…. 😉</p><h3 id="9-do-you-want-to-talk-about-future-plans-for-streamosphere">9. Do you want to talk about future plans for Streamosphere?</h3><p>Sure – the main thing I'm looking forward to is personalization – being able to log in and tell the system what you're interested in. From then on whenever you visit the site particularly relevant items will be highlighted.</p><p>The API is another key development… I'm really interested in seeing what other people can do with the data.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[PLoS One: Interview with Peter Binfield]]></title>
            <link>https://blog.martinfenner.org/posts/plos-one-interview-with-peter-binfield</link>
            <guid>02aa2efb-3245-450e-ad43-f0d1953a2f5e</guid>
            <pubDate>Sat, 15 Aug 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[At SciBar Camp Palo Alto last month
[https://web.archive.org/web/20151002113207/http://network.nature.com/people/mfenner/blog/2009/07/10/i-was-at-scibarcamp-palo-alto]
, Peter Binfield from PLoS ONE
[https://web.archive.org/web/20151002113207/http://www.plosone.org/] gave a very
interesting presentation on Article-level metrics from the PLoS perspective
[https://web.archive.org/web/20151002113207/http://friendfeed.com/scibarcamp/3577ed2e/peter-binfield-article-level-metric-from-plos]
. Particula]]></description>
            <content:encoded><![CDATA[<p>At <a href="https://web.archive.org/web/20151002113207/http://network.nature.com/people/mfenner/blog/2009/07/10/i-was-at-scibarcamp-palo-alto">SciBar Camp Palo Alto last month</a>, Peter Binfield from <a href="https://web.archive.org/web/20151002113207/http://www.plosone.org/">PLoS ONE</a> gave a very interesting presentation on <a href="https://web.archive.org/web/20151002113207/http://friendfeed.com/scibarcamp/3577ed2e/peter-binfield-article-level-metric-from-plos">Article-level metrics from the PLoS perspective</a>. Particularly interesting was his announcement that <em><em>PLoS</em></em> journals will provide usage data (HTML pageviews, PDF and XML downloads) for all their articles in September. Usage data, like <a href="https://web.archive.org/web/20151002113207/http://dx.doi.org/10.1371/journal.pone.0006022">all measures of scientific impact</a>, have their problems, but they are a welcome addition to citation-based metrics.</p><p>I’ve interviewed Pete to ask him not only about article-level metrics, but also about the publishing model of <em><em>PLoS ONE</em></em> and how these two relate to each other.</p><h3 id="1-can-you-describe-what-plos-one-is-and-does">1. Can you describe what PLoS ONE is and does?</h3><p><a href="https://web.archive.org/web/20151002113207/http://www.plosone.org/">PLoS ONE</a> is an Open Access, scholarly, peer reviewed journal for all of science. We will be three years old in December 2009, but already we are the <a href="https://web.archive.org/web/20151002113207/http://poeticeconomics.blogspot.com/2009/07/dramatic-growth-of-plos-one-soon-to-be.html">third largest journal (of any type) in the world</a>, publishing approximately 4,600 articles in 2009 alone and almost doubling in volume every year. We are online only (publishing in HTML, XML and PDF) and we publish daily (about 20 articles a day at present). In my view <em><em>PLoS ONE</em></em> is the most dynamic, innovative and exciting journal in the world, and I am proud to work on it.</p><p>In many ways <em><em>PLoS ONE</em></em> operates like any other journal however it diverges in several important respects. The founding principle of <em><em>PLoS ONE</em></em> was that there are certain aspects of publishing which are best conducted pre-publication and certain aspects which are best conducted post-publication. The advent of online publishing has allowed us to take a step back and re-evaluate these aspects of how we publish research, without the burden of centuries of tradition. In this way, we have been able to experiment with new ways of doing things which may result in dramatic improvements in the entire process of scholarly publication.</p><p>The most important thing which has come out of this premise is that unlike almost every other journal in the world, we make no judgment call whatsoever on the “impact” or “significance” or “interest level” of any submission. What this means is that if an article appropriately reports on well-conducted science, and if it passes our peer review process (which determines whether it deserves to join the scientific literature) then we will publish it. In this way, no author should ever receive the message that their article is scientifically sound but “not interesting enough” for our journal, or that their article is “only suited to a specialized audience”. As a result, we short circuit the vicious cycle of submit to a “top tier” journal; get reviewed; get rejected; submit to the next journal down the list; repeat until accepted and we are therefore able to place good science into the public domain as promptly as possible, with the minimum of burden on the academic community.</p><p>The most recent example of the way in which we separate pre-publication activity from post-publication activity is with the development of our <a href="https://web.archive.org/web/20151002113207/http://everyone.plos.org/tag/article-level-metrics/">Article-Level Metrics program</a>. <em><em>Article-level metrics</em></em> start from the assumption that the best way to measure the worth of an article is to look at the actual article itself (and not the journal it happens to have been published in). Following from this, it seems obvious that the best way to evaluate any article is to make use of the collective opinion of all experts in the field (and not a small number of peer reviewers, or a small number of people who ultimately go on to cite the article). An evaluation of this type (which requires that people actually read, and then act in a variety of ways, on the article) can only be done after the article is published (and not before, as happens when a journal rejects a paper because it thinks it is not “impactful” enough). Therefore, our development of new tools to facilitate the post-publication evaluation of individual articles is a great example of us separating out what is most appropriately conducted pre-publication vs post-publication.</p><p>I have gone into some detail on these issues in a recent peer reviewed paper – <a href="https://web.archive.org/web/20151002113207/http://conferences.aepic.it/index.php/elpub/elpub2009/paper/view/114">PLoS One: background, future development, and article-level metrics</a> and I would also recommend Shirley Wu’s excellent <a href="https://web.archive.org/web/20151002113207/http://shirleywho.wordpress.com/2009/08/06/the-evolution-of-scientific-impact/">recent blog post on this topic</a>.</p><h3 id="2-can-you-describe-what-ambra-topaz-is-and-does">2. Can you describe what Ambra/Topaz is and does?</h3><p>Basically Ambra is our publishing platform, which runs on top of the <a href="https://web.archive.org/web/20151002113207/http://everyone.plos.org/2009/05/13/all-plos-titles-now-on-the-same-publishing-platform/">Topaz</a> infrastructure. For the full technical detail, the best information is found <a href="https://web.archive.org/web/20151002113207/http://ambraproject.org/">here</a>.</p><h3 id="3-can-you-talk-a-little-bit-more-about-the-post-publication-features-of-ambra-topaz">3. Can you talk a little bit more about the post-publication features of Ambra/Topaz?</h3><p>Once an article is published, our platform allows users to leave feedback directly on the article. We were among the first journals to allow this, and we remain somewhat unique in this respect, although the concept is gaining broader acceptance and similar functionality is starting to appear at other publishers sites. Specifically, we allow users to leave a Note inline with a specific selection of text; or they can leave a general Comment on the entire article; or they can Star rate the article (on a 5 point scale in 3 categories). Comments and Notes form discussion threads, and users can then engage in debate on the points raised.</p><p>Users may not be anonymous, they must follow our guidelines for civilized academic debate, and when leaving feedback they are <a href="https://web.archive.org/web/20151002113207/http://www.plosone.org/static/commentGuidelines.action">asked to declare any competing interests</a>. I want to make it clear that the post-publication functionality that we provide is not intended to provide post-publication peer review – it is post-publication discourse and feedback. All <em><em>PLoS</em></em> titles now share this same functionality.</p><h3 id="4-are-other-publishers-besides-plos-using-the-ambra-topaz-platform">4. Are other publishers besides PLoS using the Ambra/Topaz platform?</h3><p>None that we are aware of although the NIH has an internal project. But Topaz is open source, so please contact us if you want to use it!</p><h3 id="5-what-article-level-metrics-does-plos-one-provide">5. What article-level metrics does PLoS ONE provide?</h3><p>As of today (August 2009), on every article, in every <em><em>PLoS</em></em> title we provide:</p><ul><li>Number of citations (as measured by <a href="https://web.archive.org/web/20151002113207/http://www.scopus.com/">Scopus</a> and <a href="https://web.archive.org/web/20151002113207/http://www.pubmedcentral.nih.gov/">PubMedCentral</a>)</li><li>Number of social bookmarks (as recorded by <a href="https://web.archive.org/web/20151002113207/http://www.citeulike.org/">CiteULike</a> and <a href="https://web.archive.org/web/20151002113207/http://www.connotea.org/">Connotea</a>)</li><li>Number of star ratings left by users on our system</li><li>Notes, Comments and any replies, as left by users of our system</li><li>Number of blog posts written about the article (as counted by the blog aggregators <a href="https://web.archive.org/web/20151002113207/http://www.postgenomic.com/">Postgenomic</a>, <a href="https://web.archive.org/web/20151002113207/http://blogs.nature.com/">Nature Blogs</a> and <a href="https://web.archive.org/web/20151002113207/http://www.bloglines.com/">Bloglines</a>)</li><li>Specific trackbacks to the article from any web page using our trackback protocol</li></ul><p>In September we will be adding additional citation data as measured by <a href="https://web.archive.org/web/20151002113207/http://www.crossref.org/">CrossRef</a> and, most significantly, the online usage for each article (going back to the original publication date and reported on a monthly basis, broken down by HTML pageviews, PDF downloads, and XML downloads). This development in particular is very exciting – no other publisher has made this data available for such a large corpus of articles.</p><p>After this, the next data source we will add will be blog coverage as aggregated by <a href="https://web.archive.org/web/20151002113207/http://researchblogging.org/">ResearchBlogging.org</a>, and in subsequent months we will be adding other metrics as and when we can identify high quality sources which meet our criteria.</p><p><em><em>Article-level metrics</em></em> are a major development for <em><em>PLoS</em></em> and we believe that we are unique in the publishing industry with the transparent provision of such a range of <em><em>article-level metrics</em></em>. No other publisher provides as much (or any, in most cases) article-level data in such a comprehensive and open manner. It is our belief that once we have demonstrated what is possible, as well as the power of these metrics, the academic community will quickly begin to expect, and demand, this level of information from all journals. As a result the very nature of research reporting and evaluation will be improved as a result.</p><h3 id="6-how-do-article-level-metrics-fit-in-with-how-plos-one-conducts-peer-review">6. How do article-level metrics fit in with how PLoS ONE conducts peer review</h3><p>We peer review all submissions for their <a href="https://web.archive.org/web/20151002113207/http://www.plosone.org/static/review.action">scientific content</a>, but we do NOT peer review them in order to determine whether they are high or low impact (or interest or significance or relevance etc). Therefore, from the reader’s point of view, when you encounter a <em><em>PLoS ONE</em></em> article you do not necessarily know how impactful, or interesting, or significant, or relevant that article might be (without actually reading it!). In the traditional model, you would have some indication as to the likely importance of an article by a knowledge of the journal in which it was published in (although we argue that this way of determining quality is actually <a href="https://web.archive.org/web/20151002113207/http://www.plos.org/cms/node/478">one of the worst methods you could use</a>), however in <em><em>PLoS ONE</em></em> all you know is that the article is scientifically and methodologically sound (which are the only questions that our peer review process asks). Therefore, <em><em>article-level metrics</em></em> provide the reader with an indication as to the worth of an article once it is published. Until today, people have effectively said: this article was published in journal X, therefore knowing this one fact, I now know that the article is excellent/good/average/poor. I think that any sane person who considered that statement would realize how unscientific it was. With the advent of <em><em>article-level metrics</em></em>, a reader can now say “this article was published as part of the scientific literature, it is irrelevant which journal it was published in as I have now been given a variety of information about the article itself which will help me decide whether the article is excellent / good / average / poor for my own purposes”.</p><p>Therefore, <em><em>article-level metrics</em></em> do not supplant peer review and they also do not represent post-publication peer review. However they do provide the reader with new and valuable ways to do the post-publication evaluation and filtering of journal content.</p><h3 id="7-what-are-plos-one-subject-areas-or-portals">7. What are PLoS ONE subject areas or portals?</h3><p>We actually have several ways to ‘parse’ our content by <a href="https://web.archive.org/web/20151002113207/http://www.plosone.org/static/browse.action">topic</a>: All content is assigned to one or more of our 52 topic areas (for example Pathology or Oncology). These topics are assigned by the authors themselves and an article can appear in more than one topic. Having made that classification, readers can then <a href="https://web.archive.org/web/20151002113207/http://www.plosone.org/article/browse.action">browse the topics</a> or subscribe to <a href="https://web.archive.org/web/20151002113207/http://www.plosone.org/static/rssFeeds.action">an RSS feed per topic</a>.</p><p>However, we appreciate that this is not a very flexible way to find content that doesn’t easily fall under our existing taxonomy structure. Therefore, we also have the ability to aggregate our articles into <a href="https://web.archive.org/web/20151002113207/http://www.plosone.org/article/browseVolume.action">Collections</a>. A Collection is literally just an aggregation tool (post-publication) – articles are still published as part of the normal run of the journal, but can then be assigned to join a Collection where they will also appear as part of a collection of related articles. For example, right now we have a very popular <a href="https://web.archive.org/web/20151002113207/http://www.plosone.org/article/browseIssue.action?issue=info%3Adoi%2F10.1371%2Fissue.pone.c01.i02">Paleontology Collection</a>. As we publish new Paleontology articles they get added to this Collection to form a single location for all relevant articles in the field. This Collection functionality can also be used for the output of a single research effort, and an example of this is our <a href="https://web.archive.org/web/20151002113207/http://www.plosone.org/article/browseIssue.action?issue=info%3Adoi%2F10.1371%2Fissue.pone.c01.i01">Stress-Induced Depression and Comorbidities Collection</a> which effectively replicated a “Special Issue” of a journal, and contained all the articles we published as written by the EUMOOD Research Consortia. Collections can be static, or can build up over time, and articles can appear in multiple Collections – as such they represent a very flexible way to re-present our content.</p><p>Then we have <em><em>PLoS</em></em> Hubs, which are under development right now. We see a Hub as a way to aggregate journal articles (along with other types of content) about a given topic into a single location. Once aggregated, we can then provide various community specific tools and services around this content. A Hub should not be thought of as a portal or an overlay journal – the distinguishing feature will be that a Hub will physically contain (and not just link out to) as much content as possible. Clearly, the easiest way to achieve this is by making the content Open Access, and so we also see Hubs as an opportunity to demonstrate the power of an Open Access copyright license. At the moment there is only one Hub (the <a href="https://web.archive.org/web/20151002113207/http://clinicaltrials.ploshubs.org/home.action">PLoS Clinical Trials Hub</a>) but this is a rather old implementation of the concept and only contains <em><em>PLoS</em></em> content – therefore we are proactively working on a new release which will include more of the functionality described above.</p><p>Future developments to our platform will involve the ability to tag articles (perhaps by some combination of curated and user generated tags) which will provide yet another way to dynamically aggregate the content.</p><h3 id="8-how-has-the-ambra-topaz-platform-handled-the-enormous-growth-of-plos-one">8. How has the Ambra/Topaz platform handled the enormous growth of PLoS ONE?</h3><p>Great! We had a few architecture issues in the past due to the bleeding edge nature of the platform, but all seven of our journals have now migrated to the same platform and no substantial issues have come up since the we migrated our Community Journals (back in early 2008).</p><h3 id="9-what-are-your-responsibilities-at-plos">9. What are your responsibilities at PLoS?</h3><p>I am the Managing Editor of <em><em>PLoS ONE</em></em> (one of seven titles at <em><em>PLoS</em></em>). Although this position is an Editorial one, it is the position which is ultimately responsible for everything associated with the journal. By this I mean that although other departments may not report into me, I am ultimately responsible for the marketing, production, operations, web etc of the journal. If we have a problem with any aspect of the journal, it is me that makes sure it gets solved!</p><h3 id="10-what-did-you-do-before-starting-to-work-at-plos">10. What did you do before starting to work at PLoS?</h3><p>Well, I am a physicist from way back – I have a first degree in Physics with Astrophysics and a PhD in Underwater Optical Holography (which I always tell people sounds a lot more interesting than it was!). After my PhD I realized I wasn’t interested in a life in academia and so I moved into Academic Publishing, which allowed me to stay in touch with academia and also to interact with leading researchers conducting the latest research. I started out at Institute of Physics Publishing, in Bristol UK, doing book acquisitions, then I moved to Holland to work for Kluwer Academic Publishers (KAP) to start up a reference work program for them. Via a series of moves I ended up running the KAP Earth, Environmental and Plant Sciences division – a large portfolio of books, reference works and journals in those areas. Around the time that KAP merged with Springer I moved into Business Development for a year or so, which was an interesting period working on Springer’s <a href="https://web.archive.org/web/20151002113207/http://www.springer.com/open+access/open+choice">Open Choice</a> program, online reference works and journal acquisitions among others. Then I left Springer, and also Holland, to move to California (my wife is from San Diego) where I worked for SAGE Publications, just North of LA, for 3 years. There I ran the SAGE US Journals division, which was made up of some 200 or so journals, mostly in the Social Sciences. In that position a large part of the job involved bidding on society titles, to publish them under contract. And then, finally, in March 2008 I moved to San Francisco, to work for <em><em>PLoS</em></em> and run <em><em>PLoS ONE</em></em>.</p><h3 id="11-do-you-want-to-talk-about-future-plans-for-ambra-topaz">11. Do you want to talk about future plans for Ambra/Topaz?</h3><p>The list of upcoming projects includes:</p><ul><li>More article-level metrics development</li><li>RDFa implementation</li><li>Automatic article relationships</li><li>Semantic enhancement</li><li>REST-based API</li><li>The ingest and publication of many types of content / data (structured and unstructured)</li><li>Tags</li><li>Enhanced search and browse functionality</li><li>A new process to submit articles directly to PubMed Central and other external repositories</li><li>Direct access to our underlying triple store (sparql endpoint, RDFa)</li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Recipe: Distributing papers for a journal club]]></title>
            <link>https://blog.martinfenner.org/posts/recipe-distributing-papers-for-a-journal-club</link>
            <guid>7b894842-2c6f-474b-b676-b790001c98f1</guid>
            <pubDate>Sat, 08 Aug 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[Problem
You want to distribute papers for a regular journal club in your department.

Solution
Create a group for your journal club in FriendFeed
[https://web.archive.org/web/20150922174148/http://www.friendfeed.com/]. You can
create a either a private group, where only group member can read and post
messages, or a public group that is open to everyone. Then invite all regular
participants of your journal club to FriendFeed and make them join the group.

Announce the papers that you want to disc]]></description>
            <content:encoded><![CDATA[<h3 id="problem">Problem</h3><p>You want to distribute papers for a regular journal club in your department.</p><h3 id="solution">Solution</h3><p>Create a group for your journal club in <a href="https://web.archive.org/web/20150922174148/http://www.friendfeed.com/">FriendFeed</a>. You can create a either a private group, where only group member can read and post messages, or a public group that is open to everyone. Then invite all regular participants of your journal club to <em><em>FriendFeed</em></em> and make them join the group.</p><p>Announce the papers that you want to discuss in the journal club via a <em><em>FriendFeed</em></em> message. For this go to the webpage for the paper you want to discuss (e.g. <a href="https://web.archive.org/web/20150922174148/http://www.nature.com/nature/journal/v460/n7256/full/nature08237.html">this paper</a>) and then use the <a href="https://web.archive.org/web/20150922174148/http://friendfeed.com/share/bookmarklet">FriendFeed bookmarklet</a> to announce the paper (and additional information such as the date of the journal club) in the <em><em>FriendFeed</em></em> group. If the copyright of the paper allows this, you could also post the fulltext PDF of the paper to the <em><em>FriendFeed</em></em> group.</p><p>Use <em><em>FriendFeed</em></em> comments to capture the discussion about the paper in the journal club. The comments can also contain links to other relevant papers and the slides you may have prepaped for the journal club. This is helpful for those unable to attend the journal club in person, or to look back at the journal club a few months later.</p><p><a href="https://web.archive.org/web/20150922174148/http://network.nature.com/forums">Nature Network forums</a>, <a href="https://web.archive.org/web/20150922174148/http://www.citeulike.org/">CiteULike</a>, <a href="https://web.archive.org/web/20150922174148/http://www.labmeeting.com/">Labmeeting</a> and <a href="https://web.archive.org/web/20150922174148/http://blogs.plos.org/mfenner/2009/08/08/recipe_distributing_papers_for_a_journal_club/http//www.basecamphq.com">Basecamp</a> (and probably some other tools) offer similar functionality, so use the service you are most comfortable with. Of the tools mentioned, <em><em>FriendFeed</em></em> for me is the easiest to set up and use.</p><h3 id="discussion">Discussion</h3><p>Email is not a good solution for regularly sending around large files. And discussions among a larger group of people (i.e. all members of a journal club) are difficult to follow via email. <a href="https://web.archive.org/web/20150922174148/http://wave.google.com/">Google Wave</a> is a good alternative without these limitations, but is not yet publicly available.</p><p>A <a href="https://web.archive.org/web/20150922174148/http://openwetware.org/wiki/Journal_Club">Wiki</a> or <a href="https://web.archive.org/web/20150922174148/http://blogs.nature.com/nature/journalclub/">blog</a> could also be used to organize a journal club, but requires a larger effort to set up and maintain.</p><p>Many reference managers allow their users to create private groups for sharing references. But in order to work as a tool for a journal club, we also need messages/comments. Not only to discuss the paper, but simply to provide the date and presenter for the journal club or other organisational information. But I wouldn't be surprised if more reference managers besides <em><em>CiteULike</em></em> add these features in the future.</p><p>If all journal club papers should automatically be stored in a reference manager, use either <em><em>CiteULike</em></em> or put the papers for the journal club first into the reference manager and then export them via RSS feed into <em><em>FriendFeed</em></em>. This step can be automated if you create a group/folder for the journal club in your reference manager.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Bibliographic Management meets Web 2.0]]></title>
            <link>https://blog.martinfenner.org/posts/bibliographic-management-meets-web-2-0</link>
            <guid>46a65306-d25c-4f9d-be4c-e1b314db1e93</guid>
            <pubDate>Sat, 01 Aug 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[Regular readers of this blog know that I often talk about bibliographic
management tools (most recently here
[https://blog.martinfenner.org/posts/what-is-the-right-reference-manager-for-you] 
and here [https://blog.martinfenner.org/posts/reference-manager-overview]), and
it was probably for this reason that I was invited to an interesting full-day
workshop last week at the Royal Free Hospital Medical Library
[https://web.archive.org/web/20150908064949/http://www.royalfree.nhs.uk/] in
London: Bib]]></description>
            <content:encoded><![CDATA[<p>Regular readers of this blog know that I often talk about bibliographic management tools (most recently <a href="https://blog.martinfenner.org/posts/what-is-the-right-reference-manager-for-you">here</a> and <a href="https://blog.martinfenner.org/posts/reference-manager-overview">here</a>), and it was probably for this reason that I was invited to an interesting full-day workshop last week at the <a href="https://web.archive.org/web/20150908064949/http://www.royalfree.nhs.uk/">Royal Free Hospital Medical Library</a> in London: <em><em>Bibliographic Management meets Web 2.0</em></em>. The event was organized by science librarians <a href="https://web.archive.org/web/20150908064949/http://network.nature.com/people/franknorman/profile">Frank Norman</a>, <a href="https://web.archive.org/web/20150908064949/http://network.nature.com/people/UB5280986/profile">Nathalie Cornee</a> and <a href="https://web.archive.org/web/20150908064949/http://network.nature.com/people/U8136B832/profile">Betsy Anagnostelis</a>, and was attended by about 20 science librarians. We had demos of the following software:</p><ul><li><a href="https://endnote.com/">EndnoteWeb</a>, demo by Stephanie Marshall</li><li><a href="https://web.archive.org/web/20150908064949/http://www.refworks.com/">RefWorks</a>, demo by Aaron Maierhofer</li><li><a href="https://en.wikipedia.org/wiki/Connotea">Connotea</a>, demo by Grace Barnes</li><li><a href="https://en.wikipedia.org/wiki/CiteULike">CiteULike</a>, demo by Kevin Emamy</li><li><a href="https://www.zotero.org/">Zotero</a>, demo by Rintze Zelle</li><li><a href="https://www.mendeley.com/">Mendeley</a>, demo by Victor Henning and Jan Reichelt</li></ul><p>This is a fairly complete list of bibliographic software with Web 2.0 features. Missing are some tools with similar features, as there simply wasn't enough time and money to invite them all (e.g. <a href="https://web.archive.org/web/20150908064949/http://www.2collab.com/">2collab</a> and <a href="https://web.archive.org/web/20150908064949/http://www.labmeeting.com/">Labmeeting</a>), several popular programs without these features (<a href="https://web.archive.org/web/20150908064949/http://www.refman.com/">Reference Manager</a>, <a href="https://web.archive.org/web/20150908064949/http://www.citavi.com/">Citavi</a> and <a href="https://www.papersapp.com/">Papers</a>), and <a href="https://web.archive.org/web/20150908064949/http://jabref.sourceforge.net/">JabRef</a> and similar tools for <a href="https://web.archive.org/web/20150908064949/http://www.bibtex.org/">BibTeX</a> files used by LaTeX.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20150908064949im_/http://farm4.static.flickr.com/3593/3774951723_0b9e697677.jpg" class="kg-image" alt></figure><p>I gave a brief introduction of what features I would look for in a reference manager today (available on <a href="https://web.archive.org/web/20150908064949/http://www.slideshare.net/mfenner/which-reference-manager">slideshare</a>). The six presenters then took up a challenge (Word file <a href="https://web.archive.org/web/20150908064949/http://friendfeed-media.com/f9221d4c733652d64845e3dc6e0c17eaea9a05a2">here</a>) that was given to them the week before. We used a simplified version of a set of tasks (find reference, put citation into Word document, etc.) originally developed for <a href="https://web.archive.org/web/20150908064949/http://citefest.pbworks.com/">CiteFest</a> at Northwestern University. Every participant was sitting in front of a computer and follow along with the tasks (the technical staff at the Royal Free did a marvelous job installing the software and bookmarklets on all computers before the workshop). The demos were nicely captured in the <a href="https://web.archive.org/web/20150908064949/http://friendfeed.com/bibman2">FriendFeed room</a> we have set up for the workshop, so you can follow the details of the sessions there (the room also has some excellent links to other resources).</p><p>In the discussion at the end we all felt that all reference managers used that day were up to the challenge (<em><em>Connotea</em></em> and <em><em>CiteULike</em></em> couldn't put references into a Word document, but managed the rest of the tasks perfectly), and we decided not to declare a winner. We also felt that the market is developing so fast, that a feature comparison looked very different 12 months ago (e.g. <em><em>Mendeley</em></em> just launched, no Web version of <em><em>Zotero</em></em>), and will again look very different 12 months from today. Most librarians in the room therefore felt that they probably have to support most of these tools at their institutions. We briefly talked about the cost of these tools (of the tools that were demoed, only <em><em>Endnoteweb</em></em> and <em><em>Refworks</em></em> are commercial). This might be an issue when licenses have to be renewed. I've set up a <a href="https://web.archive.org/web/20150908064949/http://network.nature.com/people/mfenner/blog/2009/03/15/reference-manager-overview">reference manager feature comparison chart</a> a few months ago, and have updated this chart after the workshop.</p><p>One feature I really like in <em><em>Papers</em></em> is full-text search of the PDF files in your library, which I think is a much easier way to find information than tagging (and David Crotty agrees with me: <a href="https://web.archive.org/web/20150908064949/http://www.cshblogs.org/cshprotocols/2009/02/23/why-article-tagging-doesnt-work/">Why article tagging doesn't work</a>). I was therefore happy to see that <em><em>Zotero</em></em>, <em><em>Mendeley</em></em>, <em><em>CiteULike</em></em> and <em><em>Refworks</em></em> all support this feature, something I have overlooked before. Full-text search is one more reason that online reference managers should be able to store PDF files. Another reason is that several people working on the same research project shouldn't all have to go out and download the full-text files themselves – or even worse, email them to each other. This sharing is not only an important features of Web 2.0 bibliographic tools, but is obviously also an area of potential problems with copyright (for papers that are not Open Access). <em><em>CiteULike</em></em>, <em><em>Mendeley</em></em> and <em><em>Refworks</em></em> all try to avoid these kinds of issues by allowing sharing of references and PDF files in private groups with a limited number of users.</p><p>It was interesting to see that the tools use very different approaches to integrate with the Web. <em><em>Connotea</em></em>, <em><em>CiteUlike</em></em> and <em><em>Refworks</em></em> are Web-based tools (the <a href="https://web.archive.org/web/20150908064949/http://www.refworks.com/refworks/WNCDownload.asp">Refworks Write-N-Cite Windows module</a> can download the reference database for read-only offline use), whereas <em><em>Zotero</em></em>, <em><em>Mendeley</em></em> and <em><em>Endnote</em></em> synchronize between a desktop version and web version. I'm really torn between these two approaches. Web-only reduces the cost of development and maintenance (no need to develop separate versions for Web/Windows/Mac/Linux, users don't have to install new versions). Desktop/Web allows offline use users always have a copy of their data on their computer. There is probably no right answer to this, email is another example where both approaches are very successful.</p><p>Only after the meeting did I realize another interesting difference between the various tools. They all have different business models behind them: <em><em>Endnoteweb</em></em>: (mostly) single-user licenses, <em><em>Refworks</em></em>: site licenses, <em><em>Connotea</em></em>: owned by a publisher, <em><em>CiteUlike</em></em>: independent with support from a publisher, <em><em>Zotero</em></em>: Open Source and <em><em>Mendeley</em></em>: Web 2.0 company that plans free and paid services. This might actually be a deciding factor in determining which of these tools will be the most successful in the long-term future.</p><p>But in the end, the similarities far outweigh the differences between these tools. We didn't have time left to talk more about this, but for Bibliographic Management to move to Web 2.0, it's not really just about technology, but rather mostly about the users. We had an interesting discussion about that topic a few weeks ago (<a href="https://blog.martinfenner.org/posts/how-to-close-the-digital-divide-among-scientists">How to close the digital divide among scientists</a>). I believe that ultimately your collection of references should sit in a database that is accessible from the web. Some references you want to share with everybody (e.g. via tools like FriendFeed), and that includes your own publications. And some references you want to share with private groups of people, e.g. people in your lab or when coauthoring a paper. All the tools presented in the workshop allow you to do that.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How does the article of the future look like?]]></title>
            <link>https://blog.martinfenner.org/posts/how-does-the-article-of-the-future-look-like</link>
            <guid>6b176afd-c274-4dc3-9478-af7c6aef1c0e</guid>
            <pubDate>Sun, 26 Jul 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[Last year Elsevier started the Article 2.0 Contest
[https://web.archive.org/web/20151002145328/http://article20.elsevier.com/contest/home.html] 
and asked for the best ideas on how research articles should be presented on the
web. The winners were announced March 31
[https://web.archive.org/web/20151002145328/http://news.prnewswire.com/ViewContent.aspx?ACCT=109&STORY=%2Fwww%2Fstory%2F03-31-2009%2F0004997400&EDATE]
:

 * Inigo Surguy from 67 Bricks
   [https://web.archive.org/web/20151002145328/h]]></description>
            <content:encoded><![CDATA[<p>Last year <em><em>Elsevier</em></em> started the <a href="https://web.archive.org/web/20151002145328/http://article20.elsevier.com/contest/home.html">Article 2.0 Contest</a> and asked for the best ideas on how research articles should be presented on the web. The winners were <a href="https://web.archive.org/web/20151002145328/http://news.prnewswire.com/ViewContent.aspx?ACCT=109&amp;STORY=%2Fwww%2Fstory%2F03-31-2009%2F0004997400&amp;EDATE">announced March 31</a>:</p><ul><li><em><em>Inigo Surguy</em></em> from <a href="https://web.archive.org/web/20151002145328/http://surguy.net/bricks/elsevier/00012998/0036/0001/05000486/about.html">67 Bricks</a> won <em><em>first prize</em></em>. His application used enhanced content navigation and allowed adding semantic data to the article as well as commenting on specific parts of the article.</li><li><em><em>Jacek Ambroziak</em></em> from <a href="https://web.archive.org/web/20151002145328/http://ambrosoft.com/cmsmadesimple/index.php?page=elsevier-application">Ambrosoft</a> won <em><em>second prize</em></em> for an application that uses server-side and client-side technology to read journal articles on the <a href="https://web.archive.org/web/20151002145328/http://www.android.com/">Android</a> mobile platform.</li><li><em><em>Stuart Chalk</em></em> from the University of North Florida won <em><em>third prize</em></em>. The link to his application is broken, but his idea was that research articles are non-linear and he provided customized interfaces on how the article is presented.</li></ul><p>This week Cell Press and Elsevier <a href="https://web.archive.org/web/20151002145328/http://www.elsevier.com/wps/find/authored_newsitem.cws_home/companynews05_01279">announced</a> the <a href="https://web.archive.org/web/20151002145328/http://beta.cell.com/">Article of the Future</a> project, again a project on how scientific articles should be presented online. It is not clear of how the material submitted in the Article 2.0 contest was used to create the Article of the Future prototypes. Some key features of the prototypes are:</p><ul><li>Tabbed browsing of article sections</li><li>hierarchical presentation of text and figures</li><li>graphical abstract and research highlights</li><li>integrated audio and video</li><li>real-time reference analyses</li></ul><p>Two sample articles are provided by Cell Press (<a href="https://web.archive.org/web/20151002145328/http://beta.cell.com/erickson/">here</a> and <a href="https://web.archive.org/web/20151002145328/http://beta.cell.com/hochstim/">here</a>). They are a good starting point to discuss what works and what doesn't. And I don't want to get into the discussion of whether naming this project <em><em>Article of the Future</em></em> is a little bit overambitious for the small changes that were proposed in the prototypes. Here are my thoughts on some of the features that might be useful in displaying research articles online (a lot of them have already been implemented by one journal or another):</p><h3 id="navigation">Navigation</h3><p>Navigation to a specific section (e.g. discussion) is an obvious feature that many journals have implemented. This navigation should also work as links from the outside, and should also allow direct linking to a figure or table.</p><h3 id="abstract">Abstract</h3><p>The traditional abstract can be extended for the online version of a manuscript. This could be an easier to read summary of the article or an audio or video abstract. Abstracts are important teasers to read the fulltext paper, and the current format might not be appropriate for everyone.</p><h3 id="integration-of-figures-and-tables">Integration of figures and tables</h3><p>Most journals don't fully integrate figures and tables in their online papers, but rather put them on a separate page and link to them. This is similar to the PDF version of a paper, where figures often have to be placed away from the text section discussing them. The online version of a paper should allow viewing the figures and tables in parallel to reading the next, preferably by using a two column layout. Supplementary information should be integrated in a similar manner.</p><h3 id="references">References</h3><p>Going back and forth between the citation in the text and the reference at the end of the article is one of the annoyances of the traditional paper format. Online papers should make this easier, e.g. by displaying the full reference when hovering over the citation. A separate column for the references could also work, but three columns (text, figures, and references) might confuse the typical reader. The reference section should include links to the referenced paper, preferably using the DOI. Additional information about these references (e.g. links to PubMed, links to the full text, number of citations) could also be provided. A “return to text” is a nice touch at <em><em>Biomed Central</em></em>.</p><h3 id="related-content">Related content</h3><p>Other papers citing the article should be listed and linked to. Showing related articles would be helpful, but I have yet to see a working implementation of that feature (maybe with the help of services such as <a href="https://web.archive.org/web/20151002145328/http://www.citulike.org/">CiteULike</a> or <a href="https://web.archive.org/web/20151002145328/http://www.mendeley.com/">Mendeley</a>?). A good starting point would be to include other articles by the same authors and articles that are cited by the paper. Links to blog posts and other online content (e.g. on <em><em>Twitter</em></em> or <em><em>FriendFeed</em></em>) talking about the article would be very helpful, but that is difficult to do. Linking to the places that bookmark the article (e.g. on <em><em>CiteULike</em></em>, <em><em>Connotea</em></em>, <em><em>2collab</em></em>) is a good starting point to find other users with related interests. Therefore it is also helpful to make it easy to share the article using these services (as they all use bookmarklets for that, this is more about providing good import filters).</p><h3 id="article-publication-history">Article Publication History</h3><p>The publication history of the article (submission and acceptance dates, as well as reviewer comments, etc.) provides interesting addition information about a paper, and is for example provided by <a href="https://web.archive.org/web/20151002145328/http://www.biomedcentral.com/1472-6963/9/127/prepub">BioMed Central journals</a>.</p><h3 id="comments-and-usage-statistics">Comments and usage statistics</h3><p>Comments should be possible not only at the end of the article, but also within specific sections (such as <a href="https://web.archive.org/web/20151002145328/http://www.plosone.org/static/commentGuidelines.action">notes</a> in PLoS journals). And comments should be implemented using an API, allowing viewing and adding comments from within other services. Usage statistics are another way to look at the popularity of a paper, and should soon be available for the <em><em>PLoS</em></em> journals.</p><h3 id="semantic-markup">Semantic markup</h3><p>This is an area where I would expect the biggest changes to the current format – papers currently have little or no support for this and that is a shame. <a href="https://web.archive.org/web/20151002145328/http://imageweb.zoo.ox.ac.uk/pub/2008/plospaper/latest/">Here</a> is an example of a semantically enhanced PLoS Neglected Tropical Diseases article. Another example is semantic markup used by <em><em>Nature Chemistry</em></em>, as discussed in <a href="https://web.archive.org/web/20151002145328/http://chem-bla-ics.blogspot.com/2009/03/nature-chemistry-improves-publishing.html">this blog post</a> by Egon Willighagen.</p><p>Kent Anderson over at the <em><em>scholarly kitchen blog</em></em> (<a href="https://web.archive.org/web/20151002145328/http://scholarlykitchen.sspnet.org/2009/07/21/the-article-of-the-future-lipstick-on-a-pig/">The Article of the Future – Just Lipstick Again?</a>) and Marshall Kirkpatrick at <em><em>ReadWriteWeb</em></em> (<a href="https://web.archive.org/web/20151002145328/http://www.readwriteweb.com/archives/elseviers_prototype_is_this_the_scientific_article.php">Elsevier's Prototype: Is This The Scientific Article of the Future?</a>) also wrote about this. And this was discussed on <a href="https://web.archive.org/web/20151002145328/http://friendfeed.com/science-online/cbbe2531/news-releases-elsevier-announces-article-of">FriendFeed</a> and in the <a href="https://web.archive.org/web/20151002145328/http://network.nature.com/groups/goodpaper/forum/topics/5099">Good Paper Journal Club</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Google Wave for a week – it’s still great!]]></title>
            <link>https://blog.martinfenner.org/posts/using-google-wave-for-a-week-its-still-great</link>
            <guid>03cea12c-13d1-458f-9019-84232feeaa16</guid>
            <pubDate>Sat, 18 Jul 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[Google Wave [https://web.archive.org/web/20150922174142/http://wave.google.com/] 
is a new tool for communication and collaboration on the web. When Wave was
first announced May 28 at the Google I/O conference
[https://web.archive.org/web/20150922174142/http://code.google.com/intl/de-DE/events/io/]
, many people immediately saw its potential as a great collaboration tool for
scientists:

 * Ricardo Vidal: Using Google Wave to surf the streams
   [https://web.archive.org/web/20150922174142/http:/]]></description>
            <content:encoded><![CDATA[<p><a href="https://web.archive.org/web/20150922174142/http://wave.google.com/">Google Wave</a> is a new tool for communication and collaboration on the web. When Wave was first announced May 28 at the <a href="https://web.archive.org/web/20150922174142/http://code.google.com/intl/de-DE/events/io/">Google I/O conference</a>, many people immediately saw its potential as a great collaboration tool for scientists:</p><ul><li>Ricardo Vidal: <a href="https://web.archive.org/web/20150922174142/http://my.biotechlife.net/2009/05/29/using-the-google-wave-to-surf-the-streams/">Using Google Wave to surf the streams</a></li><li>Me: <a href="https://web.archive.org/web/20150922174142/http://network.nature.com/people/mfenner/blog/2009/05/28/google-wave-dont-forget-the-scientists">Google Wave – don't forget the scientists</a></li><li>Cameron Neylon: <a href="https://web.archive.org/web/20150922174142/http://blog.openwetware.org/scienceintheopen/2009/05/30/omg-this-changes-everything-or-yet-another-wave-of-adulation/">OMG! This changes EVERYTHING! – or – Yet Another Wave of Adulation</a> and <a href="https://web.archive.org/web/20150922174142/http://blog.openwetware.org/scienceintheopen/2009/06/08/google-wave-in-research-the-slightly-more-sober-view-part-i-papers/">Google Wave in Research – the slightly more sober view – Part I – Papers</a></li><li>Peter Murray-Rust: <a href="https://web.archive.org/web/20150922174142/http://wwmm.ch.cam.ac.uk/blogs/murrayrust/?p=2026">Google Wave first reactions</a> and <a href="https://web.archive.org/web/20150922174142/http://wwmm.ch.cam.ac.uk/blogs/murrayrust/?p=2032">Google Wave and implications for science</a></li><li>Björn Brembs: <a href="https://web.archive.org/web/20150922174142/http://bjoern.brembs.net/news.php?item.521.3">Will science ride the Google Wave into the 21st century</a></li></ul><p>We based our first impressions on the material available online, especially the video of the <a href="https://web.archive.org/web/20150922174142/http://www.youtube.com/watch?v=v_UyVmITiYQ&amp;feature=player_embedded">Google Wave presentation</a> at Google I/O. The problem is, Wave is currently only available to selected developers, and will not be generally available until the end of the year. Wave is currently not even beta software and was announced now so that third-party developers have enough time to build (and test) extensions to Wave.</p><p>Those of us being invited to <a href="https://web.archive.org/web/20150922174142/http://www.nature.com/scifoo/index.html">SciFoo</a> (which took place July 10-12 at Google) were lucky not only in attending a great conference, but also in getting a Wave account. Wave product manager <em><em>Steph Hannon</em></em> gave us an introduction to Wave on the first evening, and <em><em>Cameron Neylon</em></em> organized a session about Wave the next day. The session was mainly about the Wave extensions that we scientists would need (and I'm sure that Cameron will blog about that), but we could also ask two developers from the Wave team a lot of questions.</p><p>After using Wave for one week, I obviously have a much better feeling for how it can help scientists to communicate and collaborate. The best way to start is to think of Wave as email on steroids. Wave is web-based (which means that it currently only works when you have an internet connection) with a nice interface similar to Gmail or other webmail products. One big advantage over email is that all reply messages are directly connected to the original message (similar to comments on a blog). This is especially helpful for longer email threads and when more than two people are involved.</p><p>But Wave is also instant messaging. You see a small green dot next to your contacts that are currently online, and you can see them typing in real-time (which looks really creepy the first time you see that).</p><p>And Wave is also like a Wiki. You can not only respond to a message, but everybody participating in the Wave can also change the original message (and several people can work on the same message simultaneously). This works great for things such as listing all the blog posts about SciFoo.</p><p>The combination of wiki-style editing plus comments make Wave an interesting alternative to project management tools such as <a href="https://web.archive.org/web/20150922174142/http://basecamphq.com/">Basecamp</a>. And this means that Wave can also be used to work on longer documents – something that the Wave developers regularly do for documentation, etc. Wave documents can also contain images, videos, links, etc. Wave supports different fonts, text colors, bold and italic text, and four different heading levels (for titles and subtitles).</p><p>But in contrast to most online collaboration tools, Wave can be extended with additional functionality that scientists require. Bibliographic references would be an obvious example, and here tools such as <a href="https://web.archive.org/web/20150922174142/http://docs.google.com/">Google Docs</a> or <a href="https://web.archive.org/web/20150922174142/http://buzzword.acrobat.com/">Adobe Buzzword</a> fall short.</p><p>Wave extensions come in the form of gadgets (that work on the client) and robots (that work on the server). Wave gadgets are XML files, whereas robots can currently be developed in either Java or Python. Other programming languages (PHP, Ruby, Perl, etc.) for robots will soon become possible when robots no longer have to be hosted on <a href="https://web.archive.org/web/20150922174142/http://code.google.com/appengine">Google App Engine</a>. Using the tools provided by Google, writing a robot is actually not that difficult and it took me only an hour to have a sample “Hello World!” robot running in Wave. It will obviously take weeks or months to develop more sophisticated robots (e.g. for management of bibliographic references), but I'm sure that a number of exciting science-related extensions will be ready by the time Wave becomes publicly available later this year.</p><p>We plan a session with a live Wave demo at the <a href="https://web.archive.org/web/20150922174142/http://www.scienceonlinelondon.org/">Science Online London</a> conference in August so that more scientists and science communicators become familiar with Wave. About 10 people that registered for the conference already have Wave accounts and I hope that some of them will come up with interesting science-related robots or gadgets. If you are a Wave user, you can reach me at mfenner@wavesandbox.com. And if you want to develop great science-related extensions for Wave, please contact <em><em>Cameron Neylon</em></em>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Value of Peer Review]]></title>
            <link>https://blog.martinfenner.org/posts/the-value-of-peer-review</link>
            <guid>02c070c8-1682-4143-ac82-e0cc103b6c5e</guid>
            <pubDate>Mon, 13 Jul 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[This weekend I was at SciFoo
[https://web.archive.org/web/20161107033419/http://www.nature.com/scifoo/everything.html]
, an invitation-only unconference by O'Reilly Media, Nature and Google that took
place at Google. I was fortunate to be invited, and I'm still digesting all the
impressions and discussions that I had (there were many). This post is the
indirect result of two sessions and several related discussions on one
particular topic that I'm most interested in – the process of scientific
p]]></description>
            <content:encoded><![CDATA[<p>This weekend I was at <a href="https://web.archive.org/web/20161107033419/http://www.nature.com/scifoo/everything.html">SciFoo</a>, an invitation-only unconference by <em>O'Reilly Media</em>, <em>Nature</em> and <em>Google</em> that took place at Google. I was fortunate to be invited, and I'm still digesting all the impressions and discussions that I had (there were <em>many</em>). This post is the indirect result of two sessions and several related discussions on one particular topic that I'm most interested in – the process of scientific publication.</p><p>Peer review <a href="https://web.archive.org/web/20161107033419/http://www.nature.com/nature/peerreview/debate/index.html">is usually seen as essential</a> for the quality of a published paper. At the same time peer review puts a large burden of work on the research community, and in general is unpaid work. But peer review is not living up to its full potential. We should start to see it not as a necessary, costly and time-consuming burden, but rather as a business opportunity. But for this the information communicated in the peer review process needs to leave the (digital) drawers of the journal editorial offices.</p><p>The published research paper is the most important piece of information used to evaluate the reputation of a scientist (the published book takes that role in many social sciences). This evaluation is most important when large sums of money and the personal careers of the involved scientists (in the form of research grants and jobs) depend on it. We usually begin with applying some sort of metrics to the publications of the scientist(s) under evaluation, most often the <a href="https://web.archive.org/web/20161107033419/http://thomsonreuters.com/products_services/science/free/essays/">Journal Impact Factor</a>. Once we have reduced the number of scientists and papers to a small enough number, we can start reading the full-text papers in order to evaluate the quality of the science. Both approaches have shortcomings that I don't want to go into detail here (citation metric: artificial number, delay of several years if relying on citation counts; reading full-text paper: time constraints, often needs outside experts as science has become so specialized). The information contained in the peer review process is a large untapped resource that could potentially overcome many of these shortcomings.</p><p>The typical revenue streams of a scientific journal are currently subscriptions and author submission fees, and to a lesser extend advertising and subscription-based added value in the form of news items and editorials. The information contained in the peer review process is extremely valuable to granting agencies and job search committees and has the potential to become an additional major revenue source for scientific journals, thus allowing a reduction in author submission fees and/or free full-text access without a subscription. Many research organizations and funding agencies currently pay for journal subscriptions and author submission fees. If they would pay the journal publishers similar amounts of money for peer review information, they would obviously get much more value out of their money. Whereas this revenue would probably come mainly from granting agencies, large research organizations or companies would also pay for this information, as would the typical academic journal reader (obviously a much smaller sum) if it helps in filtering out the most relevant scientific papers.</p><p>The peer review process obviously would have to change in this model. If peer review becomes a major source of revenue for the journals, reviewers need to get paid for their efforts. And it will affect of what reviewers and editors write in their assessment if that information might later be seen by third parties – even if they remain anonymous. It is also not clear if the peer review information of rejected papers should be used (a large body of information at journals with high rejection rates). And if journal publishers don't buy in that model of selling peer review information, nobody stops other parties from doing additional peer review of papers already published and selling that information. Which sounds pretty much like what <a href="https://web.archive.org/web/20161107033419/http://www.f1000biology.com/">Faculty of 1000</a> is doing, although they seem to be targeting the academic journal reader rather than the much more important funding agencies.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[I was at SciBarCamp Palo Alto]]></title>
            <link>https://blog.martinfenner.org/posts/i-was-at-scibarcamp-palo-alto</link>
            <guid>f551f22e-86bd-4077-bc38-91ac8535090e</guid>
            <pubDate>Fri, 10 Jul 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[
SciBarCamp Palo Alto
[https://web.archive.org/web/20161110193313/http://www.scibarcamp.org/SciBarCamp_Palo_Alto] 
took place July 8-9 in the Institute for the Future
[https://web.archive.org/web/20161110193313/http://www.iftf.org/]. I came right
from the airport and arrived too late for the general introductions and session
suggestions. But there was time for a little break before Sean Mooney started
his keynote lecture.

Keynote: Biomedical Research in the Age of Cyberinfrastructure
[https://w]]></description>
            <content:encoded><![CDATA[<p><br><a href="https://web.archive.org/web/20161110193313/http://www.scibarcamp.org/SciBarCamp_Palo_Alto">SciBarCamp Palo Alto</a> took place July 8-9 in the <a href="https://web.archive.org/web/20161110193313/http://www.iftf.org/">Institute for the Future</a>. I came right from the airport and arrived too late for the general introductions and session suggestions. But there was time for a little break before Sean Mooney started his keynote lecture.</p><h3 id="keynote-biomedical-research-in-the-age-of-cyberinfrastructure"><a href="https://web.archive.org/web/20161110193313/http://ff.im/4YEeQ">Keynote: Biomedical Research in the Age of Cyberinfrastructure</a></h3><p>Sean Mooney</p><p>Sean Mooney <a href="https://web.archive.org/web/20161110193313/http://www.buckinstitute.org/theInstitute/news.asp?id=31">just recently joined the Buck Institute for Age Research</a> as director of their bioinformatics core. He was talking both about his extensive experience building web-based bioinformatics tools, but also about <a href="https://web.archive.org/web/20161110193313/http://laboratree.org/">Laboratree</a>, a social networking tool for scientists with a focus on research management. The key argument he was trying to make is that we have a large number of online tools to store and analyze the data of our scientific experiments, but that there is still not enough effort to connect these tools. The infrasctructure networks that exist (e.g. <a href="https://web.archive.org/web/20161110193313/https://cabig.nci.nih.gov/">caBIG</a>) usually focus on one particular domain (cancer research in this case), and the 27 NIH institutes all have their own approach to informatics. Researchers and administrators have a very different approach to online tools. Whereas researchers want to analyze data, administrators focus more on the much broader picture, e.g. asset management. Administrators often misjudge their needs for the needs of the scientist. Scientists often mistake today's needs for tomorrow's needs.</p><p>Sean also talked about his experience building <em>Laboratree</em>. He thinks that the social networking for scientists killer app is one that people “have to use” not “like to use”, and that successful tools are simple and start with a preexisting community.</p><p>After the keynote we had un-dinner in smaller groups, and I went to bed after being up for 24 hours thanks to the 9 hour time difference. The next day we had the unconference sessions suggested the day before, and I talk a little bit about the sessions I attended. The rest of the sessions are well documented in the <a href="https://web.archive.org/web/20161110193313/http://friendfeed.com/scibarcamp">SciBarCamp FriendFeed group</a> and on <a href="https://web.archive.org/web/20161110193313/http://twitter.com/#search?q=%23sbcPA">Twitter</a>.</p><h3 id="open-source-textbooks"><a href="https://web.archive.org/web/20161110193313/http://ff.im/50EdW">Open Source Textbooks</a></h3><p>Chris Patil</p><p>This was a great brainstorming session. We agreed that – with very few exceptions – there is no financial incentive for scientists to write textbooks. The main motivation is reputation, and we thought that the lack of return on textbook authoring is an opportunity for new approaches. The concept of Open Access textbooks is a social problem (e.g. who pays for the textbook, what are the author incentives when there is a long list of contributors), and not a technical problem. <a href="https://web.archive.org/web/20161110193313/http://ocw.mit.edu/OcwWeb/web/home/home/index.htm">OpenCourseWare</a> was mentioned as a great educational resource that overlaps in intent with Open Access textbooks. Sean Mooney and Cameron Neylon mention the often unsolved copyright issues when creating Open Access material, and this includes potential issues with proper attribution with the <a href="https://web.archive.org/web/20161110193313/http://creativecommons.org/">Creative Commons</a> license when reusing material. We briefly talked about document formats (where PDF probably is still important), and that we probably need to print most of the textbooks, at least until eBook readers become cheaper.</p><h3 id="personal-genomics"><a href="https://web.archive.org/web/20161110193313/http://ff.im/50N35">Personal Genomics</a></h3><p>Melanie Swan</p><p>This for me was a great introduction to the topic, and the main emphasis of the session was probably on the consumer perspective. The first ever consumer genomics conference <a href="https://web.archive.org/web/20161110193313/http://www.consumergeneticsshow.com/">was held in Boston last month</a>. One important, and still only partly solved issue is huge amount of data generated, especially if we take into account not only genetics, but also epigenetic changes, and the fact that not all cells of an individual are genetically identical (which is true especially in cancer). The technology is changing rapidly and the cost of sequencing a whole genome of an individual is constantly coming down. We talked about a few examples of genes that are already useful to test, one prominent example being <a href="https://web.archive.org/web/20161110193313/http://rbaltman.wordpress.com/2009/05/21/cms-made-a-mistake-in-not-approving-genetics-for-warfarin/">CYP2C9</a> and warfarin (an anticoagulant) metabolism. Many chronic disease conditions are multigenetic, and therefore it is usually risk assessment rather than yes/no answers. Some personalized genetics testing companies are <a href="https://web.archive.org/web/20161110193313/http://www.navigenics.com/">Navigenics</a>, <a href="https://web.archive.org/web/20161110193313/http://www.decode.com/">Decode</a>, and <a href="https://web.archive.org/web/20161110193313/https://www.23andme.com/">23andMe</a>.</p><h3 id="article-level-metric-from-the-plos-perspective"><a href="https://web.archive.org/web/20161110193313/http://ff.im/50Wty">Article level metric from the PLoS perspective</a></h3><p>Peter Binfield</p><p>Peter started the session by addressing the problem: how do you access the worth of impact of journal articles, and at what level do we measure this: journal, research institution, individual researcher? he then went over currently used article level metrics that fall into six areas:</p><ul><li>citation metrics</li><li>usage metrics</li><li>expert rankings (Faculty of 1000)</li><li>Conversations (blogs, media coverage, comments)</li><li>social bookmarking (citeULike etc)</li><li>Other cool stuff (geotagging of authors, etc.)</li></ul><p>In this broader context article-level metrics can be seen as post-publication peer review. Much of this is still ongoing, ResearchBlogging.org is for example bulding an API so that blog posts about PLoS articles can be tracked. In August PLoS One will add information about usage statistics to all their papers. Because usage statistics are currently not widely available, this give a push to the role of usage statistics in evaluating a paper. Future work at PLoS will include better integration with Mendeley, Zotero, CiteULike and similar services. We briefly talked about the relative little use of commenting and tagging at PLoS One, and some people in the audience felt that we need automated tools (e.g. by looking at what papers are bookmarked or stored in Mendeley) rather than user generated content.</p><h3 id="spinning-science-the-good-and-the-bad-of-media-sensationalism"><a href="https://web.archive.org/web/20161110193313/http://ff.im/51cld">Spinning Science: The good and the bad of media sensationalism</a></h3><p><em>Naomi Most, Kiki Sanford</em></p><p>Naomi and Kiki started the session by putting up the provocative hypothesis that sensationalism is the way to report science. They cite Edwin Slosson who in 1921 said <em>Our best plan is probably to try to crowd out falsehood by truth and to present scientific information in a way that will be at least as attractive as the misinformation that now holds the field.</em> They point out that there are different realms of science communication, including</p><ul><li>science for scientists</li><li>science for scientists working in different fields</li><li>science for the educated public</li><li>science for students, and more</li></ul><p>We probably need more translators of science, rather than more people doing science. We are already overwhelmed with the massive amount of science created today. We had a longer discussion about the misunderstandings between scientists and their institution's PR people. Part of the problem is that scientists are not automatically good communicators. Should they all become good communicators? This is probably not possible and not required. We then talked about science rock stars and had the best quote of the meeting:</p><blockquote><em>Why do kids look up more to basketball players rather than scientists? They are taller.</em> (Jim Hardy)</blockquote><h3 id="scientific-publishing-in-5-10-years"><a href="https://web.archive.org/web/20161110193313/http://ff.im/51jI5">Scientific Publishing in 5-10 years</a></h3><p>Peter Binfield</p><p>This was a fun session of what we think science publishing will look like in the intermediate future. Peter asked the following questions:</p><ul><li>Do current publishers exist?</li><li>Does the journal exist as a package?</li><li>Does the article exist?</li><li>What business models dominate?</li><li>What new technical features do we seriously expect?</li><li>What new modes of scholarly communication may gain wide acceptance?</li></ul><p>Other questions that were asked by the audience include:</p><ul><li>What money exists in 5-10 years?</li><li>Who is doing the actual work of publishing?</li><li>What are the customers?</li></ul><p>We went through most of these points. We discussed a possible business model similar to iTunes where the per-article charges would be much lower than today. In the future the role of the librarian will change. It will be more about information, and less about journal subscriptions. We talked about how the process of producing and publishing a paper will become cheaper by better tools, and by using the <a href="https://web.archive.org/web/20161110193313/http://dtd.nlm.nih.gov/">NLM-DTD</a> standard for submission. Articles in the future will not contain only text and figures, and we had a longer discussion on how digital media can be preserved in the long run, and that at some point we will not be able to keep all our data because of the dramatic increase in the carbon footprint required.</p><h3 id="efficiency-and-incentives-in-research-how-to-bend-the-internet-to-scientists"><a href="https://web.archive.org/web/20161110193313/http://ff.im/51tx6">Efficiency and incentives in research – How to bend the internet to scientists</a></h3><p>Cameron Neylon, Jason Hoyt, Duncan Hull</p><p>The last session was really three separate little presentations by the three speakers. Cameron started with a presentation <a href="https://web.archive.org/web/20161110193313/http://www.slideshare.net/CameronNeylon/nesta-science-in-society">he recently gave at NESTA</a> (Science in Society). One question he asked was how we maximise the efficiency in generating impact for our research? Jason Hoyt gave a good introduction to <a href="https://web.archive.org/web/20161110193313/http://www.mendeley.com/">Mendeley</a> and used this as a basis of what social tools for scientists should look like. One important point: these tools should work with a userbase of 1, rather than needing a critical mass. Duncan Hull talked about Digital Identity on the Web, something <a href="https://web.archive.org/web/20161110193313/http://duncan.hull.name/2009/06/02/who-are-you/">he has talked about before</a>. He pointed out that many papers in Google Scholar are by “forgotten password”, “already registered”, etc. OpenID is a solution to some of the problems of author identity, but is complicated to use and not secure enough for some specific scientific applications. Peter Binfield explained that an author identifier would be really helpful for PLoS, and that their databases currently can't identify an author that has for example changed institutions. We had a long discussion about potential benefits of researcher identifiers, and what is needed to really get the ball rolling.</p><p>After the final session I went to the pub with a few people (really strange to do that in Palo Alto), and after having some food hopped on the bus that brought me back to my hotel.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Recipe: Receiving Journal Table of Contents Automatically]]></title>
            <link>https://blog.martinfenner.org/posts/recipe-receiving-journal-table-of-contents-automatically</link>
            <guid>40f2f605-bc2a-4b8e-9108-b625e19df486</guid>
            <pubDate>Sun, 21 Jun 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[Problem
You want to regularly go through the papers published in the most important
journals in your research field.

Solution
Subscribe to the journal table of contents (TOC) RSS feed. Almost all journals
now provide their TOC as RSS feed that is updated with every new issue. RSS
[https://web.archive.org/web/20150922174136/http://en.wikipedia.org/wiki/RSS] is
a standard web format used to publish frequently updated works. A journal
article RSS feed usually contains one item for every article, e]]></description>
            <content:encoded><![CDATA[<h3 id="problem">Problem</h3><p>You want to regularly go through the papers published in the most important journals in your research field.</p><h3 id="solution">Solution</h3><p>Subscribe to the journal table of contents (TOC) RSS feed. Almost all journals now provide their TOC as RSS feed that is updated with every new issue. <a href="https://web.archive.org/web/20150922174136/http://en.wikipedia.org/wiki/RSS">RSS</a> is a standard web format used to publish frequently updated works. A journal article RSS feed usually contains one item for every article, each with title, authors, abstract and link to the fulltext article. To subscribe to the RSS feed of the journal TOC, look out for the RSS icon</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20150922174136im_/http://network.nature.com/images/feed-icon-14x14.png" class="kg-image" alt></figure><p>at the table of contents page. Links to the RSS feeds of some popular scientific journals are:</p><ul><li><a href="https://web.archive.org/web/20150922174136/http://www.nature.com/nature/current_issue/rss">Nature</a></li><li><a href="https://web.archive.org/web/20150922174136/http://www.sciencemag.org/rss/current.xml">Science</a></li><li><a href="https://web.archive.org/web/20150922174136/http://www.cell.com/rssFeed/Cell/rss.NewIssueAndArticles.xml">Cell</a></li><li><a href="https://web.archive.org/web/20150922174136/http://www.pnas.org/rss/current.xml">Proceedings of the National Academy of Sciences</a></li></ul><p>Although most web browsers (e.g. Internet Explorer 7, Firefox or Safari) will understand RSS feeds (so you can just click on the links provided above), you should use a dedicated RSS reader if you subscribe to more than a few RSS feeds. There are web-based RSS readers (<a href="https://web.archive.org/web/20150922174136/http://www.google.com/reader">Google Reader</a> and <a href="https://web.archive.org/web/20150922174136/http://www.bloglines.com/">Bloglines</a> are popular choices) and dedicated programs for <a href="https://web.archive.org/web/20150922174136/http://www.dmoz.org/Computers/Software/Internet/Clients/WWW/Feed_Readers/">every platform</a> (e.g. <a href="https://web.archive.org/web/20150922174136/http://www.newsgator.com/individuals/feeddemon/default.aspx">FeedDemon</a> for Windows or <a href="https://web.archive.org/web/20150922174136/http://www.newsgator.com/Individuals/NetNewsWire">NetNewsWire</a> for Macintosh).</p><p>Dedicated RSS readers have two important features: they keep track of the journal articles you have already <em><em>read</em></em>, and they allow you to <em><em>mark</em></em> interesting articles for late use: reading the full-text article (online or after printing the PDF), and storing the article in your reference manager of choice.</p><p>RSS readers are also available for mobile devices such as the <em><em>iPhone</em></em> and are great for quickly going through a journal table of contents on the way to work.</p><h3 id="discussion">Discussion</h3><p>Regular reading of journal table of contents in your field (browsing) is still an important way to keep up with the literature, even though the use of online databases to find specific articles (searching) has become more common in recent years.</p><p>Some people prefer to regularly flip through the printed journal when the latest issue arrives. But not only is there a delay between electronic publication and arrival of the printed journal, but most individuals can't afford to personally subscribe to more than a few journals at most. And looking at the printed copy subscribed to by the department or library is often no longer practical to do on a regular basis.</p><p>Receiving the journal TOC by <em><em>email</em></em> is a popular alternative, but has several disadvantages:</p><ul><li>Receiving the TOC by email requires a few extra steps, including providing your email address, and often signing up for a (free) account with the journal</li><li>Organizing the TOC emails with your email program (e.g. moving to appropriate subfolders) requires extra work</li><li>Marking an interesting article for later reading requires extra work, because the TOC is sent in one big email message</li><li>Sharing the TOC with coworkers is more difficult than with RSS feeds</li></ul><p>Because RSS is a universal computer-readable format, receiving the journal TOC can easily be extended. One example would be the integration of the journal RSS feeds into reference managers. <em><em>CiteULike</em></em> has this feature (e.g. the most recent issue of <a href="https://web.archive.org/web/20150922174136/http://www.citeulike.org/journal/nature">Nature</a>), but I hope that more reference managers will do the same in the future.</p><p>Although almost all journals now provide RSS feeds to their TOC, how they do it might differ. Not every journal RSS feed uses the <a href="https://web.archive.org/web/20150922174136/http://www.doi.org/">DOI</a> – now the preferred way to link to a journal article. There are also small differences in what information is provided in the RSS feed.</p><p><em><em>This blog post was inspired by a recent discussion about the <a href="https://web.archive.org/web/20150922174136/http://network.nature.com/people/mfenner/blog/2009/06/14/how-to-close-the-digital-divide-among-scientists">digital divide among scientists</a>.</em></em></p><h3 id="references">References</h3><p>Hammond, T., Hannay, T., &amp; Lund, B. (2004). The Role of RSS in Science Publishing. D-Lib Magazine, 10(12). <a href="https://doi.org/10.1045/december2004-hammond">https://doi.org/10.1045/december2004-hammond</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How to close the digital divide among scientists]]></title>
            <link>https://blog.martinfenner.org/posts/how-to-close-the-digital-divide-among-scientists</link>
            <guid>db427ea3-4874-4758-8d33-a3efe4a417a0</guid>
            <pubDate>Sun, 14 Jun 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[The term digital divide usually describes the troubling gap between those who
use computers and the Internet and those who do not (Wikipedia
[https://web.archive.org/web/20151001173737/http://en.wikipedia.org/wiki/Digital_divide]
). Many if not most scientists are experienced users of computers and the
internet, and use email or public databases such as PubMed on a daily basis. But
few scientists regularly use Web 2.0 tools, which would include both general
tools such as Twitter, FriendFeed or F]]></description>
            <content:encoded><![CDATA[<p>The term <em><em>digital divide</em></em> usually describes <em><em>the troubling gap between those who use computers and the Internet and those who do not</em></em> (<a href="https://web.archive.org/web/20151001173737/http://en.wikipedia.org/wiki/Digital_divide">Wikipedia</a>). Many if not most scientists are experienced users of computers and the internet, and use email or public databases such as <em><em>PubMed</em></em> on a daily basis. But few scientists regularly use Web 2.0 tools, which would include both general tools such as <em><em>Twitter</em></em>, <em><em>FriendFeed</em></em> or <em><em>Facebook</em></em>, as well as tools specifically targeted at scientists (and this would of course include <em><em>Nature Network</em></em>).</p><p>Regular readers of this blog know that I am fascinated by technology, especially if this technology makes it easier to publish scientific papers. And like others I sometimes get carried away (<a href="https://web.archive.org/web/20151001173737/http://network.nature.com/people/mfenner/blog/2009/05/28/google-wave-dont-forget-the-scientists">Google Wave</a> is a good recent example). Even among those scientists open to blogs, wikis, etc., not everybody wants to follow every technology trend. This could simply be because that would take too much time, but most people probably just don't care that much about technology.</p><p>So what can we do about this digital divide among scientists? Science is often very specialized, and sometimes only a few people participate in a discussion about a particular topic. <a href="https://web.archive.org/web/20151001173737/http://radar.oreilly.com/">Tim O'Reilly</a> has coined the term <em><em>alpha geek</em></em> for people that are the first to use new technologies, and there certainly is a place for <em><em>science alpha geeks</em></em>. But Science Online is about science communication, and communication tools that are used by only a handful of people usually don't fulfill their purpose.</p><p>One easy solution would be to simply wait 10-20 years until most senior scientists are <a href="https://web.archive.org/web/20151001173737/http://en.wikipedia.org/wiki/Digital_native">digital natives</a> (those that have grown up with digital technology such as computers, the Internet or mobile phones), but that seems to be an awfully long time for something this important.</p><p>We could build better tools. Good tools simply work and don't need a lot of explanations. For me <a href="https://web.archive.org/web/20151001173737/http://mekentosj.com/papers/">Papers</a> is such a tool, but strictly speaking not really Web 2.0, because it has no collaboration features. <em><em>Google Wave</em></em> could be another example, but only the next few months will tell. What makes a good Web 2.0 tool for scientists? Most importantly, that the tool solves an important everyday problem. Equally important, that there aren't high hurdles in using this tool in terms of cost and learning curve. Another hurdle: some Web 2.0 tools only start to become useful once they have signed up a large enough number of users.</p><p>But we also need to do more to communicate the usefulness of online tools for scientists. The original definition of the digital divide has a negative meaning and everybody probably agrees that we should at least try to overcome this divide. Although there certainly is also a digital divide among scientists, the general perception is probably not that those scientists that are not Web 2.0-savvy are at a disadvantage. We should have a much closer look at the tools that are currently available, define the scenarios where they can be useful, and focus on that. We talk too much about the details, technical or otherwise. One example: most scientists probably want to have an idea of when an online reference manager can be helpful rather than the tools they currently use, rather than discuss the subtle differences between the very similar <a href="https://web.archive.org/web/20151001173737/http://www.citeulike.org/">CiteULike</a>, <a href="https://web.archive.org/web/20151001173737/http://www.connotea.org/">Connotea</a>, and <a href="https://web.archive.org/web/20151001173737/http://www.2collab.com/">2collab</a>. Part of the problem is that people want to make money with their Web 2.0 tools for scientists, but forget that collaboration is more important than competition when the market still has to grow and is currently probably too small for viable business opportunities.</p><p>This makes closing the digital divide among scientists very much a science education exercise, and I think that science librarians should play a central role in this. Not surprisingly, a seminar last week by our local science librarian in our department and <a href="https://web.archive.org/web/20151001173737/http://scienceblogs.com/confessions/2009/06/cool_conferences_mental_overlo.php">a blog post</a> by science librarian John Dupuis (and the <a href="https://web.archive.org/web/20151001173737/http://friendfeed.com/johndupuis/8c6723b0/cool-conferences-mental-overload">FriendFeed</a> discussion around his blog post) were the inspiration for this post (<a href="https://web.archive.org/web/20151001173737/http://friendfeed.com/coturnix/562bc610/wow-sunday-afternoon-and-i-don-t-even-know-on-what">another FriendFeed discussion</a> started by Bora Zivkovic made be write the post today instead of going to bed early).</p><p><em><em>Update 06/15/09:</em></em><br>One good strategy to overcome the digital divide among scientists would be a <em><em>Science 2.0 Cookbook</em></em>. Similar in format to the <a href="https://web.archive.org/web/20151001173737/http://oreilly.com/store/series/cookbooks.csp">O'Reilly Cookbook series</a> for programming problems, the Science 2.0 cookbook would use the format problem/solution/discussion to provide a solution for problems like <em><em>How do I share references with my coworkers in the lab?</em></em> This could be started as a Wiki project.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20151001173737im_/http://www.linkwithin.com/pixel.png" class="kg-image" alt="Related Posts Plugin for WordPress, Blogger..."></figure>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Why do we go to conferences?]]></title>
            <link>https://blog.martinfenner.org/posts/why-do-we-go-to-conferences</link>
            <guid>4fa848bd-8e3b-4407-9b6b-5b599864e2c2</guid>
            <pubDate>Sun, 07 Jun 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[I just returned from the American Society of Clinical Oncology
[https://web.archive.org/web/20151002134927/http://www.asco.org/ascov2/Meetings/ASCO+Annual+Meeting] 
(ASCO) meeting in Orlando, with approximately 30.000 participants one of the
largest oncology conferences. Like other conferences of this size, the
experience can be overwhelming, but thankfully the organizers are getting better
every year in using technology that helps in finding the most interesting
sessions. Most sessions are made]]></description>
            <content:encoded><![CDATA[<p>I just returned from the <a href="https://web.archive.org/web/20151002134927/http://www.asco.org/ascov2/Meetings/ASCO+Annual+Meeting">American Society of Clinical Oncology</a> (ASCO) meeting in Orlando, with approximately 30.000 participants one of the largest oncology conferences. Like other conferences of this size, the experience can be overwhelming, but thankfully the organizers are getting better every year in using technology that helps in finding the most interesting sessions. Most sessions are made available as video podcasts or <a href="https://web.archive.org/web/20151002134927/http://www.asco.org/ASCOv2/MultiMedia/Virtual+Meeting">online presentations</a>. There is currently still a delay of 24 hours, but I wouldn't be surprised to see the sessions streamed live as video over the internet in coming years. This year's ASCO also had the first official Twitter meet-up, although there still was relatively little <a href="https://web.archive.org/web/20151002134927/http://twitter.com/asco">Twitter activity</a> compared to other conferences.</p><p>Last week <a href="https://web.archive.org/web/20151002134927/http://www.scienceonlinelondon.org/">we announced</a> <em><em>Science Online London</em></em>, the follow-up conference of last year's <a href="https://web.archive.org/web/20151002134927/http://network.nature.com/groups/sciblog2008/forum/topics">Science Blogging 2008: London</a>, to take place August 22 at the <a href="https://web.archive.org/web/20151002134927/http://www.rigb.org/registrationControl?action=home">Royal Institution</a>. I am helping to organize the conference, and I'm finding myself in the middle of discussions about session topics, speakers and the right format to present and discuss science blogging, wikis, and other science-related activities happening online.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20151002134927im_/http://www.mendeley.com/blog/wp-content/uploads/2009/05/solologo.gif" class="kg-image" alt></figure><p>These two events started me thinking about the reasons I go to conferences. After all, traveling to conferences is not only expensive, but can also be exhausting, and too much airline travel is certainly not good for the environment (places like <a href="https://web.archive.org/web/20151002134927/http://www.dopplr.com/">Dopplr</a> can calculate your carbon profile). Here are a few points that I think make a conference a good conference worth attending in person:</p><h3 id="conferences-should-present-new-and-exciting-information-which-can-not-be-presented-differently">Conferences should present new and exciting information which can not be presented differently</h3><p>Oral presentations at conferences are usually the first public presentation of interesting research findings before they appear as published paper a little (or much) later. What we don't want to see is the presentation of the same old data that we have already seen the year before. Good educational sessions and keynote lectures find informative or entertaining ways to present their information, and again should not be simply a repeat performance (unless the audience is completely different).</p><p>Most conferences encourage the presentation of unpublished work, but speakers are often careful in doing so for a variety of reasons, e.g. fear of getting scooped, fear of problems with journal submissions or fear of problems with patentable work. This fear can make conference presentations rather boring, as presenters might hold back with the real exiting stuff until these results are at least accepted for publication.</p><p><a href="https://web.archive.org/web/20151002134927/http://meetings.cshl.edu/">Cold Spring Harbor Laboratory</a> try to solve this problem by policies that essentially make their meetings non-public. During the last week we have seen an intensive discussion (e.g. <a href="https://web.archive.org/web/20151002134927/http://scienceblogs.com/geneticfuture/2009/06/on_the_challenges_of_conferenc.php">On the challenges of conference blogging</a>) whether or not the sessions in a CSHL meeting can be communicated publicly by participating scientists via blogs or Twitter. I think that there is nothing wrong with small conferences being non-public, and that the same rules should apply to science bloggers as they do apply to journalists. But the conference organizers should clearly state their policies regarding blogging (including Twitter, FriendFeed and other microblogging tools).</p><h3 id="conferences-should-enable-as-much-active-participation-of-as-many-participants-as-possible">Conferences should enable as much active participation of as many participants as possible</h3><p>If we just want to listen to a presentation, we could do that without going to a conference, thanks to video streaming, <a href="https://web.archive.org/web/20151002134927/http://www.slideshare.net/">SlideShare</a> and other ways of presenting online. Sometimes a paper is even published on the same the day as the plenary session, as happened recently with the <a href="https://web.archive.org/web/20151002134927/http://network.nature.com/people/mfenner/blog/2008/11/23/what-are-the-right-numbers-for-jupiter">JUPITER</a> trial.</p><p>Smaller sessions that leave enough room for discussions, <a href="https://web.archive.org/web/20151002134927/http://en.wikipedia.org/wiki/Unconference">unconferences</a> and poster sessions are all good formats to encourage active participation. It should be a goal at least for smaller conferences that every attendee has had a chance for active participation in one way or another. But active participation in a session is much more focussed than disussions in coffee breaks between sessions or afterwards in the bar.</p><h3 id="conferences-should-facilitate-personal-networking">Conferences should facilitate personal networking</h3><p>Meeting someone in person is very different from interacting online via email, Twitter or social network. For many people this is the real reason to go to a conference. Or as Henry Gee puts it (slightly out of context), “the most important part is to <a href="https://web.archive.org/web/20151002134927/http://network.nature.com/people/U9556F6A5/blog/2007/08/06/someday-all-conferences-will-be-like-this">hang around in bars</a>.” Smaller conferences (e.g. 100-150 people), enough time for coffee and lunch breaks between sessions, and social activities around conferences (from <a href="https://web.archive.org/web/20151002134927/http://network.nature.com/groups/sciblog2008/forum/topics/1959">science tours</a> to skiing) all facilitate networking.</p><p>I'm looking forward to go to a very special conference in five weeks: <a href="https://web.archive.org/web/20151002134927/http://www.nature.com/nature/meetings/scifoo/index.html">Science Foo Camp</a>. And let's see whether we can put together an interesting <em><em>Science Online London</em></em> conference. Please suggest and discuss session topics and speakers in the <a href="https://web.archive.org/web/20151002134927/http://network.nature.com/groups/solondon/forum/topics">Nature Network Forum</a>, <a href="https://web.archive.org/web/20151002134927/http://friendfeed.com/solondon">FriendFeed group</a> or via <a href="https://web.archive.org/web/20151002134927/mailto:topics%40scienceonlinelondon.org">email</a> until June 19. We are still looking for interesting session ideas, speaker suggestions and other suggestions to make this an exciting conference.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[OAI-PMH: Interview with Tony Hammond]]></title>
            <link>https://blog.martinfenner.org/posts/oai-pmh-interview-with-tony-hammond</link>
            <guid>1e530d28-c683-415d-a83f-e602a9645290</guid>
            <pubDate>Mon, 25 May 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[Most of us find, store and sometimes read scientific papers electronically.
Although abstracts and full-text papers are usually available as web pages in
HTML format, PDF is clearly the preferred format for storing and printing
papers.

But publishing scientific papers in electronic form obviously requires more than
providing the content in HTML or PDF format. We want to find the papers we are
interested in on the journal homepage or in a digital library (e.g. PubMed
[https://web.archive.org/web]]></description>
            <content:encoded><![CDATA[<p>Most of us find, store and sometimes read scientific papers electronically. Although abstracts and full-text papers are usually available as web pages in HTML format, PDF is clearly the preferred format for storing and printing papers.</p><p>But publishing scientific papers in electronic form obviously requires more than providing the content in HTML or PDF format. We want to find the papers we are interested in on the journal homepage or in a digital library (e.g. <a href="https://web.archive.org/web/20151003101404/http://www.pubmed.gov/">PubMed</a>), and for this we need metadata about the paper. The metadata could simply be an digital object identifier (<a href="https://web.archive.org/web/20151003101404/http://www.doi.org/">DOI</a>), but the metadata could also contain important information required to find a paper in a search strategy (e.g. authors, title, publication date or keywords).</p><p>As <a href="https://web.archive.org/web/20151003101404/http://network.nature.com/people/duncan/profile">Duncan Hull</a> <em><em>et al.</em></em> noted in a <em><em>PLoS Computational Biology</em></em> paper last year (<a href="https://web.archive.org/web/20151003101404/http://dx.doi.org/10.1371/journal.pcbi.1000204">Defrosting the digital library: bibliographic tools for the next generation web</a>), metadata are often disconnected from the data, and there are no universally agreed standards to represent these metadata.</p><p>But why should we as scientists care about the technologies used to publish and distribute a paper? We shouldn't forget that these technologies could allow new and innovative ways to find and read scientific papers. One simple example: storing the metadata in the PDF file (using <a href="https://web.archive.org/web/20151003101404/http://blogs.nature.com/wp/nascent/2008/12/xmp_labelling_for_nature.html">XMP</a>) could make it much easier to import a large collection of PDF files into a reference manager.</p><p>One such initiative to provide the metadata of a scientific paper is OAI-PMH. I asked Tony Hammond from Nature.com a few questions about this newly supported protocol, as well as some more general questions about metadata provided by the Nature Publishing Group journals.</p><h3 id="1-can-you-describe-what-oai-pmh-is-and-does">1. Can you describe what OAI-PMH is and does?</h3><p>Well, we'll first need to unpack that double acronym. OAI-PMH is the <a href="https://web.archive.org/web/20151003101404/http://www.openarchives.org/pmh/">Protocol for Metadata Harvesting</a> which comes from the <a href="https://web.archive.org/web/20151003101404/http://www.openarchives.org/">Open Archives Initiative</a>. It's traditionally been known simply as OAI, although these days it's more properly referred to in full as OAI-PMH in view of the arrival of a new sibling protocol: OAI-ORE, which stands for <a href="https://web.archive.org/web/20151003101404/http://www.openarchives.org/ore/">Object Reuse and Exchange</a>. The two protocols are complementary: OAI-PMH deals with metadata harvesting, while OAI-ORE is concerned with content aggregation and compound digital objects.</p><p>In brief, OAI-PMH provides an interoperability framework for networked repositories to exchange metadata records on their holdings. Such metadata typically includes bibliographic-type descriptions of repository items, such as title, authors and other identifying information. At a technical level OAI-PMH provides a very simple Web-based API for querying a repository by item, by set of items or by date range with data records being returned in an XML format which can be validated by a W3C XML Schema. A basic metadata format using <a href="https://web.archive.org/web/20151003101404/http://dublincore.org/">Dublin Core</a> is available from all OAI-PMH implementations, while other richer community-specific metadata formats are encouraged for a fuller semantic exchange.</p><h3 id="2-what-is-prism-aggregator-message-pam-">2. What is PRISM Aggregator Message (PAM)?</h3><p>I posted an entry <a href="https://web.archive.org/web/20151003101404/http://www.crossref.org/CrossTech/2009/05/post_2.html">PRISM Aggregator Message</a> on CrossTech recently which delved into the <a href="https://web.archive.org/web/20151003101404/http://www.idealliance.org/industry_resources/intelligent_content_informed_workflow/about_the_prism_aggregator_message">PRISM Aggregator Message</a> (PAM) format and provided some specific details and also reasons why we chose to use it. Basically our choice of PAM stems from our earlier work with RSS and <a href="https://web.archive.org/web/20151003101404/http://prismstandard.org/">PRISM</a>.</p><p>Our first foray into semantic descriptions was with RSS feeds. While still at Elsevier I had collaborated with Timo Hannay and Ben Lund at Nature Publishing Group and wrote up a piece for <em><em>XML.com</em></em> entitled <a href="https://web.archive.org/web/20151003101404/http://www.xml.com/lpt/a/1252">Why Choose RSS 1.0?</a> which described how journal RSS feeds could be enhanced with PRISM metadata. Specifically that piece reviewed the different RSS strains and came out strongly in favour of RSS 1.0 which being an RDF application was truly extensible and could accommodate new vocabularies. The identification of PRISM as a useful vocabulary was largely based on noting the adjacency of two RDF use cases in an early edition of the W3C RDF Primer: RSS and PRISM. I had been reading up on RSS and RDF and I suddenly thought “Bingo!”, we can add in PRISM terms to the RSS feeds since PRISM is a very handy supplement to Dublin Core and provides support for enumerated fields such as volume, issue, and page number. Another advantage for publishers was that PRISM is a simple term set and did not require any specialist library knowledge. A later paper by the same authors <a href="https://web.archive.org/web/20151003101404/http://www.dlib.org/dlib/december04/hammond/12hammond.html">The Role of RSS in Science Publishing</a> published with <em><em>D-Lib Magazine</em></em> made a more in-depth review of RSS in scholarly publishing and concluded again that RSS 1.0 with its transparent support for new vocabularies such as PRISM was the right way to go.</p><p>We wanted to provide this standard article description that we were shipping with RSS also with OAI-PMH records. Unfortunately we could not simply move over the RDF properties into an OAI-PMH delivered RDF/XML payload. The problem with OAI-PMH is that it requires a record to be constrained by a W3C XML Schema. This is where <a href="https://web.archive.org/web/20151003101404/http://www.idealliance.org/industry_resources/intelligent_content_informed_workflow/about_the_prism_aggregator_message">PRISM Aggregator Message</a> (PAM) comes in.</p><p>PAM is an application of PRISM and defines a W3C XML Schema for XML content aggregators to use. The message is simply structured as a sequence of one or more articles each with a body section for XHTML content and a head section with PRISM metadata. The body sections are optional so PAM can be used exclusively as a simple metadata packaging format as we have chosen to implement for OAI-PMH.</p><h3 id="3-how-does-pam-relate-to-the-nlm-dtd">3. How does PAM relate to the NLM DTD?</h3><p>Well, both are schemas for content. This was my blind spot with regard to PAM for many years. Our focus was on metadata exchange. PRISM was a vocabulary for metadata, or rather a set of closely related vocabularies. No problem. And yet there was this thing called PAM which was busying itself with content. And we had no interest in that as we had our own Nature-specific DTD. But the PRISM folks were very enthusiastic – so clearly there was a need.</p><p>It only dawned on me relatively slowly that the PAM DTD (also available in W3C XML Schema form since PRISM 2.0) was the correspondent of the NLM DTD in the scholarly world. It provides an interchange format for content elements. So the real impetus behind the development of the PRISM metadata vocabularies was as a direct support to XML content exchange although content is not actually required to be present and the metadata alone can be used in different applications.</p><h3 id="4-what-are-typical-use-cases-for-oai-pmh">4. What are typical use cases for OAI-PMH?</h3><p>The main use of OAI-PMH is to sync, or perhaps a better term would be align, the holdings descriptions of digital repository collections, where the word “repository” should be understood in its broadest context. The protocol exposes a machine interface for the robot harvesting of records from a data provider which a service provider will build upon to provide value add services. But this machine interface can sometimes also be accessed through a user interface (such as that provided by the Nature OAI-PMH service) which adds a browser onto the repository records.</p><p>For additional uses of OAI-PMH a good start point is the paper by Herbert Van de Sompel, Jeff Young and Thom Hickey in <em><em>D-Lib Magazine</em></em> on <a href="https://web.archive.org/web/20151003101404/http://www.dlib.org/dlib/july03/young/07young.html">Using the OAI-PMH … Differently</a>. This paper notes that the metadata formats used by OAI-PMH are any that can validated by a W3C XML Schema and that therefore OAI-PMH is nothing less than “a medium for incremental, date-sensitive exchange of any form of semi-structured data”. OAI-PMH clearly has legs.</p><h3 id="5-why-should-researchers-care-about-oai-pmh">5. Why should researchers care about OAI-PMH?</h3><p>I'm not sure that researchers should want to have any specific knowledge of OAI-PMH. Bear in mind that at heart this is an infrastructural technology which exists down in the data pipes and service conduits and is analogous to the fibre or radio channels that deliver broadband services to users.</p><p>Applications that make use of the syndicated metadata records that OAI-PMH provides for, however, are another matter. Those are very definitely things that researchers will care much about. It is difficult to distil those consumer applications into any specific categories as the services platform that OAI-PMH supports is very broad, ranging from general to domain-specific descriptions to full text records, and beyond.</p><h3 id="6-what-are-the-different-data-formats-that-descriptions-of-nature-com-articles-are-provided-how-do-they-differ">6. What are the different data formats that descriptions of Nature.com articles are provided? How do they differ?</h3><p>At Nature Publishing Group we are working towards a common delivery architecture for our metadata. We are defining a core set of properties and making descriptions built from that property set available across multiple channels. Currently we are focussing on the basic bibliographic record which supports reference linking so that links can be made back to the platform. But once the channels are established they can be readily amplified to carry additional properties using the various channels' native packaging mechanisms. Metadata channels include both standalone services (e.g. RSS, OAI-PMH, etc.) as well as content objects themselves (HTML, PDF, etc.).</p><p>On the services side we began almost six years ago with RSS feeds, which being RSS 1.0 are full RDF/XML documents. The open data model implicit in RDF means that we can readily add new properties into our feeds. A case in point is the <a href="https://web.archive.org/web/20151003101404/http://www.iupac.org/inchi/">InChI</a> identifier for chemical substances which we are currently working towards adding into our RSS feeds for <em><em>Nature Chemistry</em></em>, similarly to what the Royal Society of Chemistry has done earlier with their <a href="https://web.archive.org/web/20151003101404/http://www.rsc.org/Publishing/Journals/ProjectProspect/index.asp">Project Prospect</a>.</p><p>We have just released our OAI-PMH service which provides the same basic property set as RSS but in PAM format – see the post <a href="https://web.archive.org/web/20151003101404/http://blogs.nature.com/wp/nascent/2009/05/a_catalog_for_naturecom.html">A Catalog for Nature.com</a>. As alluded to above OAI-PMH comes from an earlier generation of protocols that put more rather emphasis on validation (or packaging the elements) than on data modelling (or relating the elements). That is, while the records served by the OAI-PMH server are validatable using W3C XML Schema the properties they contain are not directly reusable within an open, cross-application context such as is provided for by RDF. We do though have a simple stylesheet that can generate RDF/XML from these OAI-PMH records.</p><p>On the content side we added in the same properties to our HTML using META tags a year ago now – see the post <a href="https://web.archive.org/web/20151003101404/http://blogs.nature.com/wp/nascent/2008/05/naturecom_adds_metadata.html">Nature.com adds metadata</a>. These properties are easily extractable by tools as simple key/value pairs. While this metadata is not directly representable as RDF it can be readily generated. We are anyway moving to make this property set more accessible by adding in <a href="https://web.archive.org/web/20151003101404/http://googlewebmastercentral.blogspot.com/2009/05/introducing-rich-snippets.html">RDFa</a> which has now gone mainstream following Google's recent <a href="https://web.archive.org/web/20151003101404/http://googlewebmastercentral.blogspot.com/2009/05/introducing-rich-snippets.html">announcement of rich snippets</a> and their <a href="https://web.archive.org/web/20151003101404/http://google.com/support/webmasters/bin/answer.py?hl=en&amp;answer=146898">support for RDFa</a>.</p><p>We also began at the end of that year a programme to embed <a href="https://web.archive.org/web/20151003101404/http://www.adobe.com/products/xmp/">XMP</a> packets into all of our newly published PDF files, again using the same basic property set – see the post <a href="https://web.archive.org/web/20151003101404/http://blogs.nature.com/wp/nascent/2008/12/xmp_labelling_for_nature.html">XMP Labelling for Nature</a>. The XMP packets are essentially simple RDF/XML documents.</p><p>You can begin to see where all this is going. We are aiming to make all our descriptions conformant to a common data model, i.e. to RDF. That way, regardless of the distribution channel used the data delivered down that pipe can be merged into the common semantic graph.</p><h3 id="7-what-tools-can-researchers-use-to-retrieve-these-descriptions">7. What tools can researchers use to retrieve these descriptions?</h3><p>Many of these channels are amenable to repurposing. The metadata they carry can be consumed within application-specific contexts, or it can be extracted from the channel medium for use in a wider generic context. Consider, for example, an RSS feed which can be used directly by a desktop or Web-based RSS reader. But it can also be mined for its metadata content, trivially in this case since the medium is already RDF/XML. Or consider again the metadata within an XMP packet in a PDF document which can be read by a viewer application (e.g. Adobe Acrobat) and presented to the user in a “Document Properties” display. But it can also be extracted simply by locating the XMP packet and reading the single XML child element which is itself a full RDF/XML document.</p><p>So one could say there are two classes of tools, those that operate at an application or specific layer and those that operate at a more generic layer, albeit with some preprocessing steps to unpack the metadata.</p><p>I should really expand here on OAI-PMH specifically since this is new for us. The primary means of interacting with an OAI-PMH server is via its service endpoint. Obviously to manage pagination seamlessly (the resumption token provides a cursor into the result record set) a library or tool is of enormous assistance. The OAI website provides a reasonably full <a href="https://web.archive.org/web/20151003101404/http://www.openarchives.org/pmh/tools/tools.php">listing of PMH tools</a> available.</p><p>Our OAI-PMH server implements Jeff Young's <a href="https://web.archive.org/web/20151003101404/http://www.oclc.org/research/software/oai/cat.htm">OAICat</a> which is a Java servlet webapp providing a repository framework which also comes with a <a href="https://web.archive.org/web/20151003101404/http://www.nature.com/oai/">forms interface</a> for testing. This interface is especially useful for occasional use, e.g. a single “<em><em>GetRecord</em></em>“:http://www.nature.com/oai/html/getRecord.html or call to “<em><em>ListRecords</em></em>“:http://www.nature.com/oai/html/listRecords.html (or “<em><em>ListIdentifiers</em></em>“:http://www.nature.com/oai/html/listIdentifiers.html), although repetitive calls to <em><em>ListRecords</em></em> (or <em><em>ListIdentifiers</em></em>) would quickly become tedious.</p><p>For harvesting we have used for testing purposes the <a href="https://web.archive.org/web/20151003101404/http://oai.rubyforge.org/">ruby-oai</a> gem for Ruby by Ed Summers and Will Groppe which includes a library and simple client which can also be run interactively as a shell. Note that this gem is not listed on the OAI-PMH tools page.</p><p>We have also made use of the open-source Java client <a href="https://web.archive.org/web/20151003101404/http://www.oclc.org/research/software/oai/harvester2.htm">OAIHarvester2</a>, again by Jeff Young of OCLC. We used this for test harvesting of our full record collection as it was a robust implementation and we had earlier run into some problems with the Ruby app as it is not as finished as it might be, although it remains very configurable and easy to use. Our intention is to proceed with the Ruby app for incremental harvesting. We're aiming to become consumers of our own services for quality control purposes.</p><h3 id="8-what-are-your-responsibilities-at-nature-com">8. What are your responsibilities at Nature.com?</h3><p>I work within the Platform Technologies group on infrastructural projects supporting discovery and access across the nature.com platform, especially those that are built upon open technologies. My job handle is <em><em>Application Architect</em></em> although we're working on deconstructing that. I don't have line responsibilities but do supervise the development of our new interfaces.</p><p>We are corralling these various standards – some come from the wider Web world, some from the digital library community, some come from industry, and some are closer to home – under the general moniker of <a href="https://web.archive.org/web/20151003101404/http://www.nature.com/libraries/public_interfaces/index.html">Public Interfaces</a>. We also have a new documentation centre for this which is located on our <a href="https://web.archive.org/web/20151003101404/http://www.nature.com/libraries/index.html">Librarian Gateway</a>.</p><p>I do maintain an active presence with various external bodies, not unsurprisingly given that the focus of my work is on defining and building interfaces. I have been from the beginning very involved in <a href="https://web.archive.org/web/20151003101404/http://www.crossref.org/">CrossRef</a> and the development of <a href="https://web.archive.org/web/20151003101404/http://www.doi.org/">DOI</a> and related technologies. Most recently I have worked with a CrossRef WG to draw up a best practices document for scholarly publishers and RSS, and we are now starting work on a companion document for embedding metadata. I am also on the <a href="https://web.archive.org/web/20151003101404/http://www.idealliance.org/industry_resources/intelligent_content_informed_workflow/prism/membership">PRISM WG</a> and a regular contributor.</p><p>Other activities I'm involved with include being a member of the <a href="https://web.archive.org/web/20151003101404/http://www.loc.gov/standards/sru/">SRU</a> Editorial Board and of the eJournal Joint Technical Panel on <a href="https://web.archive.org/web/20151003101404/http://www.bl.uk/aboutus/stratpolprog/legaldep/">Legal Deposit</a> here in the UK. I have previously been a member of the OpenURL Standard Committe that developed <a href="https://web.archive.org/web/20151003101404/http://www.niso.org/standards/z39-88-2004/">ANSI/NISO Z39.88</a>, and worked as a member of the <a href="https://web.archive.org/web/20151003101404/http://www.openarchives.org/ore/">OAI-ORE</a> TC and the JISC <a href="https://web.archive.org/web/20151003101404/http://www.jisc.ac.uk/aboutus/committees/workinggroups/palsmetadatagroup.aspx">PALS Metadata and Interoperability WG</a>.</p><h3 id="9-what-did-you-do-before-starting-to-work-for-nature-com">9. What did you do before starting to work for Nature.com?</h3><p>Before working with Nature I was with Elsevier (2001-2004) as part of their Advanced Technology Group, and prior to that I worked with Academic Press (1997-2001) as Head of the Online Resource Activity. With Academic Press I was part of Electronic Publishing team that managed IDEAL – one of the first successful large journals platforms that applied consortial site licenses.</p><p>My previous experiences included a long-ish stint (1986-1995) with NATO SACLANTCEN in La Spezia, Italy, as Scientific Editor and subsequently as Head of the Information Branch. The facility was named SACLANT ASW Research Centre when I joined and had been transmuted into NATO Undersea Research Centre by the time I left (the anti-submarine warfare aspect deftly tidied away). Same mission though, to work on basic research (from oceanography to operational research) that would aid the NATO nations in their submarine detection programmes. (It now seems to be commuted to NURC alone.)</p><p>Prior to that I worked (1982-1985) as Assistant to the Editors for the North-Holland Publishing Company journal <em><em>Nuclear Physics A</em></em> based in Copenhagen, Denmark, and previous to that as a Research Associate in the Space Physics Laboratory at the University of Kent at Canterbury, UK where I collaborated in building a micrometeoroid sensor experiment which was deployed by the Space Shuttle.</p><h3 id="10-do-you-want-to-talk-about-future-plans-for-metadata-at-nature-com">10. Do you want to talk about future plans for metadata at Nature.com?</h3><p>Sure. I guess I should separate out metadata management per se from metadata delivery and discovery. As a company we have an active programme underway to review wholesale our various ontologies and vocabularies in order to coordinate and streamline them. We also have ongoing initiatives to add in text mining to our production workflow and to address the entity extraction problem. Having better and richer vocabularies and new terms is one step. The next step is how to communicate that value in an open and structured manner to consumer applications.</p><p>My particular focus is on delivery channels. As I mentioned earlier we are working towards providing a notion of public interfaces: a set of open interfaces for delivering standard object descriptions. Complementing the OAI-PMH service which provides a catalog for nature.com we are also currently working on an SRU (<a href="https://web.archive.org/web/20151003101404/http://www.loc.gov/standards/sru/">Search and Retrieve by URL</a>) service to support structured searching. And that would also be accessible through simple <a href="https://web.archive.org/web/20151003101404/http://www.opensearch.org/">OpenSearch</a> conventions.</p><p>We will be extending our page markup to include <a href="https://web.archive.org/web/20151003101404/http://www.w3.org/TR/rdfa-syntax/">RDFa</a> which will not only provide metadata in RDF format but will also localize those descriptions to the content fragment so that cut-and-paste operations will scoop up any descriptive markup along with the actual content.</p><p>We are now close to completing our support across our full title range for <a href="https://web.archive.org/web/20151003101404/http://www.adobe.com/products/xmp/">XMP</a> in PDFs. A related development will be to embed XMP into our images (JPEGs and GIFs) so that all of our main resources then become self-describing. It is a wonder that we have gotten thus far in online publishing sending out content entities which are not unambiguously labelled.</p><p>And beyond this all lie the promises and challenges of the Semantic Web.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20151003101404im_/http://www.linkwithin.com/pixel.png" class="kg-image" alt="Related Posts Plugin for WordPress, Blogger..."></figure>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What is the right reference manager for you?]]></title>
            <link>https://blog.martinfenner.org/posts/what-is-the-right-reference-manager-for-you</link>
            <guid>21735815-20ec-42c2-afec-9d8d7fc28e37</guid>
            <pubDate>Sun, 17 May 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[Reference managers are essential tools to read and write scholarly papers. In
the last few years we have seen both a number of new reference managers (most of
them web-based), but also a trend for the established reference managers to gain
social networking features. More choice is great, but it also creates confusion
about the right tool to use. I have talked about reference managers before
[https://web.archive.org/web/20151002030110/http://network.nature.com/people/mfenner/blog/2009/03/15/refe]]></description>
            <content:encoded><![CDATA[<p>Reference managers are essential tools to read and write scholarly papers. In the last few years we have seen both a number of new reference managers (most of them web-based), but also a trend for the established reference managers to gain social networking features. More choice is great, but it also creates confusion about the right tool to use. I have talked about reference managers <a href="https://web.archive.org/web/20151002030110/http://network.nature.com/people/mfenner/blog/2009/03/15/reference-manager-overview">before</a>, but in this slideshow I look at the features that I find important.</p><p>And there are at least two features that I like, but haven't really seen implemented in a reference manager:</p><ul><li><em><em>Integration of an RSS reader for journal table of contents (TOC)</em></em>. Currently I use a standard RSS reader, and it requires too many steps to get interesting references from a TOC into my reference manager.</li><li><em><em>Tracking the post-publication discussion</em></em>. I want my reference manager to link to the papers that cite a particular reference (I currently use <a href="https://web.archive.org/web/20151002030110/http://www.scopus.com/">Scopus</a> for that) and link to <a href="https://web.archive.org/web/20151002030110/http://network.nature.com/people/mfenner/blog/2009/04/28/faculty-of-1000-interview-with-richard-grant">Faculty of 1000</a> or <a href="https://web.archive.org/web/20151002030110/http://researchblogging.org/">ResearchBlogging.org</a> comments on that paper.</li></ul><p>In the last slide I wonder whether there is a) one perfect reference manager, b) one perfect reference manager for my particular needs, or c) I will always need more than one reference manager and have to move references back and forth between them. Currently I'm at c), using mostly Papers, Endnote and Connotea. But Mendeley, Zotero, Refworks and Endnote are moving in a direction where they try to cover all requirements.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[eXtyles: Interview with Elizabeth Blake and Bruce Rosenblum]]></title>
            <link>https://blog.martinfenner.org/posts/xtyles-interview-with-elizabeth-blake-and-bruce-rosenblum</link>
            <guid>6e8c548a-3d24-4c2b-a4c3-3bf1974b11b8</guid>
            <pubDate>Fri, 01 May 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[Scientific papers are submitted to a journal as word processor files, usually in
Microsoft Word format. After the paper is accepted for publication, the journal
takes the manuscript and translates the text into a format that is better suited
for publication online and/or in print. XML and the NLM DTD
[https://web.archive.org/web/20151002164418/http://dtd.nlm.nih.gov/] – a set of
XML schema modules – have evolved as the standard data format for this purpose.
Files in the NLM DTD format can in tur]]></description>
            <content:encoded><![CDATA[<p>Scientific papers are submitted to a journal as word processor files, usually in Microsoft Word format. After the paper is accepted for publication, the journal takes the manuscript and translates the text into a format that is better suited for publication online and/or in print. XML and the <a href="https://web.archive.org/web/20151002164418/http://dtd.nlm.nih.gov/">NLM DTD</a> – a set of XML schema modules – have evolved as the standard data format for this purpose. Files in the NLM DTD format can in turn be translated into HTML and/or PDF for publication. The NLM DTD format is also used to transfer journal articles from publishers to archives (e.g. <a href="https://web.archive.org/web/20151002164418/http://www.pubmedcentral.nih.gov/">PubMed Central</a>) and for long-term archiving.</p><p><a href="https://web.archive.org/web/20151002164418/http://www.inera.com/extylesinfo.shtml">eXtyles</a> is a tool that facilitate the translation between these different document formats, and in the process also help to clean up broken references and other errors in the manscript. As paper authors usually don’t see much of what happens to their manuscript after submission, I thought I’d ask Elizabeth Blake and Bruce Rosenblum from <a href="https://web.archive.org/web/20151002164418/http://www.inera.com/">Inera</a> (the company behind eXtyles) a few questions.</p><h3 id="1-can-you-describe-what-extyles-is-and-does">1. Can you describe what eXtyles is and does?</h3><p>Elizabeth Blake: <a href="https://web.archive.org/web/20151002164418/http://www.inera.com/">Inera</a> offers several <a href="https://web.archive.org/web/20151002164418/http://www.inera.com/extylesinfo.shtml">eXtyles products</a>, all of which are designed to clean up, structure, validate, and export scholarly content. The desktop version of eXtyles is the most widely used product; it’s a plug-in to Microsoft Word that is customized according to the editorial style and production requirements of each publisher that uses it. eXtyles:</p><ul><li>collects and exports manuscript metadata</li><li>cleans up extraneous or incorrect document formatting</li><li>applies structure to the document on the paragraph and character level (using author- and editor-friendly Word styles rather than visually intrusive tags)</li><li>automatically enforces certain editorial style requirements through large-scale, context-sensitive find and replace</li><li>parses, restructures, links, and corrects bibliographic references using internal templates and databases as well as external sources such as <a href="https://web.archive.org/web/20151002164418/http://www.ncbi.nlm.nih.gov/sites/entrez">PubMed</a> and <a href="https://web.archive.org/web/20151002164418/http://www.crossref.org/">CrossRef</a></li><li>links citations and callouts to their respective references and objects</li><li>exports high-quality XML from Word according to the <a href="https://web.archive.org/web/20151002164418/http://dtd.nlm.nih.gov/">NLM DTD</a> or any other DTD required by the publisher</li></ul><p>eXtyles is a uniquely flexible tool in that it can accommodate any editorial style and the wide variety of workflows used by different publishers, and it can also process any Word-readable, author-submitted manuscript.</p><h3 id="2-what-is-the-difference-between-extyles-and-the-microsoft-word-article-authoring-add-in">2. What is the difference between eXtyles and the <a href="https://web.archive.org/web/20151002164418/http://www.microsoft.com/mscorp/tc/scholarly_communication.mspx#Tools">Microsoft Word Article Authoring Add-in</a> ?</h3><p><em><em>Bruce Rosenblum:</em></em> eXtyles makes two assumptions. First, eXtyles does not rely on the author to complete document structuring tasks – in our experience, journals have not succeeded in having authors provide sufficiently accurate styling or markup in Word files (the application used by most scholarly authors) to allow for automatic creation of high-quality XML. Second, eXtyles assumes that authors make other kinds of mistakes, whether adding inappropriate formatting, using unsupported fonts for special characters, or making informational errors in reference lists, and all of these problems must be corrected as part of the publishing process.</p><p>eXtyles is designed to overcome the limitations of how authors commonly prepare manuscripts by providing the tools that publishers need to clean up, rapidly edit, and then convert manuscripts to XML. eXtyles has been carefully developed over ten years to accurately address the reality of what publishers see in author submissions.</p><p>By contrast, the Microsoft Word Article Authoring Add-in has been developed as a content creation tool. It is designed for authors to structure articles in Word 2007 as they write. In this model, authors must add structural information rather than just submit the text of their article. The Add-in assumes authors will create reference lists with the <a href="https://web.archive.org/web/20151002164418/http://office.microsoft.com/en-us/word/HA100674921033.aspx">Word 2007 citation manager</a> and will adhere to all requirements necessary to successfully save the manuscript to NLM DTD XML when they have completed writing their article. Also, the Microsoft Add-in does not provide tools to prepare an article for XML conversion if the Add-in was not used during creation of the article, which is a key feature of eXtyles.</p><p>Fundamentally the target audience differs: the Microsoft Add-in assumes use by authors, whereas eXtyles provides tools to publishing personnel that allow authors to concentrate on great scientific research rather than the technical aspects of article publication.</p><h3 id="3-what-are-the-most-common-problems-in-submitted-manuscripts-that-can-be-fixed-by-extyles-are-there-problems-that-have-to-be-fixed-manually">3. What are the most common problems in submitted manuscripts that can be fixed by eXtyles? Are there problems that have to be fixed manually?</h3><p><em><em>Elizabeth Blake:</em></em> The most common problems range from relatively simple issues such as extraneous formatting or misapplied styles to more complex issues such as missing data or incorrect or uncited references. Hovering in between these are violations of editorial style – for example, British versus American spelling or non-standard abbreviations for units of measure – which eXtyles can also correct.</p><p>Some problems do have to be fixed manually. eXtyles does not take the place of an editor; rather, it automates as much of the low-level copy editing as can be reliably automated while drawing the editor’s attention to issues that require follow up. An example would be eXtyles flagging a callout to a table that is missing from the manuscript; this obviously requires human intervention, but the warning saves time and flags problems early in the workflow when they are easier to resolve.</p><p>On a larger scale, the problem eXtyles is designed to solve is getting the accepted author manuscript published as quickly and accurately as possible. The fully eXtyled file can be flowed into a typesetting system such as InDesign with the paragraph styles aligned to the composition template, saving a lot of labor during the typesetting stage, or the Word file can be converted directly to NLM XML with the push of a button, allowing users to create rich, valid XML without any XML knowledge or expertise.</p><h3 id="4-extyles-helps-with-editorial-tasks-such-as-document-cleanup-and-citation-checking-why-shouldn-t-these-tasks-be-left-to-the-authors-and-checked-when-a-manuscript-is-submitted">4. eXtyles helps with editorial tasks such as document cleanup and citation checking. Why shouldn’t these tasks be left to the authors and checked when a manuscript is submitted?</h3><p><em><em>Elizabeth Blake:</em></em> Theoretically these tasks are the authors’ responsibility! In practice it is an extremely rare manuscript that doesn’t have errors, particularly reference errors. Our goal is not to discourage authors from submitting clean and accurate manuscripts; our goal is to facilitate the process when that doesn’t happen, which is most of the time. The eXtyles reference-processing tools, in particular the tools that link and correct references with data retrieved from PubMed and CrossRef, go a step beyond what even a very thorough copy editor is typically able to flush out given the time constraints of a deadline-driven workflow.</p><p>As for performing these tasks at submission, the eXtyles integration with <a href="https://web.archive.org/web/20151002164418/http://www.editorialmanager.com/homepage/home.htm">Editorial Manager</a> does just that, providing an informative reference quality check early in the workflow so that responsibility for fixing the references can, if the publisher prefers, be pushed back on the authors.</p><h3 id="5-if-extyles-validates-and-corrects-references-why-do-most-journals-insist-on-bibliographies-formatted-in-a-specific-house-style">5. If eXtyles validates and corrects references, why do most journals insist on bibliographies formatted in a specific house style?</h3><p><em><em>Elizabeth Blake:</em></em> There are two ways in which eXtyles corrects references: one is by correcting the data (e.g., PubMed reports that the first author in reference 2 is incorrect) and the other is by correcting the format according to the publisher’s house style. Ensuring correct data in a reference, which eXtyles does with <a href="https://web.archive.org/web/20151002164418/http://www.inera.com/refcorrection.shtml">Automatic Reference Correction</a>, is part of ensuring accuracy in the scientific record. As for the varieties of reference styles, many developed as necessary elements of print publication – for instance, some abbreviated house styles were devised as paper-saving measures. So long as print is not dead, these concerns are still relevant. However, in an electronic world, formatting of references may be less important, though a good editor will always have an argument for why their style is preferable (I say that as a former editor). We at Inera remain neutral on the topic since eXtyles can reformat references according to any preferred editorial style if a publisher requires it.</p><h3 id="6-what-is-the-nlm-dtd-what-was-your-part-in-developing-it">6. What is the NLM DTD? What was your part in developing it?</h3><p>Bruce Rosenblum: The <a href="https://web.archive.org/web/20151002164418/http://dtd.nlm.nih.gov/">NLM DTD</a> is a family of tag sets designed for full-text XML markup of scholarly articles. The intent of the NLM DTD Suite is to mark up and preserve the intellectual content of journals independent of the form in which the content is delivered. <a href="https://web.archive.org/web/20151002164418/http://dtd.nlm.nih.gov/faq.html">The suite can be used to publish and archive journal content, and to facilitate content interchange between organizations</a>. And despite the NLM moniker, the suite was designed from day-one for full-text markup of journal content in any discipline.</p><p>Our work on the NLM DTD started in 2001 when the <a href="https://web.archive.org/web/20151002164418/http://lib.harvard.edu/">Harvard University Library</a>, under a <a href="https://web.archive.org/web/20151002164418/http://www.mellon.org/">Mellon Foundation</a> grant, was asked to study formats for long-term archiving of eJournal content. Harvard decided that PDF was not a viable archive format, and discovered that every publisher had a proprietary DTD. Harvard then approached Inera, because of our experience working with many DTDs including <a href="https://web.archive.org/web/20151002164418/http://download.www.techstreet.com/cgi-bin/pdf/free/228869/12083-a.pdf">ISO 12083</a>, to study the feasibility of developing a single DTD into which the content from all publishers could be converted for the purposes of long-term archiving. The <a href="https://web.archive.org/web/20151002164418/http://www.diglib.org/preserve/hadtdfs.pdf">e-Journal Archjive DTD Feasibility Study</a> we wrote describes the requirements for such a DTD.</p><p>At the same time, NLM was making major revisions to the <a href="https://web.archive.org/web/20151002164418/http://www.pubmedcentral.nih.gov/">PubMed Central</a> 1.0 DTD. NLM, Harvard, and Mellon decided to combine resources on a single project co-developed by NLM, <a href="https://web.archive.org/web/20151002164418/http://www.mulberrytech.com/">Mulberry Technologies</a>, and Inera. Version 1.0 of the NLM DTD was released in April 2003. The scope of use was originally focused on the needs of PubMed Central, and a long-term archive (now <a href="https://web.archive.org/web/20151002164418/http://www.portico.org/">Portico</a>) seed-funded by the Mellon Foundation.</p><p>The NLM DTD was quickly adopted by others when they discovered the coverage and flexibility. And as publishers switched from SGML to XML, they found that adopting an off-the-shelf public DTD was far less expensive than converting their existing SGML DTDs to XML. As use has expanded, support has grown so that many off-the-shelf solutions are now available for working with the DTD. Inera continues to serve on the <a href="https://web.archive.org/web/20151002164418/http://dtd.nlm.nih.gov/working-group.html">NLM DTD Advisory Board</a>.</p><h3 id="7-how-does-extyles-use-the-nlm-dtd">7. How does eXtyles use the NLM DTD?</h3><p><em><em>Bruce Rosenblum:</em></em> eXtyles users, with no knowledge of XML, can create high-quality XML according to the NLM DTD as a simple one-button action after using eXtyles to easily complete editorial preparation of a manuscript. In other words, eXtyles XML creation is a natural by-product of normal manuscript preparation for publication, and it requires no specialized user knowledge.</p><p>Because eXtyles was developed as an open platform that can be customized to the editorial and production needs of any publisher, it is internally DTD agnostic. In this regard, eXtyles can be used to convert content to any of the DTDs in the NLM DTD Suite (<a href="https://web.archive.org/web/20151002164418/http://dtd.nlm.nih.gov/publishing/">Journal Publishing</a>, <a href="https://web.archive.org/web/20151002164418/http://dtd.nlm.nih.gov/archiving/">Archive and Interchange</a>, or <a href="https://web.archive.org/web/20151002164418/http://dtd.nlm.nih.gov/book/">Book</a>).</p><p>Furthermore, the NLM DTD suite actually allows for a wide range of interpretation, and eXtyles can easily meet any of those interpretations. For example, the NLM DTD supports two table models, XHTML and <a href="https://web.archive.org/web/20151002164418/http://dtd.nlm.nih.gov/options/OASIS/tag-library/19990315/n-xe20.html">CALS</a>. eXtyles can produce content in either table model, depending on the workflow requirements of the publisher. A wide range of other configuration options are available to meet the needs of specific publisher requirements.</p><h3 id="8-can-the-nlm-dtd-also-used-by-authors-to-submit-their-papers-to-journals-or-repositories-if-not-what-are-the-limitations">8. Can the NLM DTD also used by authors to submit their papers to journals or repositories? If not, what are the limitations?</h3><p><em><em>Bruce Rosenblum:</em></em> In theory authors could submit their papers to journals or repositories in XML. However, in practice this has not occurred and we do not consider it likely to occur in the near future. There are a few key obstacles.</p><p>First, the primary job of researchers is to conduct research and report the results. Because technical knowledge of publishing formats is not a required part of the typical researcher’s job, the majority of researchers do not submit papers to journals or repositories in XML when they can much more easily do so in PDF or Word.</p><p>By extension of this point, most publishers are interested in high-quality research. One publisher told us, “If a manuscript with great science is submitted on birch bark, we’ll find a way to publish it.” We believe that in a world where journals compete for the best papers, publishers will continue to put research quality ahead of submission formats.</p><p>Second, creation of high-quality XML is just not that simple. Even with the tools that have been developed in the past ten years, publishers with experienced editorial and production teams still have problems producing consistently high-quality XML; if you doubt this point, please look at a typical first round PubMed Central publisher validation report. If publishers with experienced production teams do not find this easy, then we believe that production of consistently high-quality XML by authors is not likely to occur in the next few years.</p><h3 id="10-what-are-your-responsibilities-at-inera">10. What are your responsibilities at Inera?</h3><p><em><em>Bruce Rosenblum:</em></em> As CEO, I am involved in all aspects of Inera. My background is originally in software development, and I use this expertise to guide the development and quality assurance of eXtyles. I am also responsible for managing the business side of Inera. But probably my most important role is coordinating the incredibly creative team of people who work on eXtyles.</p><p><em><em>Elizabeth Blake:</em></em> My role is primarily customer facing, from marketing to configuration and training up through support. My goal is to ensure that potential customers get all the information they need about eXtyles to make the right decision and to ensure that, once they become customers, they continue to be happy with that decision. We rely heavily on customer feedback when planning enhancements to eXtyles, and I tend to focus on the user experience when we work through the details of any new project.</p><h3 id="11-what-did-you-do-before-starting-to-work-on-extyles">11. What did you do before starting to work on eXtyles?</h3><p><em><em>Bruce Rosenblum:</em></em> Before eXtyles, Inera spent years providing SGML and XML consulting and software development services to publishers. Prior to that, I spent 15 years developing commercial software products for companies like Microsoft, Word Perfect, Houghton Mifflin, and Broderbund. Going way back, I’ve always had an interest in problems of working with text – my first professional software project was building a word processor for Chinese on an Apple II in 1980.</p><p><em><em>Elizabeth Blake:</em></em> I started in scholarly publishing as a copy editor for the Cell Press journal <em><em>Neuron</em></em> and then worked as the managing editor of <em><em>Neuron</em></em> for several years before moving to <em><em>The New England Journal of Medicine</em></em>. At Inera, my years of real-world editorial and production experience have been very valuable and help to ensure that eXtyles development is centered on solving the problems that publishers face every day.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Faculty of 1000: Interview with Richard Grant]]></title>
            <link>https://blog.martinfenner.org/posts/faculty-of-1000-interview-with-richard-grant</link>
            <guid>cdbfba74-39cd-4774-b217-28bebcc1ad6c</guid>
            <pubDate>Tue, 28 Apr 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[Richard Grant
[https://web.archive.org/web/20151003075116/http://network.nature.com/people/rpg/blog]
, who needs no introduction here on Nature Network, has just moved to London to
start a new job
[https://web.archive.org/web/20151003075116/http://network.nature.com/people/rpg/blog/2009/04/09/on-the-payroll] 
as information architect for Faculty of 1000
[https://web.archive.org/web/20151003075116/http://www.f1000biology.com/]. I
took this opportunity to ask Richard a few questions not only about]]></description>
            <content:encoded><![CDATA[<p><a href="https://web.archive.org/web/20151003075116/http://network.nature.com/people/rpg/blog">Richard Grant</a>, who needs no introduction here on Nature Network, has just moved to London <a href="https://web.archive.org/web/20151003075116/http://network.nature.com/people/rpg/blog/2009/04/09/on-the-payroll">to start a new job</a> as <em><em>information architect</em></em> for <a href="https://web.archive.org/web/20151003075116/http://www.f1000biology.com/">Faculty of 1000</a>. I took this opportunity to ask Richard a few questions not only about Faculty of 1000, but also about his role in the company and future plans for the service that they have in mind.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20151003075116im_/http://friendfeed.s3.amazonaws.com/f353fdfe5c24bfb525680563a4a5c04b3760fafd" class="kg-image" alt></figure><h3 id="1-can-you-describe-what-faculty-of-1000-is-and-does">1. Can you describe what Faculty of 1000 is and does?</h3><p>The scientific literature is immense, and growing. It has become very difficult to keep up, especially if you’re going to keep an eye on developments not immediately in your own area. For someone new to a field it’s almost impossible to know — without insider knowledge — what papers are important, which are the key publications; where a field is going and what are the key developments.</p><p>So what we do is provide a kind of ‘filter’ on top of the literature: we have about five thousand principle investigators, who we call ‘Faculty’, across biology and medicine, who in the course of their own reading will write short evaluations on the important and influential papers in their field. We’re also recruiting <a href="https://web.archive.org/web/20151003075116/http://www.f1000biology.com/about/associateFMs">Associate Faculty members</a>: trusted junior members of a Faculty member’s lab or practice who will write their own evaluations and increase our coverage. And we’re not just talking about stuff that’s published in <em><em>Nature</em></em> or <em><em>Cell</em></em> — or <em><em>NEJM</em></em> or <em><em>The Lancet</em></em> — but in the specialized, work-a-day journals. What’s more, this expert opinion, or what we’re calling ‘post-publication peer review’, gives our users a measure of the ‘quality’ of individual papers that is independent of, and much quicker than, the impact factor of the journal.</p><h3 id="2-what-part-of-faculty-of-1000-is-free-and-what-part-needs-a-paid-subscription">2. What part of Faculty of 1000 is free and what part needs a paid subscription</h3><p>You can search or browse the entire database, and sign up for email alerts, so you can see which papers have been evaluated. You can’t actually read the evaluations themselves unless you have an institutional or personal subscription.</p><h3 id="3-how-does-faculty-of-1000-integrate-with-reference-managers-such-as-endnote-refworks-zotero-or-connotea">3. How does Faculty of 1000 integrate with reference managers such as Endnote, Refworks, Zotero or Connotea?</h3><p>You can download evaluations into a reference manager just like you can papers from PubMed. We’re working on proper integration with online tools (such as <a href="https://web.archive.org/web/20151003075116/http://www.citeulike.org/">CiteULike</a> and <a href="https://web.archive.org/web/20151003075116/http://www.connotea.org/">Connotea</a>) and other ‘social media’ tools. I’m also keen to work with <a href="https://web.archive.org/web/20151003075116/http://www.mendeley.com/">Mendeley</a> to improve the user experience.</p><h3 id="4-what-are-the-incentives-for-faculty-members-to-evaluate-papers">4. What are the incentives for faculty members to evaluate papers?</h3><p>Exposure and kudos, mainly! The Faculty member’s name is displayed prominently on the evaluations (you can see who’s written an evaluation even without a subscription). Reputation is important to scientists and being invited to become a Faculty member says to the rest of the community that your opinion is respected and your peers think highly of you. We also profile Faculty members who write timely or important evaluations, or who have news of their own (grants, papers, awards) and give them publicity through press releases, etc.</p><p>What’s more, our Faculty members <em><em>like</em></em> the combination of expert opinion and original articles. They see a value in it, and realize that there’s a kind of synergy going on here; if they contribute then others will be encouraged to too, and everybody wins.</p><p>I’d like to explore how evaluations might become citable — so that Faculty can put their work for us on their CV, how it might benefit their career, grant applications etc. We’re also considering more tangible benefits.</p><h3 id="5-how-are-faculty-members-selected">5. How are faculty members selected?</h3><p>The Heads of Faculty, for each subject or speciality, are elected or selected on the recommendation of large numbers of medics and scientists we talk to. They divide their Faculty into Sections and then select two or three Section Heads. These scientists in turn identify the sub-fields within their Section and select Faculty Members, checking with Heads of Faculty. The Section Heads select Faculty Members on the basis of various criteria:</p><ul><li>the number of Faculty Members should be proportionally representative of the number of papers published within that field;</li><li>the selected Faculty Members should be well respected by their peers and perceived as being fair-minded;</li><li>there must be a good representation of genders, nationalities and age/seniority.</li></ul><p>Faculty Members themselves are being asked to co-opt younger workers within their groups — post-docs, say — to help increase coverage and to write their own evaluations. We call these ‘Associate Faculty’.</p><h3 id="6-can-faculty-of-1000-users-comment-on-papers-or-paper-evaluations">6. Can Faculty of 1000 users comment on papers or paper evaluations?</h3><p>Not at the moment, no. Faculty members can comment, or contribute a ‘dissent’ if they disagree with an evaluation, and authors of the evaluated papers are encouraged to respond, but we feel it’s important for users to know that they can trust what we publish. However, we’re currently planning to launch a forum whereby users <em><em>can</em></em> comment freely on evaluated papers. This would be open to anyone who registers, without a subscription, but kept distinct from the main evaluation.</p><h3 id="7-can-paper-authors-comment-on-evaluations-of-their-papers">7. Can paper authors comment on evaluations of their papers?</h3><p>Yes! At the moment we get emails from authors saying that they’re pleased to have their papers selected, but we’re going to make it possible for them to comment on the evaluations directly so that a conversation with the Faculty can be initiated.</p><h3 id="8-what-is-faculty-of-1000-reports">8. What is Faculty of 1000 Reports?</h3><p>F1000 Reports carries short reviews, or commentaries, on emerging trends identified from within the F1000 database. <a href="https://web.archive.org/web/20151003075116/http://f1000medicine.com/reports/">F1000 Medicine Reports</a> features studies that are likely to change clinical practice and summarizes implications for clinicians. <a href="https://web.archive.org/web/20151003075116/http://www.f1000biology.com/reports/">F1000 Biology Reports</a> contextualizes important and exciting papers or clusters of publications. The Advisory Board (for <a href="https://web.archive.org/web/20151003075116/http://f1000biology.com/reports/advisoryboard">F1000 Biology Reports</a> and <a href="https://web.archive.org/web/20151003075116/http://f1000medicine.com/reports/advisoryboard">F1000 Medicine Reports</a>) identifies potential topics and invites appropriate Faculty members to write about them.</p><h3 id="9-what-are-your-responsibilities-at-faculty-of-1000">9. What are your responsibilities at Faculty of 1000?</h3><p>Well, my job title is ‘Information Architect’, which means quite a bit more than ‘web manager’. I have overall responsibility for the presentation of the F1000 service, and I have to ensure that the web site is fast and intuitive, and that the content is suitable for both medics and biologists. I’m also keen to keep F1000 relevant in the Web 2.0 world.</p><h3 id="10-what-did-you-do-before-starting-to-work-for-faculty-of-1000">10. What did you do before starting to work for Faculty of 1000?</h3><p>I was at the University of Sydney for three years, the token cell biologist in an NMR lab, looking at RNA-binding zinc fingers. Before that I was at the MRC-LMB in Cambridge for six years, learning how to do X-ray crystallography and NMR and applying those techniques to cell biological questions.</p><h3 id="11-do-you-want-to-talk-about-future-plans-for-faculty-of-1000">11. Do you want to talk about future plans for Faculty of 1000?</h3><p>As I’ve sorted of already hinted, we’re in the middle of a major redesign. We have some new features that I hope you’ll find very exciting — one of which I really can’t talk about yet! — including forums, a re-vamped ‘MyF1000′ site, integration of F1000 Reports, more systematic literature scanning, talking to social media sites, RSS (at last!), a blog, and a lot of behind the scenes tweaks.</p><h3 id="12-where-can-we-provide-feedback-about-faculty-of-1000">12. Where can we provide feedback about Faculty of 1000?</h3><p>Write to me! Or email info@f1000.com. The new site will also have a feedback form.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A few questions about author identifiers: the answers]]></title>
            <link>https://blog.martinfenner.org/posts/a-few-questions-about-author-identifiers-the-answers</link>
            <guid>ad5568d3-9a17-451d-9634-e6b22e6ca794</guid>
            <pubDate>Sun, 26 Apr 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[I've recently asked a few questions about author identifiers for scientists
[https://web.archive.org/web/20150922174131/http://network.nature.com/people/mfenner/blog/2009/04/13/a-few-questions-about-author-identifiers]
. Here are the results (based on 48 responses). The results are also available
as .xls file
[https://web.archive.org/web/20150922174131/http://spreadsheets.google.com/pub?key=rpPHC7BnyO2VJPglm246fKA&output=xls]
.]]></description>
            <content:encoded><![CDATA[<p>I've recently asked <a href="https://web.archive.org/web/20150922174131/http://network.nature.com/people/mfenner/blog/2009/04/13/a-few-questions-about-author-identifiers">a few questions about author identifiers for scientists</a>. Here are the results (based on 48 responses). The results are also available as <a href="https://web.archive.org/web/20150922174131/http://spreadsheets.google.com/pub?key=rpPHC7BnyO2VJPglm246fKA&amp;output=xls">.xls file</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Popularity of online reference managers]]></title>
            <link>https://blog.martinfenner.org/posts/popularity-of-online-reference-managers</link>
            <guid>f464c1ee-68b2-4e72-88fb-5249765d2b9a</guid>
            <pubDate>Sat, 18 Apr 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[Now that we have a number of online reference managers to choose from
[https://web.archive.org/web/20151003053433/http://network.nature.com/people/mfenner/blog/2009/03/15/reference-manager-overview]
, I thought it would be interesting to look at their popularity – both in
absolute numbers of visitors and the in changes during the last 12 months.
Online tools such as Compete
[https://web.archive.org/web/20151003053433/http://www.compete.com/] allow
everybody to do just that, and their basic funct]]></description>
            <content:encoded><![CDATA[<p>Now that we have a <a href="https://web.archive.org/web/20151003053433/http://network.nature.com/people/mfenner/blog/2009/03/15/reference-manager-overview">number of online reference managers to choose from</a>, I thought it would be interesting to look at their popularity – both in absolute numbers of visitors and the in changes during the last 12 months. Online tools such as <a href="https://web.archive.org/web/20151003053433/http://www.compete.com/">Compete</a> allow everybody to do just that, and their basic functions are free to use. I've picked <em><em>unique visitors</em></em>, but there are of course other statistics to look at, including total number of visits.</p><p><a href="https://web.archive.org/web/20151003053433/http://www.citeulike.org/">CiteULike</a> is the most popular online reference manager, and it is obvious that the <a href="https://web.archive.org/web/20151003053433/http://www.springer-sbm.com/index.php?id=291&amp;backPID=13041&amp;L=0&amp;tx_tnc_news=4739&amp;cHash=56bfa6b56c">announcement by Springer to sponsor them</a> last August has helped their site traffic. Only CiteULike and <a href="https://web.archive.org/web/20151003053433/http://www.labmeeting.com/">Labmeeting</a> show a significant increase in unique visitors in the last 6 months.</p><p>The statistics are more complicated for tools that include both a desktop client and online database (<a href="https://web.archive.org/web/20151003053433/http://www.endnote.com/">Endnote</a>, <a href="https://web.archive.org/web/20151003053433/http://www.mendeley.com/">Mendeley</a>, <a href="https://web.archive.org/web/20151003053433/http://www.zotero.org/">Zotero</a>) and these numbers should be interpreted with caution. I've included <a href="https://web.archive.org/web/20151003053433/http://www.refworks.com/">RefWorks</a> in both graphs for better comparison. It is probably safe to say that both Endnoteweb and the online version of Mendeley are not as popular as the online only reference managers in the first graph. This could either mean that online only tools are far more popular than desktop applications (which I doubt) or that most references are still primarily stored in desktop programs and not shared online. Something that Eva Amsen already described last year (<a href="https://web.archive.org/web/20151003053433/http://network.nature.com/people/eva/blog/2008/08/19/how-to-get-scientists-to-adopt-web-2-0-technologies">How to get scientists to adopt web 2.0 technologies</a>). To put these numbers into perspective: <a href="https://web.archive.org/web/20151003053433/http://ncbi.nlm.nih.gov/">ncbi.nlm.nih.gov</a> (the home of PubMed and other NCBI databases) sees about 2.5 million unique visitors a month.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A few questions about author identifiers]]></title>
            <link>https://blog.martinfenner.org/posts/a-few-questions-about-author-identifiers</link>
            <guid>35d651b3-8c82-4802-8df2-07821d7f3692</guid>
            <pubDate>Mon, 13 Apr 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[The scientific articles we write are uniquely identified by Digital Object
Identifiers [https://web.archive.org/web/20151001190903/http://www.doi.org/] 
(DOI). Many people believe that we also need unique identifiers for the authors
of those articles. Some of the arguments why author identifiers could be very
helpful are found in this interview with Geoffrey Bilder
[https://web.archive.org/web/20151001190903/http://network.nature.com/people/mfenner/blog/2009/02/17/interview-with-geoffrey-bilder]]]></description>
            <content:encoded><![CDATA[<p>The scientific articles we write are uniquely identified by <a href="https://web.archive.org/web/20151001190903/http://www.doi.org/">Digital Object Identifiers</a> (DOI). Many people believe that we also need unique identifiers for the authors of those articles. Some of the arguments why author identifiers could be very helpful are found in this <a href="https://web.archive.org/web/20151001190903/http://network.nature.com/people/mfenner/blog/2009/02/17/interview-with-geoffrey-bilder">interview with Geoffrey Bilder</a>, this <a href="https://web.archive.org/web/20151001190903/http://www.gen2phen.org/researcher-identification/researcher-identification-primer">Researcher Identification Primer</a>, and this <a href="https://web.archive.org/web/20151001190903/http://friendfeed.com/e/c1fd00ec-15f9-d894-4ea9-4ffeaac5ae28/A-specialist-OpenID-service-to-provide-unique/">FriendFeed discussion</a>. There is also an increasing number of scientific papers on the topic, including:</p><ul><li><a href="https://web.archive.org/web/20151001190903/http://dx.doi.org/10.1038/451766a">Scientific publishing: Identity crisis</a> (Nature)</li><li><a href="https://web.archive.org/web/20151001190903/http://dx.doi.org/10.1038/embor.2008.217">What's in a name?</a> (EMBO Reports)</li><li><a href="https://web.archive.org/web/20151001190903/http://dx.doi.org/10.1371/journal.pcbi.1000247">I Am Not a Scientist, I Am a Number</a> (PLoS Computational Biology)</li><li><a href="https://web.archive.org/web/20151001190903/http://dx.doi.org/10.1126/science.323.5922.1662">Are you ready to become a number?</a> (Science)</li><li><a href="https://web.archive.org/web/20151001190903/http://dx.doi.org/10.1038/ng0409-383">Metadata for the metaconsortium</a> (Nature Genetics)</li></ul><p>Although most people agree that we need author identifiers for scientists, many details of how this should be implemented are not clear. I've listed some of the issues below. If possible, please take a few minutes and answer the questions for yourself in <a href="https://web.archive.org/web/20151001190903/http://www.polldaddy.com/s/FF71A13C726B335C/">this poll</a>.</p><h3 id="1-what-is-the-purpose-of-an-author-identifier-for-scientists">1. What is the purpose of an author identifier for scientists?</h3><ul><li>Unique identifier</li><li>Author profile</li><li>Authentication</li><li>Other:</li></ul><p>An author identifier should obviously uniquely identify an author. Some people like to add other functions, namely the ability to add a profile (e.g. a web page listing all papers of an author) and authentication. I think that the latter two functions can better be provided by a combination of unique identifier and some of today's tools (see also question #10).</p><h3 id="2-what-is-the-best-name-for-an-author-identifier-for-scientists">2. What is the best name for an author identifier for scientists?</h3><ul><li><a href="https://web.archive.org/web/20151001190903/http://help.scopus.com/robo/projects/schelp/h_autsrch_intro.htm">Author ID</a> (Scopus)</li><li><a href="https://web.archive.org/web/20151001190903/http://www.researcherid.com/">Researcher ID</a> (Thomson Reuter)</li><li><a href="https://web.archive.org/web/20151001190903/http://www.crossref.org/CrossTech/2009/02/an_interview_about_author_ids.html">Contributor ID</a> (CrossRef)</li><li>Digital Author Identifier</li><li>Other:</li></ul><p>We need a name for author identifiers. Some of the names above are already associated with an existing (or planned) service. Just pick a name and stick with it.</p><h3 id="3-the-author-identifier-system-should-be-used-for-">3. The author identifier system should be used for:</h3><ul><li>Authors</li><li>Reviewers</li><li>Editors</li><li>Database submitters</li><li>Bloggers</li><li>Commenters</li><li>Other:</li></ul><p>I believe that the author identifier system should be applied to all of the above. But it doesn't have to be implemented for all these uses at the same time.</p><h3 id="4-who-should-pay-for-the-author-identifier-system">4. Who should pay for the author identifier system?</h3><ul><li>Journal Publisher</li><li>Blogger</li><li>Database maintainer</li><li>Author</li><li>Other:</li></ul><p>Those that gain the most from the author identifier system should pay for it. But because I think that quick adoption of the system is important, I wouldn't make authors pay.</p><h3 id="5-should-author-identifiers-be-managed-by-a-central-organization">5. Should author identifiers be managed by a central organization?</h3><ul><li>Yes</li><li>No</li></ul><p>I think yes, and for the reasons that Geoffrey Bilder explained in <a href="https://web.archive.org/web/20151001190903/http://network.nature.com/people/mfenner/blog/2009/02/17/interview-with-geoffrey-bilder">the interview with me</a>. In my opinion a distributed system would create too many new problems.</p><h3 id="6-which-organization-should-manage-author-identifiers">6. Which organization should manage author identifiers?</h3><ul><li>CrossRef</li><li>U.S. National Library of Medicine</li><li>Other:</li></ul><p>The main reason that many people are reluctant to have author identifiers managed by a single institution is that they don't want a single publisher or other commercial entity to control this system. But both CrossRef and the National Library of Medicine have a good track record for implementing publishing standards. I would prefer Crossref, as the National Library of Medicine is only concerned with a subset of papers, i.e. the biomedical literature.</p><h3 id="7-should-an-author-identifier-system-for-scientists-be-based-on-openid">7. Should an author identifier system for scientists be based on OpenID?</h3><ul><li>Yes</li><li>No</li></ul><p>I would say no. OpenID is a distributed system, which is not desirable as discussed in questions #5 and #10. And OpenID also handles authentication, something which isn't necessarily required (as discussed in question #1).</p><h3 id="8-should-there-be-one-or-several-author-identifier-systems">8. Should there be one or several author identifier systems?</h3><ul><li>One</li><li>Several</li><li>Several with one top system</li></ul><p>Similar to the DOI, I want only one unique author identifier. But this one unique identifier can then be linked to several other systems, e.g. an OpenID for authentication or a Nature Network, LinkedIn or Mendeley profile to list all your papers and other contributions.</p><p>I hope that enough people also answer these questions in the <a href="https://web.archive.org/web/20151001190903/http://www.polldaddy.com/s/FF71A13C726B335C/">poll</a> I posted.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Twitter for Peer Review]]></title>
            <link>https://blog.martinfenner.org/posts/twitter-for-peer-review</link>
            <guid>4d95be41-d8bf-4928-9c1d-9b7f3713b2fc</guid>
            <pubDate>Tue, 31 Mar 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[A recent Nature News article by Geoff Brumfiel (Science journalism: Supplanting
the old media?
[https://web.archive.org/web/20150924053130/http://www.nature.com/news/2009/090318/full/458274a.html]
) has stirred up many interesting discussions about the relationship of science
blogging and traditional science journalism. Good starting places to follow
these discussions (and engage in them) are Technorati
[https://web.archive.org/web/20150924053130/http://technorati.com/search/http://www.nature.co]]></description>
            <content:encoded><![CDATA[<p>A recent <em><em>Nature News</em></em> article by Geoff Brumfiel (<a href="https://web.archive.org/web/20150924053130/http://www.nature.com/news/2009/090318/full/458274a.html">Science journalism: Supplanting the old media?</a>) has stirred up many interesting discussions about the relationship of science blogging and traditional science journalism. Good starting places to follow these discussions (and engage in them) are <a href="https://web.archive.org/web/20150924053130/http://technorati.com/search/http://www.nature.com/news/2009/090318/full/458274a.html">Technorati</a>, <a href="https://web.archive.org/web/20150924053130/http://blogs.nature.com/stories/858">Nature.com Blogs</a> and <a href="https://web.archive.org/web/20150924053130/http://friendfeed.com/e/4f3817ea-f82c-3a94-4e1c-3368926a24ed/Defining-the-Journalism-vs-Blogging-Debate-with-a/">FriendFeed</a>.</p><p>Science blogging extends, but also threatens traditional science journalism. At the same time, aggregators and microblogging services such as <a href="https://web.archive.org/web/20150924053130/http://www.friendfeed.com/">FriendFeed</a>, <a href="https://web.archive.org/web/20150924053130/http://www.twitter.com/">Twitter</a>, but also <a href="https://web.archive.org/web/20150924053130/http://www.facebook.com/">Facebook</a> also are both an enhancementent and a threat to science blogs. Instead of writing blog posts or commenting on them, many science bloggers spend increasing amounts of time with these services, e.g. in <a href="https://web.archive.org/web/20150924053130/http://friendfeed.com/rooms/the-life-scientists">The Life Scientists</a> room on FriendFeed.</p><p>But microblogging and aggregation services have also emerged as new tools for another area of science communication, namely the <a href="https://web.archive.org/web/20150924053130/http://www.nature.com/nature/peerreview/debate/index.html">peer review</a> process. The interaction between authors and editors or editors and reviewers traditionally happens via email (because peer review is usually anonymous, authors don't communicate directly with reviewers). Twitter and similar tools fullfill the requirement for privacy (in the form of direct messages and private rooms and special services for organizations such as <a href="https://web.archive.org/web/20150924053130/http://www.yammer.com/">Yammer</a>).</p><p>What are the advantages of these tools for the peer review process? All communications can be stored in one place in the form of a discussion thread. FriendFeed and Facebook allow users to mark posts they like and this can show agreement between reviewers. Messages can also be sent to and from non-traditional devices such as cell phones. Many senior researchers are already overworked with peer review, so this way they can at least post their reviews from the golf course or their yacht. And authors want to learn about their accepted paper as soon as possible, and this is not necessarily when they sit in front of their computer.</p><p>But most importantly, microblogging enforces brevity. Virginia Walbot recently complained in a <em><em>Journal of Biology</em></em> comment (<a href="https://web.archive.org/web/20150924053130/http://dx.doi.org/10.1186/jbiol125">Are we training pit bulls to review our manuscripts?</a>) about reviewers</p><blockquote><em><em>dismissing the years of labor and stating that the manuscript can only be reconsidered with substantially more data providing definitive proof of each claim</em></em>.</blockquote><p>As Twitter messages (also known as tweets) can only be 140 characters long, reviewers are forced to write short reviews, and editors to write short notes to the authors. And if the 140 characters aren't enough, they can always point to other places with services like <a href="https://web.archive.org/web/20150924053130/http://bit.ly/5gfc9">bit.ly</a>.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20150924053130im_/http://farm4.static.flickr.com/3463/3402929290_6e010842b6.jpg" class="kg-image" alt></figure>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Editorial Manager: Interview with Richard Wynne]]></title>
            <link>https://blog.martinfenner.org/posts/editorial-manager-interview-with-richard-wynne</link>
            <guid>a6722bcb-5948-4a56-a0c8-abaa59915086</guid>
            <pubDate>Wed, 25 Mar 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[Practically all scientific journals now use web-based systems for paper
submissions and peer review. This saves the authors a lot of time compared to
paper submissions by postal mail (until 15 years ago) or email (until 5 years
ago). Unfortunately the submission process is still far from perfect and
requires authors to spend many hours formatting manuscripts, references and
images instead of focusing on the scientific content.

The tools (word processors, reference managers, graphics programs, e]]></description>
            <content:encoded><![CDATA[<p>Practically all scientific journals now use web-based systems for paper submissions and peer review. This saves the authors a lot of time compared to paper submissions by postal mail (until 15 years ago) or email (until 5 years ago). Unfortunately the submission process is still far from perfect and requires authors to spend many hours formatting manuscripts, references and images instead of focusing on the scientific content.</p><p>The tools (word processors, reference managers, graphics programs, etc.) that most authors use to write manuscripts have become more sophisticated every year, but often don't help much with creating structured documents. Structure is the most important feature of a scientific manuscript (title, authors, abstract, materials and methods, references, etc.), and this is much more relevant than the layout (fonts, page margins, etc.).</p><p>There are two different approaches to create structured manuscripts. Authors could use tools such as the <a href="https://web.archive.org/web/20150908031524/http://network.nature.com/people/mfenner/blog/2008/11/07/interview-with-pablo-fernicola">Microsoft Word Article Authoring Add-in</a> or <a href="https://web.archive.org/web/20150908031524/http://network.nature.com/people/mfenner/blog/2009/02/27/lemon8-xml-interview-with-mj-suhonos">Lemon8-XML</a> and submit structured manuscripts in the <a href="https://web.archive.org/web/20150908031524/http://www.inera.com/nlmresources.shtml">NLM DTD</a> XML format. Or journal submission systems could improve the process of creating structured manuscripts from standard word processor files (e.g. Microsoft Word). To better understand the second option, I asked Richard Wynne from <a href="https://web.archive.org/web/20150908031524/http://www.editorialmanager.com/homepage/about.html">Aries</a> a few questions about <a href="https://web.archive.org/web/20150908031524/http://www.editorialmanager.com/homepage/home.htm">Editorial Manager</a>.</p><h3 id="1-can-you-describe-what-editorial-manager-is-and-does"><em><em>1. Can you describe what Editorial Manager is and does?</em></em></h3><p>Editorial Manager is like plumbing for scholarly publishing. It manages the flow of scholarly manuscripts from submission to acceptance. Like good plumbing it should be invisible, reliable and afford some luxury.</p><p>While most of us could theoretically do our own plumbing, we usually discover that it's better to pay a professional. It's the same with online peer review systems. Most scholarly societies, publishers and university presses could develop and host their own workflow systems; but have discovered that it's less messy and less expensive to use a commercial solution such as Editorial Manager.</p><p>More than 3,100 journals from 150 publishers have adopted Editorial Manager. In the interest of fairness, I should mention other available solutions:</p><ul><li><a href="https://web.archive.org/web/20150908031524/http://pkp.sfu.ca/?q=ojs">Open Journal Systems</a>,</li><li><a href="https://web.archive.org/web/20150908031524/http://scholarone.com/products/manuscript/">Manuscript Central</a>,</li><li><a href="https://web.archive.org/web/20150908031524/http://www.ejpress.com/index.shtml">eJournal Press</a>,</li><li><a href="https://web.archive.org/web/20150908031524/http://benchpress.highwire.org/">BenchPress</a>.</li></ul><p>Thanks to a healthy competitive environment, online peer review is one of the most innovative areas of scholarly publishing.</p><h3 id="2-in-what-document-formats-can-manuscripts-be-submitted-to-editorial-manager-can-manuscripts-be-submitted-from-online-word-processors-such-as-google-docs">2. In what document formats can manuscripts be submitted to Editorial Manager? Can manuscripts be submitted from online word processors such as Google Docs?</h3><p>There are virtually no technical limitations on the types of files that can be loaded into Editorial Manager. However:</p><ul><li>Only some file types (such as Office, LaTeX, and image formats etc.) can be automatically converted to PDF format. Other types of file (e.g. Video or audio files) can be uploaded and are accessible during workflow from links in the generated PDF, and from the manuscript file inventory for authorized editors.</li><li>Superimposed on this technical handling is journal policy. Acceptable submission items (e.g. manuscript, data set etc.) are configured by the journal according to their idiosyncratic workflow preferences. So while one journal using Editorial Manager may permit the upload of supplemental data, another may choose not to make this option available.</li></ul><p>We have not interfaced with Google Docs at an API level, but this would become a priority if large numbers of authors found it a productive authoring tool for scholarly manuscripts. In the interim Google Docs provides many supported download file format options such as RTF.</p><h3 id="3-why-do-most-publishers-prefer-not-to-have-manuscripts-submitted-as-pdf-file-s-">3. Why do most publishers prefer not to have manuscripts submitted as PDF file(s)?</h3><p>Editorial Manager does support upload of manuscripts in PDF format. However many publishers discourage this practice for good reasons:</p><ul><li>Some journals ask reviewers to directly edit the manuscript in word processing format – obviously not possible if the author uploads a PDF.</li><li>If the manuscript is accepted, the publisher will need access to original source files to undertake copy editing, image formatting, composition etc. Obtaining source files up-front allows the publisher to accelerate the workflow and eliminates the time-consuming and cumbersome step of obtaining source files from the author after acceptance. Publishers that do accept PDF submissions must ensure that subsequently submitted source files match the revised manuscript that was accepted by the editors as an error prone and unnecessary step.</li></ul><p>It's unfortunate that publishers don't take the time to explain their reasoning regarding PDF submissions. This contributes to the scholar street wisdom that publishers are out of touch.</p><h3 id="4-can-manuscripts-be-transferred-from-preprint-servers-such-as-arxiv-or-nature-preceedings">4. Can manuscripts be transferred from preprint servers such as arXiv or Nature Preceedings?</h3><p>Yes, manuscripts can be directly transferred from the <a href="https://web.archive.org/web/20150908031524/http://arxiv.org/">arXiv</a> server by entering the appropriate arXiv number during the submission process. Editorial Manager then automatically collects the source files from the arXiv server. The feature is journal configurable.<br>We have not yet implemented a similar feature for <a href="https://web.archive.org/web/20150908031524/http://precedings.nature.com/">Nature Precedings</a>.</p><h3 id="5-how-does-editorial-manager-support-the-nlm-dtd-format-can-manuscripts-be-submitted-in-that-format">5. How does Editorial Manager support the <a href="https://web.archive.org/web/20150908031524/http://www.inera.com/nlmresources.shtml">NLM DTD</a> format? Can manuscripts be submitted in that format?</h3><p>Editorial Manager does process files with structured DTDs, but that's not really the point of your question. Here's the issue: reliably structured manuscripts theoretically present workflow benefits:</p><ul><li>During submission, manuscript metadata could be automatically harvested from the file, thereby avoiding author re-keying of title, abstract, etc.</li><li>Journal styling and composition could be applied automatically, thereby avoiding manual steps currently required to structure unformatted manuscripts – a massive cost and time saving.</li></ul><p>Today these benefits remain largely theoretical because they depend entirely on authors uploading standardized, structured manuscripts. The question is: who should have ultimate responsibility for manuscript structure quality? In my opinion not authors – their primary focus should be manuscript content not manuscript format. Part of the publisher's role is to take care of manuscript structuring. Trying to offload this responsibility to authors is not a good use of their time.</p><p>Eventually authoring tools could solve the problem by enabling transparent insertion of structure during manuscript authoring, but initiatives in this area are still immature in terms of technical feasibility, operational convenience and economic sustainability. Until this changes, weâ€™re focused on adding server-side tools to Editorial Manager that donâ€™t place any extra technical or financial burden on the author.</p><h3 id="6-does-editorial-manager-help-with-the-formatting-of-bibliographies-e-g-by-checking-the-references-against-online-databases-or-the-formatting-of-references-in-the-journal-style">6. Does Editorial Manager help with the formatting of bibliographies, e.g. by checking the references against online databases or the formatting of references in the journal style?</h3><p>Yes, Editorial Manager provides this facility, but format styles are determined by the individual journals/publishers that use the system.</p><p>Journals can select an Editorial Manager option that automatically links author submitted bibliographies to PubMed and/or CrossRef. The system can also format the author's bibliography to journal style. This means that we broadly accept whatever style the author has used. We power the service with <a href="https://web.archive.org/web/20150908031524/http://www.inera.com/extylesinfo.shtml">eXtyles</a>. The output of the process is clean XML of the bibliography.</p><p>This is an excellent example of how Editorial Manager improves workflow without displacing work back to the author. Alternative approaches are burdensome to the author because they require her to pre-format the bibliography or mandate the purchase/use of reference management tools and plug-ins.</p><h3 id="7-what-is-the-preferred-format-for-graphics-can-editorial-manager-help-with-conversions-into-a-different-format-e-g-tiff-to-pdf-">7. What is the preferred format for graphics? Can Editorial Manager help with conversions into a different format (e.g. TIFF to PDF)?</h3><p>Editorial Manager has no preferred graphic format. The preference is determined by the journal/publisher using the system.</p><p>Preferences are the result of the publisher's production and content delivery objectives. For example, a journal that re-draws graphics may not care about format. A journal that produces high quality print may reject RGB images, but RGB images would typically be acceptable for an online-only journal.</p><p>Editorial Manager does include an automatic image checking option. Journals that configure this feature can provide feedback to authors concerning the acceptability of submitted images. Just to be clear, this tool provides feedback and education, it does not prevent submission.</p><h3 id="8-does-editorial-manager-support-the-sword-simple-web-service-offering-repository-deposit-protocol">8. Does Editorial Manager support the <a href="https://web.archive.org/web/20150908031524/http://www.elearning.ac.uk/features/sword">SWORD</a> (Simple Web-service Offering Repository Deposit) protocol?</h3><p>Back in 2002 Aries proposed an XML-based standard and anticipated that the “Submission and Manuscript eXchange Format” (SMXF) would provide a system-neutral standard for the exchange of manuscript metadata and content . The broad adoption of such a standard would provide key benefits (see 2002 <a href="https://web.archive.org/web/20150908031524/http://www.editorialmanager.com/homepage/press-releases/200211.pdf">Press Release</a>). Despite our best efforts, there was little interest at the time, and as a consequence Editorial Manager supports dozens of XML input/output formats. So, from our point of view, the emergence of a standardized manuscript transfer format is a great boon and I've no doubt that SWORD deposit will soon be an Editorial Manager feature.</p><h3 id="9-how-can-authors-give-feedback-e-g-to-report-problems-or-request-features">9. How can authors give feedback, e.g. to report problems or request features?</h3><p>Most user feedback comes indirectly via publishers so that they can filter editorial and policy questions. However, we are also happy to hear suggestions directly from users. They can reach us at <a href="https://web.archive.org/web/20150908031524/mailto:marketing%40edmgr.com">marketing@edmgr.com</a>. Users are also invited to talk to their journals/publishers about participating in User Group meetings (London and Boston) or the <a href="https://web.archive.org/web/20150908031524/http://www.editorialmanager.com/homepage/emdiscussion.html">Listserv</a> discussions.</p><h3 id="10-what-are-your-responsibilities-within-the-editorial-manager-team">10. What are your responsibilities within the Editorial Manager team?</h3><p>A distinguishing characteristic of Editorial Manager is that it is genuinely the result of a broad-based team effort. I joined Aries approximately 10 years ago and have been privileged to participate in the growth of Editorial Manger from idea to sustainable solution processing more than 1,000,000 submissions per year. Along the way I have forged key relationships, and led the product management, sales and marketing team.</p><h3 id="11-what-did-you-do-before-starting-to-work-in-the-editorial-manager-team">11. What did you do before starting to work in the Editorial Manager team?</h3><p>After graduating from the University of Edinburgh, I started my career in software but jumped at the opportunity to start a multimedia company for <a href="https://web.archive.org/web/20150908031524/http://www.ovid.com/site/products/tools/silverplatter/access_tools.jsp">SilverPlatter</a> in the early 90's. Working with scientists we published interactive video, audio, graphics and text on CD-ROM. In those days few scientists owned CD-ROM drives so I'd carry one around with me. At one point I remember a professor at Cornell excitedly showing me something called <a href="https://web.archive.org/web/20150908031524/http://de.wikipedia.org/wiki/NCSA_Mosaic">Mosaic</a> and thinking: that's just hypertext. Since then I have been a lot more inquisitive about innovations that originate in academia!</p><h3 id="12-do-you-want-to-talk-about-future-plans-for-editorial-manager">12. Do you want to talk about future plans for Editorial Manager?</h3><p>There are many opportunities to innovate and improve the experience for authors, reviewers and editors; and we work on a rich list of suggestions and enhancements. We do not announce innovation details until they are close to being deployed, but there are a couple of great releases coming this year.</p><p>Recently we received the following comment form an editor: Editorial Manger has the “feel” of actually responding to the user. I think that a number of subtleties account for this impression, including the language used, the flow of the algorithm, and the customized real time feedback to the user. Our ambition is to achieve and surpass this level of satisfaction for all users.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Word processor support in citation managers: Is there a better way?]]></title>
            <link>https://blog.martinfenner.org/posts/word-processor-support-in-citation-managers-is-there-a-better-way</link>
            <guid>175956bf-2d08-4101-a548-88709222418c</guid>
            <pubDate>Sat, 21 Mar 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[Citations of the relevant literature are an essential feature of scientific
papers. Reference Manager software helps adding these citations and creating a
bibliography. Are there differences in how reference managers work together with
your word processor of choice?

Support for your favorite word processor
Support for word processors other than Microsoft Word is spotty. The Mendeley
plugin
[https://web.archive.org/web/20150906104449/http://www.mendeley.com/download_client] 
is only a few months]]></description>
            <content:encoded><![CDATA[<p>Citations of the relevant literature are an essential feature of scientific papers. Reference Manager software helps adding these citations and creating a bibliography. Are there differences in how reference managers work together with your word processor of choice?</p><h3 id="support-for-your-favorite-word-processor">Support for your favorite word processor</h3><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20150906104449im_/http://farm4.static.flickr.com/3447/3371450153_62cbe77494.jpg" class="kg-image" alt></figure><p>Support for word processors other than Microsoft Word is spotty. The <a href="https://web.archive.org/web/20150906104449/http://www.mendeley.com/download_client">Mendeley plugin</a> is only a few months old, and the latest Zotero release (1.5b) <a href="https://web.archive.org/web/20150906104449/http://www.zotero.org/support/word_processor_integration">broke the plugin for Word 2004</a>. I expect the Word for Macintosh support of both these tools to become better over time. Google Docs doesn't have any reference manager integration. This greatly limits its usefulness for writing scientific papers. The RefWorks plugin connects to an online database, so you can't add or edit references without an internet connection.</p><p>Most word processor plugins add an extra menu that allows to add and edit citations (allowing the user to search the reference manager database), and to add and edit bibliographies (allowing the user to pick a citation style, see below). Most reference managers also allow scanning for reference tags in documents produced by other word processors (e.g. in the .rtf format), but that process requires a few extra steps.</p><h3 id="support-for-your-favorite-journal">Support for your favorite journal</h3><p>I think it is very unfortunate that paper authors have to deal with a large number of different citation styles. All that we really need for paper references is the DOI (e.g. <a href="https://web.archive.org/web/20150906104449/http://dx.doi.org/10.1038/455708a">doi:10.1038/455708a</a>) to make the reference automatically identifiable and some basic information (authors, title, journal, year, issue) to make the reference readable. But it is beyond my understanding why anybody would care about formatting details such as whether the pulication year appears before or after the journal name. There have been initiatives to standardize the formatting of references (e.g. <a href="https://web.archive.org/web/20150906104449/http://www.nlm.nih.gov/citingmedicine/">Citing Medicine</a>), but for now paper authors have to format their bibliographies in the style required by the journal. Citation styles are an important asset for those that write reference manager software. Many people will recall that Thomson Reuters (who makes Endnote) <a href="https://web.archive.org/web/20150906104449/http://www.nature.com/nature/journal/v455/n7214/full/455708a.html">sued</a> George Mason University (who makes Zotero) last year, because Zotero added a feature that could convert Endnote .ens citation style files into <a href="https://web.archive.org/web/20150906104449/http://xbiblio.sourceforge.net/csl/">Citation Style Language</a> .csl files. Mendeley is also using .csl for citation styles.</p><h3 id="is-there-a-better-way">Is there a better way?</h3><p>Word processor plugins are fragile and usually break when a new software version is released (see for example <a href="https://web.archive.org/web/20150906104449/http://www.endnote.com/support/en_wpchart_mac.asp">this chart</a> for Endnote and Word for Macintosh). Native word processor support for references allows a much tighter integration into the word processor interface. Lastly, documents produced by different reference managers are not interchangeable, as each plugin uses a slightly different formatting approach. This means you can't write on a paper using Endnote and send it to your coauthor who uses Zotero.</p><p>The latest versions of Microsoft Word (2007 and 2008 Macintosh) have <a href="https://web.archive.org/web/20150906104449/http://blogs.msdn.com/joe_friend/archive/2006/07/13/664960.aspx">built-In support for citations and bibliographies</a>, but this feature is severely limited for the requirements of academic papers. Only a handful of citation styles are supported, adding more styles is possible but requires <a href="https://web.archive.org/web/20150906104449/http://blogs.msdn.com/microsoft_office_word/archive/2007/12/14/bibliography-citations-1011.aspx">some serious skills in XML editing</a>. References are stored in one flat file (<a href="https://web.archive.org/web/20150906104449/https://www.uwec.edu/help/Word07/bib-srcfile.htm">Sources.xml</a>) and can't be searched. OpenOffice is also struggling with built-in <a href="https://web.archive.org/web/20150906104449/http://bibliographic.openoffice.org/">bibliographic support</a>.</p><p>LaTex has long included support for references using <a href="https://web.archive.org/web/20150906104449/http://www.bibtex.org/About">BibTex</a> and shows how citation support should be done. Tools like <a href="https://web.archive.org/web/20150906104449/http://jabref.sourceforge.net/">JabRef</a> or <a href="https://web.archive.org/web/20150906104449/http://bibdesk.sourceforge.net/">BibDesk</a> extend this functionality, and most reference managers will import/export bibtex files.</p><p>Both Microsoft Word and OpenOffice should open up their citation APIs to third-party tools. This would create better citation tools, allows the easier exchange of documents between authors (journal submissions), and would it make easier for smaller tools such as Papers to integrate with word processors (a workaround is described <a href="https://web.archive.org/web/20150906104449/http://vnoel.wordpress.com/2008/02/14/papers-word-2008-bibliography-heaven/">here</a>). If they wait too long, we will probably see the online word processors such as Google Docs, Zoho Writer start adding an API for citations and bibliographies and all of the sudden become very serious alternatives for writing scientific papers. <a href="https://web.archive.org/web/20150906104449/http://network.nature.com/people/mfenner/blog/2009/02/27/lemon8-xml-interview-with-mj-suhonos">Lemon8-XML</a> already has very good bibliography support.</p><p><em><em>P.S. Bruce D'Arcus has recently come to similar conclusions (<a href="https://web.archive.org/web/20150906104449/http://community.muohio.edu/blogs/darcusb/archives/2009/03/01/the-babel-of-citations">The Babel of Citations</a>).</em></em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Reference Manager Overview]]></title>
            <link>https://blog.martinfenner.org/posts/reference-manager-overview</link>
            <guid>41501d61-5038-4f4c-88c7-36f0dfd8848f</guid>
            <pubDate>Sun, 15 Mar 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[18 April 2009: I've updated the chart and added 2collab, the “Read” category,
sharing of PDF files and the Mendeley bookmarklet and fulltext search.

15 July 2009: I've updated the chart to indicate that Mendeley and labmeeting
have integrated PDF viewers, and that 2collab can search Scopus.

20 July 2009: I've added the offline version of RefWorks for Windows, EndNote
OpenOffice plugin and fulltext search in several tools.

22 February 2010: Many small changes, including a few more categories. ]]></description>
            <content:encoded><![CDATA[<figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20150905233309im_/http://blogs.plos.org/mfenner/files/2010/11/Reference%20Manager%20Overview%202.3.jpg" class="kg-image" alt></figure><p><em><em>18 April 2009: I've updated the chart and added 2collab, the “Read” category, sharing of PDF files and the Mendeley bookmarklet and fulltext search.</em></em></p><p><em><em>15 July 2009: I've updated the chart to indicate that Mendeley and labmeeting have integrated PDF viewers, and that 2collab can search Scopus.</em></em></p><p><em><em>20 July 2009: I've added the offline version of RefWorks for Windows, EndNote OpenOffice plugin and fulltext search in several tools.</em></em></p><p><em><em>22 February 2010: Many small changes, including a few more categories. Added Citavi and dropped 2collab, LabMeeting and Connotea.</em></em></p><p><em><em>6 August 2010: Added Mendeley API and iPhone app, EndNote X4 features.</em></em></p><p><em><em>19 September 2010: Moved chart to <a href="https://web.archive.org/web/20150905233309/http://blogs.plos.org/mfenner/reference-manager-overview/">different location</a>, PDF download, Creative Commons license.</em></em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Zotero: Interview with Trevor Owens]]></title>
            <link>https://blog.martinfenner.org/posts/zotero-interview-with-trevor-owens</link>
            <guid>05781106-bf0e-43b2-bd27-d69f2e0ae38a</guid>
            <pubDate>Wed, 04 Mar 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[Zotero [https://web.archive.org/web/20150906100323/http://www.zotero.org/] is a
reference manager built as an extension of the Firefox
[https://web.archive.org/web/20150906100323/http://www.mozilla-europe.org/en/firefox/] 
web browser. The best introduction to Zotero is probably this short video.

Last week the newest version (1.5 beta) was announced
[https://web.archive.org/web/20150906100323/http://www.zotero.org/blog/zotero-15-beta-released-join-us-in-the-clouds/] 
on the Zotero blog. Among t]]></description>
            <content:encoded><![CDATA[<p><a href="https://web.archive.org/web/20150906100323/http://www.zotero.org/">Zotero</a> is a reference manager built as an extension of the <a href="https://web.archive.org/web/20150906100323/http://www.mozilla-europe.org/en/firefox/">Firefox</a> web browser. The best introduction to Zotero is probably this short video.</p><p>Last week the newest version (1.5 beta) <a href="https://web.archive.org/web/20150906100323/http://www.zotero.org/blog/zotero-15-beta-released-join-us-in-the-clouds/">was announced</a> on the Zotero blog. Among the most exciting new features is the synchronization of library data with the Zotero server, which in the future will allow a lot of interesting social features. I asked <em><em>Trevor Owens</em></em> from Zotero a few questions about Zotero, particularly some of the new features.</p><h3 id="1-can-you-describe-what-zotero-is-and-does">1. Can you describe what Zotero is and does?</h3><p>Zotero is an easy-to-use yet powerful research tool that helps you gather, organize, and analyze sources (citations, full texts, web pages, images, and other objects), and lets you share the results of your research in a variety of ways. An extension to the popular open-source web browser Firefox, Zotero includes the best parts of older reference manager software — the ability to store author, title, and publication fields and to export that information as formatted references — and the best parts of modern software and web applications (like iTunes and <a href="https://web.archive.org/web/20150906100323/http://www.delicious.com/">del.icio.us</a>), such as the ability to interact, tag, and search in advanced ways. Zotero integrates tightly with online resources; it can sense when users are viewing a book, article, or other object on the web, and on most major research and library sites it will find and automatically save the full reference information for the item. Since it lives in the web browser, it can effortlessly transmit information to, and receive information from, other web services and applications; since it runs on one's personal computer, it can also communicate with software running there (such as Microsoft Word). And it can be used offline as well (e.g., on a plane, in an archive without WiFi).</p><h3 id="2-why-is-zotero-a-firefox-plugin-and-not-a-desktop-application">2. Why is Zotero a Firefox plugin and not a desktop application?</h3><p>For most researchers the web is the first and primary point of entry for their research process. We thought it would be ideal to integrate Zotero as tightly as possible with the interface researchers already use to interact with the the journals, libraries, and databases they regularly consult.</p><h3 id="3-why-is-zotero-a-firefox-plugin-and-not-a-web-based-application">3. Why is Zotero a Firefox plugin and not a web-based application?</h3><p>Two reasons, first as a extension Zotero can sit alongside any page a researcher visits. Many of our users will keep Zotero partway open as they work on research online, allowing them to organize and annotate their research without leaving the page they are on. Second, as an extension users have full access to their collections when they are offline. This is particularly important for researchers working in remote locations or with flakey connections. For example, researchers working in offline archives can manually add items and attach scans and photos. If you're writing a paper on a plane you can add citations to your documents. In short being inside the browser gives us the best of both worlds. Zotero offers direct connectivity to web content, while still always remaining accessible. The last thing I would note is that Zotero is rapidly becoming a web application. With our newest release users can browse and share their collections online and in the near future users will be able to further manipulate their collections through our web application.</p><h3 id="4-did-you-consider-google-gears-for-offline-access">4. Did you consider Google Gears for offline access?</h3><p>Both <a href="https://web.archive.org/web/20150906100323/http://gears.google.com/">Google Gears</a> and Zotero rely on a local instance of <a href="https://web.archive.org/web/20150906100323/http://www.sqlite.org/about.html">sqlite</a> for data storage, but Zotero predates Gears by over a year. Google Gears is intended more to synchronize a web application for offline use, while Zotero fundamentally is a research database that users expect to be able to interact with fully regardless of their network connectivity.</p><h3 id="5-does-zotero-work-with-google-docs">5. Does Zotero work with Google Docs?</h3><p>Yes, users can drag and drop citations and bibliographic entries into google documents, or for that matter any sort of text field. We have a <a href="https://web.archive.org/web/20150906100323/http://www.zotero.org/support/zotero_and_google_tools">short screencast</a> which demos this functionality.</p><h3 id="6-can-you-briefly-describe-the-citation-style-language-csl-">6. Can you briefly describe the Citation Style Language (CSL)?</h3><p><a href="https://web.archive.org/web/20150906100323/http://www.zotero.org/support/dev/creating_citation_styles">CSL</a> is an XML language for citation formatting. It is designed to provide a nice balance between power and ease-of-use. It is also designed to be independent of any particular application, document format, or programming language.</p><h3 id="7-how-can-zotero-users-share-their-references-with-others">7. How can Zotero users share their references with others?</h3><p>At the moment users can export collections and libraries and email them to associates. Users can also share their library online, and by next week users will be able to import references directly from any users shared library. By next month users will be able to create groups for more seamless sharing of references and attachments.</p><h3 id="8-for-zotero-2-0-you-plan-to-offer-users-the-ability-to-share-collections-with-others-through-the-zotero-server-is-there-a-difference-to-similar-services-e-g-connotea-citeulike-or-mendeley-and-do-you-plan-integration-with-them">8. For Zotero 2.0 you plan to offer users the ability to share collections with others through the Zotero Server. Is there a difference to similar services (e.g. Connotea, CiteULike or Mendeley) and do you plan integration with them?</h3><p>In many ways the collaborative features currently in the works for Zotero are similar to the other services you mention. I think the biggest difference is the way in which sharing collections through groups will be tightly coupled into the Zotero client, and writing applications through our Word and Open Office plugins. The social and collaborative features we are launching directly connect into our hundreds of thousands of users' existing workflows. Zotero is also a non-commercial, open-source project directed by academics who are committed to enabling scholarship. Finally, Zotero is oriented toward storing anything related to your research (papers, books, audio, video, datasets, images, etc) while other solutions are almost entirely oriented toward working with research papers.</p><h3 id="9-what-are-your-responsibilities-within-the-zotero-project">9. What are your responsibilities within the Zotero project?</h3><p>Over the last two years as the community lead and evangelist I have been responsible for spreading the word about Zotero through workshops and presentations at conferences and institutions, as well as helping support the ever growing community of users, evangelists, and developers through Zotero's forums and by writing a majority of Zotero's user documentation.</p><h3 id="10-what-did-you-do-before-starting-to-work-on-zotero">10. What did you do before starting to work on Zotero?</h3><p>Before working on Zotero I worked as the press coordinator for the Games Learning and Society Conference in Madison and as a Academic Advisor at the University of Wisconsin. My undergraduate degree is in the History of Science.</p><h3 id="11-do-you-want-to-talk-about-future-plans-for-zotero">11. Do you want to talk about future plans for Zotero?</h3><p>You can consult our development roadmap online <a href="https://web.archive.org/web/20150906100323/http://www.zotero.org/support/development_roadmap">here</a>. Beyond that I would recommend taking a look at the work we are doing with the internet archive <a href="https://web.archive.org/web/20150906100323/http://www.dancohen.org/2007/12/12/zotero-and-the-internet-archive-join-forces/">here</a>. Once you take a look at those I would be happy to <a href="https://web.archive.org/web/20150906100323/http://www.zotero.org/support/contact_us">answer any questions</a> that come up.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Lemon8-XML: Interview with MJ Suhonos]]></title>
            <link>https://blog.martinfenner.org/posts/lemon8-xml-interview-with-mj-suhonos</link>
            <guid>ad9abc92-a6d3-448a-a4b1-18a2dfa3c102</guid>
            <pubDate>Fri, 27 Feb 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[Finishing an exciting research project and writing it up in a paper are the
first two steps in getting your work published. The next two steps – submitting
your paper to a journal and getting it through the review process – have changed
dramatically in the last 10-15 years. No longer do we have to print out our
manuscript using one of the few available laser printers in the department,
paste our gel pictures on cardboard and number the figures with Letraset. And
then send it off with the mail. A]]></description>
            <content:encoded><![CDATA[<p>Finishing an exciting research project and writing it up in a paper are the first two steps in getting your work published. The next two steps – submitting your paper to a journal and getting it through the review process – have changed dramatically in the last 10-15 years. No longer do we have to print out our manuscript using one of the few available laser printers in the department, paste our gel pictures on cardboard and number the figures with Letraset. And then send it off with the mail. And then repeat the process for every revision of the manuscript.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/02/2276034665_456342fb50_n.jpg" class="kg-image" alt><figcaption><a href="https://web.archive.org/web/20151003091211/http://www.flickr.com/photos/synaptichappenings/2276034665/">Flickr Photo</a> by synaptic.happenings.</figcaption></figure><p>Now of course we submit our manuscripts online using a manuscript submission system such as <a href="https://web.archive.org/web/20151003091211/http://www.editorialmanager.com/homepage/home.htm">Editorial Manager</a> or <a href="https://web.archive.org/web/20151003091211/http://www.ejpress.com/index.shtml">eJournalPress</a>. Which is not to say that the process is necessarily easy or fun. Many of us can tell stories of spending hours or whole days until the manuscript is finally submitted. We struggle with the conversion of the different parts of the manuscript into a single PDF file, have problems with fonts, have to deal with different graphics formats (e.g. PDF, JPEG, EPS, TIFF), don’t use the correct style for our references, etc.</p><p>The flip side of this is the time and money spent at the journal to format your accepted manuscript into something that can be turned into a journal article published online or in print.</p><p>Some of these problems wouldn’t exist if we used a common document format for manuscript writing, manuscript revisions and manuscript printing and online viewing. That common document format does exist and is called <a href="https://web.archive.org/web/20151003091211/http://dtd.nlm.nih.gov/publishing/">NLM Journal Publishing DTD</a> (and no, it’s not LaTex). This document format is used in the workflow of many journals, but until now has rarely been used by authors. Last November <a href="https://web.archive.org/web/20151003091211/http://network.nature.com/people/mfenner/blog/2008/11/07/interview-with-pablo-fernicola">I talked with Pablo Fermicola</a> about a free tool that allows Microsoft Word to save manuscripts in that format (<a href="https://web.archive.org/web/20151003091211/http://www.microsoft.com/DOWNLOADS/details.aspx?FamilyID=09c55527-0759-4d6d-ae02-51e90131997e&amp;displaylang=en">Microsoft Word Authoring Add-In</a>). <a href="https://web.archive.org/web/20151003091211/http://www.inera.com/extylesinfo.shtml#NLM">eXtyles NLM</a> is another tool for Microsoft Word with similar functionality.</p><p><a href="https://web.archive.org/web/20151003091211/http://pkp.sfu.ca/lemon8">Lemon8-XML</a> takes a different approach in helping academic authors convert their manuscripts into the NLM DTD format. Version 1.0 of the software <a href="https://web.archive.org/web/20151003091211/http://pkp.sfu.ca/node/1891">was released today</a>, so this was a great opportunity to talk with the lead developer <em><em>MJ Suhonos</em></em> about Lemon8-XML.</p><h3 id="1-can-you-describe-what-lemon8-xml-is-and-does">1. Can you describe what Lemon8-XML is and does?</h3><p>Lemon8-XML is a freely-available, open source web application that aims to help academic authors and editors convert scholarly articles from layout formats like Microsoft Word to structured XML formats without requiring a great deal of knowledge of XML or the schema that they’re working with. It’s based on the <a href="https://web.archive.org/web/20151003091211/http://opendocument.xml.org/overview">OpenDocument</a> format internally, and the <a href="https://web.archive.org/web/20151003091211/http://dtd.nlm.nih.gov/publishing/">NLM Journal Publishing DTD</a> as the default export format.</p><p>Originally, Lemon8-XML came from a bunch of scripts that performed a series of tasks:</p><ul><li>convert an author’s article from a myriad of document formats to OpenDocument,</li><li>parse the article and try to extract metadata, determine section structure based on layout information, and parse citations into their disparate elements,</li><li>export this semantic data as XML.</li></ul><p>The work to date on Lemon8-XML has been building these functions into an easy-to-use web UI that also provides some editing facilities; for example, to fix problems from incomplete or incorrect parsing. One of the most promising features of Lemon8-XML is its ability to parse citations in a wide range of formats, and automatically try to correct or enhance them by searching for their complete records in, eg. PubMed, CrossRef, and WorldCat.</p><h3 id="2-what-are-the-advantages-of-using-lemon8-xml-over-traditional-word-processors-such-as-microsoft-word">2. What are the advantages of using Lemon8-XML over traditional word processors such as Microsoft Word?</h3><p>I should start by clarifying that Lemon8-XML wasn’t really intended to be used as a word processor, but rather a conversion tool to be used after the writing was done. This was to fit in with existing practices, where authors upload papers to a journal and the editing is done afterwards. Of course, the advent of online word processors challenges this notion, and has caused us to think differently about the editing aspects of Lemon8-XML. Cross-platform deployment and online collaboration are things that obviously the web is great for. An author could, for example, upload a paper to Lemon8-XML, and work together with an editor on that same copy, regardless of what desktop tools they have. One advantage of this idea is that the burden of work is shared between more people, whether that’s an author and a copyeditor, or multiple editors. This is a big deal for small journals who don’t have funds for dedicated staff.</p><p>Within PKP in general, we also try to keep the requirements for our software as low as possible, both technically and financially. So, for journals like <a href="https://web.archive.org/web/20151003091211/http://www.openmedicine.ca/">Open Medicine</a>, who are entirely volunteer-run, paying $230 per copy for Microsoft Word on each computer actually represents a significant cost. Using OpenOffice on the desktop and a single copy of Lemon8-XML on a server — both of which are free — lets them spend this out-of-pocket expense on other things to keep their journal alive and growing. Microsoft Word also has an unfortunate history of being variable and inconsistent across platforms and versions. We wanted to help people escape that, which is one of the reasons for converting everything to OpenDocument as early as possible. In addition, stewarding documents through an online workflow can be a laborious and frustrating process, so the possibility for journals to make the editing process more centralized and interactive by integrating Lemon8-XML is a compelling one.</p><h3 id="3-how-does-lemon8-xml-compare-to-the-microsoft-word-article-authoring-add-in-that-also-produces-nlm-journal-publishing-dtd-output">3. How does Lemon8-XML compare to the Microsoft Word Article Authoring Add-In, that also produces NLM Journal Publishing DTD output?</h3><p>In many ways, we’re working in parallel with Pablo Fermicola’s group at Microsoft — <a href="https://web.archive.org/web/20151003091211/http://perspectives.on10.net/blogs/jonudell/Word-for-scientific-publishing/">they’re basically building an editor</a> for placing a structured NLM schema on top of the Microsoft WordML (DOCX) XML format, while Lemon8-XML places the NLM schema on top of the OpenDocument (ODT) XML format. Of course, there’s nothing saying that Lemon8-XML couldn’t be modified to read DOCX files generated with the Article Authoring Add-In or vice-versa. This difference is really a reflection of the two competing standards, and the tools they use, with the same ultimate goal: to allow users to generate semantically-structured documents from layout-based ones. In addition, the Article Authoring Add-In is built on Microsoft Word 2007 as a platform, which I think has some limitations; for example, most of the PKP team don’t use Windows, so the Article Authoring Add-In is basically inaccessible to us. But overall, our goals appear to be very similar.</p><p>To my mind, the main difference between the projects is in how they approach the user: the Article Authoring Add-In presents new tools for adding semantic mark-up in a (somewhat) familiar interface; many authors are already familiar with Microsoft Word and are comfortable working in that environment. My concern with this approach is that it still has strong ties to the layout paradigm, so, for example, marking text as “article title” may not be sufficiently different in authors’ minds from marking text as “16 point bold”. I also think the idea of presenting an entire document in a single free-form editor reinforces the layout paradigm, as opposed to identifying the distinct elements which comprise a scholarly article. Lemon8-XML, on the other hand, builds from these individual elements, and assembles them within the user interface based on their structural relations. You can see this reflected in the tabs in the current interface: “Metadata, Sections, Citations” — these are the front, body, back matter of an article.</p><p>This places restrictions on how a user can edit their article, which is sometimes frustrating, but it forces them to think about what they’re doing and the meaning of their content: why do I have to place it here, what content is valid in this element, etc. In a future version, this will all be on the same web page, similar to how it would appear in Microsoft Word, but again with the semantic structure visually enforced instead of being totally free-form.</p><h3 id="4-what-is-the-difference-between-lemon8-xml-and-online-word-processors-such-as-google-docs">4. What is the difference between Lemon8-XML and online word processors such as Google Docs?</h3><p>Unlike most word processor software, Lemon8-XML is built around the semantic notion of a document, not its appearance. We wanted to help people begin to think about the meaning and structure of their articles, not just whether they look good on the screen or as a PDF. It turns out this is a tough challenge, though — people have a hard time with WYSIWYM (What You See Is What You Mean) editors, and there’s often a lot of complex structure to present visually for editing. One thing people seem to be comfortable with, or at least used to, is entering metadata and information in web forms, so much of the Lemon8-XML user interface is built that way.</p><p>Lemon8-XML is also specifically aimed at modeling and editing scholarly articles, which have a long history of practice and some very rigid conventions that aren’t applicable more broadly. This means it’s not a general-purpose editor like most word processors, but that also means we can focus on the specific things it should do, and refine them rather than trying to please everyone and falling victim to feature-creep.</p><h3 id="5-why-did-you-pick-the-nlm-journal-publishing-dtd-as-document-format">5. Why did you pick the NLM Journal Publishing DTD as document format?</h3><p>I did a survey in 2003 of available DTDs to represent scholarly journal articles, looking for something I could use as a common source for generating HTML and PDF. I selected the NLM DTD above the others (eg. BioMed Central, <a href="https://web.archive.org/web/20151003091211/http://www.docbook.org/whatis">DocBook</a>, <a href="https://web.archive.org/web/20151003091211/http://www.erudit.org/apropos/info.html">Erudit</a>, <a href="https://web.archive.org/web/20151003091211/http://www.tei-c.org/index.xml">Text Encoding Initiative</a>) since it seemed to strike a good balance between comprehensiveness and granularity; it can be as simple or as complex as you need it to be. It also had a very thorough citation model and a modular design, so extensibility wasn’t a concern — I liked the “journal articles plus” concept that it was built with. And, of course, being stewarded by the National Library of Medicine meant it would likely be well-maintained and remain open. The fact that it became <a href="https://web.archive.org/web/20151003091211/http://www.pubmedcentral.nih.gov/about/faq.html#q21">the central XML standard for PubMed Central as an archival format</a> was just icing on the cake.</p><p>Another reason is that, among the STM journals at least, a lot of established practice has developed around generating HTML and PDF from NLM XML specifically, and we want to support that. Actually, we want to expand adoption of the NLM DTD by making it accessible to humanities and social science journals, as well. The information contained in the XML is essentially identical regardless of subject, which means we just need to develop discipline-specific rendering; for example, for different citation styles. I think there’s an incredible amount of room for growth and improvement in this area in particular.</p><h3 id="6-does-lemon8-xml-integrate-with-manuscript-submission-systems-such-as-open-journal-systems">6. Does Lemon8-XML integrate with manuscript submission systems such as Open Journal Systems?</h3><p>Not yet, but that’s the next major area of development immediately following the 1.0 release. <a href="https://web.archive.org/web/20151003091211/http://pkp.sfu.ca/?q=ojs">OJS</a> has been so successful, one of our biggest challenges has been keeping it flexible enough to support a wide range of workflows, and we want to continue with that by taking as much of the technology from Lemon8-XML as possible and folding it into OJS. I don’t know if we’ll see a side-by-side kind of integration immediately, but rather strategic enhancements of certain aspects of OJS: automatic document conversion, annotation directly within articles, extraction and stripping of metadata to improve blindedness, automatically parsing and linking citations, etc. By going this way, we give users the ability to choose specific features that are useful for them, and at the same time build upon the huge community that already exists for OJS in terms of development, testing, and feedback.</p><p>There’s definitely still value in a stand-alone Lemon8-XML for cases where people don’t want journal workflow, or where they’re integrating with a different kind of system. So, we will be working on a Lemon8-XML 2.0 that’s built on the same modular, reliable framework as the rest of the PKP suite, and back-porting code from OJS.</p><h3 id="7-what-is-the-public-knowledge-project">7. What is the Public Knowledge Project?</h3><p><a href="https://web.archive.org/web/20151003091211/http://pkp.sfu.ca/node/1410">PKP</a> is a small research group distributed between Simon Fraser University, University of British Columbia, Stanford University and Arizona State University that has been quietly <a href="https://web.archive.org/web/20151003091211/http://www.openmedicine.ca/article/view/276">developing open source software for online publishing and knowledge sharing</a> for the past 10 years, under the direction of Dr. <a href="https://web.archive.org/web/20151003091211/http://ed.stanford.edu/suse/faculty/displayRecord.php?suid=willinsk">John Willinsky</a> at Stanford. Our major aim is to provide tools for improving access to academic research, as well as helping improve the quality and efficiency of its production. Our philosophy is also to create partnerships between researchers, librarians, publishers, to help them build sustainable and globally accessible scholarly infrastructure. In my mind, it’s also about giving people options and choices aside from what’s being offered commercially, especially to those who can’t afford them, like a lot of developing-world journals.</p><p>We have four applications in addition to Lemon8-XML: <a href="https://web.archive.org/web/20151003091211/http://pkp.sfu.ca/?q=ojs">Open Journal Systems</a>, our most popular, is being used by over 2500 journals in over 50 countries. We also produce a conference management system (<a href="https://web.archive.org/web/20151003091211/http://pkp.sfu.ca/?q=ocs">Open Conference Systems</a>), an OAI metadata indexing system (<a href="https://web.archive.org/web/20151003091211/http://pkp.sfu.ca/?q=harvester">Open Archives Harvester</a>), and a monograph publishing system (<a href="https://web.archive.org/web/20151003091211/http://pkp.sfu.ca/omp">Open Monograph Press</a>), currently under development. All of our software is freely available under the GPL open source license, and we have an active community of over 1500 users. We’ve worked with small, volunteer-run humanities journals, to major international society conferences, to high-profile, ISI-ranked medical journals.</p><h3 id="8-what-are-your-responsibilities-within-the-lemon8-xml-project">8. What are your responsibilities within the Lemon8-XML project?</h3><p>Because our group’s so small, we all share responsibilities — there are only a handful of people to do almost all of the development, and that’s in addition to handling the support, correspondence, and software maintenance of the rest of the PKP suite, not to mention providing various international workshops. So, we often divvy roles based on expertise or interest. Since bringing Lemon8-XML to PKP, I do the vast majority of Lemon8-XML development, but I also manage collaborations with our development and research partners, and write white papers on the software design we use to share our ideas, on top of contributing to the daily stuff above.</p><p>Probably the biggest struggle in developing Lemon8-XML has been that it’s one person, about half-time, doing all of the coding, testing, etc. We already have a number of very exciting partnerships with groups who will be testing and providing feedback, and have contributed a talented developer who’s recently started half-time as well. Of course, we can always use more help with the coding side of things. Our team is quite spread-out geographically, so we have a lot of experience with working across time zones and with distributed development practices. I’m hoping that after the 1.0 release, more people will become interested and will want to help get involved. Whether contributing plugins for citation processing, or helping us improve the UI — really, anything that helps the software grow will benefit more authors and editors, which is the main goal.</p><h3 id="9-what-did-you-do-before-starting-to-work-on-lemon8-xml">9. What did you do before starting to work on Lemon8-XML?</h3><p>I worked for a two-person medical informatics journal that was publishing around 50 articles a year using Microsoft FrontPage and managed entirely via email. My job was to transition it to an online manuscript management system (naturally, I chose OJS), convert around 300 back articles from FrontPage HTML into valid NLM XML, develop a custom rendering system that would create HTML and PDF galleys from that XML, get the journal accepted into PubMed Central, and establish an impact factor from ISI (which is now 3.0) — oh, and continue publishing 50 articles a year. The journal also ran an international medical conference during that time (using OCS). Three years later, I left to do my Masters degree in Library/Information Studies, during which time I joined PKP and started working on Lemon8-XML, based on a lot of the things I’d learned from my time at the journal.</p><h3 id="10-do-you-want-to-talk-about-future-plans-for-lemon8-xml">10. Do you want to talk about future plans for Lemon8-XML?</h3><p>The most common question I’m asked is, “when will it be ready?”, which for many people really means, “when will it be a one-click magic bullet for getting my journal into PubMed Central?”. I’m proud of the Lemon8-XML 1.0 release, and I feel that it represents a significant milestone given how far we’ve come — but there’s still a long way to go before we get to that point. I hope people will view the 1.0 version as a stable tool that’s already being used in production by at least one journal, and a strong indicator of PKP‘s commitment to developing research and software in this area.</p><p>I mentioned the Open Journal Systems integration, which is both a strategic and pragmatic move; this will be our main focus, as well as integration with OMP from its earliest inception. We also want to improve the editing aspect of Lemon8-XML — there is some great work being done in the area of using style templates for providing structural mark-up in traditional word processors; we will be working more closely with Peter Sefton’s group on the ICE project, who have a lot of experience. I’d also like to re-visit the idea of using web-based WYSIWYM editors like <a href="https://web.archive.org/web/20151003091211/http://www.wymeditor.org/">WYMeditor</a> or an enhanced <a href="https://web.archive.org/web/20151003091211/http://tinymce.moxiecode.com/">TinyMCE</a> as part of a user interface overhaul, and possibly integrating more closely with Google Docs if that’s an option. Certainly, there’s room for adding more citation lookup plugins: <a href="https://web.archive.org/web/20151003091211/http://www.oaister.org/about.html">OAIster</a>, Amazon.com, <a href="https://web.archive.org/web/20151003091211/http://citeseer.ist.psu.edu/citeseer.html">CiteSeer</a>, etc. as well as improving the existing ones. Finally, we want to try applying the approach we’ve used with citation parsing and lookup with other scholarly article elements; for example, checking quotations for correctness and plagiarism, extracting and linking author/contributor identifiers, and so on. We also have some ideas around <a href="https://web.archive.org/web/20151003091211/http://www.dlib.org/dlib/may06/apps/05apps.html">OpenURL</a> that may or may not come into Lemon8-XML development.</p><p>We’re also starting a major side-project based entirely around the NLM DTD in two parts: 1) building mappings from various other XML schemas to NLM; and 2) building a standard set of rendering tools for generating HTML and PDF from NLM XML, in a way that can be easily customized for an individual journal. There are already a lot of groups out there using NLM but currently the practice is quite fragmented — we’d like to see these journals and publishers become more connected and share their work as a community. As I say, we want to help increase adoption as a way to raise the bar on journal quality and improve options for publishing and archiving.</p><p>One of the most important things we’ve learned during the development of Lemon8-XML has been the value of user feedback early and often, and remaining aware of related work that’s going on, so we can remain efficient by building partnerships instead of working in parallel. We’ll continue pursuing this approach, and I’d encourage anyone who is interested in any of these areas or has ideas of their own to contribute to <a href="https://web.archive.org/web/20151003091211/http://pkp.sfu.ca/contact">get in touch with us</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Papers for iPhone released – time for more poetry]]></title>
            <link>https://blog.martinfenner.org/posts/papers-for-iphone-released-time-for-more-poetry</link>
            <guid>9b3ec5c0-2801-4cfb-addc-d0ea797b86d0</guid>
            <pubDate>Thu, 19 Feb 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[Papers [https://web.archive.org/web/20150924053123/http://mekentosj.com/papers/] 
is a great Macintosh program to manage the PDF files of scientific papers on
your (Macintosh) computer. I've mentioned several times that I like the program
and I interviewed
[https://web.archive.org/web/20150924053123/http://network.nature.com/people/mfenner/blog/2008/10/03/interview-with-alexander-griekspoor] 
the author Alex Griekspoor back in October. Today the iPhone version of Papers
[https://web.archive.org/]]></description>
            <content:encoded><![CDATA[<p><a href="https://web.archive.org/web/20150924053123/http://mekentosj.com/papers/">Papers</a> is a great Macintosh program to manage the PDF files of scientific papers on your (Macintosh) computer. I've mentioned several times that I like the program and I <a href="https://web.archive.org/web/20150924053123/http://network.nature.com/people/mfenner/blog/2008/10/03/interview-with-alexander-griekspoor">interviewed</a> the author Alex Griekspoor back in October. Today the <a href="https://web.archive.org/web/20150924053123/http://mekentosj.com/papers/iphone/">iPhone version of Papers</a> was released and it is a great companion for those moments when you <em><em>really</em></em> have to look up that particular <em><em>Nature</em></em> paper while discussing <em><em>the release of calcium from intracellular stores</em></em> in the bar.</p><p>As a thank you for beta testing the iPhone version, Alex Griekspoor is giving me three copies of Papers for Mac (or rather three serial numbers, Papers can be downloaded and used for 30 days without registration). As I already own Papers, and in the spirit of some recent poetry <a href="https://web.archive.org/web/20150924053123/http://network.nature.com/people/mfenner/blog/2008/11/30/why-do-we-blog-and-other-important-questions-answered-by-34-science-bloggers">here on Nature Network</a> (and elsewhere in the blogosphere, answers to question #11), I'm giving these serial numbers away to the best poetry about using the iPhone for finding, reading or writing science papers. Please post your poetry in the comments section of this blog post, the submission deadline is next Thursday at 8 PM.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Interview with Geoffrey Bilder]]></title>
            <link>https://blog.martinfenner.org/posts/interview-with-geoffrey-bilder</link>
            <guid>9226f9e4-0604-4faa-a58d-466f7900d1d1</guid>
            <pubDate>Tue, 17 Feb 2009 20:27:00 GMT</pubDate>
            <description><![CDATA[Almost exactly two years ago, CrossRef
[https://web.archive.org/web/20090221213233/http://www.crossref.org/] invited a
number of people to discuss unique identifiers for researchers (CrossRef Author
ID meeting
[https://web.archive.org/web/20090221213233/http://www.crossref.org/CrossTech/2007/02/crossref_author_id_meeting.html]
). One year ago Thomson Reuters launched ResearcherID (Thomson Scientific
launches ResearcherID to uniquely identify authors
[https://web.archive.org/web/20090221213233/ht]]></description>
            <content:encoded><![CDATA[<p>Almost exactly two years ago, <a href="https://web.archive.org/web/20090221213233/http://www.crossref.org/"><strong>CrossRef</strong></a> invited a number of people to discuss unique identifiers for researchers (<a href="https://web.archive.org/web/20090221213233/http://www.crossref.org/CrossTech/2007/02/crossref_author_id_meeting.html"><strong>CrossRef Author ID meeting</strong></a>). One year ago Thomson Reuters launched <strong>ResearcherID</strong> (<a href="https://web.archive.org/web/20090221213233/http://network.nature.com/people/mfenner/blog/2008/01/21/thomson-scientific-launches-researcherid-to-uniquely-identify-authors"><strong>Thomson Scientific launches ResearcherID to uniquely identify authors</strong></a>). And two months ago Phil Bourne and Lynn Fink wrote about this topic in a  <strong>PLoS Computational Biology</strong> paper (<a href="https://web.archive.org/web/20090221213233/http://dx.doi.org/10.1371/journal.pcbi.1000247"><strong>I Am Not a Scientist, I Am a Number</strong></a>).</p><p>So it comes as no surprise that we also talked about author identifiers at the recent <a href="https://web.archive.org/web/20090221213233/http://www.scienceonline09.com/index.php/wiki/"><strong>ScienceOnline09</strong></a> meeting in North Carolina (both in the session <a href="https://web.archive.org/web/20090221213233/http://www.scienceonline09.com/index.php/wiki/Reputation_authority_and_incentives/"><strong>Impact Factors and researcher incentives</strong></a> and over drinks afterwards). <a href="https://web.archive.org/web/20090221213233/http://network.nature.com/people/U42E63119/profile"><strong>Cameron Neylon</strong></a> wrote down his thoughts after the meeting in a blog post (<a href="https://web.archive.org/web/20090221213233/http://blog.openwetware.org/scienceintheopen/2009/01/20/a-specialist-openid-service-to-provide-unique-researcher-ids/"><strong>A specialist OpenID service to provide unique researcher IDs?</strong></a>), and this resulted in <a href="https://web.archive.org/web/20090221213233/http://friendfeed.com/e/c1fd00ec-15f9-d894-4ea9-4ffeaac5ae28/A-specialist-OpenID-service-to-provide-unique"><strong>a very interesting</strong></a> discussion on <strong>FriendFeed</strong>. And at about the same time both <a href="https://web.archive.org/web/20090221213233/http://network.nature.com/people/jandot/profile"><strong>Jan Aerts</strong></a> (<a href="https://web.archive.org/web/20090221213233/http://saaientist.blogspot.com/2009/01/who-o-o-are-you-who-who-who-who.html"><strong>Who-o-o are you? Who who? Who who?</strong></a>) and Christopher Leonard (<a href="https://web.archive.org/web/20090221213233/http://blogs.openaccesscentral.com/blogs/pmcblog/entry/some_thoughts_on_unique_author"><strong>Some thoughts on unique author IDs</strong></a>) independently wrote blog posts about the same topic. This week <a href="https://web.archive.org/web/20090221213233/http://network.nature.com/people/U42E63119/profile"><strong>Cameron Neylon</strong></a> summarized the discussion in another blog post (<a href="https://web.archive.org/web/20090221213233/http://blog.openwetware.org/scienceintheopen/2009/02/15/contributor-ids-an-attempt-to-aggregate-and-integrate/"><strong>Contributor IDs – an attempt to aggregate and integrate</strong></a>).</p><p>Science bloggers have put a lot of thought into the idea of a unique author identifier and I’ve collected more reading material about author ID at <strong>Connotea</strong> using the tag <a href="https://web.archive.org/web/20090221213233/http://www.connotea.org/tag/authorid"><strong>authorid</strong></a>. But I was also very curious to learn more about the work that has already been done. That’s why I asked <a href="https://web.archive.org/web/20090221213233/http://network.nature.com/people/gbilder/profile"><strong>Geoffrey Bilder</strong></a> from <strong>CrossRef</strong> a few questions. In the end Geoffrey talked not only about author identifiers, but also about CrossRef, DOIs and many other aspects of scholarly publishing.</p><h3 id="1-can-you-describe-what-crossref-is-and-does">1. Can you describe what CrossRef is and does?</h3><p>Let me start with what it does because this is a little less likely to make your eyes glaze-over.</p><p>CrossRef was originally founded by scholarly publishers to fight <a href="https://web.archive.org/web/20090221213233/http://en.wikipedia.org/wiki/Link_rot"><strong>link-rot</strong></a>.</p><p>Web links have a half-life of about six years. That is, after six years a link is likely to break because the content that it pointed to has been moved. To a lay-audience this might be a mere annoyance, but to scholarly and professional publishers broken links are anathema. The scholarly record is built on a foundation of links in the form of citations. If these online citation links break, the online scholarly citation record breaks.<br><br>But surely fighting link-rot should be simple, right? After all, the glory of the web is its decentralized architecture, one in which the domain name that you own can be used as a namespace for identifiers. <a href="https://web.archive.org/web/20090221213233/http://en.wikipedia.org/wiki/Tim_Berners-Lee"><strong>Tim Berners-Lee</strong></a> has said that “cool URIs never die”. Aren’t “persistent URIs” merely a matter of being disciplined in the way that you mint URIs and in being conscientious about sensibly redirecting URIs when things change location on your web server?</p><p>Well, certainly the majority of broken links on the web are the result of careless web administrators not taking the time to structure and redirect their web site’s URIs properly, but there are a significant percentage of links that will break despite the best efforts of webmasters. This is because in some cases the domain name in the link will change, and in these cases the whole “domain name as URI minting name-space” starts to crumble. When otherwise sensible technorati refer to “owning” a domain name, it makes me want to stick forks in my eyeballs. We do not “own” domain names. At best, we only lease them and there are manifold ways in which we could lose control of a domain name – through litigation, through forgetfulness, through poverty, through voluntary transfer, etc. Once you don’t control a domain name anymore, then you can’t control your domain-name-based persistent identifiers either.</p><p>Incidentally, another technorati meme that makes me want to self-harm with cutlery is the notion that “persistent identifiers don’t matter in the age of the search engine. If a link breaks, we can just find the content again wherever it has moved.” This, naturally, is the self-serving argument often used by Google and it’s starry-eyed acolytes. Even just few minutes’ reflection reveals the gaping hole in this approach.</p><p>The hole is this – how do you cite a specific copy of something if there are multiple <strong>almost identical</strong> copies of it located in different places? For instance, lets say there are 2 copies (X and Y) of an article which only differ in a few paragraphs, but those few paragraphs are crucial and likely to change the reader’s interpretation of the work. How, in a world where persistent linking is maintained by search engines, do you create a citation link to article X instead of article Y? To create a search-based link that is more likely to resolve to article X, you would essentially have to encode the entire article in the search URI! Even this wouldn’t guarantee you that the link directed a future reader to article X first; it is still possible that article Y might end up getting a higher ranking because more people have linked to it and it therefore has a higher page-rank (or equivalent). Believe me, even variations of the search engine scenario (using document hashes for citations, pingbacks, etc.) quickly unravel after a little reflection.</p><p>This is all a long-winded and ranty way of saying that the issue of persistent identifiers on the web is just a wee bit more complex than most people think. So how does CrossRef address the persistent identifier issue?</p><p>From our perspective, the persistent identifier problem is much more a social problem than a technical one.</p><p>In fact, the technical part of our service is relatively straightforward. CrossRef provides a level of indirection (i.e. a pointer) between an identifier and a URL. When publishers put something online, they assign a CrossRef <a href="https://web.archive.org/web/20090221213233/http://www.doi.org/"><strong>Digital Object Identifier</strong></a> (DOI) to it and submit a record for that item with CrossRef. The record includes the CrossRef DOI, basic bibliographic metadata for the content and a URL that points to the current location of the content. People citing the publisher’s content are encouraged to use the CrossRef DOI for the citation instead of the publisher’s URL. When a researcher clicks on a CrossRef DOI, the CrossRef service redirects the URL to whatever URL the publisher has currently registered for that CrossRef DOI. This means that the publisher can update their CrossRef DOI record to point to a completely new URI (including a new domain name) and any CrossRef DOI citations will continue to work. We provide a few other services based on this infrastructure too. So, for instance, we can resolve an <a href="https://web.archive.org/web/20090221213233/http://www.oclc.org/research/projects/openurl/default.htm"><strong>OpenURL</strong></a> to a CrossRef DOI (by querying the publisher-submitted bibliographic metadata), resolve a free-text query to a CrossRef DOI and we can also return bibliographic metadata instead of redirecting if that is what the user wants.</p><p>So-far, so good, but this isn’t anything that couldn’t be accomplished using other redirection tools such as <a href="https://web.archive.org/web/20090221213233/http://www.purl.org/"><strong>PURLs</strong></a>, <a href="https://web.archive.org/web/20090221213233/http://www.handle.net/"><strong>CNRI Handles</strong></a>, <a href="https://web.archive.org/web/20090221213233/http://www.oasis-open.org/committees/tc_home.php?wg_abbrev=xri"><strong>XRIs</strong></a>, <a href="https://web.archive.org/web/20090221213233/http://numly.com/numly/default.asp"><strong>NUmly Numbers</strong></a>, etc.? The crucial question to ask of any such service is, “what guarantees that the publisher will actually update their URL pointers?” If the publisher doesn’t update these pointers, then the links will break anyway. It isn’t enough that a publisher decides to use PURLs, if they then don’t update their PURLs- in perpetuity.</p><p>This is where it is important to explain the organizational structure and the social effect that this has on the service.</p><p>CrossRef was founded as a non-profit, membership organization for publishers. Note that we are entirely catholic in our definition of what a publisher is, so our membership includes commercial publishers, non-profit publishers, open access publishers, institutional repositories, NGOs and IGOs, Video publishers, Wiki-based publishers, etc. We are also open to publishers of all disciplines (humanities, social sciences, sciences, professional), geographies and content types (journals, books, database records, videos, etc.)</p><p>In practice, what unites our membership is a concern that their content should be considered worthy of trust by professional researchers. One way in which researchers assess the trustworthiness of content is by determining how it sits within the scholarly record. Does it provide evidence for its assertions in citations? Do other people cite it?</p><p>When a publisher joins CrossRef, they agree to a <a href="https://web.archive.org/web/20090221213233/http://www.crossref.org/02publishers/59pub_rules.html"><strong>set of enforceable terms and conditions</strong></a> that govern the way in which they use CrossRef’s persistent citation infrastructure. Specifically, they agree to:</p><ul><li>Register DOIs within a week of something being published online</li><li>Update the URLs associated with a DOI when said URLs change</li><li>Link citations in their content via the DOI</li></ul><p>In joining CrossRef they also agree that CrossRef can fine them or throw them out of the service if they do not meet the terms and conditions of the service. Note that the penalty of being thrown out can be quite severe as it effectively means that the publisher would become invisible in the online scholarly citation record. In short, the system has a built-in social feedback loop that strongly enforces good citizenship.</p><p>As I said, the technical infrastructure of CrossRef is pretty mundane, and it is the social aspect of the service that does the most to guarantee the persistency of CrossRef citation links.</p><h3 id="2-what-are-your-responsibilities-within-crossref">2. What are your responsibilities within CrossRef?</h3><p>Thinking of, gathering the requirements for, designing and (most importantly) launching new services.</p><p>Last year we launched a plagiarism detection service called <a href="https://web.archive.org/web/20090221213233/http://www.crossref.org/crosscheck.html"><strong>CrossCheck</strong></a>. This year I am working on Contributor ID, another project tentatively named CrossMark and a bunch of smaller projects designed to encourage the use of DOIs in citations.</p><h3 id="3-what-did-you-do-before-starting-to-work-for-crossref">3. What did you do before starting to work for CrossRef?</h3><p>In the early nineties Allen Renear and I co-founded Brown University’s Scholarly Technology Group, where we were charged with providing advanced consulting and support to Brown’s research community. In the mid-nineties I grew tired of the politics, resource constraints and institutional paralysis that seems to grip so many universities and I decided to do something as far away from the academic sphere as possible. In short, I worked at a management consultancy doing R&amp;D for their IT group. In 2000 I was lured into managing the web development efforts for an Information Architecture firm called Dynamic Diagrams. In 2001 we were bought by <a href="https://web.archive.org/web/20090221213233/http://www.ingentaconnect.com/"><strong>Ingenta</strong></a> in the UK. I became Ingenta’s CTO and I moved to Oxford in 2002. I left Ingenta in 2005, did a brief spell of consulting for publishers in 2006 and joined CrossRef in 2007.</p><h3 id="4-what-are-your-thoughts-on-how-an-author-identifier-should-look-like">4. What are your thoughts on how an author identifier should look like? </h3><p>First of all, I think we need to stop talking about “author” identifiers. One of the first requirements we found when interviewing publishers, researchers and librarians is that we would ideally like to be able to identify any party who contributes to the scholarly literature in any way. That is, we would also like to be able to identify reviewers, editors, correspondents, bloggers, commenters, etc. This is why we have taken to calling our project the “CrossRef Contributor ID” project. This isn’t just playing with words either. For instance, as soon as you start thinking about things like “how do you accommodate reviewers” in this system you need to think of things like pseudo-anonymity. That is, you want somebody to be able to get credit for doing reviews in a way that doesn’t necessarily reveal who reviewed what. In turn, the pay-off for designing a system whereby anonymous reviewers might be credited with reviews could be profound. It might ultimately result in researchers having much more incentive to review if reviewing were something that could be counted and rated in the same way that authorship is.</p><p>Second, I think that people conflate a lot of issues when they talk about “author identifiers” [sic]. Are they talking about the simple token used (e.g. a unique string or a number assigned to an individual like a social security number), are they talking about an authentication mechanism (e.g. <a href="https://web.archive.org/web/20090221213233/http://openid.net/what/"><strong>OpenID</strong></a>, <a href="https://web.archive.org/web/20090221213233/http://shibboleth.internet2.edu/"><strong>Shibboleth</strong></a>) or are they talking about the profile information associated with an identifier (e.g. publications, affiliation, contact info, etc.)? Obviously, these all overlap in some ways, but how they relate and what you choose to focus on depends largely on your use cases.</p><p>Third, speaking of use cases, our requirements gathering has identified two broad categories of use cases that, though related, have profoundly different implementation implications. One category of use cases identified revolves around “knowledge discovery” and the other category of cases revolves around “authentication.”</p><p>The “knowledge discovery” use cases are probably the most obvious things that people would like to be able to do with a contributor ID such as:</p><ul><li>Determine what IDs authored/edited/reviewed document X</li><li>What documents where authored/edited/reviewed by ID Y</li><li>What IDs are related to ID Z and what is the nature of that relationship (e.g. co-authored, edited, reviewed)</li><li>What (subject to privacy settings) is the profile information for ID Z (e.g. institutional affiliation, email address, etc.)</li><li>All the author IDs and their respective publications where the institutional affiliation recorded by the author is X</li><li>Etc.</li></ul><p>At this point I feel obliged to point out that the bulk of our requirements gathering has been focused on trying to understand the needs of our member publishers. The reason I mention this here is that the bulk of the “authentication” use cases that we identified are all focused around making publisher back-office systems less cumbersome. So, for instance, publishers are interested in using a “contributor id” for:</p><ul><li><a href="https://web.archive.org/web/20090221213233/http://en.wikipedia.org/wiki/Single_sign-on"><strong>single sign-on</strong></a> (SSO) for manuscript tracking systems</li><li>Disambiguating contact information for use by editorial offices, royalty payments systems, copyright clearances, etc.</li><li>Automatic updating of email addresses for table of contents (TOC) alerts and other automated email communications</li><li>Automated tools for detecting potential reviewers, including tools for detecting potential conflicts of interest</li><li>Synchronization with publisher web site user profiles and granting researchers customized, privileged access to content based on profiles</li><li>Understanding all of the manifold ways in which an individual “contributes” to a publisher or a field (e.g. As an editor, reviewer, letter writer, conference chair, etc.).</li><li>Etc.</li></ul><p>As I said, these are very publisher-focused use cases, but this is not to say that we are not interested in the use cases posed by librarians, researchers and funding agencies. We have actively been talking to people from each of these constituencies and we are trying to understand if there are ways in which we can help them. For instance, we have recently been speaking to a group of researchers who are interested in using some sort of authenticated contributor ID as a mechanism for controlling who gets trusted access to sensitive genome-wide aggregate genotype data.</p><p>The interesting thing to note about these “authentication” use cases is that they have far more stringent requirements than the “knowledge discovery” use cases. In other words if you are only trying to address the knowledge discovery problem, it might be fine to use automated techniques to disambiguate authors and assign IDs to them. State-of-the-art mechanisms for automatic disambiguation of authors from a defined corpus can be 96-97% accurate, which sounds pretty good. At least until you realize that CrossRef has ~200K new article DOIs deposited each month, each of which on average has about 3 authors. This could potentially leave you with upwards of 20K in mis/un-identified authors. This error rate might be an acceptable tradeoff for knowledge discovery type applications, but it certainly isn’t suitable for authentication type applications.</p><p>Speaking of authentication, I think the fourth thing to note is that, though I think <strong>OpenID</strong> will probably play an important role in any service we provide, by itself it makes a pretty bad identity token and would provide little utility on its own. This all gets back to some of the issues that I raised above when discussing persistent identifiers: URI-based identifiers are fragile because they depend on the domain name. What happens if your OpenID is tied to a domain that you don’t control (e.g. a company, an institution a country)? How can you guarantee that, should you leave that company/institution/country that they will do the right thing and let you maintain or redirect that identifying credential?</p><p>The traditional geeky response to this scenario is “don’t get yourself into that situation. Only tie your <strong>OpenID </strong>to a domain that you own.” (Insert forks in eyeballs). Again, you do not “own” a domain name. You lease it. What happens if you lose control of it due to litigation, forgetfulness, poverty, divorce, death? Death? Yes, what happens when somebody dies? When I die does the not-yet-born Georgia Bilder get to buy “my” domain “gbilder.com” and make it the basis of her identity? Mmm… Gets kind of complicated doesn’t it?</p><p>Of course, lots of the same issues can be raised with CrossRef, right? What guarantees that CrossRef won’t become evil and co-opt all of our identities? This, of course is the big fear underlining the knee-jerk reaction against “centralized systems” in favor of “distributed systems”. The problem with this, as I mentioned in the <a href="https://web.archive.org/web/20090221213233/https://friendfeed.com/e/c1fd00ec-15f9-d894-4ea9-4ffeaac5ae28/A-specialist-OpenID-service-to-provide-unique/"><strong>FriendFeed thread</strong></a> is that my personal and unfashionable observation is that “distributed” begets “centralized.” For every distributed service created, we’ve then had to create a centralized service to make it useable again (ICANN, Google, Pirate Bay, CrossRef, DOAJ, ticTocs, WorldCat, etc.). This gets us back to square one and makes me think the real issue is- how do you make the centralized system that eventually emerges accountable? This is, of course, a social issue more than a technical issue and involves making sure that whatever entity emerges has clearly defined data portability policies and a “living will” that attempts to guarantee that the service can be run in perpetuity- even if by another organization. For the record, I don’t think adopting the slogan “don’t be evil” is enough ;).</p><p>Anyway- I could go on talking about what the contributor ID “should look like” for a very, very long time, but I think that the above probably addresses some of the major points that are raised when the topic is discussed.</p><h3 id="5-what-are-the-benefits-and-maybe-disadvantages-if-crossref-manages-the-author-identifier">5. What are the benefits (and maybe disadvantages) if CrossRef manages the author identifier?</h3><p>I think the biggest potential disadvantage that CrossRef has is that it is a consensus-based organization that is governed by sometimes fierce competitors. This aspect of the organization can sometimes slow things down. On the other hand, this can also be a huge strength for us. Once a consensus is agreed, we can move very quickly and push uptake across the industry.</p><p>Research increasingly transcends institutional, geographic and discipline boundaries, so I think another advantage that we have is that we are well positioned to provide a service that is similarly unconstrained.</p><p>Finally, I think that we have a very interesting advantage by virtue of the fact that our infrastructure is already integrated upstream in the publication process. There is a useful property of the system that we are designing in that, as researchers used the CrossRef identifier in their interactions with publishers and this data is fed back into our system via DOI deposits, you could start to develop a trust-metric based on the types of claims attached to an author’s profile. For instance, an author profile that consisted of nothing but self-claims (e.g. I claim I wrote paper X) might not be very worthy of trust whereas an author profile that consisted of publications that had been verified by the publisher (by virtue of those publications having been processed along with the CrossRef contributor ID) would have far more credibility. You can start to see an interesting hierarchy of publication claims emerging such as:</p><ul><li>Proxy claims (Leigh claims Geoffrey wrote article X)</li><li>Self Claims (Geoffrey claims Geoffrey wrote article X)</li><li>Verified claims (Geoffrey claims Geoffrey wrote article X <strong>and</strong> the “Journal of Psychoceramics” confirms this claim)</li><li>Verified Proxy Claims (Geoffrey (who has already been verified as an author of article X) claims that Kirsty was also an author of article X)</li></ul><h3 id="6-how-does-your-author-identifier-relate-to-other-identifiers-e-g-researcherid-scopus-author-idor-openid">6. How does your author identifier relate to other identifiers, e.g. <a href="https://web.archive.org/web/20090221213233/http://www.researcherid.com/">ResearcherID</a>, <a href="https://web.archive.org/web/20090221213233/http://help.scopus.com/robo/projects/schelp/h_autsrch_intro.htm">Scopus Author ID</a>or <a href="https://web.archive.org/web/20090221213233/http://openid.net/what/">OpenID</a>?</h3><p>OpenID is a different kettle of fish, and I discussed it already above. As for the others (I’d add <a href="https://web.archive.org/web/20090221213233/http://www.authorresolver.com/"><strong>Author Resolver</strong></a>, <a href="https://web.archive.org/web/20090221213233/http://repec.org/"><strong>RePEC</strong></a>, <a href="https://web.archive.org/web/20090221213233/http://www.scilink.com/start.action"><strong>SciLink</strong></a>, <a href="https://web.archive.org/web/20090221213233/http://bibserver.berkeley.edu/cgi-bin/mathweb/index.py"><strong>MathPeople</strong></a>, <a href="https://web.archive.org/web/20090221213233/http://network.nature.com/"><strong>Nature Network</strong></a>, etc.), we’ve actually been talking to some of these parties in order to understand how they might relate to a CrossRef Contributor ID. One obvious difference is in the use-cases being addressed. All of the above are focused on “knowledge discovery” use-cases. None of them pretends to provide any sort of authentication services. It is also interesting to note that in a lot of the above cases, the parties see their author identification functionality as a means to an end. For instance, their primary application is “creating better metrics” or “running a social network” or “expert identification” for recruiting purposes. In these cases they don’t necessarily see a CrossRef system as being competitive and, in fact, they think that such a service might even improve their primary application.</p><h3 id="7-can-you-talk-about-the-current-status-and-next-planned-steps-of-the-contributorid-project">7. Can you talk about the current status and next planned steps of the ContributorID project?</h3><p>We just ended lengthy period of investigation and requirements gathering. In the process we went down a few blind alleys. Now we are working on a prototype that we will test with a few publishers. It is hard to say how long this will take as we are just in the process of planning this phase.</p><h3 id="8-satisfying-many-different-interests-is-one-of-the-biggest-challenges-in-creating-an-author-identifier-what-are-the-lessons-learned-from-implementing-the-digital-object-identifier-doi-">8. Satisfying many different interests is one of the biggest challenges in creating an author identifier. What are the lessons learned from implementing the digital object identifier (<a href="https://web.archive.org/web/20090221213233/http://www.doi.org/">DOI</a>)?</h3><p>I’ll give you one tactical lesson and one strategic lesson.</p><p>The tactical lesson is foremost in my mind because I have recently been trying to build tools to encourage researchers to use DOIs in their citations. The problem arrises when a researcher occasionally encounters a DOI that is 80 characters long. There is just no way that a researcher is going to insert <strong>that</strong> in a citation. The tactical lesson here is that it is sometimes better to make an identifier opaque and short. This is also a tremendously unfashionable position to take, but I think that one of Clay Shirky’s observations about hierarchical categorization systems also applies to identifiers. If you make the identifier human-interpretable and add semantics, then people will be extremely tempted to start hard-coding ontologies into their identifiers. This makes said identifiers both long and inherently brittle. The ontologies will inevitably evolve, and then people will want to change the identifiers- at which point they will either break or you have a giant identifier mapping subsystem to create.</p><p>We see a manifestation of this syndrome already with the DOI. Each DOI has a four-digit “prefix” which is effectively a namespace for the assigning publisher. Note that I said the “assigning” publisher- this is not necessarily the publisher who currently “owns” the DOI with that prefix. What this often means is that, when publisher A acquires publisher B, publisher A will ask CrossRef if we can create new DOIs for all of publisher B’s backfiles so that they all have the same prefix! The answer to their request is “no”, but you wouldn’t believe how stroppy publishers can get about this. They somehow imbue this ridiculous four-digit prefix with branding significance. This, of course, is absolutely mental, but it is a predictable form of mental. The French went mad when they had to replace their region-encoded license plates with opaque EU ones. People in the US go mad when they are given new area codes. In short, when people associate semantic significance in identifiers, you will face problems.</p><p>The strategic lesson is basically a recapitulation of the "technical vs “social” theme I’ve been banging on about. I think that, at first, even our membership thought of the CrossRef DOI as being a technical solution to a problem, not a social one. It has become much clearer to us over the years that CrossRef DOIs are only as persistent as CrossRef staff. That is, we sometimes have to bang on lots of heads and threaten members with fines and worse in order to make sure that they are meeting their terms &amp; conditions. The good news is that CrossRef has become essential infrastructure for a wide variety of publishers who are often at each other’s throats in any other circumstances. In many ways these “different interests” are our strength. Everybody wants it to work better, nobody wants to see it die and nobody wants it to be co-opted. We are working hard to put the social structures into place that will guarantee its longevity. Part of this is making sure that we are fiscally sound (which we are) and part of this is making sure that, even if we do disappear, other stakeholders can run the system if need be.</p><h3 id="9-what-can-researchers-interested-in-author-identifiers-do-to-help">9. What can researchers interested in author identifiers do to help?</h3><ul><li>Feed CrossRef more use cases.</li><li>Let CrossRef know what you think will/won’t work.</li><li>Make sure you let your publishers know if you think this is a good idea. Naturally, I expect you will also let them know if you think it is a bad idea ;-)</li></ul><p>I can be reached at <strong>gbilder at crossref dot org</strong>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Mutation, selection and metastasis]]></title>
            <link>https://blog.martinfenner.org/posts/mutation-selection-and-metastasis</link>
            <guid>e6118cd4-955d-4aaa-b1c5-a284571b1e6b</guid>
            <pubDate>Sat, 14 Feb 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[Mutation and selection are important concepts in cancer biology. One well-known
example is hereditary colon cancer. Patients with mutations in the DNA mismatch
repair genes MLH1 or MSH2 develop colon cancer because of an increased rate of
mutations. And tumors in patients with familial polyposis coli have a growth
advantage because of a mutation in the tumor suppressor gene APC. Mutations and
selection also help to explain the process of metastasis, the formation of
secondary tumor foci at dista]]></description>
            <content:encoded><![CDATA[<figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20151003091150im_/http://citizenship.typepad.com/blogfordarwin/DarwinBadge.gif" class="kg-image" alt></figure><p>Mutation and selection are important concepts in cancer biology. One well-known example is hereditary colon cancer. Patients with mutations in the DNA mismatch repair genes <em><em>MLH1</em></em> or <em><em>MSH2</em></em> develop colon cancer because of an increased rate of mutations. And tumors in patients with familial polyposis coli have a growth advantage because of a mutation in the tumor suppressor gene APC. Mutations and selection also help to explain the process of metastasis, the formation of secondary tumor foci at distant sites in the body. Understanding metastasis is important, because it is often this spread of the tumor that makes a previously localized cancer an incurable disease.</p><p>In 1977 a landmark paper in <em><em>Science</em></em> by Isaiah Fidler and Margaret Kripke (Fidler 1977) described a mouse model of lung metastasis using the syngeneic B16 mouse melanoma cell line. This melanoma cell line originated spontaneously in a C57BL/6 mouse in 1954 and is known to metastasize to the lung. They produced several clones of the B16 cells, each clone originating from a single cell and therefore genetically identical. Cells from these clones were then injected into the tail vein of C57BL/6 mice. The number of lung metastases in these mice varied dramatically between clones and was also different from the parental cell line. Fidler and Kripke concluded that a subpopulation of highly metastatic cells preexists in the parent population and is selected during the metastatic process.</p><p>In 1994 I started to work on a research project that tried to identify molecules responsible for the selection process in this B16 lung metastasis model. This was before microarrays and the sequencing of the mouse genome, and we used a technique called differential display to identify differentially expressed genes in two variant B16 cell lines that created low and high numbers of lung metastases. We identified a novel gene that turned out to be a transcriptional regulator (Shioda 1996) but probably is not that critical for the metastatic process.</p><p>In 2009 we have learned a lot more about the metastatic process, but we still know much more about genes involved in tumor cell proliferation, apoptosis, etc. than the genes critical for the metastatic process. And we still have not come up with a clever way to specifically treat that malignant subpopulation of tumor cells that will later produce metastatic disease.</p><h3 id="references">References</h3><p>Fidler, I., &amp; Kripke, M. (1977). Metastasis results from preexisting variant cells within a malignant tumor. Science, 197(4306), 893–895. <a href="https://doi.org/10.1126/science.887927">https://doi.org/10.1126/science.887927</a></p><p>Shioda, T., Fenner, M. H., &amp; Isselbacher, K. J. (1996). msg1, a novel melanocyte-specific gene, encodes a nuclear protein and is associated with pigmentation. Proceedings of the National Academy of Sciences, 93(22), 12298–12303. <a href="https://doi.org/10.1073/pnas.93.22.12298">https://doi.org/10.1073/pnas.93.22.12298</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Interview with Kevin Emamy]]></title>
            <link>https://blog.martinfenner.org/posts/interview-with-kevin-emamy</link>
            <guid>afb09dd6-f96e-4477-b834-45d0c25aa1bf</guid>
            <pubDate>Fri, 30 Jan 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[One interesting session at ScienceOnline09
[https://web.archive.org/web/20150922174120/http://scienceonline09.com/] was 
Social networking for scientists
[https://web.archive.org/web/20150922174120/http://www.scienceonline09.com/index.php/wiki/Social_networking_for_scientists/]
, moderated by Cameron Neylon and Deepak Singh. We now have so many of these
social networking sites, that it becomes difficult to differentiate between them
and to see how they can interact with each other. One important]]></description>
            <content:encoded><![CDATA[<p>One interesting session at <a href="https://web.archive.org/web/20150922174120/http://scienceonline09.com/">ScienceOnline09</a> was <a href="https://web.archive.org/web/20150922174120/http://www.scienceonline09.com/index.php/wiki/Social_networking_for_scientists/">Social networking for scientists</a>, moderated by Cameron Neylon and Deepak Singh. We now have so many of these social networking sites, that it becomes difficult to differentiate between them and to see how they can interact with each other. One important category is social bookmarking sites for scientists. I spoke with Kevin Emamy from <a href="https://web.archive.org/web/20150922174120/http://www.citeulike.org/">CiteULike</a> to find out more.</p><p><em><em>Most of the CiteULike team. Kevin is the second person from the right.</em></em></p><h3 id="1-can-you-describe-what-citeulike-is-and-does">1. Can you describe what CiteULike is and does?</h3><p>CiteULike is social bookmarking for research papers. It enables you to easily store references online, share them in groups and discover new ones.</p><p>It's run as a web service, so everything is always stored online. You can access it from any computer (most of us use more than one these days) and if you're lazy and stupid like me you're much less likely to lose anything. Sharing and discovery obviously work better this way too.</p><p>The social side of it is simply that by default everyones posts are public and you can see who has bookmarked the same papers as you, what tags they are giving them and from there you can go on to browse what else they are bookmarking.</p><p>And then we have groups which are shared libraries of posts, newsfeeds for every user action, RSS feeds, watchlists, <a href="https://web.archive.org/web/20150922174120/http://www.citeulike.org/citegeist">CiteGeist</a> (which is a hotlist of posts) and obviously search as ways to encourage this social discovery.</p><h3 id="2-do-your-users-basically-just-store-their-own-bookmarks-or-do-you-also-see-a-lot-of-use-of-the-social-networking-features">2. Do your users basically just store their own bookmarks or do you also see a lot of use of the social networking features?</h3><p>In the 30 days up to the Christmas Holidays 08, CiteULike received 116,477 posts and 6,189 of those were copies made directly from other user's librarys, which is the successful end result of the social stuff. That doesn't count people who find an article, go to the original source to read it, and then post it.</p><p>The other social stuff is hard to quantify without diving into the weblogs. Also, once you get into the realms of counting pageviews and linkouts etc. it becomes easy to mislead yourself about what is really going on, which is why we use “posts” as a genuine metric (assuming you are on top of the spam).</p><p>But the social discovery stuff is certainly used. Groups for example; well over 50% are active, by which I mean people have posted to them in the last 90 days. Watchlists (following another user's posts) and RSS are very active. Many people search the site, either directly or through RSS feeds.</p><p>One thing that is definitely true is that many more people browse the site than actually register and post, by a factor of say 5:1, and I'm discounting the random Google traffic that just bounces on and off.</p><p>Of course, the service would never have got off the ground if it didn't provide individual value to a user without any social features, so yes, many people use it just like that and always will.</p><h3 id="3-how-is-citeulike-different-from-other-social-bookmarking-tools-e-g-connotea-2collab-or-delicious">3. How is CiteULike different from other social bookmarking tools, e.g. Connotea, 2collab or delicious?</h3><p>It differs from <a href="https://web.archive.org/web/20150922174120/http://delicious.com/">delicious</a> because we extract and bookmark citation metadata along with the URL, so it's aimed at professional researchers and scientists. We have over 50 sites where we do this, covering most of the online sources of journal papers. Because of this specialization our userbase is very different and therefore much more relevant when you look at the community features. The tags, as one example, are very specialized.</p><p>There are lots of differences in detail with the other two 'scholarly' services but it seems that the users have voted with their feet (or should I say mice); CiteULike is far and away the most popular service. If you count the number of papers posted we estimate that CiteULike is currently 3-5 times the size of <a href="https://web.archive.org/web/20150922174120/http://www.connotea.org/">Connotea</a> both in total posts and posts on a daily basis (2 million+ posts and very little spam for CiteULike vs. 650k posts including a significant proportion of spam for connotea, 3k-5k daily posts for CiteULike vs. 1k to 1.5k claimed for Connotea).</p><h3 id="4-does-citeulike-integrate-with-other-social-networking-sites-and-services-for-scientists-e-g-connotea-mendeley-or-friendfeed-does-it-integrate-with-desktop-reference-managers">4. Does CiteULike integrate with other social networking sites and services for scientists, e.g. Connotea, Mendeley or FriendFeed? Does it integrate with desktop reference managers?</h3><p>You can export and import files to and from pretty much all these services.</p><p>In the case of <a href="https://web.archive.org/web/20150922174120/http://friendfeed.com/">FriendFeed</a> I have seen many people using RSS feeds to display their CiteULike posts there, which is great, it's a really good service.</p><p><a href="https://web.archive.org/web/20150922174120/http://www.mendeley.com/">Mendeley</a> are based in London like us and we have begun to discuss ways where we can integrate more tightly, the point being to make the workflow better for users of both services.</p><h3 id="5-what-is-your-policy-regarding-personal-pdf-files-uploaded-to-citeulike">5. What is your policy regarding personal PDF files uploaded to CiteULike?</h3><p>Only the person who uploaded a PDF can download it, so CiteULike is acting as an online storage drive.</p><p>We have also allowed uploads to “private groups” which are invite only and otherwise invisible. In this case the user will commit that they have a right to distribute the document to the people in the group,<br>who they already know.</p><h3 id="6-what-are-your-responsibilities-within-citeulike">6. What are your responsibilities within CiteULike?</h3><p>There are only five of us, so we don't really have roles. Unlike the rest of the team, I can't write code or design applications, so I have to leave that to my esteemed colleagues.</p><p>One of the things I try and do is to promote CiteULike and increase our userbase and traffic, which is a bit of a capricious art, but one way that has worked for us is to try to engage with, dare I say it,<br>the publishers.</p><p>Springer, who we <a href="https://web.archive.org/web/20150922174120/http://www.springer.com/company/citeulike?SGWID=0-164102-0-0-0">recently agreed a sponsorship with</a>, have been invaluable to us in this regard. They are one of the few major publishers who are really progressive and actually understand this stuff.</p><h3 id="7-what-did-you-do-before-starting-to-work-for-citeulike">7. What did you do before starting to work for CiteULike?</h3><p>We all worked in the software industry, sometimes together. Richard wrote CiteULike as a tool for his own use when he was back doing research at university.</p><h3 id="8-do-you-want-to-talk-about-future-plans-for-citeulike">8. Do you want to talk about future plans for CiteULike?</h3><p>Well the team are continuously improving the service, making it perform better, fixing things that break etc. I wish I could count the number of improvements that have been made over the last 18 months. That is part of the secret to it's growth, the users can tell when the developers are making an effort. We have a very active newsgroup where users make feature and functionality requests or report problems. I have regularly seen my colleagues put stuff live in a matter of hours following a user request, which is one of the advantages of a service that is run by it's developers.</p><p>We have wanted to do some kind of recommendation system for a long time; automated collaborative filtering or whatever. We certainly have the data to do it. But it is, I'm told, a hard problem if you want to get useful results. It's possible we'll work with someone else on this.</p><p>It was really interesting to see <a href="https://web.archive.org/web/20150922174120/http://www.plos.org/">PLoS</a> announcing the variety of impact metrics they want to publish about their articles, a small part of which is going to be social bookmarking posts. We are currently giving them this data on request, it would be nice to allow anyone to get this sort of data out of CiteULike themselves (we do already make the whole CiteULike dataset freely available for download).</p><p>Another example of that is that a lot of other sites want to display CiteULike tag data along with the relevant articles; there are 6.7 million user created tags on CiteULike now (that's total not distinct)<br>and those tags have been given by people who know the subjects. I suppose I am talking about some kind of API, which would also allow reference managers etc. to integrate with CiteULike more easily, as you asked about above.</p><p>Having said that, there is a great temptation to continually add more features or try to make it into something different to what it really is and I'm not sure if that is the right approach (though we have been guilty of it).</p><p>For example, one of the biggest issues we have is that CiteULike is good at matching articles that have been posted by hand but not articles imported by file upload. By “match” I mean identify that two articles from different sources are in fact the same, which is the basis of the social stuff; seeing who is reading what you're reading etc. Seeing as 40% or so of our posts come via file upload, it would be great to fix. Matching is done by DOI and PMID; however we ran some tests that showed only 5% of file uploaded articles contain these, so maybe we'll try a different approach.</p><p>So what we try to focus on is to make it better and more efficient at what it does well: social bookmarking for research papers. There is plenty of work to do in that regard.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Interview with Moshe Pritsker]]></title>
            <link>https://blog.martinfenner.org/posts/interview-with-moshe-pritsker</link>
            <guid>8bf7ed73-c38b-4940-858c-47560783477b</guid>
            <pubDate>Sat, 24 Jan 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[The ScienceOnline09
[https://web.archive.org/web/20150924053056/http://scienceonline09.com/] meeting
last weekend not only had a number of interesting presentations and discussions,
but even more importantly, was a great opportunity to personally interact with a
great number of people that share your ideas and interests. It was the first
time I personally met Moshe Pritsker
[https://web.archive.org/web/20150924053056/http://network.nature.com/people/U06343707/profile]
, the CEO, Editor-in-Chief ]]></description>
            <content:encoded><![CDATA[<p>The <a href="https://web.archive.org/web/20150924053056/http://scienceonline09.com/">ScienceOnline09</a> meeting last weekend not only had a number of interesting presentations and discussions, but even more importantly, was a great opportunity to personally interact with a great number of people that share your ideas and interests. It was the first time I personally met <a href="https://web.archive.org/web/20150924053056/http://network.nature.com/people/U06343707/profile">Moshe Pritsker</a>, the CEO, Editor-in-Chief and co-founder of the Journal of Visualized Experiments (<a href="https://web.archive.org/web/20150924053056/http://www.jove.com/">JoVE</a>), <em><em>a peer-reviewed, free access, online journal devoted to the publication of biological research in a video format</em></em> (taken from <a href="https://web.archive.org/web/20150924053056/http://www.jove.com/index.stt">About JoVE</a>). As I like to write about new technologies that help scientists publish their research, I asked Moshe a few questions after the meeting.</p><h3 id="1-can-you-describe-what-jove-is-and-does">1. Can you describe what JoVE is and does?</h3><p><em><em>JoVE</em></em> is the first video journal for biological and biomedical research. We publish articles that include step-by-step video demonstrations of experimental techniques and procedures. For example, <a href="https://web.archive.org/web/20150924053056/http://www.jove.com/index/details.stp?ID=923">A Behavioral Assay to Measure Responsiveness of Zebrafish to Changes in Light Intensities</a> by the group of John Dowling at Harvard or <a href="https://web.archive.org/web/20150924053056/http://www.jove.com/index/details.stp?ID=1067">Calcium Imaging of Cortical Neurons using Fura-2 AM</a> by the group of Ricardo Dolmetsch at Stanford. We call these articles video-articles or video-protocols. They also include a text part which is similar to traditional scientific articles (abstract, introduction, experiment, materials and references).</p><p>This novel video-based approach to scientific publishing is applied to increase reproducibility and transparency of experimental studies, which is the <em><em>bottleneck</em></em> problem in the life sciences today. As every bench scientist knows, it is very difficult to repeat biological experiments based on their text description in traditional scientific journals. This is because the text format presents a requirement for scientists to understand the complex reality and numerous <em><em>small</em></em> details of the experiment from reading. It is very difficult. Visualization through video provides a solution to this problem by clear unambiguous demonstration of experimental techniques and procedures. Using this new approach will enable increase efficiency and productivity across all the areas of biological research and drug discovery.</p><p>Being a scientific journal, <em><em>JoVE</em></em> is indexed in PubMed and MEDLINE, and has an editorial board of 22 distinguished professors from Harvard, Princeton, NIH and other leading institutions in US, Europe and Japan. After two years, since its foundation in October 2006, <em><em>JoVE</em></em> has published nearly 300 articles across all the areas of experimental biology including neuroscience, cell biology, developmental biology, stem cell research, immunology, bioengineering and plant biology. Most of the articles are produced at the laboratories in the leading academic research institutions including Harvard, MIT, Berkeley, Stanford, UCSF, Yale and others.</p><h3 id="2-what-kind-of-research-is-well-suited-to-be-published-in-jove-and-what-kind-of-research-doesn-t-work-well">2. What kind of research is well-suited to be published in JOVE? And what kind of research doesn't work well?</h3><p>The <em><em>JoVE</em></em> video-based format of publication is very effective for description of experimental techniques in all the areas of biological and clinical research. We have accumulated a lot of experience in production and publication of video-articles in these areas. This format can be also applied for experiments in chemistry and physics. For example, we have recently published a video-article on a <a href="https://web.archive.org/web/20150924053056/http://www.jove.com/index/details.stp?ID=942">creation of chemical libraries using Ugi reaction</a> by <a href="https://web.archive.org/web/20150924053056/http://network.nature.com/people/jcbradley/profile&amp;#39;s">Jean-Claude Bradley</a> group at Drexel University.</p><p>We have less experience with video-publication on theoretical and computational research, e.g. bioinformatics. We currently explore developing different formats for these non-experimental areas of science.</p><h3 id="3-how-do-you-help-authors-with-video-production">3. How do you help authors with video production?</h3><p>We know that most scientists do not have experience in video-production, and therefore cannot make good quality videos on their own experiments. Therefore, we take complete responsibility for this part. Specifically, we send video-professionals to film at the laboratories that want to publish their experiments in <em><em>JoVE</em></em>.</p><p>The entire <em><em>JoVE</em></em> publication process works as following:</p><ul><li>authors submits a text description (protocol) of their experiment to <em><em>JoVE</em></em></li><li><em><em>JoVE</em></em> sends one of its video-professionals to film the experiment in the authorsâ€™ laboratory</li><li><em><em>JoVE</em></em> editors edit the video</li><li>the video is submitted to the approval by authors and reviewers</li></ul><p>To facilitate integration of video into scientific publishing, <em><em>JoVE</em></em> has developed a network of video-professionals to conduct production of scientific videos in research labs across 30 cities in USA, Canada, UK, Germany and Japan including such centers of academic research as Boston, San Diego, San Francisco, New York, Chicago, Seattle, Toronto, Vancouver, London, Berlin and others. These video-professionals are selected, interviewed and trained by <em><em>JoVE</em></em> before they are sent to film in the laboratories. This infrastructure enables production and publication of video-articles from university laboratories around the world.</p><h3 id="4-can-you-talk-a-little-bit-about-the-peer-review-process">4. Can you talk a little bit about the peer review process?</h3><p>The video-articles are sent to reviewers in a regular fashion: 2-3 anonymous reviewers at different universities. The reviewers provide their comments according to the timeline in videos, e.g. "introduce changes at 2 minutes 35 seconds". They also provide comments on the text part, e.g. "introduce changes in paragraph 4".</p><p>We are more interested in the applicability and technical clarity rather than novelty of the methods published. To achieve our goal, to increase transparency and efficiency in biological research, we need to visualize all the experimental techniques applied today. It is less important whether they are <em><em>old</em></em> or <em><em>new</em></em>. For example, purification and transplantation of hematopoetic stem cells are used for more than 40 years. Yet, it is very difficult, even impossible, to learn these techniques based on text descriptions. So, it should be published on video.</p><h3 id="5-what-are-your-responsibilities-within-jove">5. What are your responsibilities within JoVE?</h3><p>As every CEO of a start-up, I wear multiple hats. I oversee and coordinate the work of different parts of <em><em>JoVE</em></em> including editorial, publishing, IT, video-production, business development, marketing and sales. My personal responsibilities as Editor-in-Chief include management of the editorial and publishing processes, marketing in academia and publishing industry, and PR.</p><p>I consider myself <em><em>EXTREMELY</em></em> lucky to meet very smart and hard-working people working with me on <em><em>JoVE</em></em>, including my two partners and <em><em>JoVE</em></em> co-founders, Nikita Bernstein and Klaus Korak (M.D.). Among others are Aaron Kolski-Andreaco (Ph.D.), Nandita Singh (Ph.D.), Mark Shalinsky (Ph.D.), Alvin Liang and Lori Chesla. A large fraction of doctorate-holders in the team enables us to adapt our innovative product to the high demands of the scientific community.</p><h3 id="6-what-did-you-do-before-starting-to-work-for-jove">6. What did you do before starting to work for JoVE?</h3><p>I was doing my Ph.D. in Molecular Biology at Princeton, working on embryonic stem cells and bioinformatics. This is where the <em><em>JoVE</em></em> idea was born since I, like any other scientists around, was suffering from low reproducibility of experiments. Then I was working as a post-doc at Harvard Medical School, Massachusetts General Hospital, for one year. Then I met my partners, Nikita Bernstein and Klaus Korak, and <em><em>JoVE</em></em> was started.</p><h3 id="7-do-you-want-to-talk-about-future-plans-for-jove">7. Do you want to talk about future plans for JoVE?</h3><p>My dream is to build a large comprehensive online video-library that will include a video-protocol for every possible experimental technique in biological and medical research. This will tremendously increase productivity of research in academia and biotech industry, accelerating development of new technologies and drug discovery. This will also have a strong impact on scientific education and science policy at all levels.</p><p>Being initially focused on basic biological research, we received numerous requests to expand our approach into clinical medicine, psychology and other fields. We are doing our first <a href="https://web.archive.org/web/20150924053056/http://www.jove.com/index/details.stp?ID=992">steps</a> in these directions too.</p><p><em><em>Further Reading</em></em></p><ul><li><a href="https://web.archive.org/web/20150924053056/http://jove-blog.blogspot.com/">Official JoVE blog</a> with posts from <a href="https://web.archive.org/web/20150924053056/http://network.nature.com/people/U2929A0EA/profile">Anna Kushnir</a> (<a href="https://web.archive.org/web/20150924053056/http://network.nature.com/people/U2929A0EA/blog/2008/01/07/blogging-can-help-you-get-a-job-continued">Blogging Can Help You Get a Job, Continued</a>) and <a href="https://web.archive.org/web/20150924053056/http://network.nature.com/people/steelgraham/profile">Graham Steel</a>.</li><li><a href="https://web.archive.org/web/20150924053056/http://scienceblogs.com/clock/2008/02/visualize_this_interview_with.php">Visualize This! Interview with Moshe Pritsker</a> (A Blog around the Clock)</li><li><a href="https://web.archive.org/web/20150924053056/http://arstechnica.com/journals/science.ars/2009/01/20/science-online-09-moving-beyond-text">Science Online 09: moving beyond text</a> (Nobel Intent)</li><li><a href="https://web.archive.org/web/20150924053056/http://www.boston.com/news/local/massachusetts/articles/2008/08/21/out_in_the_open_some_scientists_sharing_results/?page=2">Out in the open: Some scientists sharing results</a> (The Boston Globe)</li><li><a href="https://web.archive.org/web/20150924053056/http://usefulchem.blogspot.com/2008/11/from-ons-to-peer-review-our-jove.html">From ONS to Peer Review: our JoVE Article is Published</a> (Useful Chemistry)</li><li><a href="https://web.archive.org/web/20150924053056/http://network.nature.com/groups/goodpaper/forum/topics/3785">JOURNAL CLUB: A Behavioral Assay to Measure Responsiveness of Zebrafish to Changes in Light Intensities</a> (Good Paper Journal Club)</li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ScienceOnline09: Providing public health and medical information to all]]></title>
            <link>https://blog.martinfenner.org/posts/scienceonline09-providing-public-health-and-medical-information-to-all</link>
            <guid>e4efb994-f9d4-456c-808b-d0c93cd0ba70</guid>
            <pubDate>Fri, 16 Jan 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[This Sunday I will moderate a session called Providing public health and
medical
information to all
[https://web.archive.org/web/20151003022812/http://www.scienceonline09.com/index.php/wiki/Public_health_and_medical_information/] 
at ScienceOnline09
[https://web.archive.org/web/20151003022812/http://www.scienceonline09.com/index.php/wiki]
. I didn't pick the title, but it is a topic I care a lot about. Because the
session is intended as an open discussion, I thought that a blog post would be
goo]]></description>
            <content:encoded><![CDATA[<p>This Sunday I will moderate a session called <a href="https://web.archive.org/web/20151003022812/http://www.scienceonline09.com/index.php/wiki/Public_health_and_medical_information/">Providing public health and medical information to all</a> at <a href="https://web.archive.org/web/20151003022812/http://www.scienceonline09.com/index.php/wiki">ScienceOnline09</a>. I didn't pick the title, but it is a topic I care a lot about. Because the session is intended as an open discussion, I thought that a blog post would be good way to organize my thoughts and ideas that I have for this session<sup><a href="https://web.archive.org/web/20151003022812/http://blogs.plos.org/mfenner/2009/01/16/scienceonline09_providing_public_health_and_medical_information_to_all/#fn1">1</a></sup>. And even though there are only two days left, I might even get some valuable feedback.</p><p>We science bloggers have many recurring themes, and this session is again about access to information and filtering out the important information.</p><h3 id="access">Access</h3><p>The traditional format to provide important medical information is the peer-reviewed journal article. Unless you work at a large research institution or university hospital, only part of these papers will be available as full-text articles. Notable examples of journals with immediate open access to peer reviewed research are the <a href="https://web.archive.org/web/20151003022812/http://www.bmj.com/channels/research.dtl">British Medical Journal</a>, <a href="https://web.archive.org/web/20151003022812/http://medicine.plosjournals.org/">PLoS Medicine</a>, <a href="https://web.archive.org/web/20151003022812/http://www.biomedcentral.com/bmcmed/">BMC Medicine</a> and <a href="https://web.archive.org/web/20151003022812/http://www.openmedicine.ca/">Open Medicine</a>. Many other medical journals provide free fulltext access 6-12 months after publication, and this is now a requirement for research funded by many funding agencies, including the <a href="https://web.archive.org/web/20151003022812/http://publicaccess.nih.gov/policy.htm">NIH</a>, the <a href="https://web.archive.org/web/20151003022812/http://www.wellcome.ac.uk/About-us/Policy/Spotlight-issues/Open-access/Policy/index.htm">Wellcome Trust</a>, <a href="https://web.archive.org/web/20151003022812/http://www.hhmi.org/about/research/journals/main?action=search">Howard Hughes Medical Institute</a>.</p><p>Clinical trials are essential to improve the prevention, diagnosis and treatment of many medical conditions. Since 2005 all clinical trials have to registered in one of several central databases (e.g. <a href="https://web.archive.org/web/20151003022812/http://www.clinicaltrials.gov/">clinicaltrials.gov</a>) before the first patient is treated. The information in these publicly accessible databases includes the trial design and participating centers. Since September 2008, clinicaltrials.gov started to also report key results, as required by the <a href="https://web.archive.org/web/20151003022812/http://network.nature.com/people/mfenner/blog/2008/08/02/fdaaa-push-to-open-data-in-clinical-medicine">Federal Drug Administration Amendment Act</a>. It is important to remember that <a href="https://web.archive.org/web/20151003022812/http://network.nature.com/people/mfenner/blog/2009/01/10/what-science-is-worth-shouting-about">not all clinical trials eventually will be published</a>, so that looking just at the published literature will not give a complete picture.</p><h3 id="filtering">Filtering</h3><p>Access to important medical information is only the first step. The filtering of this information is at least as important. With filtering I mean both finding the important research papers and evaluating them in a larger context. A meta-analysis is a systematic review that follows a standard set of rules for finding and evaluating research papers, a format championed by the <a href="https://web.archive.org/web/20151003022812/http://www.cochrane.org/">Cochrane Collaboration</a>. A meta-analysis can not only summarize published research papers, but sometimes also uses the raw data of the published research (the so-called individual patient data meta-analysis). I would wish that the meta-analysis receives the same kind of attention in the blogosphere as the open access to research papers.</p><p>Whereas a positive outcome in a good meta-analysis is the best evidence for the usefulness of a healthcare intervention, most filtering of biomedical research papers comes to us in different ways:</p><ul><li>Review articles</li><li>Journal editorials</li><li>Science blogging</li><li>Articles by science journalists</li><li> <a href="https://web.archive.org/web/20151003022812/http://network.nature.com/people/mfenner/blog/2008/12/05/open-access-is-not-enough-the-source-is-also-important">Direct to consumer advertising</a> (United States and New Zealand only)</li><li>Social networking sites such as <a href="https://web.archive.org/web/20151003022812/http://www.connotea.org/">Connotea</a>, <a href="https://web.archive.org/web/20151003022812/http://www.citeulike.org/">CiteULike</a> or <a href="https://web.archive.org/web/20151003022812/http://www.friendfeed.com/">FriendFeed</a></li></ul><p>I would argue that science articles in traditional media (e.g. newspapers) and direct to consumer advertising may work in drawing your attention to a research paper, but they are usually useless in evaluating biomedical research. And obviously I believe that science blogging has great potential to help in both the finding and evaluation of important biomedical research. Meta-analyses take a very long time and review articles take a long time. A thoughtful blog post on a recently published paper can provide an important service. <a href="https://web.archive.org/web/20151003022812/http://researchblogging.org/">Research Blogging</a> and <a href="https://web.archive.org/web/20151003022812/http://blogs.nature.com/">Nature Blogs</a> are two services to help find these blog posts. Social networking sites for scientists are a great tool to find interesting research papers, but I don't know how they can help evaluate a research paper. <a href="https://web.archive.org/web/20151003022812/http://www.f1000medicine.com/">Faculty of 1000 Medicine</a> is a very interesting approach to the filtering problem, but I haven't used the service enough to comment on it here.</p><p><em><em>Addendum (01/18/09)</em></em><br>It can be a good thing to have your presentation on the second day of a conference. You come up with new ideas. I changed the title to better convey what I would like to talk about in this session: <em><em>Reporting medical research:</em></em> <em><em>specific problems and specific solutions</em></em>. And I've added another topic besides access and filtering:</p><h3 id="motivation">Motivation</h3><p>There are many reasons why people are interested in reporting or learning about medical research. But betroffenheit and financial interests are two very strong reasons that should always be kept in mind. Betroffenheit is a German word that means something that is directly affecting your life. If you are a patient with a serious medical condition, or have a sick friend or relative, you obviously have very different needs than someone who just wants to catch up on the latest research on the release of calcium from intracellular stores. Betroffenheit is not confined to medical research, climate change is another example.<br>Obviously there are often important financial interests behind the outcome of medical research. And this not only means the interests of drug companies or device manufacturers. Most presentations of medical research in larger meetings now begin with a financial disclosure slide.</p><p>I've updated the <a href="https://web.archive.org/web/20151003022812/http://www.slideshare.net/mfenner/providing-public-health-and-medical-information-to-all-presentation-924116">Slideshare Presentation</a> that I want to give as an introduction to the session. And the conference Wiki page for the session is <a href="https://web.archive.org/web/20151003022812/http://www.scienceonline09.com/index.php/wiki/Public_health_and_medical_information/">here</a>.</p><p>fn1. Yet another argument for the usefulness of science blogging.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What science is worth shouting about?]]></title>
            <link>https://blog.martinfenner.org/posts/what-science-is-worth-shouting-about</link>
            <guid>c11f2872-1769-45c3-9b5d-b72900fbc980</guid>
            <pubDate>Sat, 10 Jan 2009 00:00:00 GMT</pubDate>
            <description><![CDATA[A few weeks ago I wrote about the different ways results of a clinical trial can
be reported (What are the right numbers for JUPITER
[https://web.archive.org/web/20151003033024/http://network.nature.com/people/mfenner/blog/2008/11/23/what-are-the-right-numbers-for-jupiter]
). Inspired by blog posts by Eva Amsen (Failure
[https://web.archive.org/web/20151003033024/http://network.nature.com/people/eva/blog/2008/12/29/failure]
) and Sally Church (Over hyped cancer drugs or sensational journalism?
[]]></description>
            <content:encoded><![CDATA[<p>A few weeks ago I wrote about the different ways results of a clinical trial can be reported (<a href="https://web.archive.org/web/20151003033024/http://network.nature.com/people/mfenner/blog/2008/11/23/what-are-the-right-numbers-for-jupiter">What are the right numbers for JUPITER</a>). Inspired by blog posts by <em><em>Eva Amsen</em></em> (<a href="https://web.archive.org/web/20151003033024/http://network.nature.com/people/eva/blog/2008/12/29/failure">Failure</a>) and <em><em>Sally Church</em></em> (<a href="https://web.archive.org/web/20151003033024/http://www.pharmastrategyblog.com/2009/01/over-hyped-cancer-drugs-or-sensational-journalism.html">Over hyped cancer drugs or sensational journalism?</a>), I thought more about what makes scientific findings worth talking about outside of your immediate research community. I will look at cancer research, and it is obvious that some research findings are more exciting than others. But it less obvious that it is as important who is communicating the research and what is the intended audience. This is especially true for translational research<sup><a href="https://web.archive.org/web/20151003033024/http://blogs.plos.org/mfenner/2009/01/10/what_science_is_worth_shouting_about/#fn1"> </a></sup>(Translational Research 2008), where research findings can change the way we treat patients and drug companies (and others) can potentially earn a lot of money.</p><h3 id="the-academic-researcher">The academic researcher</h3><p>Academic researchers involved in basic or clinical cancer research are interested in publications and grant money. As Eva <a href="https://web.archive.org/web/20151003033024/http://network.nature.com/people/eva/blog/2008/12/29/failure">said</a>: the unit of success is the publication record. Basic cancer research can be very exciting and touches many related fields from signal transduction to epigenetics. But what confuses me is when basic research findings are related to the treatment of human cancer, which is a completely different story. Cell lines are not living organisms, mice are not men and very few basic research findings make it into a clinical trial.<br>Unfortunately less than one in five studies in cancer (they all have to be registered with <a href="https://web.archive.org/web/20151003033024/http://www.clinicaltrials.gov/">clinicaltrials.gov</a> since 2005) have been published in peer-reviewed journals (Ramsey 2008). Negative results will usually not be published, and this so-called publication bias creates many problems. Just imagine 5 clinical trials with a new drug where one trial will show an advantage for the new treatment. This trial will be published in a nice journal, probably one negative trial will be published in a small journal and the remaining 3 trials will remain unpublished. Just looking at the published literature will of course give the wrong impression, but these are the only data that are available to most people.</p><h3 id="the-drug-company">The drug company</h3><p>A drug company is interested in marketing approval of a drug. This approval is obtained from the <em><em>Food and Drug Administration</em></em> (FDA) in the United States and from the <em><em>European Medicines Agency</em></em> EMEA for most of Europe. Approval of a cancer drug usually requires one or more large randomized trials (so-called phase III trials) that shows that the drug improves survival. And up to 50% of these phase III trials fail to show the desired effect, even when earlier (so-called phase II) trials were positive<sup><a href="https://web.archive.org/web/20151003033024/http://blogs.plos.org/mfenner/2009/01/10/what_science_is_worth_shouting_about/#fn3"> </a></sup>(Chan 2008). <a href="https://web.archive.org/web/20151003033024/http://www.gpc-biotech.com/en/anticancer_programs/satraplatin/clinical_trials/index.html">Satraplatin</a> is one recent example of a drug that showed promising results in the treatment of prostate cancer but failed to prolong survival – and therefore was not approved by the FDA.</p><h3 id="the-insurance-company-or-whoever-pays-for-medical-care-">The insurance company (or whoever pays for medical care)</h3><p>The drug erlotinib was approved for the treatment of pancreatic cancer, but the median benefit in overall survival was less than 2 weeks<sup><a href="https://web.archive.org/web/20151003033024/http://blogs.plos.org/mfenner/2009/01/10/what_science_is_worth_shouting_about/#fn4"> </a></sup>(Moore 2007). Insurance companies are interested in treatments that are not only effective, but also cost-effective. For cancer treatments one can calculate the cost per quality-adjusted life year (QALY). Adding erlotinib to gemcitabine in the treatment of pancreatic cancer would cost about $410,000 per year of life saved<sup><a href="https://web.archive.org/web/20151003033024/http://blogs.plos.org/mfenner/2009/01/10/what_science_is_worth_shouting_about/#fn5"> </a></sup>(Miksad 2007). Costs over $100.000 per year of life saved are considered high and the high cost is the reason that the British National Institute for Health and Clinical Excellence (NICE) decided to not recommend <a href="https://web.archive.org/web/20151003033024/http://www.medscape.com/viewarticle/579628">4 new drugs for the treatment of renal cancer</a> in August 2008. NICE doesn't question the effectiveness of these drugs, but wants these drugs to become cheaper.</p><h3 id="the-media">The media</h3><p>Cancer research sells. Cancer is very common and many people can tell sad stories of friends or relatives that died because of cancer. And we want to hear encouraging stories. But the media often confuse promising findings in basic research or early clinical trials with a new cure for cancer. One of the more famous examples is the statement <em><em>Judah is going to cure cancer in two years</em></em> by James Watson in a 1998 <em><em>New York Times</em></em> story on angiogenesis research (<a href="https://web.archive.org/web/20151003033024/http://query.nytimes.com/gst/fullpage.html?res=9F04E6D6113EF930A35756C0A96E958260&amp;sec=&amp;spon=&amp;partner=permalink&amp;exprod=permalink">A Cautious Awe Greets Drugs That Eradicate Tumors in Mice</a>). Another example is this recent article (cited by <em><em>Sally Church</em></em> in her blog post mentioned above) in the <em><em>Times</em></em> (<a href="https://web.archive.org/web/20151003033024/http://www.timesonline.co.uk/tol/life_and_style/men/article4375429.ece">Cancer drug could save the lives of 10,000 a year</a>). <em><em>Ben Goldacre</em></em> has written more than one critical blog post about how traditional media report cancer research (e.g. <a href="https://web.archive.org/web/20151003033024/http://www.guardian.co.uk/commentisfree/2008/jul/19/cancer.foodtech">Still no cure for cancer hysteria</a>).</p><h3 id="the-patient">The patient</h3><p>A patient with cancer needs a drug treatment that either cures his cancer or prolongs survival. The new treatment should either be better or have fewer side effects than the current standard of care. But many cancer patients can't wait until a new cancer drug is approved and available for prescription. So whenever possible, patients should participate in a clinical trial. Clinical trial participation by adult cancer patients is unfortunately low (less than 5% compared to more than 50% for children with cancer<sup><a href="https://web.archive.org/web/20151003033024/http://blogs.plos.org/mfenner/2009/01/10/what_science_is_worth_shouting_about/#fn6"> </a></sup>(Sateren 2002)), but this is the best way to receive a promising new drug as early as possible (and to stop receiving it when the drug has been shown to be ineffective or toxic).</p><h3 id="conclusions">Conclusions</h3><p>We should be careful how we report research findings outside of specialist journals or research meetings, especially if these findings could have important consequences outside our immediate research community. And as a critical reader we should assume that most published research findings are false<sup><a href="https://web.archive.org/web/20151003033024/http://blogs.plos.org/mfenner/2009/01/10/what_science_is_worth_shouting_about/#fn7"> </a></sup>(Ioannidis 2005).</p><h3 id="references">References</h3><p>Translational Research: Getting the message across. (2008). Nature, 453(7197), 839–839. <a href="https://doi.org/10.1038/453839a">https://doi.org/10.1038/453839a</a></p><p>Ramsey, S., &amp; Scoggins, J. (2008). Commentary: Practicing on the Tip of an Information Iceberg? Evidence of Underpublication of Registered Clinical Trials in Oncology. The Oncologist, 13(9), 925–929. <a href="https://doi.org/10.1634/theoncologist.2008-0133">https://doi.org/10.1634/theoncologist.2008-0133</a></p><p>Chan, J. K., Ueda, S. M., Sugiyama, V. E., Stave, C. D., Shin, J. Y., Monk, B. J., … Kapp, D. S. (2008). Analysis of Phase II Studies on Targeted Agents and Subsequent Phase III Trials: What Are the Predictors for Success? Journal of Clinical Oncology, 26(9), 1511–1518. <a href="https://doi.org/10.1200/jco.2007.14.8874">https://doi.org/10.1200/jco.2007.14.8874</a></p><p>Moore, M. J., Goldstein, D., Hamm, J., Figer, A., Hecht, J. R., Gallinger, S., … Parulekar, W. (2007). Erlotinib Plus Gemcitabine Compared With Gemcitabine Alone in Patients With Advanced Pancreatic Cancer: A Phase III Trial of the National Cancer Institute of Canada Clinical Trials Group. Journal of Clinical Oncology, 25(15), 1960–1966. <a href="https://doi.org/10.1200/jco.2006.07.9525">https://doi.org/10.1200/jco.2006.07.9525</a></p><p>Miksad, R. A., Schnipper, L., &amp; Goldstein, M. (2007). Does a Statistically Significant Survival Benefit of Erlotinib Plus Gemcitabine for Advanced Pancreatic Cancer Translate Into Clinical Significance and Value? Journal of Clinical Oncology, 25(28), 4506–4507. https://doi.org/10.1200/jco.2007.13.0401</p><p>Sateren, W. B., Trimble, E. L., Abrams, J., Brawley, O., Breen, N., Ford, L., … Christian, M. C. (2002). How Sociodemographics, Presence of Oncology Specialists, and Hospital Cancer Programs Affect Accrual to Cancer Treatment Trials. Journal of Clinical Oncology, 20(8), 2109–2117. <a href="https://doi.org/10.1200/jco.2002.08.056">https://doi.org/10.1200/jco.2002.08.056</a></p><p>Ioannidis, J. P. A. (2005). Why Most Published Research Findings Are False. PLoS Medicine, 2(8), e124. <a href="https://doi.org/10.1371/journal.pmed.0020124">https://doi.org/10.1371/journal.pmed.0020124</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My Personal Plans for 2009]]></title>
            <link>https://blog.martinfenner.org/posts/my-personal-plans-for-2009</link>
            <guid>fc675ae6-711a-461e-9bd2-f56e3819221f</guid>
            <pubDate>Sun, 28 Dec 2008 00:00:00 GMT</pubDate>
            <description><![CDATA[The end of the year is always a time to think about the past and the future.
Even more so if you also have your birthday (FemaleScienceProfessor calls it 
Christmas Time Birthdays
[https://web.archive.org/web/20150923121708/http://science-professor.blogspot.com/2007/12/christmas-time-birthdays.html]
). Below are some of my plans for the next year. The general theme: more overlap
of science blogging with my daytime job as physician treating cancer patients
and doing cancer research.

Meet fellow ]]></description>
            <content:encoded><![CDATA[<p>The end of the year is always a time to think about the past and the future. Even more so if you also have your birthday (FemaleScienceProfessor calls it <a href="https://web.archive.org/web/20150923121708/http://science-professor.blogspot.com/2007/12/christmas-time-birthdays.html">Christmas Time Birthdays</a>). Below are some of my plans for the next year. The general theme: more overlap of science blogging with my daytime job as physician treating cancer patients and doing cancer research.</p><h3 id="meet-fellow-science-bloggers">Meet fellow science bloggers</h3><p>I'm looking forward to <a href="https://web.archive.org/web/20150923121708/http://www.scienceonline09.com/">ScienceOnline'09</a> in January. Please contact me by email if you have an interesting idea for the session Providing public health and medical information to all. <a href="https://web.archive.org/web/20150923121708/http://www.chiswick.demon.co.uk/CISB09.html">Cromer is so Bracing</a> in February will be very different, maybe something like conversations at the fireplace? And I hope we repeat <a href="https://web.archive.org/web/20150923121708/http://www.nature.com/natureconferences/sciblog2008/index.html">Science Blogging 2008: London</a>.</p><h3 id="start-a-new-job">Start a new job</h3><p>If everything works out as expected, I will start a new job next year. It will still be at my institution and will in fact be very similar, but will present new challenges and opportunities. One of these new opportunities is science communication, or how to communicate our efforts in cancer research and treating cancer patients both within our institution and to the public.</p><h3 id="blog-more-about-research">Blog more about research</h3><p>Science blogging <a href="https://web.archive.org/web/20150923121708/http://network.nature.com/people/mfenner/blog/2008/08/31/science-blogging-is-the-new-email">means many different things to different people</a>. Blogging about research (your own or that of other people) is one important part of it. In 2008 I have written only a few blog posts about research (e.g. <a href="https://web.archive.org/web/20150923121708/http://network.nature.com/people/mfenner/blog/2008/11/23/what-are-the-right-numbers-for-jupiter">this one</a>), but I want to do more of it in 2009. <a href="https://web.archive.org/web/20150923121708/http://blogs.nature.com/">Nature.com Blogs</a> and <a href="https://web.archive.org/web/20150923121708/http://www.researchblogging.org/">Research Blogging</a> are great tools to find blogging about research, and I hope the <a href="https://web.archive.org/web/20150923121708/http://researchblogging.org/news/?p=109">broken integration</a> of Nature Network blog posts with Research Blogging will soon be fixed.</p><h3 id="start-a-new-blog">Start a new blog</h3><p>Related to the last two topics, I might start a new blog. This would be a German-language blog and would be an official blog of our institution. But first I have to convince a few people that such a blog is a good idea. I might ask <a href="https://web.archive.org/web/20150923121708/http://network.nature.com/people/edyong/profile">Ed Yong</a> for advice, as he writes for <a href="https://web.archive.org/web/20150923121708/http://info.cancerresearchuk.org/">Cancer Research UK</a>.</p><h3 id="find-good-papers">Find good papers</h3><p>The <a href="https://web.archive.org/web/20150923121708/http://network.nature.com/groups/goodpaper/forum/topics">Good Paper Journal Club</a> is a Nature Network Forum to promote good scientific writing. It is a good idea and the forum has over 200 members and some very interesting discussions, but it is very time-consuming to find good examples of well-written papers. The best approach would be regular contributions by a large number of people, but the incentives for doing so are probably not there. But it is possible to pick well-written papers from a journal I read anyway, so I will try to do that for <em><em>Nature</em></em> in regular intervals in 2009. And in January I will give my first seminar on science writing.</p><h3 id="collaborate-with-fellow-science-bloggers">Collaborate with fellow science bloggers</h3><p>We constantly have many interesting and (sometimes) important discussions. Why shouldn't more of these discussions turn into formal projects, e.g. a paper or a presentation at a science conference? Science bloggers are a very diverse mix of people, and this could lead to some very interesting approaches.</p><h3 id="spend-more-time-cooking">Spend more time cooking</h3><p>I think it is important to also do stuff not related to science. Some fellow Nature Network bloggers write books (<a href="https://web.archive.org/web/20150923121708/http://network.nature.com/people/UE19877E8/blog/2008/11/15/in-which-books-are-judged-by-covers">Jennifer Rohn</a>, <a href="https://web.archive.org/web/20150923121708/http://stores.lulu.com/siegeofstars">Henry Gee</a>), write about books (<a href="https://web.archive.org/web/20150923121708/http://petrona.typepad.com/">Maxine Clarke</a>), write about music (<a href="https://web.archive.org/web/20150923121708/http://scientistmusicians.wordpress.com/">Eva Amsen</a>), write about food (<a href="https://web.archive.org/web/20150923121708/http://sunday-night-dinner.blogspot.com/">Anna Kushnir</a>) or write about their favorite city (<a href="https://web.archive.org/web/20150923121708/http://network.nature.com/hubs/london/blog/2008/12/09/going-solo">Matt Brown</a>). I like cooking. I don't write about it, but I hope to do more cooking in 2009. Preparing good food is not only tasty and fun, but also a good distraction from the daytime work. And reading a good cookbook (for example <a href="https://web.archive.org/web/20150923121708/http://www.becomingachef.com/culinary_artistry.php">Culinary Artistry</a>) can be as entertaining as reading a good novel.</p><h3 id="go-to-a-warm-place-in-the-summer">Go to a warm place in the summer</h3><p>This year's summer vacation was basically a trip to London for a long weekend. Next year I need a longer summer vacation, and also a place with more sun. We have almost finished planning our trip to this place:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://web.archive.org/web/20150923121708im_/http://farm1.static.flickr.com/197/487031747_5f63a95cd9_m_d.jpg" class="kg-image" alt><figcaption><a href="https://web.archive.org/web/20150923121708/http://www.flickr.com/photos/scingram/487031747/">Flickr Picture by Scott Ingram</a></figcaption></figure>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Just DOI it!]]></title>
            <link>https://blog.martinfenner.org/posts/just-doi-it</link>
            <guid>02d01c3d-5a0a-43ed-95c0-8c8289a6d136</guid>
            <pubDate>Mon, 22 Dec 2008 00:00:00 GMT</pubDate>
            <description><![CDATA[With the December 18 issue
[https://web.archive.org/web/20151003053153/http://www.nature.com/nature/journal/v456/n7224/] 
 Nature started to support XMP markup in article PDFs (reported last week on the 
Nascent blog by Tony Hammond)1
[https://web.archive.org/web/20151003053153/http://blogs.plos.org/mfenner/2008/12/22/just_doi_it/#fn1]
. XMP stands for Extensible Metadata Platform and is a technology to embed
metadata in files, including PDFs2
[https://web.archive.org/web/20151003053153/http://b]]></description>
            <content:encoded><![CDATA[<p>With the <a href="https://web.archive.org/web/20151003053153/http://www.nature.com/nature/journal/v456/n7224/">December 18 issue</a> <em><em>Nature</em></em> started to support XMP markup in article PDFs (reported last week on the <em><em>Nascent</em></em> blog by <em><em>Tony Hammond</em></em>)<sup><a href="https://web.archive.org/web/20151003053153/http://blogs.plos.org/mfenner/2008/12/22/just_doi_it/#fn1">1</a></sup>. XMP stands for Extensible Metadata Platform and is a technology to embed metadata in files, including PDFs<sup><a href="https://web.archive.org/web/20151003053153/http://blogs.plos.org/mfenner/2008/12/22/just_doi_it/#fn2">2</a></sup>. XMP was created by Adobe (with XMP support in PDF files since 2001), but is an open standard with backing by others, including Creative Commons<sup><a href="https://web.archive.org/web/20151003053153/http://blogs.plos.org/mfenner/2008/12/22/just_doi_it/#fn3">3</a></sup>. The Digital Object Identifier (<a href="https://web.archive.org/web/20151003053153/http://www.doi.org/">DOI</a>) is the most important piece of information in the metadata, as the DOI provides a link to the journal publisher website where more metadata can be retrieved. XMP support in scientific PDFs is unfortunately still very uncommon and probably hasn't changed much since <a href="https://web.archive.org/web/20151003053153/http://network.nature.com/people/lindenb/profile">Pierre Lindenbaum</a> checked last year<sup><a href="https://web.archive.org/web/20151003053153/http://blogs.plos.org/mfenner/2008/12/22/just_doi_it/#fn4">4</a></sup>.</p><p>Adding metadata to PDFs seems to be a no-brainer. We have done the same with music (mp3 ID) and photos (IPTC and EXIF) for years and it has been a tremendous help in organizing these files stored on our computers. Unfortunately there aren't too many tools that can extract the DOI or other metadata from the XMP in article PDFs. But I expect more desktop software to support XMP, once XMP support in scientific articles is more widespread. We will then be able to add a journal PDF to our reference manager of choice and have the relevant metadata (including authors, title, journal and issue) automatically filled in. As well as many other creative uses. Until then we need tools like <a href="https://web.archive.org/web/20151003053153/http://mekentosj.com/papers/">Papers</a> or <a href="https://web.archive.org/web/20151003053153/http://www.mendeley.com/">Mendeley</a> that can extract metadata from PDF files without this XMP information.</p><p>For a more technical discussion of XMP in scientific articles, please read the set of blog posts by Tony Hammond<sup><a href="https://web.archive.org/web/20151003053153/http://blogs.plos.org/mfenner/2008/12/22/just_doi_it/#fn5">5</a></sup>,<sup><a href="https://web.archive.org/web/20151003053153/http://blogs.plos.org/mfenner/2008/12/22/just_doi_it/#fn6">6</a></sup>,<sup><a href="https://web.archive.org/web/20151003053153/http://blogs.plos.org/mfenner/2008/12/22/just_doi_it/#fn7">7</a></sup>.</p><p>fn1. <a href="https://web.archive.org/web/20151003053153/http://blogs.nature.com/wp/nascent/2008/12/xmp_labelling_for_nature.html">XMP Labelling for Nature</a></p><p>fn2. <a href="https://web.archive.org/web/20151003053153/http://www.adobe.com/products/xmp/">Adding intelligence to media</a></p><p>fn3. <a href="https://web.archive.org/web/20151003053153/http://wiki.creativecommons.org/XMP">XMP</a></p><p>fn4. <a href="https://web.archive.org/web/20151003053153/http://plindenbaum.blogspot.com/2007/05/is-there-any-xmp-in-scientific-pdf-no.html">Is there any XMP in scientific pdf? No</a></p><p>fn5. <a href="https://web.archive.org/web/20151003053153/http://www.crossref.org/CrossTech/2007/08/metadata_in_pdf_1_strategies.html">Metadata in PDF: 1. Strategies</a></p><p>fn6. <a href="https://web.archive.org/web/20151003053153/http://www.crossref.org/CrossTech/2007/08/metadata_in_pdf_2_use_cases.html">Metadata in PDF: 2. Use Cases</a></p><p>fn7. <a href="https://web.archive.org/web/20151003053153/http://www.crossref.org/CrossTech/2007/08/metadata_in_pdf_3_deployment_1.html">Metadata in PDF: 3. Deployment</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Open access is not enough – the source is also important]]></title>
            <link>https://blog.martinfenner.org/posts/open-access-is-not-enough-the-source-is-also-important</link>
            <guid>e312f4d8-9318-4b12-85f1-dd4307786d61</guid>
            <pubDate>Fri, 05 Dec 2008 00:00:00 GMT</pubDate>
            <description><![CDATA[Direct to consumer advertising (DTCA) – advertising for prescription drugs – is
only allowed in the United States (since 1997, when restrictions were loosened)
and New Zealand. Drug companies pay for direct to consumer advertising (more
than $4 billion in 2005 (Donohue 2007)) because they believe that it increases
prescription rates. In a clever study published in the British Medical Journal 
in September 
[https://web.archive.org/web/20150922174115/http://blogs.plos.org/mfenner/2008/12/05/open_]]></description>
            <content:encoded><![CDATA[<p>Direct to consumer advertising (DTCA) – advertising for prescription drugs – is only allowed in the United States (since 1997, when restrictions were loosened) and New Zealand. Drug companies pay for direct to consumer advertising (more than $4 billion in 2005 (Donohue 2007)) because they believe that it increases prescription rates. In a clever study published in the <em><em>British Medical Journal</em></em> in September<sup><a href="https://web.archive.org/web/20150922174115/http://blogs.plos.org/mfenner/2008/12/05/open_access_is_not_enough_the_source_is_also_important/#fn2"> </a></sup>(Law 2008) Michael Law and colleagues looked at the prescription rates of the drugs etanercept, mometasone, and tegaserod in Canada. DTCA is not allowed in Canada, but English-speaking Canadians see these ads in US magazines and US TV commercials. The study authors found that prescriptions were higher compared to French-speaking Canada (supposedly not using the US media) for one of the three drugs studied (tegaserod). The study authors explain:</p><p>The European Union is considering changes in legislation that would allow direct to consumer advertising (<a href="https://web.archive.org/web/20150922174115/http://www.pharmalot.com/2008/10/eu-plan-gives-pharma-direct-access-to-patients/">EU Plan Gives Pharma Direct Access To Patients</a>). Not a good idea. Direct to consumer advertising not only means easily identified ads, but also information campaigns about diseases and treatments that are biased towards prescribing a particular drug.</p><p>The problems surrounding direct to consumer advertising are a reminder that open access to scientific and medical information is not enough. Similar conflicts of interest exist when car manufacturers would talk about climate change or crop producers about genetically engineered crops. We need not access to as much information as possible, but rather access to independent and objective information of high quality. Scientific journals and learned societies have traditionally played an important role in this. Once they've built up more reputation, science bloggers could have a bigger role in this in the future.</p><h3 id="references">References</h3><p>Donohue, J. M., Cevasco, M., &amp; Rosenthal, M. B. (2007). A Decade of Direct-to-Consumer Advertising of Prescription Drugs. New England Journal of Medicine, 357(7), 673–681. <a href="https://doi.org/10.1056/nejmsa070502">https://doi.org/10.1056/nejmsa070502</a></p><p>Law, M. R., Majumdar, S. R., &amp; Soumerai, S. B. (2008). Effect of illicit direct to consumer advertising on use of etanercept, mometasone, and tegaserod in Canada: controlled longitudinal study. BMJ, 337(sep02 1), a1055–a1055. <a href="https://doi.org/10.1136/bmj.a1055">https://doi.org/10.1136/bmj.a1055</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Why do we blog and other important questions, answered by 34 science bloggers]]></title>
            <link>https://blog.martinfenner.org/posts/why-do-we-blog-and-other-important-questions-answered-by-34-science-bloggers</link>
            <guid>c696a914-2633-4511-ac06-dc94f99c053e</guid>
            <pubDate>Sun, 30 Nov 2008 00:00:00 GMT</pubDate>
            <description><![CDATA[What started out as a few questions to science bloggers
[https://web.archive.org/web/20151003123037/http://network.nature.com/groups/nnbloggername/forum/topics/3392] 
in the Nature Network Bloggers Forum, has turned into a collection of more than
30 blog posts not limited to Nature Network (big thanks to Bora
[https://web.archive.org/web/20151003123037/http://scienceblogs.com/clock/2008/11/the_science_blog_meme.php] 
and others for spreading the word). The following science bloggers answered a
s]]></description>
            <content:encoded><![CDATA[<p>What started out as <a href="https://web.archive.org/web/20151003123037/http://network.nature.com/groups/nnbloggername/forum/topics/3392">a few questions to science bloggers</a> in the <em><em>Nature Network Bloggers Forum</em></em>, has turned into a collection of more than 30 blog posts not limited to <em><em>Nature Network</em></em> (big thanks to <a href="https://web.archive.org/web/20151003123037/http://scienceblogs.com/clock/2008/11/the_science_blog_meme.php">Bora</a> and others for spreading the word). The following science bloggers answered a set of 10 questions about their blogging (roughly in chronological order):</p><ul><li><a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/henrygee/blog/2008/11/14/that-martin-fenner-effect">Henry Gee</a></li><li><a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/U27CE62BB/blog/2008/11/14/blog-survey">Eva Amsen</a></li><li><a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/stuffysour/blog/2008/11/14/its-the-need-to-communicate">Steffi Suhr</a></li><li><a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/scurry/blog/2008/11/14/about-a-blog">Stephen Curry</a></li><li><a href="https://web.archive.org/web/20151003123037/http://petrona.typepad.com/petrona/2008/11/pinning-ones-hamster-to-the-mast.html">Maxine Clarke</a></li><li><a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/mfenner/blog/2008/11/14/some-answers-for-henry-gee">Martin Fenner</a></li><li><a href="https://web.archive.org/web/20151003123037/http://scienceblogs.com/clock/2008/11/the_science_blog_meme.php">Bora Zivkovic</a></li><li><a href="https://web.archive.org/web/20151003123037/http://keeperofthesnails.blogspot.com/2008/11/nature-meme.html">Clare Dudman</a></li><li><a href="https://web.archive.org/web/20151003123037/http://genomicron.blogspot.com/2008/11/why-do-we-blog-and-other-important.html">T. Ryan Gregory</a></li><li><a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/massimopinto/blog/2008/11/15/whats-this-bel-paese">Massimo Pinto</a> (and <a href="https://web.archive.org/web/20151003123037/http://www.galileonet.it/postdoc/article/74/alt-si-ma-quanti-siete">another post</a> in Italian)</li><li><a href="https://web.archive.org/web/20151003123037/http://www.tuibguy.com/?p=2407">Mike Haubrich</a></li><li><a href="https://web.archive.org/web/20151003123037/http://sandwalk.blogspot.com/2008/11/why-do-we-blog-and-other-important.html">Larry Moran</a></li><li><a href="https://web.archive.org/web/20151003123037/http://scienceblogs.com/evolvingthoughts/2008/11/another_goldang_meme.php">John Wilkins</a></li><li><a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/U3EABC9C8/blog/2008/11/15/oh-noes-meta-blogging">Kristi Vogel</a></li><li><a href="https://web.archive.org/web/20151003123037/http://blindscientist.genedrift.org/2008/11/16/the-science-blog-meme/">Paolo Nuin</a></li><li><a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/etchevers/blog/2008/11/16/me-me-meme">Heather Etchevers</a></li><li><a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/U71147CBA/blog/2008/11/16/on-the-theme-of-martins-meme">Lee Turnpenny</a></li><li><a href="https://web.archive.org/web/20151003123037/http://my.biotechlife.net/2008/11/16/the-science-blog-meme/">Ricardo Vidal</a></li><li><a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/boboh/blog/2008/11/16/memetic-selections">Bob O'Hara</a></li><li><a href="https://web.archive.org/web/20151003123037/http://mndoci.com/blog/2008/11/16/why-do-we-blog/">Deepak Singh</a></li><li><a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/U81B5C465/blog/2008/11/16/la-meme-chose">Frank Norman</a></li><li><a href="https://web.archive.org/web/20151003123037/http://fredcobio.wordpress.com/2008/11/17/scientistist-their-desks">Jim Hardy</a></li><li><a href="https://web.archive.org/web/20151003123037/http://blog.pansapiens.com/2008/11/17/that-science-blog-meme-thing-going-around/">Andrew Perry</a></li><li><a href="https://web.archive.org/web/20151003123037/http://pbeltrao.blogspot.com/2008/11/why-do-we-blog.html">Pedro Beltrao</a></li><li><a href="https://web.archive.org/web/20151003123037/http://shirleywho.wordpress.com/2008/11/17/the-science-blog-meme-why-do-we-blog/">Shirley Wu</a></li><li><a href="https://web.archive.org/web/20151003123037/http://www.amarkos.gr/blog/2008/11/252/">Angelos Markos</a></li><li><a href="https://web.archive.org/web/20151003123037/http://www.corporeality.net/museion/2008/11/17/why-do-we-blog-and-other-important-questions-reply-to-martin-fenner-nature-networks/">Thomas Soderqvist</a></li><li><a href="https://web.archive.org/web/20151003123037/http://the-mouse-trap.blogspot.com/2008/11/science-blog-meme.html">Sandy Gautam</a></li><li><a href="https://web.archive.org/web/20151003123037/http://duncan.hull.name/2008/11/17/science-blog-meme-why-do-we-blog/">Duncan Hull</a></li><li><a href="https://web.archive.org/web/20151003123037/http://jdupuis.blogspot.com/2008/11/science-blog-meme-why-do-we-blog.html">John Dupuis</a></li><li><a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/mike/blog/2008/11/18/follow-those-lemmings-where">Mike Fowler</a></li><li><a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/erikacule/blog/2008/11/18/meme-scheme">Erika Cule</a></li><li><a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/strippedscience/blog/2008/11/18/100th-comic-strip">Viktor Po</a></li><li><a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/rpg/blog/2008/11/19/the-martin-fenner-effect">Richard Grant</a></li><li><a href="https://web.archive.org/web/20151003123037/http://scienceblogs.com/notrocketscience/2008/11/why_blog_the_meme_returns.php">Ed Yong</a></li><li><a href="https://web.archive.org/web/20151003123037/http://scienceblogs.com/grrlscientist/2008/11/navelgazing_courtesy_of_nature.php">GrrlScientist</a></li><li><a href="https://web.archive.org/web/20151003123037/http://www.pharmastrategyblog.com/2008/12/science-blog-meme.html">Sally Church</a></li></ul><p><em><em>Please contact me if I missed a blog post.</em></em></p><p>Reading these blog posts is not only interesting and entertaining, but probably also a very good introduction to the current state of science blogging. Below is a personal summary of some of the answers. Oh, the best title was probably from <a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/U81B5C465/blog/2008/11/16/la-meme-chose">Frank Norman</a>: <em><em>La meme chose</em></em>.</p><h3 id="1-what-is-your-blog-about">1. What is your blog about?</h3><p>Most bloggers seem to write about many different science-related topics. And only a minority about the actual science they are doing. Some bloggers gave more specific answers:</p><ul><li><em><em>My blog is about science, in particular evolution and genomes.</em></em> (<a href="https://web.archive.org/web/20151003123037/http://genomicron.blogspot.com/2008/11/why-do-we-blog-and-other-important.html">T. Ryan Gregory</a>)</li><li><em><em>It</em>'<em>s a sort of a window of transparency over a weird scientific environment, the Italian science jobs and funding market.</em></em> (<a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/massimopinto/blog/2008/11/15/whats-this-bel-paese">Massimo Pinto</a>)</li><li><em><em>Basically the philosophical implications of science.</em></em> (<a href="https://web.archive.org/web/20151003123037/http://scienceblogs.com/evolvingthoughts/2008/11/another_goldang_meme.php">John Wilkins</a>)</li><li><em><em>Anything releted to Biotechnology in Frederick County, MD.</em></em> (<a href="https://web.archive.org/web/20151003123037/http://fredcobio.wordpress.com/2008/11/17/scientistist-their-desks">Jim Hardy</a>)</li><li><em><em>The general theme is how we can bring the worlds of information technology and the life sciences together.</em></em> (<a href="https://web.archive.org/web/20151003123037/http://mndoci.com/blog/2008/11/16/why-do-we-blog/">Deepak Singh</a>)</li><li><em><em>I am trying to put the people behind the science into the spotlight: the technicians, operational support, science management and others.</em></em> (<a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/stuffysour/blog/2008/11/14/its-the-need-to-communicate">Steffi Suhr</a>)</li><li><em><em>We write about making sense of medicine and medical science in museums.</em></em> (<a href="https://web.archive.org/web/20151003123037/http://www.corporeality.net/museion/2008/11/17/why-do-we-blog-and-other-important-questions-reply-to-martin-fenner-nature-networks/">Thomas Soderqvist</a>)</li><li><em><em>Ostensibly about theoretical population biology.</em></em> (<a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/mike/blog/2008/11/18/follow-those-lemmings-where">Mike Fowler</a>)</li><li><em><em>I am interested in how the internet is changing the way we publish and communicate science.</em></em> (<a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/mfenner/blog/2008/11/14/some-answers-for-henry-gee">me</a>)</li></ul><h3 id="2-what-will-you-never-write-about">2. What will you never write about?</h3><p>Several people mentioned that they would not give out personal information about other people, or comment directly on what's going on in their institution/company. Most people also avoid to talk religious beliefs or politics. Confidential information, including unreleased papers, was mentioned several times. The <a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/henrygee/blog/2008/03/05/the-release-of-calcium-from-intracellular-stores-and-other-stuff">release of calcium from intracellular stores</a> is another topic that several people would never blog about. Also:</p><ul><li><em><em>Only write about things I actually understand.</em></em> (<a href="https://web.archive.org/web/20151003123037/http://scienceblogs.com/notrocketscience/2008/11/why_blog_the_meme_returns.php">Ed Yong</a>)</li><li><em><em>I will never ask anyone to give me money via this blog.</em></em> (<a href="https://web.archive.org/web/20151003123037/http://petrona.typepad.com/petrona/2008/11/pinning-ones-hamster-to-the-mast.html">Maxine Clarke</a>)</li><li><em><em>I hope that I will not have to write blog posts that are evaluated, measured and put on a resumé.</em></em> (<a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/mfenner/blog/2008/11/14/some-answers-for-henry-gee">me</a>)</li></ul><h3 id="3-have-you-ever-considered-leaving-science">3. Have you ever considered leaving science?</h3><p>Several people said something similar to <a href="https://web.archive.org/web/20151003123037/http://scienceblogs.com/clock/2008/11/the_science_blog_meme.php">Bora Zivkovic</a>: <em><em>Leaving research – yes, I already did that. Leaving science – never.</em></em></p><h3 id="4-what-would-you-do-instead">4. What would you do instead?</h3><p>Some interesting answers. And a science background would be helpful in most of the jobs:</p><ul><li><em><em>Zookeeper, or an animal trainer in a circus</em></em> (<a href="https://web.archive.org/web/20151003123037/http://scienceblogs.com/clock/2006/06/how_to_become_a_biologist.php">Bora Zivkovic</a>)</li><li><em><em>Jazz trumpeteer</em></em> (<a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/massimopinto/blog/2008/11/15/whats-this-bel-paese">Massimo Pinto</a>)</li><li><em><em>I</em>'<em>d live on a beach somewhere, just fishing all day</em></em> (<a href="https://web.archive.org/web/20151003123037/http://fredcobio.wordpress.com/2008/11/17/scientistist-their-desks">Jim Hardy</a>)</li><li><em><em>Own a used book store</em></em> (<a href="https://web.archive.org/web/20151003123037/http://jdupuis.blogspot.com/2008/11/science-blog-meme-why-do-we-blog.html">John Dupuis</a>)</li><li><em><em>I</em>'<em>d try my best to get a job in video game development</em></em> (<a href="https://web.archive.org/web/20151003123037/http://blog.pansapiens.com/2008/11/17/that-science-blog-meme-thing-going-around/">Andrew Perry</a>)</li><li><em><em>Playing football for England</em></em> (<a href="https://web.archive.org/web/20151003123037/http://www.pharmastrategyblog.com/2008/12/science-blog-meme.html">Sally Church</a>)</li><li><em><em>Tend olive trees in Greece</em></em> (<a href="https://web.archive.org/web/20151003123037/http://duncan.hull.name/2008/11/17/science-blog-meme-why-do-we-blog/">Duncan Hull</a>)</li><li><em><em>Yoga instructor</em></em> (<a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/erikacule/blog/2008/11/18/meme-scheme">Erika Cule</a>)</li><li><em><em>Graphic designer</em></em> (<a href="https://web.archive.org/web/20151003123037/http://my.biotechlife.net/2008/11/16/the-science-blog-meme/">Ricardo Vidal</a>)</li><li><em><em>Write very bad science fiction</em></em> (<a href="https://web.archive.org/web/20151003123037/http://scienceblogs.com/evolvingthoughts/2008/11/another_goldang_meme.php">John Wilkins</a>)</li></ul><h3 id="5-what-do-you-think-will-science-blogging-be-like-in-5-years">5. What do you think will science blogging be like in 5 years?</h3><p>This was a difficult question that some didn't answer. <a href="https://web.archive.org/web/20151003123037/http://sandwalk.blogspot.com/2008/11/why-do-we-blog-and-other-important.html">Larry Moran</a> said: <em><em>pretty much the same as it is now.</em></em> <a href="https://web.archive.org/web/20151003123037/http://genomicron.blogspot.com/2008/11/why-do-we-blog-and-other-important.html">T. Ryan Gregory</a> thinks that <em><em>more professional researchers will join the blogosphere as this becomes socially acceptable.</em></em> <a href="https://web.archive.org/web/20151003123037/http://blog.pansapiens.com/2008/11/17/that-science-blog-meme-thing-going-around/">Andrew Perry</a> thinks that <em><em>research groups will be tied together more and more by their blogs.</em></em> <a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/U27CE62BB/blog/2008/11/14/blog-survey">Eva Amsen</a> thinks that <em><em>there will be so many science blogs that we have to specialize.</em></em> And I wrote that <em><em>some science bloggers will be able to make enough money to earn a living from it.</em></em></p><h3 id="6-what-is-the-most-extraordinary-thing-that-happened-to-you-because-of-blogging">6. What is the most extraordinary thing that happened to you because of blogging?</h3><p><a href="https://web.archive.org/web/20151003123037/http://www.corporeality.net/museion/2008/11/17/why-do-we-blog-and-other-important-questions-reply-to-martin-fenner-nature-networks/">Thomas Soderqvist</a> said that <em><em>there is no the most extraordinary thing. But I hadn</em>'<em>t expected to get so many interesting contacts with colleagues around the world.</em></em> Many people (including myself) had similar answers. Going to <a href="https://web.archive.org/web/20151003123037/http://www.nature.com/nature/meetings/scifoo/index.html">SciFoo</a> is certainly an extraordinary thing an SciFoo invitation was mentioned by <a href="https://web.archive.org/web/20151003123037/http://pbeltrao.blogspot.com/2008/11/why-do-we-blog.html">Pedro Beltrao</a>, <a href="https://web.archive.org/web/20151003123037/http://fredcobio.wordpress.com/2008/11/17/scientistist-their-desks">Jim Hardy</a>, <a href="https://web.archive.org/web/20151003123037/http://duncan.hull.name/2008/11/17/science-blog-meme-why-do-we-blog/">Duncan Hull</a> and <a href="https://web.archive.org/web/20151003123037/http://mndoci.com/blog/2008/11/16/why-do-we-blog/">Deepak Singh</a>. Some other answers:</p><ul><li><em><em>The Editor-in-Chief of Nature once told David Attenborough that he should read my blog.</em></em> (<a href="https://web.archive.org/web/20151003123037/http://scienceblogs.com/notrocketscience/2008/11/why_blog_the_meme_returns.php">Ed Yong</a>)</li><li><em><em>Getting a job with PLoS in the comments of one of my posts.</em></em> (<a href="https://web.archive.org/web/20151003123037/http://scienceblogs.com/clock/2008/11/the_science_blog_meme.php">Bora Zivkovic</a>)</li><li><em><em>Co-founded a network of science bloggers (The DNA Network) and been invited (and accepted!) to work at MIT.</em></em> (<a href="https://web.archive.org/web/20151003123037/http://my.biotechlife.net/2008/11/16/the-science-blog-meme/">Ricardo Vidal</a>)</li><li><em><em>Getting interviewed by Jon Udell.</em></em> (<a href="https://web.archive.org/web/20151003123037/http://mndoci.com/blog/2008/11/16/why-do-we-blog/">Deepak Singh</a>)</li><li><em><em>I loaned a power cable to a Nature editor.</em></em> (<a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/boboh/blog/2008/11/16/memetic-selections">Bob O'Hara</a>)</li></ul><h3 id="7-did-you-write-a-blog-post-or-comment-you-later-regretted">7. Did you write a blog post or comment you later regretted?</h3><p>For most people that was not a big issue. <a href="https://web.archive.org/web/20151003123037/http://scienceblogs.com/notrocketscience/2008/11/why_blog_the_meme_returns.php">Ed Yong</a> regrets to have written nice things about studies that later turned out to be rubbish not so good.</p><h3 id="8-when-did-you-first-learn-about-science-blogging">8. When did you first learn about science blogging?</h3><p>Many different answers. <a href="https://web.archive.org/web/20151003123037/http://www.nodalpoint.org/">Nodalpoint</a> was mentioned by several bloggers, including <a href="https://web.archive.org/web/20151003123037/http://duncan.hull.name/2008/11/17/science-blog-meme-why-do-we-blog/">Duncan Hull</a>, <a href="https://web.archive.org/web/20151003123037/http://blindscientist.genedrift.org/2008/11/16/the-science-blog-meme/">Paolo Nuin</a> and <a href="https://web.archive.org/web/20151003123037/http://pbeltrao.blogspot.com/2008/11/why-do-we-blog.html">Pedro Beltrao</a>. <a href="https://web.archive.org/web/20151003123037/http://genomicron.blogspot.com/2008/11/why-do-we-blog-and-other-important.html">T. Ryan Gregory</a> was introduced to science blogging by his graduate student. The most hilarious answer is from <a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/henrygee/blog/2008/11/14/that-martin-fenner-effect">Henry Gee</a> and involves a garage, an old washing-machine motor and heavier-than-air flight.</p><h3 id="9-what-do-your-colleagues-at-work-say-about-your-blogging">9. What do your colleagues at work say about your blogging?</h3><p>The standard answer seems to be that most of them don't know or don't care. I would hope that in the future we will have more answers like the one from <a href="https://web.archive.org/web/20151003123037/http://scienceblogs.com/clock/2008/11/the_science_blog_meme.php">Bora Zivkovic</a>: <em><em>That's what they are paying me for and I hope they are happy.</em></em></p><h3 id="10-how-the-heck-do-you-have-time-to-blog-and-do-research-at-the-same-time">10. How the heck do you have time to blog and do research at the same time?</h3><p>Most people blog in their spare time. I hope to see more <em><em>daytime bloggers</em></em> that blog as part of their science job in 5 years (this relates to questions #5, #8 and #9).</p><h3 id="11-extra-credit-are-you-able-to-write-an-entry-to-your-blog-that-takes-the-form-of-a-poem-about-your-research">11. Extra credit: are you able to write an entry to your blog that takes the form of a poem about your research?</h3><p>Not all bloggers answered that question, but you can find poetry by <a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/etchevers/blog/2008/11/16/me-me-meme">Heather Etchevers</a> (who suggested that question), <a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/scurry/blog/2008/11/12/public-engagement-ring">Stephen Curry</a> (who inspired it), <a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/henrygee/blog/2007/11/09/the-noble-five-hundred">Henry Gee</a>, <a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/U27CE62BB/blog/2008/11/04/unexpected-haikus">Eva Amsen</a>, <a href="https://web.archive.org/web/20151003123037/http://scienceblogs.com/clock/2008/08/well_versed_in_science.php">Bora Zivkovic</a>, <a href="https://web.archive.org/web/20151003123037/http://petrona.typepad.com/petrona/2006/07/petronarati.html">Maxine Clarke</a> (a play), <a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/massimopinto/blog/2008/11/15/whats-this-bel-paese">Massimo Pinto</a> (science fiction), <a href="https://web.archive.org/web/20151003123037/http://shirleywho.wordpress.com/karaoke/">Shirley Wu</a> (karaoke), <a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/mike/blog/2008/11/18/follow-those-lemmings-where">Mike Fowler</a>, <a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/erikacule/blog/2008/11/18/meme-scheme">Erika Cule</a>, <a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/U3EABC9C8/blog/2008/11/15/oh-noes-meta-blogging">Kristi Vogel</a>, <a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/boboh/blog/2008/03/17/the-deeper-meaning-of-a-residual-plot">Bob O'Hara</a> (art), <a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/strippedscience/blog/2008/11/18/100th-comic-strip">Viktor Po</a> (a dance) and <a href="https://web.archive.org/web/20151003123037/http://network.nature.com/people/mfenner/blog/2008/11/14/some-answers-for-henry-gee">myself</a>. <a href="https://web.archive.org/web/20151003123037/http://blindscientist.genedrift.org/2008/11/16/the-science-blog-meme/">Paolo Nuin</a> needs a few more encouraging comments before he will write a poem.</p><p>Update: <a href="https://web.archive.org/web/20151003123037/http://blog.pansapiens.com/2008/11/17/that-science-blog-meme-thing-going-around/">Andrew Perry</a> has created two great wordle images for <a href="https://web.archive.org/web/20151003123037/http://tinyurl.com/5aeq88">question #1</a> and <a href="https://web.archive.org/web/20151003123037/http://tinyurl.com/5z498r">question #2</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[New ways to look at your presentation]]></title>
            <link>https://blog.martinfenner.org/posts/new-ways-to-look-at-your-presentation</link>
            <guid>ca9ef7fb-2ec8-4a89-a358-e399962e7dc6</guid>
            <pubDate>Fri, 26 Sep 2008 04:45:00 GMT</pubDate>
            <description><![CDATA[This blog post is about presentations. And this usually means PowerPoint
presentations, although some people do well without it1
[https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn1]
. Edward Tufte argues that PowerPoint can be a really bad tool to create slides2
[https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn2]
. ]]></description>
            <content:encoded><![CDATA[<p>This blog post is about presentations. And this usually means PowerPoint presentations, although some people do well without it<sup><a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn1"><strong>1</strong></a></sup>. Edward Tufte argues that PowerPoint can be a really bad tool to create slides<sup><a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn2"><strong>2</strong></a></sup>. But it is probably not the software, but rather the people that produce these slides that are responsible for the quality<sup><a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn3"><strong>3</strong></a></sup>. The Neurotic Physiology blog published a list of things you shouldn’t do during a Powerpoint presentation<sup><a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn4"><strong>4</strong></a></sup>. But there are also many tips to create better presentations. A May 2008 <em>Nature Methods</em> editorial<sup><a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn5"><strong>5</strong></a></sup> gives ten such suggestions. Links to some more Powerpoint tips were collected in a Nautilus blog post<sup><a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn6"><strong>6</strong></a></sup> by <a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/maxine/profile"><strong>Maxine Clarke</strong></a>. One positive example is this presentation by <a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mza/profile"><strong>Matt Wood</strong></a>from the Science Blogging London conference:</p><p><a href="https://web.archive.org/web/20080929085351/http://www.slideshare.net/mza/how-to-make-friendfeeds-and-influence-people-presentation?type=powerpoint">How to make Friendfeeds and influence people</a></p><p>View SlideShare <a href="https://web.archive.org/web/20080929085351/http://www.slideshare.net/mza/how-to-make-friendfeeds-and-influence-people-presentation?type=powerpoint"><strong>presentation</strong></a> or <a href="https://web.archive.org/web/20080929085351/http://www.slideshare.net/upload?type=powerpoint"><strong>Upload</strong></a> your own. (tags: <a href="https://web.archive.org/web/20080929085351/http://slideshare.net/tag/science"><strong>science</strong></a> <a href="https://web.archive.org/web/20080929085351/http://slideshare.net/tag/blogs"><strong>blogs</strong></a>)</p><p>The Nature Network <a href="https://web.archive.org/web/20080929085351/http://network.nature.com/groups/scivis/forum/topics"><strong>Visualization &amp; Science Forum</strong></a> is a great place for further discussions.</p><p>Presentations can also be created online. Google Docs and Zoho Show have been around for a while now, but 280Slides<sup><a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn7"><strong>7</strong></a></sup> is a fairly new offering with a very slick interface. The advantages of these programs: slides can be created by several authors working together and slides can be easily shared. But presentations created with Powerpoint can also be shared online. Slideshare and Scribd are the most popular tools for this, and since last week these presentations can be embedded into Nature Network blog posts. By default, these presentations are public and can be seen by everybody. But they can also be uploaded as private presentations and only those that know the secret URL can see them. Presentations in the life sciences can also be uploaded to Nature Preceedings<sup><a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn8"><strong>8</strong></a></sup>. This way the scientific presentation receives a DOI and becomes citable. But Nature Preceedings has still a long way to go with currently only about 50 presentations available. Which is a bit suprising, since it looks like the perfect platform to host conference presentations.</p><p>YouTube videos or podcasts are probably the preferred format to share presentations that also include the recorded audio. Having the audio available is especially important for those presentations that have little text on their slides. Many presentations from the TED (Technology, Entertainment, Design) conference<sup><a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn9"><strong>9</strong></a></sup> have been made available as TEDTalks, including this one by Neuroanatomist Jill Bolte Taylor:</p><p>If you want to give a presentation remotely (i.e. to one or more people in a different location), you could use that feature in Google Docs<sup><a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn10"><strong>10</strong></a></sup>. Or use a full-fledged web conferencing solution such as Dimdin<sup><a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn11"><strong>11</strong></a></sup>, which is free for up to 20 users and also is available as Open Source community edition.</p><p><sup>1</sup> <a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/henrygee/blog/2008/04/18/power-point-to-the-people"><strong>Powerpoint to the People</strong></a></p><p><sup>2</sup> <strong>Kemp M.</strong> PowerPoint presentations and the culture of pitch. <em>Nature</em>2006; <a href="https://web.archive.org/web/20080929085351/http://dx.doi.org/10.1038/442140a"><strong>doi:10.1038/442140a</strong></a></p><p><sup>3</sup> <a href="https://web.archive.org/web/20080929085351/http://freakonomics.blogs.nytimes.com/2007/06/20/dont-hate-powerpoint-hate-the-powerpointers/"><strong>Don’t hate Powerpoint; Hate the Powerpointers</strong></a></p><p><sup>4</sup> <a href="https://web.archive.org/web/20080929085351/http://scicurious.wordpress.com/2008/08/12/and-now-a-powerpoint-presentation/"><strong>And Now, a Powerpoint Presentation</strong></a></p><p><sup>5</sup> Talking points. <em>Nature Methods</em> 2008; <a href="https://web.archive.org/web/20080929085351/http://dx.doi.org/10.1038/nmeth0508-371"><strong>doi:10.1038/nmeth0508-371</strong></a></p><p><sup>6</sup> <a href="https://web.archive.org/web/20080929085351/http://blogs.nature.com/nautilus/2008/05/how_to_give_a_good_presentatio.html"><strong>How to give a good presentation</strong></a></p><p><sup>7</sup> <a href="https://web.archive.org/web/20080929085351/http://280slides.com/"><strong>280Slides</strong></a></p><p><sup>8</sup> <a href="https://web.archive.org/web/20080929085351/http://precedings.nature.com/"><strong>Nature Preceedings</strong></a></p><p><sup>9</sup> <a href="https://web.archive.org/web/20080929085351/http://www.ted.com/"><strong>TED</strong></a></p><p><sup>10</sup> <a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/andrewsun/blog/2007/10/01/google-docs-now-with-presentation"><strong>Google Docs – Now with Presentation</strong></a></p><p><sup>11</sup> <a href="https://web.archive.org/web/20080929085351/http://www.dimdim.com/"><strong>Dimdim</strong></a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How to lure (German) researchers back to Germany]]></title>
            <link>https://blog.martinfenner.org/posts/how-to-lure-german-researchers-back-to-germany</link>
            <guid>8c66b4ed-f989-4481-8c26-8a6cd508191e</guid>
            <pubDate>Fri, 19 Sep 2008 19:21:00 GMT</pubDate>
            <description><![CDATA[The german academic international network (GAIN)1
[https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn1] 
informs German researchers working in North America about research opportunities
in Germany. The implied intention is to lure German researchers back to Germany.
GAIN project director Katja Simons explains:

Many german researchers abroad are highly interested in returning but they need
support ]]></description>
            <content:encoded><![CDATA[<p>The <strong>german academic international network (GAIN)</strong><sup><a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn1"><strong>1</strong></a></sup> informs German researchers working in North America about research opportunities in Germany. The implied intention is to lure German researchers back to Germany. GAIN project director Katja Simons explains:</p><p><em>Many german researchers abroad are highly interested in returning but they need support creating networks and receiving information on career opportunities in Germany. Germany has invested a great deal in their education and is in need of these bright minds and their experience they gained abroad.</em></p><p>GAIN is a joint initiative by the <strong>Alexander von Humboldt Foundation (AvH)</strong>, the <strong>German Academic Exchange Service (DAAD)</strong> and the <strong>German Research Foundation (DFG)</strong>. Their 8th annual meeting took place two weeks ago in Boston<sup><a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn2"><strong>2</strong></a></sup>. More than 200 researchers working in North America participated, together with representatives from many German research organizations, including Matthias Kleiner, president of the German Research Foundation (DFG)<sup><a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn3"><strong>3</strong></a></sup> and Margeret Wintermantel, president of the <strong>German Recotors’ Conference</strong> (HRK, the association of all higher education institutions in Germany)<sup><a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn4"><strong>4</strong></a></sup>. Representatives from business and politics (members of the parliament) were also present.</p><p>To get a more personal perspective, I talked to two researchers that attented the meeting. <a href="https://web.archive.org/web/20080921133726/http://network.nature.com/profile/avmaier"><strong>Alexander Maier</strong></a>, a research fellow at the <a href="https://web.archive.org/web/20080921133726/http://network.nature.com/affiliations/735"><strong>NIH</strong></a>, thinks that the GAIN meeting was a success. Sceptical at the beginning of the conference, he acknowledges that things have changed and that doing reseach in Germany has become much more attractive. The <strong>excellence initiative</strong> by the German Government is one big reason for that change<sup><a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn5"><strong>5</strong></a></sup>. The most rewarding part of the program for him was the workshop on how to apply for professorhips.</p><p>Florian Jaeger, an assistant professor from the University of Rochester (his blog is here<sup><a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn6"><strong>6</strong></a></sup>) is on the GAIN advisory board. He also got the impression that the German reseach system is changing, and that</p><p><em>Many institutions in Germany seem to be inspired to learn from the positive aspects of the American system (and maybe even to improve on it).</em></p><p>But both Alexander and Florian felt that the research environment in Germany is still far from perfect, and the German research organizations should not think that all problems have been solved. Startup grants are often relatively low and junior research groups are usually not as independent as in the United States. And Florian thinks that the research atmosphere – the way people interact and approach problems – is still more stimulating in the United States.</p><p>Most reports about the GAIN meeting are in the German media, including the newspapers Hamburger Abendblatt<sup><a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn7"><strong>7</strong></a></sup> and Süddeutsche Zeitung<sup><a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn8"><strong>8</strong></a></sup>. I found one blog post from a German postdoc attending the meeting<sup><a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn9"><strong>9</strong></a></sup>. Nature Network is a good place to have a discussion not only about the research environment in different countries, but also to learn more about similar strategies carried out by other countries, e.g. France, Italy, Japan or China. Feel free to leave your comments about why you left your home country to do research somewhere else<sup><a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn10"><strong>10</strong></a></sup>, or why you returned after finishing your PhD or postdoc<sup><a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn11"><strong>11</strong></a></sup>.</p><p><sup>1</sup> <a href="https://web.archive.org/web/20080921133726/http://www.gain-network.org/"><strong>GAIN Homepage</strong></a></p><p><sup>2</sup> <a href="https://web.archive.org/web/20080921133726/http://www.eurekalert.org/pub_releases/2008-09/df-wc091008.php"><strong>DFG news report of the meeting</strong></a></p><p><sup>3</sup> <a href="https://web.archive.org/web/20080921133726/http://www.dfg.de/en/index.html"><strong>DFG Homepage</strong></a></p><p><sup>4</sup> <a href="https://web.archive.org/web/20080921133726/http://www.hrk.de/index_eng.php"><strong>HRK Homepage</strong></a></p><p><sup>5</sup> <a href="https://web.archive.org/web/20080921133726/http://www.dfg.de/en/research_funding/coordinated_programmes/excellence_initiative/general_information.html"><strong>Excellence initiative</strong></a></p><p><sup>6</sup> <a href="https://web.archive.org/web/20080921133726/http://hlplab.wordpress.com/"><strong>HLP/Jaeger lab blog</strong></a></p><p><sup>7</sup> <a href="https://web.archive.org/web/20080921133726/http://www.abendblatt.de/daten/2008/09/15/937232.html"><strong>Lockrufe der deutschen Forschung</strong></a></p><p><sup>8</sup> <a href="https://web.archive.org/web/20080921133726/http://jetzt.sueddeutsche.de/texte/anzeigen/446385"><strong>In Boston werben Politiker für deutsche Unis</strong></a></p><p><sup>9</sup> <a href="https://web.archive.org/web/20080921133726/http://sonjatoots.blogspot.com/2008/09/one-with-boston.html"><strong>Sonja in the City</strong></a></p><p><sup>10</sup> <a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/etchevers/blog"><strong>A Developing Passion</strong></a></p><p><sup>11</sup> <a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/massimopinto/blog"><strong>Science in the Bel Paese</strong></a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[It's time for Conference 2.0]]></title>
            <link>https://blog.martinfenner.org/posts/its-time-for-conference-2-0</link>
            <guid>50b818e8-281d-46f7-9924-1583e94d8b28</guid>
            <pubDate>Fri, 12 Sep 2008 16:16:00 GMT</pubDate>
            <description><![CDATA[Conference 2.0 – A scheduled meeting of people sharing a common interest that
takes advantage of Web 2.01
[https://web.archive.org/web/20080929012015/http://network.nature.com/people/mfenner/blog/2008/09/12/its-time-for-conference-2-0#fn1] 
concepts.

Scientific conferences are essential both for the exchange of ideas and for
networking. But they don’t have to be organized the same way as 10-20 years ago.
Web 2.0 tools now allow much broader user participation before, during and after
the confer]]></description>
            <content:encoded><![CDATA[<p><em><strong>Conference 2.0</strong> – A scheduled meeting of people sharing a common interest that takes advantage of Web 2.0<sup><a href="https://web.archive.org/web/20080929012015/http://network.nature.com/people/mfenner/blog/2008/09/12/its-time-for-conference-2-0#fn1"><strong>1</strong></a></sup> concepts.</em></p><p>Scientific conferences are essential both for the exchange of ideas and for networking. But they don’t have to be organized the same way as 10-20 years ago. Web 2.0 tools now allow much broader user participation before, during and after the conference. Technology conferences have seen this change already<sup><a href="https://web.archive.org/web/20080929012015/http://network.nature.com/people/mfenner/blog/2008/09/12/its-time-for-conference-2-0#fn2"><strong>2</strong></a></sup>. We also already have open source software to organize conferences<sup><a href="https://web.archive.org/web/20080929012015/http://network.nature.com/people/mfenner/blog/2008/09/12/its-time-for-conference-2-0#fn3"><strong>3</strong></a></sup>. I’ve collected a few of those ideas and concepts below.</p><p><strong>1. Keep the conference small</strong><br>Active user participation works better in smaller conferences, e.g. not more than maybe 150 participants (derived from Dunbar’s number<sup><a href="https://web.archive.org/web/20080929012015/http://network.nature.com/people/mfenner/blog/2008/09/12/its-time-for-conference-2-0#fn4"><strong>4</strong></a></sup> introduced by <a href="https://web.archive.org/web/20080929012015/http://network.nature.com/profile/duncan"><strong>Duncan Hull</strong></a>). This will exclude large or very large conferences – the largest scientific conferences today have more than 10.000 participants. But those larger conferences can still adopt some of the principles discussed below.</p><p><strong>2. Allow for user input to the conference program</strong><br>Even though most scientific conferences ask for abstract submissions well before the conference, the conference schedule is ultimately decided my a small program committee. But conference organizers could well ask for user input about session topics. In the BarCamp or unconference format, the conference program is even decided on the first day of the conference<sup><a href="https://web.archive.org/web/20080929012015/http://network.nature.com/people/mfenner/blog/2008/09/12/its-time-for-conference-2-0#fn5"><strong>5</strong></a></sup>.</p><p><strong>3. Provide free WiFi</strong><br>This is essential for liveblogging about the conference. And free WiFi in combination with a good conference website with detailed schedule, message boards and practical information would greatly reduce the amount of printed material that needs to be handed out at the conference.</p><p><strong>4. Set aside time for networking</strong><br>There should be enough time (and space) between sessions to talk to the other conference participants. After all, this is one main reason for many people to attend a conference. And the conference organizers can facilitate networking in other ways. Poster sessions (see below) are one way, a very short introduction by every participant (either in person or on the conference website) is another idea.</p><p><strong>5. Pay attention to poster sessions and discussions</strong><br>Conferences can have other session formats than oral presentations. Poster sessions are an often neglected part of many conferences. But they are a great tool for networking, especially with younger scientists. The conference organizers should set aside enough time and avoid parallel oral sessions. Providing drinks and food also helps. Round-table discussions are another underused format with a lot of potential.</p><p><strong>6. Encourage blogging</strong><br>There should be a clear policy regarding blogging stated at the conference website. And this policy should make it easy for conference participants to blog. This means no preregistration and no required affiliation with a news service or journal. Conference organizers should provide a tag for the conference so that blog entries can be tracked.</p><p><em>Blogging about the conference is encouraged by the conference organizers. Please use the tag *conference</em>name* for all your blog posts. Please don’t blog about sessions marked <strong>non_public</strong> in the conference program. They contain information that should not become become public at this time, e.g. because they discuss unpublished results. For further questions regarding blogging at the conference, please contact …_</p><p>There are many different ways to blog about a conference, microblogging via FriendFeed is currently a very popular option<sup><a href="https://web.archive.org/web/20080929012015/http://network.nature.com/people/mfenner/blog/2008/09/12/its-time-for-conference-2-0#fn6"><strong>6</strong></a></sup>.</p><p><strong>7. Produce podcasts</strong><br>Podcasts with audio and video of the slides are a great way to capture oral sessions at a conference. They are espcially valuable for those unable to attend. This week’s <strong>Science in the 21st Century</strong> conference is a good example of how this can be done<sup><a href="https://web.archive.org/web/20080929012015/http://network.nature.com/people/mfenner/blog/2008/09/12/its-time-for-conference-2-0#fn7"><strong>7</strong></a></sup>.</p><p><strong>8. Organize parallel local conferences</strong><br>The costs and annoyances of traveling, combined with concerns about the carbon footprint<sup><a href="https://web.archive.org/web/20080929012015/http://network.nature.com/people/mfenner/blog/2008/09/12/its-time-for-conference-2-0#fn8"><strong>8</strong></a></sup> have led to new concepts. Instead of following the conference from the distance via live-streaming or live-blogging, why not organize several parallel local conferences<sup><a href="https://web.archive.org/web/20080929012015/http://network.nature.com/people/mfenner/blog/2008/09/12/its-time-for-conference-2-0#fn9"><strong>9</strong></a></sup>? The Singularity web conference next month is using this concept<sup><a href="https://web.archive.org/web/20080929012015/http://network.nature.com/people/mfenner/blog/2008/09/12/its-time-for-conference-2-0#fn10"><strong>10</strong></a></sup>. Will the next science blogging conference happen in parallel in several locations?</p><p><sup>1</sup> <a href="https://web.archive.org/web/20080929012015/http://oreilly.com/pub/a/oreilly/tim/news/2005/09/30/what-is-web-20.html"><strong>What is Web 2.0</strong></a></p><p><sup>2</sup> <a href="https://web.archive.org/web/20080929012015/http://money.cnn.com/2008/03/11/technology/fost_conference.fortune/"><strong>Welcome to Conference 2.0</strong></a></p><p><sup>3</sup> <a href="https://web.archive.org/web/20080929012015/http://pkp.sfu.ca/?q=ocs"><strong>Open Conference Systems</strong></a></p><p><sup>4</sup> <a href="https://web.archive.org/web/20080929012015/http://en.wikipedia.org/wiki/Dunbar%27s_number"><strong>Dunbar’s number</strong></a></p><p><sup>5</sup> <a href="https://web.archive.org/web/20080929012015/http://network.nature.com/blogs/user/UE19877E8/2008/08/11/in-which-i-am-utterly-fooed"><strong>In which I am utterly Fooed</strong></a></p><p><sup>6</sup> <a href="https://web.archive.org/web/20080929012015/http://scienceblogs.com/principles/2008/09/microblogging_conference_talks.php"><strong>Micro-Blogging Conference Talks</strong></a></p><p><sup>7</sup> <a href="https://web.archive.org/web/20080929012015/http://pirsa.org/C08021"><strong>Perimeter Institute Recorded Seminar Archive</strong></a></p><p><sup>8</sup> <a href="https://web.archive.org/web/20080929012015/http://www.carbonfootprint.com/"><strong>Carbon Footprint</strong></a></p><p><sup>9</sup> <a href="https://web.archive.org/web/20080929012015/http://www.insideria.com/2008/06/building-conference-20.html"><strong>Building Conference 2.0</strong></a></p><p><sup>10</sup> <a href="https://web.archive.org/web/20080929012015/http://www.headconference.com/"><strong>Head – the global web conference</strong></a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Interview with Victor Henning from Mendeley]]></title>
            <link>https://blog.martinfenner.org/posts/interview-with-victor-henning-from-mendeley</link>
            <guid>5d76d882-6ebb-4021-b4b4-3175696f0e6e</guid>
            <pubDate>Fri, 05 Sep 2008 06:18:00 GMT</pubDate>
            <description><![CDATA[In the last few months we have seen an ever increasing number of new social
networking (Web 2.0) sites for scientists. Good Web 2.0 tools for scientists
primarily try to solve a problem. But by adding a social aspect, they will gain
useful features that would otherwise not be possible. Eva Amsen
[https://web.archive.org/web/20080920001317/http://network.nature.com/profile/U27CE62BB] 
has recently written a great blog post about this1
[https://web.archive.org/web/20080920001317/http://network.nat]]></description>
            <content:encoded><![CDATA[<figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20080920001317im_/http://farm4.static.flickr.com/3120/2827515376_ac2f0d81a1_o.jpg" class="kg-image" alt></figure><p>In the last few months we have seen an ever increasing number of new social networking (Web 2.0) sites for scientists. Good Web 2.0 tools for scientists primarily try to solve a problem. But by adding a social aspect, they will gain useful features that would otherwise not be possible. <a href="https://web.archive.org/web/20080920001317/http://network.nature.com/profile/U27CE62BB"><strong>Eva Amsen</strong></a> has recently written a great blog post about this<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn1"><strong>1</strong></a></sup>. Many of these new services have overlapping functions, e.g. almost all of them allow the user to maintain a list of contacts. This raises two questions:</p><ol><li>Which of these sites has (or have) the features that I’m most interested in?</li><li>Do any of these sites work with the commonly used desktop tools and with each other, so that I don’t have to maintain duplicate information, e.g. the list of my publications?</li></ol><p><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/profile/U42E63119"><strong>Cameron Neylon</strong></a> in August posted an open letter to the developers of these sites<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn2"><strong>2</strong></a></sup> and also started a comprehensive list<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn3"><strong>3</strong></a></sup>. <a href="https://web.archive.org/web/20080920001317/http://network.nature.com/profile/http-network-nature-comprofilegerrymck"><strong>Gerry McKiernan</strong></a><strong> </strong>collects information about social networking sites for scientists on his SciTechNet blog<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn4"><strong>4</strong></a></sup>. Mendeley<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn5"><strong>5</strong></a></sup> is one of these new Web 2.0 sites for scientists (they launched in August), and I spoke with <a href="https://web.archive.org/web/20080920001317/http://network.nature.com/profile/U4C4A58A2[6"><strong>Victor Henning</strong></a>, one of the founders of Mendeley, about it.</p><p><strong>1. Can you describe what Mendeley is and does?</strong><br>Mendeley is actually two things: Mendeley Desktop and Mendeley Web. Mendeley Desktop is free academic software (available for Windows, Mac and Linux) for managing &amp; sharing research papers. Mendeley Web lets you back up your research papers online, shows you research trends in your academic discipline, and connects you to like-minded researchers.</p><p><strong>2. What is the connection to Last.fm?</strong><br>There is a conceptual as well as a personal connection. Conceptually, in the long run Mendeley aims to do for research what Last.fm did for music. For those of your readers who don’t know Last.fm, this is how it works: When you install Last.fm’s desktop software on your computer, it will monitor which music you listen to and automatically build a profile of your musical taste on the Last.fm website. The website then recommends you music that you might like, shows you statistics about the most popular songs and artists in your favourite genre, and lets you discover people with a similar taste in music. By aggregating the listening habits and tags of its 20 million users, Last.fm has managed to create the largest ontological classification (and the largest open database) of music in the world – it would be great if we could achieve the same for research papers.</p><p>So, if you install Mendeley Desktop on your computer, you can manage and share research papers on your machine, but you can also upload your papers to your private account on Mendeley Web to access them online. Mendeley Web anonymously aggregates the metadata of these papers to generate statistics about the most popular authors and papers in your research discipline, and – in the future – generates recommendations for papers which you might like. One thing that we handle very differently from Last.fm is privacy: The Last.fm website lets everyone know which music you listen to, whereas Mendeley Web keeps all your research data in your private account which can’t be accessed by anyone else. The conceptual parallels between Last.fm and Mendeley are outlined in more detail in a talk I gave both at the EuroScience Open Forum 2008 in Barcelona and at the Southampton Open Science Workshop<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn7"><strong>7</strong></a></sup>.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20080920001317im_/http://farm4.static.flickr.com/3129/2822784112_f01ed9673b_o.jpg" class="kg-image" alt></figure><p>Besides the conceptual similarities, the personal connection to Last.fm is Dr. Stefan Glänzer. He was Last.fm’s first investor and executive chairman, and now has the same role at Mendeley. My co-founder Jan and I first met him back in 2003, when he was a guest lecturer in Entrepreneurship at our university, the WHU Vallendar<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn8"><strong>8</strong></a></sup>, and we contributed a case study to the book he published together with our professors. So when we were looking for funding, we approached him again in June 2007, he was fascinated by the idea of Mendeley and luckily decided to join us. He also brought us in touch with the former founding engineers of Skype, who became investors as well.</p><p><strong>3. What are your responsibilities within Mendeley?</strong><br>I do most of the conceptual work behind Mendeley and write the specifications for our developers: What is the vision of Mendeley Desktop and Mendeley Web in the long term, which features should we develop next to get there, how does each feature work in detail, right down to questions like “where do we place this button and how do we label it?”. So you can blame me for all the usability problems you might encounter.</p><p>I’m also responsible for staying in touch with the academic community and incorporating its wishes into the Mendeley development roadmap, which has the enjoyable side effect that I get to travel to all these wonderful academic conferences. Next week, I’ll be at the Science in the 21st Century conference at the Perimeter Institute for Theoretical Physics, Waterloo/Ontario<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn9"><strong>9</strong></a></sup>.</p><p><strong>4. What did you do before starting to work for Mendeley?</strong><br>Until a little more than a year ago, I thought I’d pursue an academic career – I’m currently finishing my Ph.D. on decision-making and choice at the Bauhaus-University of Weimar. I’ve been saying “currently finishing” for almost a year now, but I’m hopeful that I’ll manage to submit my thesis by the end of this year :-) Prior to that, I’ve worked in film production and the music industry a lot, and I opened a café/bar<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn10"><strong>10</strong></a></sup> opposite the WHU in parallel to writing my master’s thesis. I actually left Vallendar the day after the opening night to pursue my Ph.D. in Weimar, so I had all of the work and little of the fun of owning a café/bar!</p><p><strong>5. What is your policy regarding users sharing their PDF files of publications with others?</strong><br>I think it’s important to point out that we’re not a “Napster for research papers” – i.e. no free-for-all peer-to-peer filesharing. Quite a lot of people are disappointed when I tell them that! Sharing is currently limited to “Shared Document Groups” of max. 10 people (e.g. a lab, or collaborators on a research paper), although you can create and join as many Shared Document Groups as you like. Also, as we state in our terms of use<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn11"><strong>11</strong></a></sup>, you may only share PDFs with the permission of the copyright owner – e.g. your own articles or working papers, articles from Open Access databases, articles under a Creative Commons/Scientific Commons license, or perhaps when you and the person you are sharing the PDF with both have licensed access to the database the PDF was taken from.</p><p>We also encourage users to post their own papers and working papers on their Mendeley profiles – depending on whether they have permission to do so from their publishers. As you can imagine, we’re big fans of Open Access!</p><p><strong>6. How is Mendeley different from other desktop reference managers such as Endnote?</strong><br>There are a number of things that make Mendeley Desktop unique, but I’d probably highlight the collaboration aspect, the “automatic metadata extraction”, the online back-up/multi-machine support, and the cross-platform support:</p><ul><li>As far as I know, Mendeley Desktop is the only desktop reference management software that lets you share and collaboratively annotate research papers. We’re also working on a “groups” feature in Mendeley Web which labs can use for discussions, sharing files, setting up a lab blog/wiki etc. – all of this will tie into the reference management seamlessly.</li><li>The “automatic metadata extraction” is quite unique as well: When you drop your PDFs into Mendeley Desktop, it will automatically extract the full-text to make it searchable, try to guess the correct metadata from the full-text (author, title, journal, volume, issue etc.) so you don’t have to type it in manually, and also parse each documents’ cited references, so you can add them to your library as well.</li><li>Due to the integration with Mendeley Web, you can back-up your entire library for online access through simple drag &amp; drop in Mendeley Desktop. Also, this means that you can install Mendeley Desktop on multiple computers and easily synchronize your PDF library across them via the Mendeley Online Library.</li><li>Last but not least, Mendeley Desktop is the only desktop reference manager that is available on all three major platforms (Windows, Mac, and Linux).</li></ul><p>Not to mention that, in comparison to solutions such as EndNote, RefMan, RefWorks etc., which cost hundreds of Euros per license, Mendeley Desktop is completely free.</p><p><strong>7. How is Mendeley different from other social networking sites for scientists?</strong><br>While social networking is an aspect of Mendeley Web, we don’t see ourselves primarily as a social network. We don’t believe that social networking in itself is the killer feature that researchers are looking for – rather we’re using a social network to enable researchers to share their data. I think that <a href="https://web.archive.org/web/20080920001317/http://network.nature.com/profile/neilfws"><strong>Neil Saunders</strong></a>, who is also here on Nature Network, said it best: “Really, it’s our data that needs to be social, not ourselves”. So we’ve tried to develop a research tool which is useful without any network effects, and which uses networking as a means rather than an end.</p><p>To invoke the analogy to Last.fm again: Even though people have profiles on Last.fm and can connect to each other, Last.fm is not primarily a social network. Last.fm connects the music first, and networks of people form around the music as a second step.</p><p><strong>8. Does Mendeley integrate with other social networking sites and services for scientists, e.g. Connotea or CiteULike? Does it integrate with desktop reference managers?</strong><br>At the moment we’re focusing on increasing the speed and stability of Mendeley, as well as introducing more features which make Mendeley useful as a standalone software. However, compatibility with Connotea or CiteULike is something we’ll be working on in the near future.</p><p>Regarding integration with other desktop reference managers: At the Science Blogging Conference<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn11"><strong>11</strong></a></sup> I briefly spoke to <a href="https://web.archive.org/web/20080920001317/http://network.nature.com/profile/mekentosj"><strong>Alexander Griekspoor</strong></a>, the developer of the Mac software Papers<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn12"><strong>12</strong></a></sup>, whether we couldn’t make our software compatible so that Papers users and Mendeley Desktop users could share and synchronize their collections online. As for EndNote: It’s already possible to import/export data to/from Mendeley in the EndNote XML format – but, sadly, I don’t think that EndNote’s developers would be inclined to enable online sharing between EndNote and Mendeley.</p><p><strong>9. Do you want to talk about future plans for Mendeley?</strong><br>Sure! Besides speed and stability, which I already mentioned, we’ll be working on two main issues to better integrate Mendeley into the research workflow. First, there will be a “cite-while-you-write” plugin for Microsoft Word (or Open Office, if our users would prefer that), so that you can generate reference lists from your Mendeley library automatically. Similarly, we’ll improve the integration with LaTeX by automating the BibTeX file export from Mendeley Desktop. Second, we’ll introduce a “bookmarklet” like the ones CiteULike or Connotea have, which lets you import metadata/papers from websites into your Mendeley Online Library with a single click. This metadata will then be automatically synchronized with the Mendeley Desktop library on your computer.</p><p>The Microsoft Word/LaTeX integration and the online bookmarklet will be available very soon. Over the coming months, we’ll also introduce OCR to Mendeley Desktop, so that you can extract metadata, full-text and references from older scanned-image documents; we’ll integrate Mendeley Desktop with external databases such as PubMed, Scopus, and Web of Science; we’ll implement the groups/lab management feature on Mendeley Web that I already mentioned; there will be much more detailed research trend statistics on Mendeley Web; we’ll introduce the recommendation engine for academic papers, and many more little goodies. For example, a frequent user request has been automatic PDF file renaming based on the metadata – so that’s going to be in one of the next versions.</p><p><strong>10. If a user is interested to learn more about Mendeley or give feedback, who should he contact?</strong><br>You can always contact me directly at <a href="https://web.archive.org/web/20080920001317/mailto:victor.henning@mendeley.com"><strong>victor.henning@mendeley.com</strong></a>. There is also a Mendeley team blog on which there is plenty of behind-the-scenes information<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn13"><strong>13</strong></a></sup>. You can also submit feature requests or bug reports on our homepage (the buttons on the top left).</p><p>Victor, thank you very much for giving me this interview.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20080920001317im_/http://farm4.static.flickr.com/3212/2822759480_c330822084_o.jpg" class="kg-image" alt></figure><p><em>The Mendeley founders Stefan Glänzer, Victor Henning, Paul Föckler and Jan Reichelt</em></p><p><sup>1</sup> <a href="https://web.archive.org/web/20080920001317/http://network.nature.com/blogs/user/U27CE62BB/2008/08/19/how-to-get-scientists-to-adopt-web-2-0-technologies"><strong>How to get scientists to adopt web 2.0 technologies</strong></a></p><p><sup>2</sup> <a href="https://web.archive.org/web/20080920001317/http://blog.openwetware.org/scienceintheopen/2008/08/06/an-open-letter-to-the-developers-of-social-network-and-%E2%80%98web-20%E2%80%99-tools-for-scientists"><strong>An open letter to the developers of social network and web 2.0 tools for scientists</strong></a></p><p><sup>3</sup> <a href="https://web.archive.org/web/20080920001317/http://docs.google.com/View?docid=dhs5x5kr_572hccgvcct"><strong>A critical analysis Google Docs</strong></a></p><p><sup>4</sup> <a href="https://web.archive.org/web/20080920001317/http://scitechnet.blogspot.com/"><strong>SciTechNet</strong></a></p><p><sup>5</sup> <a href="https://web.archive.org/web/20080920001317/http://www.mendeley.com/"><strong>Mendeley</strong></a></p><p><sup>6</sup> <a href="https://web.archive.org/web/20080920001317/http://www.mendeley.com/profiles/victor-henning"><strong>Victor Henning’s profile on Mendeley</strong></a></p><p><sup>7</sup> <a href="https://web.archive.org/web/20080920001317/http://www.youtube.com/watch?v=UzJbrA9EY7A"><strong>A Last.fm for Research</strong></a></p><p><sup>8</sup> <a href="https://web.archive.org/web/20080920001317/http://www.whu.edu/cms/index.php?id=1959&amp;amp;L=1&amp;amp;1354"><strong>WHU Otto Beisheim School of Management</strong></a></p><p><sup>9</sup> <a href="https://web.archive.org/web/20080920001317/http://www.science21stcentury.org/"><strong>Science in the 21st Century</strong></a></p><p><sup>10</sup> <a href="https://web.archive.org/web/20080920001317/http://www.korova-bar.de/korova/"><strong>Korova Bar</strong></a></p><p><sup>11</sup> <a href="https://web.archive.org/web/20080920001317/http://www.mendeley.com/terms/"><strong>Mendeley terms of use</strong></a></p><p><sup>12</sup> <a href="https://web.archive.org/web/20080920001317/http://www.nature.com/natureconferences/sciblog2008/index.html"><strong>Science Blogging 2008: London</strong></a></p><p><sup>13</sup> <a href="https://web.archive.org/web/20080920001317/http://mekentosj.com/papers/"><strong>Papers</strong></a></p><p><sup>14</sup> <a href="https://web.archive.org/web/20080920001317/http://www.mendeley.com/blog"><strong>Mendeley blog</strong></a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Looking back on a year of gobbledygook]]></title>
            <link>https://blog.martinfenner.org/posts/looking-back-on-a-year-of-gobbledygook</link>
            <guid>83abbe61-4297-4d75-9d57-2ef4afe5c076</guid>
            <pubDate>Sun, 03 Aug 2008 00:00:00 GMT</pubDate>
            <description><![CDATA[A year ago today I wrote my first blog post on Nature Network (Open access may
become mandatory for NIH-funded research
[https://web.archive.org/web/20080929033935/http://network.nature.com/blogs/user/mfenner/2007/08/03/open-access-may-become-mandatory-for-nih-funded-research]
). This is blog post #84 one year later and a good time to reflect on the
experience. In May of last year I started the science blog in a nutshell
[https://web.archive.org/web/20080929033935/http://blog.xartrials.com/], ho]]></description>
            <content:encoded><![CDATA[<p>A year ago today I wrote my first blog post on Nature Network (<a href="https://web.archive.org/web/20080929033935/http://network.nature.com/blogs/user/mfenner/2007/08/03/open-access-may-become-mandatory-for-nih-funded-research"><strong>Open access may become mandatory for NIH-funded research</strong></a>). This is blog post #84 one year later and a good time to reflect on the experience. In May of last year I started the science blog <a href="https://web.archive.org/web/20080929033935/http://blog.xartrials.com/"><strong>in a nutshell</strong></a>, hosted on my own server and written just for fun. I discovered Nature Network in July and started <strong>Publish or Perish 2.0</strong>. In November 2007 I <a href="https://web.archive.org/web/20080929033935/http://network.nature.com/blogs/user/mfenner/2007/11/11/a-case-for-goobledygook"><strong>changed the blog name</strong></a> to <strong>Gobbledygook</strong>.</p><p>I try to write about the paper writing process from the perspective of a researcher. I’m interested in the technical changes in paper writing thanks to Web 2.0. Open access is another important topic and the perspective of a researcher is obviously very different from a journal publisher, science library or the interested public. I am sometimes not comfortable to write about open access, as this is a very political topic and the discussion can move away from arguments and into something about doing the right thing. That’s why I would never write about Evolution vs. Intelligent Design or some of the other hotly debated topics in science blogging.</p><p>The blog post that received the most comments is <a href="https://web.archive.org/web/20080929033935/http://network.nature.com/blogs/user/mfenner/2008/06/14/my-paper-writing-dream-machine-1-0"><strong>My Paper Writing Dream Machine 1.0</strong></a>. That was also one of my favorite blog posts as I would love to see more of the potential of Web 2.0 technologies in our paper writing tools. I also enjoyed the dicussion on posters at scientific meetings (<a href="https://web.archive.org/web/20080929033935/http://network.nature.com/blogs/user/mfenner/2008/03/01/are-posters-worth-the-effort"><strong>Are posters worth the effort?</strong></a>) and on blogging from conferences (<a href="https://web.archive.org/web/20080929033935/http://network.nature.com/blogs/user/mfenner/2008/05/17/scientific-meetings-need-more-bloggers"><strong>Scientific meetings need more bloggers</strong></a>).</p><p>I participated in a wonderful SynchroBlogging effort on April Fools Day (organized by <a href="https://web.archive.org/web/20080929033935/http://phylogenomics.blogspot.com/"><strong>Jonathan Eisen</strong></a> and with “help” from the <a href="https://web.archive.org/web/20080929033935/http://homepage.mac.com/jonathan_eisen/Wabda/Wabda.html"><strong>World Anti-Brain Doping Authority</strong></a>) with <a href="https://web.archive.org/web/20080929033935/http://network.nature.com/blogs/user/mfenner/2008/04/01/what-can-erythopoetin-do-for-you"><strong>What can Erythopoetin do for you?</strong></a>. I think we should do more SynchroBlogging, and not just on April 1st. <strong>Public Access Week</strong> was another SynchroBlogging effort and I learned a lot about access to my own papers in <a href="https://web.archive.org/web/20080929033935/http://network.nature.com/blogs/user/mfenner/2008/04/11/public-access-week-who-could-read-my-papers"><strong>Public Access Week: Who could read my papers?</strong></a>.</p><p>Only two blog posts are about scientific research. <a href="https://web.archive.org/web/20080929033935/http://network.nature.com/blogs/user/mfenner/2008/02/07/using-rna-interference-to-identify-genes-that-protect-from-cancer"><strong>Using RNAinterference to identify genes that protect from cancer</strong></a> was my contribution to <a href="https://web.archive.org/web/20080929033935/http://www.justscience.net/2008/?page_id=1368"><strong>Just Science 2008</strong></a>. In <a href="https://web.archive.org/web/20080929033935/http://network.nature.com/blogs/user/mfenner/2008/07/14/mouse-models-of-human-cancer-and-the-need-for-more-translational-research"><strong>Mouse models of human cancer and the need for more translational research</strong></a> I wrote about a presentation by Mario Capecchi at the International Genetics Conference. I would love to do more <a href="https://web.archive.org/web/20080929033935/http://www.researchblogging.org/index.php"><strong>ResearchBlogging</strong></a>, but I think that we have to wait a few more years before science blogging has attracted enough people that read and comment on specific research findings.</p><p>Thanks to this blog I have met a number of very interesting and intelligent people with similar interests (see <a href="https://web.archive.org/web/20080929033935/http://scienceblogs.com/clock/2008/05/eurotrip_08_berlin_part_iii_we.php"><strong>this blog entry</strong></a> by Bora Zivkovic and <a href="https://web.archive.org/web/20080929033935/http://network.nature.com/london/news/blog/matt/2008/07/12/dinner-with-the-nobel-prize-winners"><strong>this blog entry</strong></a> by Matt Brown). That’s why I’m very much looking forward to the <a href="https://web.archive.org/web/20080929033935/http://www.nature.com/natureconferences/sciblog2008/index.html"><strong>Science Blogging 2008: London</strong></a> conference at the end of this month. My goal for the next year: help to make reading and writing science blogs part of the everyday life at more universities and reseach institutions.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My Paper Writing Dream Machine 1.0]]></title>
            <link>https://blog.martinfenner.org/posts/my-paper-writing-dream-machine-1-0</link>
            <guid>7c832e7d-2d5b-4452-89ff-7a9c3b716550</guid>
            <pubDate>Sat, 14 Jun 2008 14:41:00 GMT</pubDate>
            <description><![CDATA[I’ve written a similar post before
[https://web.archive.org/web/20081014105337/http://network.nature.com/blogs/user/mfenner/2008/03/31/pubmed-and-other-annoyances-in-the-paper-writing-process]
, put I would like to talk about some of the features that I would like to see
in an ideal paper writing application.

Intelligent Formatting
Content and formatting should be separated from each other. A manuscript should
require as little formatting as possible, and that formatting (including the
format o]]></description>
            <content:encoded><![CDATA[<p>I’ve written a similar post <a href="https://web.archive.org/web/20081014105337/http://network.nature.com/blogs/user/mfenner/2008/03/31/pubmed-and-other-annoyances-in-the-paper-writing-process"><strong>before</strong></a>, put I would like to talk about some of the features that I would like to see in an ideal paper writing application.</p><p><strong>Intelligent Formatting</strong><br>Content and formatting should be separated from each other. A manuscript should require as little formatting as possible, and that formatting (including the format of references) should be defined in a Journal style that is automatically applied to the manuscript. <a href="https://web.archive.org/web/20081014105337/http://www.wolfram.com/products/publicon/index.html"><strong>Publicon</strong></a> by Wolfram Research tried to achieve this, but unfortunately appears to be a dead product.</p><p><strong>References</strong><br>A reference database should be integrated into the paper writing application. Ideally this would be a web-based database such as <a href="https://web.archive.org/web/20081014105337/http://www.connotea.org/"><strong>Connotea</strong></a>, <a href="https://web.archive.org/web/20081014105337/http://www.citeulike.org/"><strong>CiteULike</strong></a>. Both <a href="https://web.archive.org/web/20081014105337/http://www.refworks.com/"><strong>Refworks</strong></a> and <a href="https://web.archive.org/web/20081014105337/http://www.endnoteweb.com/enwebinfo.asp"><strong>EndNote Web</strong></a> already offer some level of integration.</p><p><strong>Versioning</strong><br>Storing all versions of a manuscript is very important for obvious reasons: backup, keeping track of revisions and coordinating the input from more than one author. Version control is standard practice in software development, using tools like <a href="https://web.archive.org/web/20081014105337/http://subversion.tigris.org/"><strong>Subversion</strong></a> or the newer <a href="https://web.archive.org/web/20081014105337/http://git.or.cz/index.html"><strong>Git</strong></a>.</p><p><strong>Integration with Online Submission Systems</strong><br>Submitting a manuscript to an online manuscript submission system such as <a href="https://web.archive.org/web/20081014105337/http://www.editorialmanager.com/homepage/home.htm"><strong>EditorialManager</strong></a> or <a href="https://web.archive.org/web/20081014105337/http://www.topazproject.org/trac/"><strong>Topaz</strong></a> is too complicated. This process could and should be automated.</p><p><strong>Summary</strong><br>My Paper Writing Dream Machine will in all likelihood turn out to be a web-based application, using on one of the more advanced platforms <a href="https://web.archive.org/web/20081014105337/http://gears.google.com/"><strong>Google Gears</strong></a>, <a href="https://web.archive.org/web/20081014105337/http://silverlight.net/"><strong>Microsoft Silverlight</strong></a> or <a href="https://web.archive.org/web/20081014105337/http://www.adobe.com/de/products/flex/"><strong>Adobe Flex</strong></a>. And the data will be in XML using a standard <a href="https://web.archive.org/web/20081014105337/http://en.wikipedia.org/wiki/Document_Type_Definition"><strong>DTD</strong></a>. Some of the pieces of the puzzle already exist, but nobody has yet put them together in a way that it creates a compelling alternative to the currently used systems.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Tired of Impact Factors? Try the SJR indicator]]></title>
            <link>https://blog.martinfenner.org/posts/tired-of-impact-factors-try-the-sjr-indicator</link>
            <guid>8861a890-51f8-40c4-8600-856450d598d3</guid>
            <pubDate>Mon, 07 Jan 2008 00:00:00 GMT</pubDate>
            <description><![CDATA[Picking the right journal is one of the most important decisions when you start
to work on a paper. You probably have a gut feeling of the journals that are
best suited for your paper in progress. To make this decision more objective,
you can rely on the Impact Factor
[https://web.archive.org/web/20170910200523/http://scientific.thomson.com/free/essays/journalcitationreports/impactfactor/] 
of a journal. The Impact Factor is roughly the average number of citations per
paper in a given journal an]]></description>
            <content:encoded><![CDATA[<p>Picking the right journal is one of the most important decisions when you start to work on a paper. You probably have a gut feeling of the journals that are best suited for your paper in progress. To make this decision more objective, you can rely on the <a href="https://web.archive.org/web/20170910200523/http://scientific.thomson.com/free/essays/journalcitationreports/impactfactor/">Impact Factor</a> of a journal. The Impact Factor is roughly the average number of citations per paper in a given journal and is published by  <a href="https://web.archive.org/web/20170910200523/http://scientific.thomson.com/index.html">Thomson Scientic</a>. Higher Impact Factors mean more prestigious journals. This information is also frequently used for job or grant applications.</p><p>Impact factors have been around for more than 40 years and they generally been very helpful. But there are two big problems:</p><p><em>Impact Factors are published by one privately owned company</em><br>Given the importance of Impact Factors for many aspects of scientific publishing, it would be preferable if there were alternatives. And Impact Factors are not freely available, but must be purchased from Thomson Scientific.</p><p><em>Impact Factors might not be the best tool to measure scientific quality</em><br>Impact factors have several shortcomings. Because they are a convenient way to judge the scientific output of a person, organization, journal or country, they are often overused. They should for example not be used to compare journals in different fields, e.g. cell biology and particle physics. Measures like the <a href="https://web.archive.org/web/20170910200523/http://network.nature.com/blogs/user/mfenner/2007/08/17/do-you-know-your-hirsch-number">Hirsch Number</a> might be a better tool to measure the scientific output of an individual scientist. And sometimes the judgement of your peers in the field is more important than simple numbers.</p><p>The <a href="https://web.archive.org/web/20170910200523/http://www.scimagojr.com/">SCImago Journal Rank</a> indicator tries to overcome these two shortcomings. The index was created by the <a href="https://web.archive.org/web/20170910200523/http://www.scimago.es/">SCImage Research Group</a>, located at several Spanish universities. The index uses information from the <a href="https://web.archive.org/web/20170910200523/http://www.scopus.com/">Scopus</a> abstract and citation database of research literature owned by <a href="https://web.archive.org/web/20170910200523/http://www.elsevier.com/">Elsevier</a>.</p><p>In contrast to the Impact Factor, the SJR indicator measures not simply the number of citations per paper. Citations from a journal with a higher SJR indicator lead to a higher SJR indicator for the cited journal (more details <a href="https://web.archive.org/web/20170910200523/http://www.scimagojr.com/SCImagoJournalRank.pdf">here</a>). This approach is similar to PageRank (described in <a href="https://web.archive.org/web/20170910200523/http://infolab.stanford.edu/~backrub/google.html">this</a> paper), the algorithm for web searches by Sergey Brin and Lawrence Page that made <a href="https://web.archive.org/web/20170910200523/http://www.google.com/">Google</a> what it is today. <a href="https://web.archive.org/web/20170910200523/http://www.eigenfactor.org/">Eigenfactor</a> is another scientific ranking tool that uses a PageRank algorithm.</p><p>Most of the time, journals with high Impact Factors have high SJR indicators. Nature and Science are still head to head. We will find unexpected results and discrepancies between the two over time. In my field of oncology, both the Journal of the NCI and Cancer Research are ranked higher than the Journal of Clinical Oncology.</p><p>You can read more about the SJR indicator in this <a href="https://web.archive.org/web/20170910200523/http://www.nature.com/news/2008/080102/full/451006a.html">Nature News article</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Open access may become mandatory for NIH-funded research]]></title>
            <link>https://blog.martinfenner.org/posts/open-access-may-become-mandatory-for-nih-funded-research</link>
            <guid>562258f0-2a28-4091-b827-fda27b76642e</guid>
            <pubDate>Fri, 03 Aug 2007 14:17:00 GMT</pubDate>
            <description><![CDATA[The National Institutes of Health (NIH) is currently recommending public access
of all papers from NIH-funded research. Fewer than 5% of research papers
[https://web.archive.org/web/20080517063146/http://publicaccess.nih.gov/Final_Report_20060201.pdf] 
have gone this route since the policy went into effect in 2005. On July 19, 2007
[https://web.archive.org/web/20080517063146/http://www.scientificblogging.com/news/house_of_representatives_backs_faster_public_access_to_nih_studies] 
the House of R]]></description>
            <content:encoded><![CDATA[<p>The National Institutes of Health (NIH) is currently recommending public access of all papers from NIH-funded research. <a href="https://web.archive.org/web/20080517063146/http://publicaccess.nih.gov/Final_Report_20060201.pdf"><strong>Fewer than 5% of research papers</strong></a> have gone this route since the policy went into effect in 2005. On <a href="https://web.archive.org/web/20080517063146/http://www.scientificblogging.com/news/house_of_representatives_backs_faster_public_access_to_nih_studies"><strong>July 19, 2007</strong></a> the House of Representatives passed the FY2008 Labor, HHS, and Education Appropriations Bill that will make public access within 12 months of publication a requirement for all NIH-funded work. The bill still has to be approved by the Senate.</p><p>What is the policy of other granting agencies on open access requirements for funded work?</p><p><strong>Howard Hughes Medical Institute (HHMI)</strong><br>On June 26, 2007 the <a href="https://web.archive.org/web/20080517063146/http://www.hhmi.org/news/20070626.html"><strong>HHMI</strong></a> announced that original research papers by HHMI scientists have to be publicly available within 6 months of publication. The policy goes into effect for all papers submitted on or after January 1, 2008 where a HHMI scientist is first author, last author and/or corresponding author.</p><p><strong>Welcome Trust</strong><br>All publications of research funded by the British <a href="https://web.archive.org/web/20100122231246/http://www.wellcome.ac.uk/doc_WTD002766.html"><strong>Welcome Trust</strong></a> have to be publicly available within 6 months of publication since October 1, 2006.</p><p><strong>Deutsche Forschungsgemeinschaft (DFG)</strong><br>The German DFG has issued recommendations for open access in <a href="https://web.archive.org/web/20100122231246/http://www.dfg.de/aktuelles_presse/information_fuer_die_wissenschaft/andere_verfahren/info_wissenschaft_04_06.html"><strong>January 2006</strong></a>. Although scientists receiving DFG grants are encouraged to have their papers publicly available, there is no requirement to do so.</p><p><strong>CNRS, INSERM, INRA and INRIA</strong><br>The French National Center for Scientific Research (CNRS), the National Institute of Health and Medical Research (INSERM), the National Institute for Agricultural Research (INRA) and the National Research Institute for IT and Robotics (INRIA) have created <a href="https://web.archive.org/web/20100122231246/http://www2.cnrs.fr/en/332.htm"><strong>publicly available institutional repositories</strong></a> for their researchers. Deposition of papers in these repositories is voluntary.</p><p>In summary, there is a clear trend towards making open access a requirement for funding. The taxpayer-funded research agencies – or rather the legislators – have more problems with making this a requirement than the private charities (Welcome Trust and HHMI).</p>]]></content:encoded>
        </item>
    </channel>
</rss>