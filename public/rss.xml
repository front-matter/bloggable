<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Gobbledygook</title>
        <link>https://blog.martinfenner.org/</link>
        <description>Martin Fenner writes about how the internet is changing scholarly communication.</description>
        <lastBuildDate>Sat, 06 Feb 2021 12:26:12 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <copyright>Copyright © 2007-2021 Martin Fenner. Distributed under the terms of the Creative Commons Attribution 4.0 License.</copyright>
        <item>
            <title><![CDATA[Thank you PLOS]]></title>
            <link>https://blog.martinfenner.org/posts/thank-you-plos</link>
            <guid>ae704746-6ec6-4d90-b126-d88206036d2f</guid>
            <pubDate>Wed, 29 Jul 2015 10:52:00 GMT</pubDate>
            <description><![CDATA[Starting next week
[https://www.datacite.org/news/martin-fenner-and-laura-rueda-join-datacite-team.html] 
I will work as the DataCite Technical Director, and I am excited about this new
opportunity. But this is material for another post, here I want to reflect on
the last three years working as Technical Lead for the PLOS Article-Level
Metrics [http://lagotto.io/plos/] project.

It feels much longer than three years, but until May 2012 I worked as medical
oncologist at Hannover Medical School, t]]></description>
            <content:encoded><![CDATA[<p><a href="https://www.datacite.org/news/martin-fenner-and-laura-rueda-join-datacite-team.html">Starting next week</a> I will work as the DataCite Technical Director, and I am excited about this new opportunity. But this is material for another post, here I want to reflect on the last three years working as Technical Lead for the <a href="http://lagotto.io/plos/">PLOS Article-Level Metrics</a> project.</p><p>It feels much longer than three years, but until May 2012 I worked as medical oncologist at Hannover Medical School, treating patient with cancer, attending interdisciplinary tumor boards and helping with clinical trials. It was a very brave move by PLOS to hire me at this point, especially since I <a href="https://martinfenner.ghost.io/2015/06/28/why-work-where-we-live/">worked remotely</a> from Germany rather than in the San Francisco office. I will be forever thankful to PLOS for giving me this opportunity.</p><p>Two factors probably played a role in this decision: I have been blogging about how the internet is changing scholarly communication since 2007, and since September 2010 I had <a href="http://blogs.plos.org/mfenner/">my blog on the PLOS Blogs Network</a>. I had also visited the PLOS offices in San Francisco, and had met several PLOS people at conferences, including Pete Binfield, Rich Cave, Mark Patterson, Brian Mossop, Jennifer Lin and Liz Allen. I had <a href="http://blogs.plos.org/mfenner/2009/08/15/plos_one_interview_with_peter_binfield/">interviewed</a> Pete Binfield about PLOS ONE and the PLOS Article-Level Metrics project in August 2009, shortly after the project was launched.</p><p>The other factor was the hackathon at the <a href="http://www.nature.com/spoton/event/science-online-london-2011/">2011 Science Online London</a> conference. We were a really small group of people (I remember Jason Hoyt, Victor Henning, Kristi Holmes and Cameron Neylon, Mendeley was hosting the event), but I had the idea to hack the open source PLOS Article-Level Metrics application. This hack turned into <a href="http://blogs.plos.org/mfenner/2011/09/28/announcing-sciencecard/">ScienceCard</a>, a version of the PLOS Article-Level Metrics application focussing on people rather than articles, and the application was a finalist for the <a href="http://blog.mendeley.com/highlighting-research/the-top-101-apps-in-the-mendeley-plos-binary-battle/">Mendeley/PLOS API Binary Battle</a>. ScienceCard doesn’t exist anymore, but the concept of organizing metrics around a person lives on in ImpactStory (see my profile <a href="https://impactstory.org/mfenner">here</a>), facilitated by the launch of ORCID in October 2012. More importantly - without me knowing it - ScienceCard demonstrated that I could work with and extend the PLOS Article-Level Metrics code, and I think I was the first person outside of PLOS doing this. Which must have helped when PLOS was looking for a technical lead for the project a few months later.</p><p>In other words, blogging and hacking code can lead to great job opportunities.</p><p>While at PLOS I not only learned a ton of things about article-level metrics and all its challenges and opportunities, but also many basic skills needed in software development. Which is important, as my formal training is in clinical medicine and molecular biology, and doing software development in your free time (which I had done since the 1990s) only gets you so far. Some of the unexpected things I learned:</p><ul><li><strong><strong>Visualizations</strong></strong>: while it was clear that I was expected to generate visualizations for the PLOS Article-Level Metrics data, I didn’t expect this to go so deep, first with R and later with <a href="http://d3js.org/">d3.js</a>. Najko Jahn introduced me to using R to analyze the PLOS data, and I later worked closely with Scott Chamberlain from the <a href="https://ropensci.org/">rOpenSci</a> project to help improve their <a href="https://ropensci.org/tutorials/alm_tutorial.html">alm package</a>. The Javascript work with d3.js started with <a href="http://article-level-metrics.plos.org/alm-workshop-2012/hackathon/#altviz">AlmViz</a> at the 2012 ALM hackathon and later was done in close collaboration with Juan Alperin from the <a href="https://pkp.sfu.ca/">Public Knowledge Project</a>.</li><li><strong><strong>DevOps</strong></strong>: the intersection of software development and system administration. I became a big fan and have spent endless hours learning how to automate the configuration and deployment of servers and other infrastructure.</li><li><strong>O<strong>pen source community building</strong></strong>: again something I was expected to do around the PLOS article-level metrics open source application, but I never expected this to be so challenging and time-consuming, but also rewarding.</li></ul><p>I thank everyone at PLOS who I had the pleasure to work with over the years, in particular Kristen Ratan, Cameron Neylon, Donna Okubo, Mei Yan Leung, Liz Allen, Catriona MacCallum, Matt Hodgkinson, Theo Bloom, Damian Pattinson, Ginny Barbour, Emma Ganley, Roli Roberts, Eric Martens, Susan Au, Matt Willman, Edgar Munoz, Rachel Drysdale, CJ Rayhill, Lisa Siegel, Jennifer Song, Polina Grinbaum, John Bertrand, Mike Baehr, Clark Hartsock, Adam Hyde, and Holly Allen. A very special thanks goes to Jennifer Lin, Rich Cave and John Chodacki who worked with me on a daily basis.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Component DOIs Revisited]]></title>
            <link>https://blog.martinfenner.org/posts/component-dois-revisited</link>
            <guid>0983610b-64df-4dff-96dc-5de43c56a237</guid>
            <pubDate>Thu, 09 Jul 2015 10:19:00 GMT</pubDate>
            <description><![CDATA[Four years ago I wrote a blog post
[http://blogs.plos.org/mfenner/2011/03/26/direct-links-to-figures-and-tables-using-component-dois/] 
about component DOIs. It is time to revisit the topic, in particular since our
approach to citing data associated with a publication has changed since 2011.

Component DOIs are explained in the CrossRef Help System
[http://help.crossref.org/components]:

> DOIs may be assigned to items that are part of a journal article, book chapter,
or any other content item. ]]></description>
            <content:encoded><![CDATA[<p>Four years ago I wrote a <a href="http://blogs.plos.org/mfenner/2011/03/26/direct-links-to-figures-and-tables-using-component-dois/">blog post</a> about component DOIs. It is time to revisit the topic, in particular since our approach to citing data associated with a publication has changed since 2011.</p><p>Component DOIs are explained in the <a href="http://help.crossref.org/components">CrossRef Help System</a>:</p><blockquote>DOIs may be assigned to items that are part of a journal article, book chapter, or any other content item. A component would typically be a figure, table, or image which is part of or referred to by the parent item. Assigning a DOI to a component allows direct linking to the component item.</blockquote><p>Component DOIs are DOIs, i.e. persistent identifiers that link directly to the resource in question, e.g. a figure in a publication. The component DOI for a figure in a PLOS paper used in the 2011 post still <a href="http://doi.org/10.1371/journal.pone.0006022.g002">works as expected</a>, despite changes to the URL of the journal landing page.</p><p>The problem with component DOIs is the problem with DOIs in general: there is basic functionality common to all DOIs, and there are additional services specific to subgroups of DOIs. This confuses users - in particular since there is no easy way to immediately see what kind of DOI they have in front of them - and in the case of component DOIs there is one important feature missing.</p><p>DOis are assigned by registration agencies (CrossRef and DataCite are the most relevant ones for scholarly content), and these RAs have built different services around DOIs, e.g. different ways to describe and search the metadata (title, authors, etc.) associated with a DOI. Component DOIs are again different, the most important difference is that in the CrossRef implementation they they are not discoverable by querying the CrossRef system (Feeney, 2010). Component DOIs are also always associated with a parent DOI (for the article, book, etc.). Although this is the expected behaviour, we shouldn’t expect component DOIs to always look like an extension of the parent DOI, as in <code>10.1371/journal.pone.0006022.g002</code> used in the example above.</p><p>In essence, a component DOI is a <strong><strong>DOI light</strong></strong>. We can use them for persistent linking, but we can’t use them for discovery via the CrossRef Metadata Search (and by extension other indexing services). A common use case for component DOIs is supplementary information in a journal article. Content in supplementary information files is already much harder to find than content in the body of an article, using component DOIs instead of regular DOIs makes the content again harder to find.</p><p>All of this might not have been much of an issue when I wrote the 2011 post, but making the data underlying a publication publicly available and discoverable is increasingly becoming something that funders, publishers and institutions expect. Most of these data are not deposited in dedicated data repositories, but in supplementary information files (for PLOS articles published since March 2014 this is true for more than 50% of papers). Using regular DOIs for supplementary information files with proper metadata and proper inclusion in indexing services will make it easier to find, access and reuse these data.</p><p>Unfortunately that still leaves us with the problem that the supplementary information files then will have CrossRef DOIs, whereas data repositories typically use DataCite DOIs, so that we need to search for these datasets in two different places. But that is material for another post.</p><h2 id="references">References</h2><p>Feeney, P. (2010). DOIs for Journals: Linking and Beyond. <em>Information Standards Quarterly</em>, <em>22</em>(3), 27. <a href="https://doi.org/10.3789/isqv22n3.2010.06">https://doi.org/10.3789/isqv22n3.2010.06</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Why should we work where we live?]]></title>
            <link>https://blog.martinfenner.org/posts/why-should-we-work-where-we-live</link>
            <guid>66d00cb0-23c6-4e9d-9561-7dab13ffaa63</guid>
            <pubDate>Sun, 28 Jun 2015 10:23:00 GMT</pubDate>
            <description><![CDATA[At the SciFoo Camp [http://www.digital-science.com/events/scifoo-camp-2015/] 
this weekend Erin McKiernan [https://emckiernan.wordpress.com/] and I moderated
an unconference session on the topic Why should we work where we live? This was
a spontaneous idea after we had talked about this topic on Friday (Erin lives in
Mexico with a job in Canada, I live in Germany and work for an organization in
San Francisco).

We quickly realized that this situation is far from uncommon in the space we
work in ]]></description>
            <content:encoded><![CDATA[<p>At the <a href="http://www.digital-science.com/events/scifoo-camp-2015/">SciFoo Camp</a> this weekend <a href="https://emckiernan.wordpress.com/">Erin McKiernan</a> and I moderated an unconference session on the topic <strong><strong>Why should we work where we live?</strong></strong> This was a spontaneous idea after we had talked about this topic on Friday (Erin lives in Mexico with a job in Canada, I live in Germany and work for an organization in San Francisco).</p><p>We quickly realized that this situation is far from uncommon in the space we work in (science and science communication). Most commonly the reason is compromises we have to make when both partners have to find an adequate job. It can be a big challenge for a couple to find senior jobs in academia in the same city or region, especially outside of academic clusters such as Boston, New York or London.</p><p>The other big reason for work remote is that some research can only happen in special places, for example in high-energy physics, astronomy or the geosciences. And of course there are other flavors of the same situation, e.g. when a principal investigator moves to a new institution and PhD students or postdocs can’t or don’t want to move with him/her. And most academics have to do at least some remote work, since they will spend a good amount of time travelling to conferences or collaboration partners.</p><p>The discussion in the session centered on the social and technical challenges of working remotely. We didn’t have time to go into the legal aspects (e.g. taxes when you work in a different country), or the challenges organizing your personal life, particular difficult when you have children.</p><p>We shared our experience with online collaboration tools, and video conferencing with Skype, Google Hangouts or similar was central to this. Videoconferencing can be a challenge with slow internet connectivity, a situation that luckily is constantly improving.</p><p>Private group chat tool such as <a href="https://www.hipchat.com/">HipChat</a> or <a href="https://slack.com/">Slack</a> are becoming increasingly popular outside the Tech sector and are a great alternative to email. They not only provide a platform for quick messages between two people, but also serve as a backchannel for informal “water cooler” discussions in an organization.</p><p>Another essential category is tools that track your work so that your remote colleagues not only can collaborate with you, but also see the work you are doing. As a supervisor you quickly see the work that was done the past week, a much more reasonable approach than looking at physical presence at work (where people might be doing all kinds of other things and personal productivity varies). Tracking your work is easy if you are a software developer like me and can look at code commited to version control, tickets closed, etc. For research this is more challenging, in particular if the workflow is not digital yet and for example all experiments are documented in a paper notebook. It seems that one requirement for remote work in science is digitalization of your work, but that is a direction we are heading anyway and which has other advantages (e.g. improving reproducibility). If there are no specialized tools for documenting your work available, then a note-taking tool such as <a href="https://www.onenote.com/">OneNote</a> or <a href="https://evernote.com/">Evernote</a> can be helpful. The digitization and automation of work is obviously limited in wet labs that require direct interactions with samples and instruments.</p><p>The social aspects of remote work might be the bigger challenge. There is still a big reluctance in supervisors and administrators to this, assuming that people will only be productive if someone is watching them. This assumption is very short sighted, as what drives PhDs and postdocs to work hard is not supervision, but the intrinsic motivation to accomplish something, in particular in light of the very competitive situation for permanent jobs in academia. The book <a href="http://37signals.com/remote/">Remote</a> by Jason Fried talks about this in great detail in the context of software development, but the same principles apply to work in science. What supervisors and administrators loose in direct oversight they can in attracting talent they would otherwise not get. Remote work only works if supported by the host institution, for example by adapting internal workflows and communications to make remote work the default rather than an exception.</p><p>Remote work is usually more successful and satisfying if combined with physical presence at the workplace. Reasons for this are not only the part of the work that can’t be done remotely, but more importantly the social aspect. How extensive this physical presence is depends on the circumstances. Some level of remote work has become part of almost everyone’s job in science, as it includes working at home in the evenings or on weekends, or work while traveling.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Persistent Identifiers and URLs]]></title>
            <link>https://blog.martinfenner.org/posts/persistent-identifiers-and-urls</link>
            <guid>f89e2997-a9e4-4924-bd70-16231baec3e6</guid>
            <pubDate>Wed, 03 Jun 2015 10:43:00 GMT</pubDate>
            <description><![CDATA[Just like the rest of the internet, much of our scholarly infrastructure is
built around the Hypertext Transfer Protocol (HTTP), increasingly HTTPS for
security, and soon HTTP/2 [https://http2.github.io/] for better performance. In
this infrastructure Universal Resource Locators (URLs) are essential to locate
resources (sic) such as scholarly articles, datasets, researchers,
organizations, or grants. Read this
[http://site.thomsonreuters.com/site/data-identifiers/] recent Thomson Reuters
report ]]></description>
            <content:encoded><![CDATA[<p>Just like the rest of the internet, much of our scholarly infrastructure is built around the Hypertext Transfer Protocol (HTTP), increasingly HTTPS for security, and soon <a href="https://http2.github.io/">HTTP/2</a> for better performance. In this infrastructure Universal Resource Locators (URLs) are essential to locate resources (sic) such as scholarly articles, datasets, researchers, organizations, or grants. Read <a href="http://site.thomsonreuters.com/site/data-identifiers/">this</a> recent Thomson Reuters report for a good recent perspective on this topic. While this works for the most part, there are some issues with URLs - not specific to scholarly content, but particularly import here:</p><ol><li>multiple URLs can point to the same resource</li><li>URLs can be long and look ugly</li><li>URLs can change or break, making it hard or impossible to locate the resource</li><li>we are used to central indexes (or databases) describing these resources, allowing us to do sophisticated queries not possible in a generic web search, e.g. find all publications by author John Doe, published since 2012.</li></ol><p>No. 1 is a problem relevant to all URLs, e.g. web searches or liking/commenting a particular web page. Originally suggested by Google, <a href="https://support.google.com/webmasters/answer/139066?hl=en">Canonical URLs</a> are essential for services such as Facebook or <a href="https://hypothes.is/blog/cross-format-annotation/">Hypothes.is</a>. They have been formalized in <a href="http://tools.ietf.org/html/rfc6596">rfc6596</a> and are commonly used.</p><p>No. 2 can be a problem, in particular if we are not careful in designing appropriate URLs for landing pages (see next paragraph), but rather use something long and unreadable that also includes query parameters, etc. If we have no control over how the URL looks like, we can use URL shortener services such as <a href="https://bitly.com/">bit.ly</a>, which of course have become a common sight on the web. <a href="http://shortdoi.org/">ShortDOIs</a> are an URL shortener for DOIs, but they don’t seem to have gained much traction.</p><p>No. 3 is a particularly important issue, commonly referred to as <strong><strong>link rot</strong></strong> and described extensively for the scholarly literature, e.g. by (Klein et al., 2014). There are several technical solutions to this problem, a common approach is to use a landing page for the resource that will never change (and follows the recommendations by Tim Berners-Lee for <a href="http://www.w3.org/Provider/Style/URI.html">Cool URIs</a>, and then use redirection to point to the current location of the resource. This is easily for changes of the URL path using web server <a href="http://httpd.apache.org/docs/2.4/rewrite/remapping.html">redirect rules</a>. It gets more complicated if the server name also changes, in particular if it is the server holding the landing page. Thinking this through you realize that the only way this can be done on a larger scale is via one or more centralized services that not only provide the technical infrastructure for a central redirection (or resolver) service, but also come with a social contract of rules that everyone submitting URLs to the service has to follow - a major difference to URL shorteners, which don’t solve the link rot problem.</p><p>The above is of course a description of the DOI service provided by CrossRef, DataCite, and others, as well as similar persistent identifier services. Unfortunately some persistent identifier services don’t do the above: they create and use persistent identifiers, but there is no central resolver service that maps these identifiers back to URLs. This breaks the integration with the bigger scholarly infrastructure based on URLs. One common example are nucleotide sequences such as U65091 (Shioda, Fenner, &amp; Isselbacher, 1997), there is no single corresponding URL because the sequence can be found in all three main nucleotide databases: <a href="http://www.ncbi.nlm.nih.gov/nuccore/U65091">http://www.ncbi.nlm.nih.gov/nuccore/U65091</a>, <a href="http://www.ebi.ac.uk/ena/data/view/U65091">http://www.ebi.ac.uk/ena/data/view/U65091</a>, and <a href="http://getentry.ddbj.nig.ac.jp/getentry/na/U65091">http://getentry.ddbj.nig.ac.jp/getentry/na/U65091</a>. It would help to have a central resolver, e.g. <a href="http://nucleotide.org/U65091">http://nucleotide.org/U65091</a> that then redirects to one of the three databases based on geographical location or user preference.</p><p>There are also problems with DOIs. They use the <a href="http://www.handle.net/">Handle</a> system to resolve the identifier to a location, and this system was built in the 1990s as infrastructure <a href="http://www.handle.net/faq.html">independent of</a> URLs or DNS (Domain Name Service), at a time when it wasn’t clear yet that URLs and associated standards would become ubiquitous. I don’t have numbers, but practically all DOIs are of course now resolved to URLs using the <a href="http://www.doi.org/factsheets/DOIProxy.html">DOI proxy server</a> at <a href="http://doi.org/">http://doi.org</a> (preferred) or <a href="http://dx.doi.org/">http://dx.doi.org</a>. One main consequence of this is that DOIs are frequently not written as URLs - e.g. doi:10.5555/12345678 instead of <a href="http://doi.org/10.5555/12345678">http://doi.org/10.5555/12345678</a> - again breaking the integration with the bigger scholarly infrastructure. The CrossRef <a href="http://www.crossref.org/02publishers/doi_display_guidelines.html">DOI display guidelines</a> clearly state that DOIs should be written as URLs in <em>the online environment</em>, which basically is whenever DOIs are used, as PDFs and even Word documents know how to handle URLs. Unfortunately this guideline is still frequently ignored. The above is of course also true for other persistent identifiers using the Handle system, e.g. <a href="http://www.pidconsortium.eu/">EPIC</a>.</p><p>The other problem with the DOI system is that it doesn’t address issue No. 4, i.e. provide a central metadata index for the resources that use the system. This job is left to the DOI registration agencies such as CrossRef and DataCite, who have implemented a central metadata store (e.g. <a href="http://search.crossref.org/">CrossRef</a>, <a href="http://search.datacite.org/">DataCite</a>) in different ways (e.g. using different metadata schemata), or not at all. This means that we have to look in several places to find all DOis associated with author John Doe, published since 2012. Obviously we are used to looking up information in multiple places, but not being able to look up the metadata for a DOI without some extra work (finding out the registration agency for the DOI and then going to the respective metadata store) is a problem. One way around these problems is to use the <a href="http://www.crosscite.org/cn/">DOI Content Negotiation Service</a>.</p><p>Another problem with the DOI system is more a social than a technical issue. Neither CrossRef nor DataCite seem to enfource that DOIs should alsways resolve to URLs when using a computer program. DOI resolution for humans works fine, but computers, e.g. command line tools such as cURL, can run into issues such as requiring cookies, javascript or user input, or permission problems getting to the journal landing page (see <a href="https://martinfenner.ghost.io/2013/10/13/broken-dois">this earlier blog post</a> for some numbers). People seem to forget that a DOI that is not actionable is not really useful, and that scholarly infrastructure is not only used by people, but of course also by automated tools.</p><p>The persistent identifiers used in our scholarly infrastructure would benefit from a clearer focus on the problems they should solve, startin with No. 1-4 above. One problem is that we probably focus too much on the persistence problem, implied also by the term <strong><strong>persistent identifier</strong></strong> or <strong><strong>PID</strong></strong>. What we have neglected is the resolvable problem, i.e. making as easy as possible to get from the persistent identifier to the resource and/or its metadata. Based on the <a href="http://www.knowledge-exchange.info/Default.aspx?ID=462">Den Haag Manifesto</a> and suggested by Todd Vision, we therefore proposed the term <strong><strong>trusted identifier</strong></strong> with the following characteristics in the conceptual model of interoperability for the <a href="http://odin-project.eu/">ODIN Project</a> (ODIN Project, Fenner, Thorisson, Ruiz, &amp; Brase, 2013):</p><ul><li>are unique on a global scale, allowing large numbers of unique identifiers</li><li>resolve as HTTP URI’s with support for content negotiation, and these HTTP URI’s should be persistent.</li><li>come with metadata that describe their most relevant properties, including a minimum set of common metadata elements. A search of metadata elements across all trusted identifiers of that service should be possible.</li><li>are interoperable with other identifiers through metadata elements that describe their relationship.</li><li>are issued and managed by an organization that focuses on that goal as its primary mission, has a sustainable business model and a critical mass of member organizations that have agreed to common procedures and policies, has a governing body, and is committed to using open technologies.</li></ul><p>While not directly relevant for resolving persistent identifiers as URLs, the last point is really important for any persistent identifier infrastructure, described in detail recently by (Bilder, Lin, &amp; Neylon, 2015).</p><p>If I would design a persistent identifier service today (as if we would need yet another persistent identifier service), I would build the system around an URL shortening service that I control. The URLs could look very similar to what we have with DOIs now, e.g. <a href="http://doi.org/10.5555/12345678">http://doi.org/10.5555/12345678</a>, but it would be clear that persistent identifiers are URLs, not something separate. Plus we could take adavantage of all the lessons learned - and possibly even reuse open source code - with URL shorteners, which are much more widely used than scholarly persistent identifiers.</p><p><em>Update 6/4/15: added link to Thomson Reuters <a href="http://site.thomsonreuters.com/site/data-identifiers/">report</a> on identifiers and open data.</em></p><h2 id="references">References</h2><p>Bilder, G., Lin, J., &amp; Neylon, C. (2015). Principles for open scholarly infrastructures-v1. <a href="https://doi.org/10.6084/m9.figshare.1314859">https://doi.org/10.6084/m9.figshare.1314859</a></p><p>Klein, M., Van de Sompel, H., Sanderson, R., Shankar, H., Balakireva, L., Zhou, K., &amp; Tobin, R. (2014). Scholarly context not found: one in five articles suffers from reference rot. <em>PLoS ONE</em>, <em>9</em>(12), e115253. <a href="https://doi.org/10.1371/journal.pone.0115253">https://doi.org/10.1371/journal.pone.0115253</a></p><p>ODIN Project, Fenner, M., Thorisson, G., Ruiz, S., &amp; Brase, J. (2013). D4.1 conceptual model of interoperability. <a href="https://doi.org/10.6084/m9.figshare.824314">https://doi.org/10.6084/m9.figshare.824314</a></p><p>Shioda, T., Fenner, M. H., &amp; Isselbacher, K. J. (1997). Mus musculus melanocyte-specific gene 1 (msg1) mRNA, complete cds. ENA. Retrieved from <a href="http://www.ebi.ac.uk/ena/data/view/U65091">http://www.ebi.ac.uk/ena/data/view/U65091</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Human-readable and machine-readable Persistent Identifiers]]></title>
            <link>https://blog.martinfenner.org/posts/human-readable-and-machine-readable-persistent-identifiers</link>
            <guid>e92e15e3-4449-4e6e-b901-f36aabc6fb36</guid>
            <pubDate>Wed, 27 May 2015 10:45:00 GMT</pubDate>
            <description><![CDATA[Yesterday Julie McMurry and co-authors published a preprint 10 Simple rules for
design, provision, and reuse of persistent identifiers for life science data 
(McMurry et al., 2015). This is an important paper trying to address a
fundamental problem: how can we make persistent identifiers both human-readable
and machine-readable?

Don’t be fooled by the title (used frequently by PLOS Computational Biology
[http://www.ploscollections.org/article/browse/issue/info%3Adoi%2F10.1371%2Fissue.pcol.v03.i]]></description>
            <content:encoded><![CDATA[<p>Yesterday Julie McMurry and co-authors published a preprint <strong><strong>10 Simple rules for design, provision, and reuse of persistent identifiers for life science data</strong></strong> (McMurry et al., 2015). This is an important paper trying to address a fundamental problem: how can we make persistent identifiers both human-readable and machine-readable?</p><p>Don’t be fooled by the title (used frequently by <a href="http://www.ploscollections.org/article/browse/issue/info%3Adoi%2F10.1371%2Fissue.pcol.v03.i01">PLOS Computational Biology</a>) - the paper doesn’t describe simple rules that help the average life sciences researcher. Rather, the paper deals with rather complex issues, and has 36 authors.</p><p>There is general agreement that we need persistent identifiers for scholarly communication, and that also includes life sciences datasets, the focus of the paper. What is less clear is how to express these persistent identifiers. An identifier such as <strong><strong>AB020317</strong></strong> - for the mouse p53 gene - is ambiguous. It is not clear without additional information that this is an identifier for the GenBank nucleotide database, rather than <a href="https://www.flickr.com/photos/alexcycu/8936663973/">something completely different</a>. One common approach to make this identifier unambiguous is to use URIs (Uniform Resource Identifiers), e.g. <a href="http://www.ncbi.nlm.nih.gov/nuccore/AB020317">http://www.ncbi.nlm.nih.gov/nuccore/AB020317</a> in this case.</p><p>The paper doesn’t like this approach, and even states that “URIs are still among the most commonly used and most problematic identifiers in the bio-data ecosystem”. The text also states that “their length makes them unwieldy for humans working with the data or for referencing in publications or other text”, but doesn’t go into any detail why URIs are “problematic identifiers”, or why length is an issue in an online environment.</p><p>This is an important weakness of the paper, because the authors propose an alternative: CURIEs or <strong><strong>compact URIs</strong></strong>. CURIEs were <a href="http://www.w3.org/TR/curie/">proposed</a> by the W3C a few years ago, as a way to make URIs <a href="http://crosstech.crossref.org/2008/12/curies_a_cure_for_uris.html">more human-readable</a>. The idea is simple, we use a namespace in addition to the local identifier, separated by a colon, e.g. <strong><strong><a href="http://www.ebi.ac.uk/ena/data/view/AB020317">Genbank:AB020317</a></strong></strong>.</p><p>This approach has of course been common practice in the life sciences before CURIEs or even the WWW existed, and is still the most common approach how identifiers for life sciences data are referenced in the scholarly literature. Unfortunately there are important problems with CURIEs, most of them mentioned in the paper:</p><ul><li>Persistent identifiers need to be resolvable, without additional information we don’t know what to do with <strong><strong><a href="http://www.ebi.ac.uk/ena/data/view/AB020317">Genbank:AB020317</a></strong></strong>. Most life sciences researchers understand this CURIE, but that might not necessarily be true for less commonly used namespaces</li><li>Namespaces are not necessarily unique, the paper uses <strong><strong>GEO</strong></strong> (which could mean Gene Expression Omnibus or GeoNames Ontology) as an example</li><li>Rule 3 in the paper goes into great detail what characters and patterns should be avoided in local identifiers that are part of a CURIE. It is not clear whether these recommendations will always be followed or how to check them</li><li>CURIEs should follow a pattern (regular expression) so that they can be extracted from a text. We know (Kafkas, Kim, &amp; McEntyre, 2013) that extracting identifiers from journal articles is possible, but difficult</li></ul><p>URIs don’t have the problems listed above: they resolve, are unique, and there is good understanding (and available tools) of how a valid URI should look like and how to extract URIs from text documents. That is why URIs are good representations of persistent identifiers.</p><p>Another problem I have with CURIEs: the idea doesn’t seem to have caught on from the initial work more than five years ago (background reading <a href="http://manu.sporny.org/2011/case-for-curies/">here</a>). I’m not even sure what percentage of persistent identifier experts know about CURIEs.</p><p>My recommendation for life sciences data: express persistent identifiers as URIs. Now that can go into 10 simple rules for the average life sciences researcher.</p><p><em>P.S. This blog uses a tool <a href="http://blog.martinfenner.org/2013/07/02/auto-generating-links-to-data-and-resources/">I wrote two years ago</a> that automatically turns CURIEs in the text into links.</em></p><h2 id="references">References</h2><p>Kafkas, Ş., Kim, J.-H., &amp; McEntyre, J. R. (2013). Database Citation in Full Text Biomedical Articles. <em>PLoS ONE</em>. <a href="http://doi.org/10.1371/journal.pone.0063184">doi:10.1371/journal.pone.0063184</a></p><p>McMurry, J., Blomberg, N., Burdett, T., Conte, N., Dumontier, M., Fellows, D. K., … Parkinson, H. (2015). 10 Simple rules for design, provision, and reuse of persistent identifiers for life science data. <a href="http://doi.org/10.5281/zenodo.18003">doi:10.5281/zenodo.18003</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Introducing the Scholarly Markdown Bundle]]></title>
            <link>https://blog.martinfenner.org/posts/introducing-the-scholarly-markdown-bundle</link>
            <guid>c53ee3a5-a102-4482-be12-09f53966469a</guid>
            <pubDate>Thu, 23 Apr 2015 11:48:00 GMT</pubDate>
            <description><![CDATA[Using Markdown to author scholarly documents is an attractive alternative to the
standard authoring tools Microsoft Word and LaTeX. The feeling shared by many is
that Scholarly Markdown
[http://blog.martinfenner.org/2013/06/17/what-is-scholarly-markdown/] is 80%
there, and that more effort is needed for the remaining 20% - moving markdown
from a niche into the mainstream. What is mainly needed is building tools that
connect the existing tools and ideas, resulting in one or more services
attracti]]></description>
            <content:encoded><![CDATA[<p>Using Markdown to author scholarly documents is an attractive alternative to the standard authoring tools Microsoft Word and LaTeX. The feeling shared by many is that <a href="http://blog.martinfenner.org/2013/06/17/what-is-scholarly-markdown/">Scholarly Markdown</a> is 80% there, and that more effort is needed for the remaining 20% - moving markdown from a niche into the mainstream. What is mainly needed is building tools that connect the existing tools and ideas, resulting in one or more services attractive to a critical number of users. But maybe we also need to rethink the essential parts of Scholarly Markdown. In this post I propose that we expand the concept and define the <em>Scholarly Markdown Bundle</em>.</p><p>It is becoming increasingly clear that scholarly work can’t be adaequately described in a single text document, most commonly the journal article. Not only are there associated metadata, assets such as figures and supplementary information, but also the research data and software needed to produce the work described in the publication. The obvious next step is to think of scholarly work as a collection of objects, most clearly described by Carol Goble and others as <a href="https://researchobject.github.io/specifications/bundle/">Research Object Bundle</a>.</p><p>There will probably never be a single authoring tool and format that pleases everyone. Markdown has particular inherent strengths and weaknesses, complex math or tables will probably always be easier with other formats. The strength of markdown is the simplicity of the format. Some things are hard or impossible to do, but many other things are much simpler. Creating a useful markdown editor is much easier than a word processor reading/writing <code>docx</code> format. Markdown is also a perfect format to <a href="http://blog.martinfenner.org/2014/08/25/using-microsoft-word-with-git/">work with</a> version control systems such as git.</p><p>This low barrier of entry makes markdown perfect to be integrated into many workflows. And we can go one step further than ePub and Research Object Bundle, which use the related Universal Container Format (<a href="https://wikidocs.adobe.com/wiki/display/PDFNAV/Universal+Container+Format">UCF</a>) and ePub Open Container Format (<a href="http://www.idpf.org/epub/301/spec/epub-ocf.html">OCF</a>), respectively. Instead of using zip to compress a folder into a single file we can use git version control instead: git provides the commands <code>git bundle</code> and <code>git archive</code> to compress a project under version control with or without version history. I feel this format is both more powerful So I propose the <em>Scholarly Markdown Bundle</em>:</p><ul><li>a git repository with one or more markdown files, either as a folder, or compressed into a single file using <code>git bundle</code></li><li>a particular flavor or markdown called Scholarly Markdown, and discussed here and elsewhere before</li><li>a <code>citeproc.json</code> file in the root of the project that contains all metadata relevant to the container, including references</li></ul><p>The <code>citeproc.json</code> file is similar to the minimal metadata schema <a href="https://github.com/mbjones/codemeta">codemeta</a> proposed by Matt Jones and others, but is in the format used by Pandoc today. This is <a href="http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/">important</a> because it adds citation parsing support out of the box. The last two points rely on the <a href="http://pandoc.org/">Pandoc</a> document conversion tool, so Scholarly Markdown bundles are really <strong><strong>markdown</strong></strong> + <strong><strong>Pandoc</strong></strong> + <strong><strong>Citeproc/CSL</strong></strong> + <strong><strong>git</strong></strong>. The format is flexible enough to not only describe scholarly articles, but also other kinds of scholarly works, including scientific software managed with git version control. And it integrates nicely with a number of existing workflows, e.g. an R project using RStudio for both code and text (in Rmarkdown). This format should also work for blogs like this one, but I would have to separate the blog posts from the Jekyll site generator code, a direction I suggested in the <a href="http://blog.martinfenner.org/2015/03/23/blogging-beyond-jekyll/">last</a> post.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Blogging Beyond Jekyll]]></title>
            <link>https://blog.martinfenner.org/posts/blogging-beyond-jekyll</link>
            <guid>4bdd1c2c-0b6f-4035-88a2-5c51e9a32055</guid>
            <pubDate>Mon, 23 Mar 2015 11:50:00 GMT</pubDate>
            <description><![CDATA[This blog has been on four different platforms since starting in 2007: a custom
blogging engine and then Movable Type [https://movabletype.org/] on Nature
Network [http://network.nature.com/] 2007-2010, Wordpress on the PLOS Blogs
Network [http://blogs.plos.org/mfenner/] 2010-2013, and the static blogging
engine Jekyll [http://jekyllrb.com/] hosted on Github Pages since 2013. It might
be time for yet another blogging platform change.

The main reason to switch from Wordpress to Jekyll was the co]]></description>
            <content:encoded><![CDATA[<p>This blog has been on four different platforms since starting in 2007: a custom blogging engine and then <a href="https://movabletype.org/">Movable Type</a> on <a href="http://network.nature.com/">Nature Network</a> 2007-2010, Wordpress on the <a href="http://blogs.plos.org/mfenner/">PLOS Blogs Network</a> 2010-2013, and the static blogging engine <a href="http://jekyllrb.com/">Jekyll</a> hosted on Github Pages since 2013. It might be time for yet another blogging platform change.</p><p>The main reason to switch from Wordpress to Jekyll was the concept of a static site generator: write posts in <a href="http://commonmark.org/">markdown format</a>, store them in a Github repository, and then have Jekyll automatically generate the HTML pages hosted on <a href="https://pages.github.com/">Github Pages</a>. The main attraction was the blog posts in markdown format stored in git version control without the need of a database. Jekyll is the glue to make all this work, and I was able to customize Jekyll to my needs, e.g. by using <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a> for the markdown to html conversion.</p><p>While this workflow still makes sense for this blog, there are a number of shortcomings:</p><ul><li>Jekyll needs to rebuild the entire site every time I publish a new post. While this isn’t much of a problem for the size of this blog, it doesn’t scale well for larger sites. And the process is more complex if you use custom jekyll plugins like this blog, as you can’t use the automatic Jekyll pipeline provided by Github (hint: use a Travis continous integration server <a href="http://blog.martinfenner.org/2014/03/10/continuous-publishing/">to build the site</a>)</li><li>the web is moving to increasingly sophisticated javascript frontends, using frameworks such as <a href="https://angularjs.org/">Angular.js</a>, <a href="http://emberjs.com/">Ember.js</a>, or frontend libraries for scholarly documents such as <a href="http://elifesciences.org/elife-news/lens">Lens</a>. While they can be used together with Jekyll, that is not a typical use case.</li><li>the tight integration between the code to generate the website and the content (Wordpress and other blogging engines have the same approach) is not always the best solution, e.g. when you want to want to generate the pages for something that is not a blog (e.g. a <a href="http://book.openingscience.org/">book</a>).</li></ul><p>What could we do instead?</p><blockquote>Build a Javascript frontend where the content is served via an API built around markdown documents, stored in git version control.</blockquote><h3 id="api">API</h3><p>The blog posts are still written in markdown, stored (and version-controlled in a Github repository), but we would now access the content via API. The easiest solution is to use the <a href="https://developer.github.com/v3/repos/contents/">Github Contents API</a> and either do the markdown to html conversion in javascript yourself, or let the Github API do the conversion to HTML for you. Alternatively we could build our own API, e.g. because we want to control the markdown to html conversion, or need additional functionality such as fulltext search. And of course the two approaches can be combined, e.g. via a Github webhook that triggers the markdown to html coversion every time a document is added or updated, and stores the converted documents in the same repo.</p><h3 id="frontend">Frontend</h3><p>The frontend should be written as a one-page javascript application, not requiring a server backend. In contrast to the Jekyll workflow the frontend code doesn’t need to be updated every time we post a blog post. Since this is a very common scenario, there are probably several solutions out there already. Please mention them in the comments if you have suggestions. One candidate is <a href="https://github.com/elifesciences/lens/">Lens</a> mentioned above - a beautiful frontend for scholarly documents. Lens displays documents in the <a href="http://jats.nlm.nih.gov/publishing/tag-library/1.0/index.html">JATS</a> XML format, so your API would have to provide that format.</p><h3 id="conclusions">Conclusions</h3><p>The separation into API and frontend is of course old news. But for blogs this seems to still be a fairly new concept, in particular when combined with a backend using documents stored in git version control rather than in a database. Wordpress added a <a href="https://wordpress.org/plugins/json-rest-api/">REST API Plugin</a> in 2014, and the Ghost blogging framework (which uses a database backend) also seems to <a href="https://trello.com/b/EceUgtCL/ghost-roadmap">go into that general direction</a>. Please ping me if you like the idea and want to contribute, or have implemented something like this already.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Metadata in Microsoft Word documents]]></title>
            <link>https://blog.martinfenner.org/posts/metadata-in-microsoft-word-documents</link>
            <guid>5398a86a-fd4c-439d-9773-34527b79ac7d</guid>
            <pubDate>Fri, 20 Mar 2015 11:52:00 GMT</pubDate>
            <description><![CDATA[Metadata such as author, title, journal or persistent identifier are essential
for scholarly documents, and some of us are spending a significant part of our
time adding or fixing metadata. Unfortunately we sometimes don’t pay enough
attention to the flow of metadata, i.e. we ignore already existing metadata, or
reinvent the wheel in how we describe or store them.

Storing metadata in text-based formats is usually straightforward. This blog
post is written in markdown with a YAML header [http://]]></description>
            <content:encoded><![CDATA[<p>Metadata such as author, title, journal or persistent identifier are essential for scholarly documents, and some of us are spending a significant part of our time adding or fixing metadata. Unfortunately we sometimes don’t pay enough attention to the flow of metadata, i.e. we ignore already existing metadata, or reinvent the wheel in how we describe or store them.</p><p>Storing metadata in text-based formats is usually straightforward. This blog post is written in markdown with a <a href="http://yaml.org/">YAML header</a> - think of YAML as the more human-readable version of JSON - at the beginning of the document:</p><pre><code>---
title: Metadata in Microsoft Word documents
---</code></pre><p>This is then translated into this HTML when the blog post is published:</p><pre><code>&lt;meta property="dc:title" content="Metadata in Microsoft Word documents" /&gt;</code></pre><p>XML is of course a very natural format for metadata, here for example <a href="http://jats.nlm.nih.gov/publishing/tag-library/1.0/index.html">JATS</a> used for scholarly articles:</p><pre><code>&lt;article-title&gt;Metadata in Microsoft Word documents&lt;/article-title&gt;</code></pre><p>Many scholarly documents start out as Microsoft Word documents. And while the <code>docx</code> format introduced by Microsoft in Microsoft Office 2007 <a href="http://officeopenxml.com/">is XML-based</a>, few users are aware of this fact. And probably even fewer users (including myself) ever go to the <code>Properties…</code> settings of a <code>docx</code> document and add a <code>title</code>, <code>keywords</code> or other metadata (the <code>author</code> is usually set automatically).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/IC164149.gif" class="kg-image" alt><figcaption>Microsoft Word 2007 Properties. Image from <a href="https://msdn.microsoft.com/en-us/library/bb308936(v=office.12).aspx">Microsoft Developer Network</a></figcaption></figure><p>This is very unfortunate, as these metadata are very often required, e.g. in a journal article submission, and then need to be collected again, usually either by asking the author to fill out a web form, and/or by extracting the metadata (e.g. title) from the document.</p><p>The best place for metadata is with the document (not <em>in</em> the document), and if the file format (<code>docx</code> in this case) supports it, we should take advantage of this. The main benefit: metadata stay with the text when the document is sent to co-authors via email, or put on a file server, or into Dropbox.</p><p>In the case of <code>docx</code>, the metadata support is actually pretty good, using the standard <a href="http://dublincore.org/">Dublin Core</a>, and storing the metadata in a separate file called <code>core.xml</code>. You can see this file if you unzip your <code>docx</code> file (e.g. after giving it a <code>zip</code> extension). The <code>core.xml</code> file for this blog post (after converting the markdown file to <code>docx</code> using <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a>) looks like this:</p><pre><code>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;cp:coreProperties xmlns:cp="http://schemas.openxmlformats.org/package/2006/metadata/core-properties" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcterms="http://purl.org/dc/terms/" xmlns:dcmitype="http://purl.org/dc/dcmitype/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"&gt;&lt;dc:title&gt;Metadata in Microsoft Word documents&lt;/dc:title&gt;&lt;dc:creator&gt;&lt;/dc:creator&gt;&lt;/cp:coreProperties&gt;</code></pre><p>Because <code>docx</code> is XML, we can read/write this file not only in Microsoft Word, e.g. using macros, but also outside of Microsoft Word, e.g. in workflows that converts <code>docx</code> documents into other formats, or tools that check <code>docx</code> files for required metadata (e.g. by using <a href="https://martinfenner.ghost.io/2015/03/20/metadata-in-microsoft-word-documents/2014/08/18/introducing-rakali/">rakali</a> that I wrote last year). So please encourage authors to use the Microsoft Word <code>Properties…</code> settings, and update existing tools to take advantage of the Dublin Core metadata stored in every <code>docx</code> file.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[First analysis of software metrics]]></title>
            <link>https://blog.martinfenner.org/posts/first-analysis-of-software-metrics</link>
            <guid>f66851ce-9224-4c1b-b880-4fa675d01b05</guid>
            <pubDate>Sat, 28 Feb 2015 11:55:00 GMT</pubDate>
            <description><![CDATA[Last week I wrote about [/2015/02/19/metrics-for-scientific-software/] 
software.lagotto.io [http://software.lagotto.io/], an instance of the lagotto
[https://github.com/articlemetrics/lagotto] open source software collecting
metrics for the about 1,400 software repositories included in Sciencetoolbox
[http://sciencetoolbox.org/]. In this post I want to report the first results
analyzing the data.

If you want to follow along, please go to 
https://github.com/mfenner/software-analysis, this repo]]></description>
            <content:encoded><![CDATA[<p>Last week <a href="https://martinfenner.ghost.io/2015/02/19/metrics-for-scientific-software/">I wrote about</a> <a href="http://software.lagotto.io/">software.lagotto.io</a>, an instance of the <a href="https://github.com/articlemetrics/lagotto">lagotto</a> open source software collecting metrics for the about 1,400 software repositories included in <a href="http://sciencetoolbox.org/">Sciencetoolbox</a>. In this post I want to report the first results analyzing the data.</p><p>If you want to follow along, please go to <a href="https://github.com/mfenner/software-analysis">https://github.com/mfenner/software-analysis</a>, this repository holds all the data, as well as the R code used for analysis. A special thanks goes to <a href="http://scottchamberlain.info/">Scott Chamberlain</a> who greatly helped me by tweaking the <a href="https://github.com/ropensci/alm">alm</a> R package to support URLs instead of DOIs as identifiers.</p><p>The first step in the analysis is to get an overview of the external sources citing or discussing the software package:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/software.lagotto.io_2.png" class="kg-image" alt srcset="https://martinfenner.ghost.io/content/images/size/w600/2021/01/software.lagotto.io_2.png 600w, https://martinfenner.ghost.io/content/images/size/w1000/2021/01/software.lagotto.io_2.png 1000w, https://martinfenner.ghost.io/content/images/2021/01/software.lagotto.io_2.png 1444w" sizes="(min-width: 720px) 720px"><figcaption>Number of software repositories (out of 1,404) with at least one event. Data from <a href="http://software.lagotto.io">software.lagotto.io</a></figcaption></figure><p>This is basically the same figure as in the <a href="https://martinfenner.ghost.io/2015/02/19/metrics-for-scientific-software/">previous post</a>, but with two differences: I have added a <a href="http://www.nature.com/opensearch/">Nature.com OpenSearch</a> data source, and I have found an additional 64 repositories cited in scholarly articles via an Europe PMC fulltext Search that also includes the reference lists (thanks to <a href="http://www.ebi.ac.uk/about/people/johanna-mcentyre">Jo McEntyre</a>).</p><p>I am not sure why we are not picking up any Wikipedia citations, and have to take a closer look. The ORCID source also needs tweaking, and there are some issues with the <a href="http://wordpress.com/" rel="nofollow">Wordpress.com</a> data that I have to look into as well. Citations in the scholarly literature are obviously the most interesting data, and we have three Github repos with more than 25 citations, including <a href="https://github.com/najoshi/sickle">https://github.com/najoshi/sickle</a> with 54 citations. As most repositories in our sample are cited only once if at all, a correlation with Github stars and forks is not useful. Sickle is popular on Github (52 stars and 32 forks), but it is not clear that this activity is correlated to citations (e.g. because there are more citations than stars).</p><p>The vast majority of software repos in this analysis are hosted by Github, so we have the numbers of stars and forks for those. It is interesting, although probably not very surprising, that the number of Github stargazers and forks is highly correlated:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/github_likes_readers-1.png" class="kg-image" alt srcset="https://martinfenner.ghost.io/content/images/size/w600/2021/01/github_likes_readers-1.png 600w, https://martinfenner.ghost.io/content/images/2021/01/github_likes_readers-1.png 672w"><figcaption>Correlation between Github stargazers and forks, log-log scale. Data from <a href="http://software.lagotto.io">software.lagotto.io</a></figcaption></figure><p>We can find Facebook activity (likes, comments or shares) for one third of the repositories. There is a reasonably good correlation between Facebook activity and number of Github forks:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/facebook_github_readers-1.png" class="kg-image" alt srcset="https://martinfenner.ghost.io/content/images/size/w600/2021/01/facebook_github_readers-1.png 600w, https://martinfenner.ghost.io/content/images/2021/01/facebook_github_readers-1.png 672w"><figcaption>Correlation between combined Facebook activity and Github forks, log-log scale. Data from <a href="http://software.lagotto.io">software.lagotto.io</a></figcaption></figure><p>One interesting analysis would be to look at the repositories that have been forked much more often relative to their Facebook activity, e.g. <a href="https://github.com/cloudera/impala">Impala</a> with 1,207 Github stars and 458 forks, but only 5 Facebook shares. One limitation of the analysis is that we are not tracking Facebook (or other social media) activity for all forks of a repo.</p><p>We found Reddit discussions mentioning one of the repositories in 7% of cases. Once we have a larger sample size it would be interesting to correlate this activity with Github stars and forks, similar to what we did for Facebook. By far the most popular repository from our sample on Reddit is <a href="https://github.com/Bitcoin/Bitcoin">Bitcoin</a>, followed by <a href="https://github.com/jquery/jquery">JQuery</a>. Twitter activity is notoriously difficult to collect since Twitter doesn’t keep tweets very long, hence probably the low numbers compared to Facebook and Reddit.</p><p>Feel free to play with the data and scripts provided at <a href="https://github.com/mfenner/software-analysis">https://github.com/mfenner/software-analysis</a>, my next step is probably to include a much larger number of software repositories.</p><p>It has not escaped our notice that the kind of analysis described above could be applied to any software repository, not just scientific software.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Why there is no iTunes for science papers]]></title>
            <link>https://blog.martinfenner.org/posts/why-there-is-no-itunes-for-science-papers</link>
            <guid>b898d6d8-3030-455c-b08a-108215ba2b97</guid>
            <pubDate>Mon, 23 Feb 2015 11:58:00 GMT</pubDate>
            <description><![CDATA[The iTunes Store was opened by Apple in 2003 to sell digital music and other
digital assets. Since 2009 music purchased in the iTunes store is free of
Digital Rights Management (DRM). Apple became the largest music vendor worldwide
in 2010, and by 2013 had sold 25 billion songs.

Scholarly articles are distributed almost exclusively in digital form. While
there is an increasing number of journal articles freely available via green or
gold open access, the majority of them still can only be read ]]></description>
            <content:encoded><![CDATA[<p>The iTunes Store was opened by Apple in 2003 to sell digital music and other digital assets. Since 2009 music purchased in the iTunes store is free of Digital Rights Management (DRM). Apple became the largest music vendor worldwide in 2010, and by 2013 had sold 25 billion songs.</p><p>Scholarly articles are distributed almost exclusively in digital form. While there is an increasing number of journal articles freely available via green or gold open access, the majority of them still can only be read if the reader works at an institution with a subscription to the journal. Many journals also allow the reader to buy a single article of interest, for prices between $10 and more than $30:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/pay_per_view_nature-1.png" class="kg-image" alt srcset="https://martinfenner.ghost.io/content/images/size/w600/2021/01/pay_per_view_nature-1.png 600w, https://martinfenner.ghost.io/content/images/size/w1000/2021/01/pay_per_view_nature-1.png 1000w, https://martinfenner.ghost.io/content/images/2021/01/pay_per_view_nature-1.png 1150w" sizes="(min-width: 720px) 720px"><figcaption>For an article in Nature</figcaption></figure><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/pay_per_view_lancet-1.png" class="kg-image" alt srcset="https://martinfenner.ghost.io/content/images/size/w600/2021/01/pay_per_view_lancet-1.png 600w, https://martinfenner.ghost.io/content/images/2021/01/pay_per_view_lancet-1.png 606w"><figcaption>For an article in Lancet</figcaption></figure><p>There is also a document delivery service provided by libraries, but that option varies considerably by country and in Germany for example means a scanned article as printout rather than the original PDF because of a change in German copyright law a few years ago. There are also the services <a href="https://www.deepdyve.com/">DeepDyve</a> and <a href="https://www.readcube.com/">ReadCube</a>, but again you don’t get the PDF (or only for prices similar to those quoted above), but rather limited access for reading and printing.</p><p>In summary, affordable access to scholarly content by subscription publishers is in a dire state: you either have to work at an academic institution subscribing to the desired journal, get only a crippled version of the article (online viewing only), or pay up to $30 for a single article, which clearly doesn’t scale beyond very occasional use.</p><p>With this background it is obvious that several people have discussed the iTunes Store-like model to sell scholarly articles:</p><ul><li><a href="http://crosstech.crossref.org/2009/09/prc_report_and_ipub_revisited.html">PRC Report and “iPub” revisited</a></li><li><a href="http://www.popsci.com/science/article/2009-10/deepdyve-launches-itunes-science-papers">DeepDyve launches iTunes Store-like service for science papers</a></li><li><a href="http://scienceblogs.com/digitalbio/2012/01/10/could-an-itunes-like-model-wor/">Could an iTunes-like model work with scientific publishing?</a></li><li><a href="http://www.bostonglobe.com/business/2012/10/07/start-readcube-program-uses-itunes-payment-model-for-access-scientific-articles/1UopCX1qfEE3uO2UEzuM7L/story.html">A plan to open up science journals</a></li><li><a href="http://www.newyorker.com/tech/elements/when-the-rebel-alliance-sells-out">When the Rebel Alliance Sells Out</a></li></ul><p>The best already existing platforms to build such as service are reference managers, as most of them have learned now to manage PDF files, and have an online component. ReadCube is offering a pay-per-view option already, Papers, Mendeley, Endnote or others could get into this business.</p><p>One of the big advantages of payments for single articles is transparency, as institutions and users only pay for what they actually use. Price transparency is one of the big problems with the <em>big deal</em> contracts that academic institutions have with publishers - read <a href="http://dx.doi.org/10.1073/pnas.1403006111">this article</a> for more info.</p><p>But rather than becoming the predominant way to pay for digital music, services such as DeepDyve and ReadCube are only playing a marginal role. Why is that so?</p><ul><li>whereas digital music is paid for by the consumer, there is usually a middleman in the form of the library for scholarly articles, which makes the payment process more complex.</li><li>subscription publishers have focused all their efforts on selling big deals with increasing numbers of journals to libraries. Prices of $30 per article are clearly intended to discourage payment for single articles (which could jeopardize journal bundles) rather than offering an earnest payment option.</li><li>Apple was in a strong negotiation position with record labels when starting the iTunes store (the extremely popular iPod, record labels scared of file-sharing platforms such as Napster). No organization is in a similar position with scientific publishers, and services such as ReadCube or Mendeley are handicapped because they are associated with a particular publisher</li></ul><p>Unless several large publishers and/or a smart third-party with enough muscle start an initiative in this space, e.g. by bringing the pay-per-view prices to a reasonable level (e.g. $4.99), we will never see an iTunes Store-like service for scholarly articles, and this currently looks like the most likely outcome. We may have reached the point where it is too late, as most publishers seem to already work towards another payment model: gold open access where the authors pay the article costs.</p><p><em>Update 3/2/15: added link to 2009 CrossTech blog post.</em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Metrics for scientific software]]></title>
            <link>https://blog.martinfenner.org/posts/metrics-for-scientific-software</link>
            <guid>31c00475-7abf-4bc8-830c-4b37bafa0359</guid>
            <pubDate>Thu, 19 Feb 2015 12:00:00 GMT</pubDate>
            <description><![CDATA[One of the challenges of collecting metrics for scholarly outputs is persistent
identifiers. For journal articles the Digital Object Identifier (DOI) has become
the de-facto standard, other popular identifiers are the pmid from PubMed, the
identifiers used by Scopus and Web of Science, and the arxiv ID for ArXiV
preprints.

For other research outputs the picture is less clear. DOIs are also used for
datasets, but so are many other identifiers, in particular in the life sciences.

To collect metr]]></description>
            <content:encoded><![CDATA[<p>One of the challenges of collecting metrics for scholarly outputs is persistent identifiers. For journal articles the Digital Object Identifier (DOI) has become the de-facto standard, other popular identifiers are the pmid from PubMed, the identifiers used by Scopus and Web of Science, and the arxiv ID for ArXiV preprints.</p><p>For other research outputs the picture is less clear. DOIs are also used for datasets, but so are many other identifiers, in particular in the life sciences.</p><p>To collect metrics for research outputs, the requirements are slightly different. We need identifiers understood by the services collecting the metrics, not by the data repository or other service that is holding the research output (the only exception is usage stats, which are generated locally). For many services, in particular social media such as Facebook, Twitter or Reddit, the primary identifier for a resource is a URL. This means that we should have one or more URLs for every research output where we want to track the metrics - typically the publisher or data repository landing page. Since URLs can be messy, Google, Facebook and others have come up with the concept of a <a href="http://googlewebmastercentral.blogspot.de/2009/02/specify-your-canonical.html">canonical URL</a>, and some care should go into constructing proper canonical URLs (see <a href="http://blog.martinfenner.org/2013/10/13/broken-dois/">this blog post</a> for examples of what can go wrong).</p><p>The <a href="http://www.knowledge-exchange.info/Default.aspx?ID=462">Den Haag Manifesto</a> is the result of a <strong><strong>Knowledge Exchange</strong></strong> workshop held in June 2011 and tries to bring Persistent Identifiers and Linked Open Data together. The first principle is very much in line with what I said above:</p><blockquote>Make sure PIDs can be referred to as HTTP URI’s, including support for content negotiation.</blockquote><p>Or, to put this differently: URLs are good enough to start collecting metrics for scholarly outputs. Scientific software is a good example where persistent identifiers are not commonly used (despite efforts such as <a href="https://guides.github.com/activities/citable-code/">this one</a>), but we can still collect many meaningful metrics using the repository URL (and the open source software <a href="https://github.com/articlemetrics/lagotto">lagotto</a>):</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/software.lagotto.io.png" class="kg-image" alt="Number of software repositories (out of 1,404) with at least one event. Data from software.lagotto.io"><figcaption>Number of software repositories (out of 1,404) with at least one event. Data from <a href="http://software.lagotto.io/" style="box-sizing: border-box; background: transparent; color: rgb(52, 152, 219); text-decoration: none;">software.lagotto.io</a></figcaption></figure><p>The last three rows are citations in the scholarly literature found via fulltext search of BioMed Central, Europe PMC and PLOS. URLs (in contrast to persistent identifiers represented as strings and/or numbers) are easy to find, the main limitation is not so much using a URL rather than a DOI, but that scientific software typically is mentioned in the text without appearing in the reference list. This makes it hard to impossible to find articles mentioning the software that are not open access, which unfortunately is still the majority of them.</p><p>We are of course also tracking the discussion of the software in social media, and are collecting the number of stars and forks in Github and Bitbucket. Overall there is quite a lot of activity, here are some examples:</p><ul><li><a href="http://software.lagotto.io/works/url/https://github.com/najoshi/sickle">Windowed Adaptive Trimming for fastq files using quality</a></li><li><a href="https://github.com/lh3/wgsim">Reads simulator</a></li><li><a href="http://software.lagotto.io/works/url/https://github.com/lh3/seqtk">Toolkit for processing sequences in FASTA/Q formats</a></li></ul><p>All three software repos have been cited in the scholarly literature at least ten times. What is missing is infrastructure that tracks the citations of scientific software, so that we can give proper scientific credit to the authors of the software, and can discover other research projects using the same tools. <a href="http://software.lagotto.io/">software.lagotto.io</a> uses a list of software repos collected by Jure Triglav for <a href="http://sciencetoolbox.org/">ScienceToolbox</a>, and a scientific software index is indeed one of the important missing pieces.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Manifests and Reference Lists]]></title>
            <link>https://blog.martinfenner.org/posts/manifests-and-reference-lists</link>
            <guid>c19c5ec6-6e50-47fc-b51a-c5c2a97a9860</guid>
            <pubDate>Thu, 05 Feb 2015 12:03:00 GMT</pubDate>
            <description><![CDATA[Last month at the Force15 conference
[https://www.force11.org/meetings/force2015/pre-conference-meeting-list] in
Oxford Ian Mulvany [https://twitter.com/IanMulvany] and I ran a workshop on 
data
citation support in reference managers
[/2015/01/05/data-citation-support-in-reference-managers/]. The report of that
workshop isn’t done yet, but I can say that it was a success - we now have a
pretty good idea what the problems are and what needs to be done to fix them.
The short summary of the worksho]]></description>
            <content:encoded><![CDATA[<p>Last month at the <a href="https://www.force11.org/meetings/force2015/pre-conference-meeting-list">Force15 conference</a> in Oxford <a href="https://twitter.com/IanMulvany">Ian Mulvany</a> and I ran a workshop on <a href="https://martinfenner.ghost.io/2015/01/05/data-citation-support-in-reference-managers/">data citation support in reference managers</a>. The report of that workshop isn’t done yet, but I can say that it was a success - we now have a pretty good idea what the problems are and what needs to be done to fix them. The short summary of the workshop is in <a href="https://speakerdeck.com/mfenner/workshop-summary-reference-managers-and-data-citation">this</a> slidedeck of the presentation that summarized the workshop for the other Force15 attendees.</p><p>The whole idea of the workshop was to treat data citation as similar as possible to the citation of journal articles, i.e. to allow authors to use the same tools (reference managers) and conventions (citation styles). Putting a data citation into a reference list makes it easier to find that data citation because reference lists contain more metadata, are more structured, and more accessible than data citations in the form of identifiers or links within the body text of the article.</p><p>But I have to admit that there is one problem with reference lists: although there is always some self-citation, reference lists usually contain references to articles (and other resources) created by other people and before the article was published. It feels a little bit odd to put a dataset created by the same group of people and published at the same time into the reference list. And although we could use a separate reference list or highlight the data associated with the article in some other way, what we really want is something slightly different, a manifest file.</p><p>The journal article has been a (mainly) textual document for many centuries not because this is the essence of science communication, but rather because there was no practical way to include all the other information (raw data, tools used for experiments, etc.). Very few of these limitations remain with the digital journal article that we have since the 1990s, but we have for the most part failed to change the format other than going from paper to PDF. One of many examples: figures in publications typically still are has limited as they were decades ago with no way to see the data underlying the figure, options for selecting what data points are shown, or animation for time-based information.</p><p>So what we really care about is the sum of artifacts and resources that together make what Carol Goble and others call research object (Bechhofer et al., 2010), the journal article is an important part, but clearly doesn’t include everthing that is needed to understand and reproduce the work. Reference lists can help with linking to some of the resources not included in the article text, but they typically don’t link to supplementary information or other places where the underlying data are made available, or to the figures of the article. Although some publishers provide navigation tools for readers to get to this information, what we really need is a machine-readable list of all the resources used in an article.</p><p>As it happens, this is exactly what the ePub format for electronic books is doing, as every ePub must include a manifest file that lists all the files that are part of the publication, defined in the <a href="http://www.idpf.org/epub/20/spec/OPF_2.0.1_draft.htm">Open Packaging Format (OPF)</a>. I need to do more research to figure out how to do this with <a href="http://jats.nlm.nih.gov/archiving/tag-library/1.0/index.html">JATS</a>, the standard for scholarly articles, and how to generate something similar to the manifest file when using different formats, e.g. html or markdown. This has to be linked to some of the information we are collecting already, e.g. described in JATS (Beck, 2011), or the <code>relatedIdentifier</code> in the DataCite metadata (Starr, 2014).</p><h2 id="references">References</h2><p>Bechhofer, S., Bechhofer, S., De Roure, D., Gamble, M., Goble, C., &amp; Buchan, I. (2010). Research Objects: Towards Exchange and Reuse of Digital Knowledge. <em>Nature Precedings</em>, (713). <a href="https://doi.org/10.1038/npre.2010.4626.1">https://doi.org/10.1038/npre.2010.4626.1</a></p><p>Beck, J. (2011). NISO Z39.96 The Journal Article Tag Suite (JATS): What Happened to the NLM DTDs? <em>The journal of electronic publishing : JEP</em>, <em>14</em>(1). <a href="https://doi.org/10.3998/3336451.0014.106">https://doi.org/10.3998/3336451.0014.106</a></p><p>Starr, J. (2014). DataCite Metadata Schema for the Publication and Citation of Research Data, 1–38. <a href="https://doi.org/10.5438/0010">https://doi.org/10.5438/0010</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Data Citation Support in Reference Managers]]></title>
            <link>https://blog.martinfenner.org/posts/data-citation-support-in-reference-managers</link>
            <guid>b36a3870-6498-4514-9102-2ee4763ef739</guid>
            <pubDate>Mon, 05 Jan 2015 14:55:00 GMT</pubDate>
            <description><![CDATA[This is the title of an upcoming workshop next Sunday organized by Ian Mulvany
and myself. The workshop is a pre-conference event
[https://www.force11.org/meetings/force2015/pre-conference-meeting-list] of the 
Force15 [https://www.force11.org/meetings/force2015] conference in Oxford. This
blog post summarizes some of the issues and work that needs to be done.

Data Citation is one of the big themes of the Force15 conference, and a lot of
progress has been made, including the Joint Declaration o]]></description>
            <content:encoded><![CDATA[<p>This is the title of an upcoming workshop next Sunday organized by Ian Mulvany and myself. The workshop is a <a href="https://www.force11.org/meetings/force2015/pre-conference-meeting-list">pre-conference event</a> of the <a href="https://www.force11.org/meetings/force2015">Force15</a> conference in Oxford. This blog post summarizes some of the issues and work that needs to be done.</p><p>Data Citation is one of the big themes of the Force15 conference, and a lot of progress has been made, including the <a href="https://www.force11.org/datacitation">Joint Declaration of Data Citation Principles</a> that start with the following paragraph on <strong><strong>Importance</strong></strong>:</p><blockquote>Data should be considered legitimate, citable products of research. Data citations should be accorded the same importance in the scholarly record as citations of other research objects, such as publications.</blockquote><p>Convincing researchers, funders, university administrators and others that data citation is important is crucial. But for researchers to actually adopt data citation to the same degree as citations of the scholarly literature, more needs to be done:</p><ul><li>incentives (both carrots and sticks) by funders, institutions, and scholarly societies</li><li>training in data management</li><li>data repositories and other tools and services for the public sharing of data</li><li>tools and services that help citing those datasets</li></ul><p>The focus of the workshop is on the last bullet point, and I would argue that more work still needs to be done here compared to the first three bullet points.</p><h2 id="reference-managers">Reference Managers</h2><p>Researchers use reference managers to handle the citations in the manuscripts they write. This is both a common practice that everybody understands, and there are a plethora of tools - both free and paid - available. Most reference managers were originally built to handle citations of journal articles and maybe books or book chapters, and many of them also help with managing the associated PDF files. In the last 15 years we have seen an dramatic increase of non-article citations in reference lists, mainly to web resources (Klein et al., 2014):</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/journal.pone.0115253.g002.png" class="kg-image" alt="From Fig. 2: STM articles and URI references per publication year - Elsevier corpus (Klein et al., 2014)."><figcaption>From Fig. 2: STM articles and URI references per publication year - Elsevier corpus <span class="citation" data-cites="klein_2014bj" style="box-sizing: border-box;">(Klein et al., 2014)</span>.</figcaption></figure><p>References managers have started to adapt to these changes in citation patterns. Similarly they have become better in handling non-textual resources such as slide decks, datasets, or movies. Nobody should type in references by hand in 2015, as reference managers have come up with several ways of importing metadata about citations:</p><ul><li>import references stored in a file using a format such as BibTex or RIS</li><li>import references by talking to an external API</li><li>import references via a bookmarklet that grabs information from the current webpage in the browser</li></ul><p>Endnote and Papers typically use the second approach whereas Mendeley, Zotero (and others) work almost exclusively via bookmarklets (and there are of course combinations of both). Bookmarklets in general work better for web resources and other content that is not indexed in a central service such as Web of Science or Scopus. This is also true for research data, as there are currently few central research data indexing services - the Thomson Reuters <a href="http://wokinfo.com/products_tools/multidisciplinary/dci/">Data Citation Index</a> and <a href="https://www.datacite.org/">DataCite</a> are two examples in this category.. But there are also thousands of data repositories, many of them listed in re3data (Pampel et al., 2013).</p><p>The reference manager <a href="https://www.zotero.org/">Zotero</a> has built a large open source ecosystem around bookmarklets (what they call <a href="https://github.com/zotero/translators">web translators</a>), making it straightforward to add support for a new resource, as I have done for <a href="https://github.com/zotero/translators/blob/master/NCBI%20Nucleotide.js">GenBank nucleotide sequence datasets</a> in November after learning the basics in a <a href="http://blog.martinfenner.org/2014/10/17/webinar-on-writing-zotero-translators/">webinar</a> given by Sebastian Karcher, a frequent contributor to Zotero web translators.</p><p>There is no technical reason that reference managers can’t support a broad range of objects to cite, including datasets. And integration of data citation into the reference manager workflow is not only the easiest and most natural way for the author of a paper, but also makes it easier to discover these citations - reference lists are simply much better for that than links in the text, in particular if the content is behind subscription walls. There is a long tradition in the life sciences to put identifiers for genetic sequences used in a publication right into the text (usually into the methods section). Links in the body text are worse than references in reference lists, <a href="http://blog.martinfenner.org/2013/07/02/auto-generating-links-to-data-and-resources/">identifiers without a link</a> are even worse, as they are very hard to find in an automated way (Kafkas, Kim, &amp; McEntyre, 2013).</p><p>Please come to our workshop on Sunday afternoon if you are in Oxford and are interested in this topic. <a href="https://www.eventbrite.com/e/data-citation-support-in-reference-managers-tickets-15136593960">Registration</a> is free, and the workshop will include both presentations about the current state of data citation support in the reference managers Endnote, Papers, Mendeley and Zotero, and work in smaller groups on practical implementations.</p><h2 id="references">References</h2><p>Kafkas, Ş., Kim, J.-H., &amp; McEntyre, J. R. (2013). Database Citation in Full Text Biomedical Articles. <em>PLoS ONE</em>. <a href="http://doi.org/10.1371/journal.pone.0063184">doi:10.1371/journal.pone.0063184</a></p><p>Klein, M., Van de Sompel, H., Sanderson, R., Shankar, H., Balakireva, L., Zhou, K., &amp; Tobin, R. (2014). Scholarly context not found: one in five articles suffers from reference rot. <em>PLoS ONE</em>, <em>9</em>(12), e115253. <a href="http://doi.org/10.1371/journal.pone.0115253">doi:10.1371/journal.pone.0115253</a></p><p>Pampel, H., Vierkant, P., Scholze, F., Bertelmann, R., Kindling, M., Klump, J., … Dierolf, U. (2013). Making Research Data Repositories Visible: The re3data.org Registry. <em>PLoS ONE</em>, <em>8</em>(11), e78080. <a href="http://doi.org/10.1371/journal.pone.0078080">doi:10.1371/journal.pone.0078080</a><br></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Webinar on Writing Zotero Translators]]></title>
            <link>https://blog.martinfenner.org/posts/webinar-on-writing-zotero-translators</link>
            <guid>a7068104-63a6-4807-b229-9bf24eac6693</guid>
            <pubDate>Fri, 17 Oct 2014 14:59:00 GMT</pubDate>
            <description><![CDATA[In a blog post two weeks ago
[http://blog.martinfenner.org/2014/10/01/please-keep-it-simple-citations-links-and-references/] 
I argued for the need for reference managers to properly support data citation,
if we want data citation to become a standard activity. I am happy to announce
two events working towards that goal.

November 3rd: Webinar on writing Zotero web translators
Sebastian Karcher
[https://www.zotero.org/blog/community-spotlight-sebastian-karcher/], one of the
most prolific authors]]></description>
            <content:encoded><![CDATA[<p>In a <a href="http://blog.martinfenner.org/2014/10/01/please-keep-it-simple-citations-links-and-references/">blog post two weeks ago</a> I argued for the need for reference managers to properly support data citation, if we want data citation to become a standard activity. I am happy to announce two events working towards that goal.</p><h2 id="november-3rd-webinar-on-writing-zotero-web-translators">November 3rd: Webinar on writing Zotero web translators</h2><p><a href="https://www.zotero.org/blog/community-spotlight-sebastian-karcher/">Sebastian Karcher</a>, one of the most prolific authors of Zotero web translators (and citation styles), has kindly offered to hold an introductory webinar on writing Zotero web translators. These web translators allow Zotero to import metadata about a scholarly work from a variety of places, and new web translators for repositories that hold research data (or software) would go a long way towards making data citation easier for authors. <a href="https://www.zotero.org/support/dev/translators">Web translators</a> are written in Javascript and only basic Javascript knowledge is required. The free webinar takes place on November 3rd on 5 PM UK time (12 PM EST) and the registration form is <a href="http://www.eventbrite.com/e/writing-zotero-translators-webinar-tickets-13768797845">here</a>.</p><h2 id="january-11-force11-pre-conference-workshop-on-data-citation-support-in-reference-managers">January 11: Force11 Pre-Conference workshop on Data Citation Support in Reference Managers</h2><p><a href="https://www.force11.org/meetings/force2015/pre-conference-meeting-list">This workshop</a>, coorganized with Ian Mulvany, will extend the Zotero web translator work to other reference managers, including Papers and Mendeley. This will be a hackathon with the goal to get some things working in these reference managers, but it should also be interesting for others, as we will discuss what is missing to make data citation work in reference managers.</p><p>My personal goal is to learn to write a Zotero web translator in the webinar, and then write a working web translator for the three biological databases ENA, PDB and Uniprot before the January workshop. And hopefully these activities generate enough interest that other people write web translators for their favorite research data database or software repository, and that the proprietary reference managers Papers and Mendeley (and hopefully others) also add support for these data sources.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Let's do an unconference]]></title>
            <link>https://blog.martinfenner.org/posts/lets-do-an-unconference</link>
            <guid>03829097-0a63-4baf-b901-ab3d0b84b631</guid>
            <pubDate>Tue, 14 Oct 2014 15:02:00 GMT</pubDate>
            <description><![CDATA[This year’s SpotOn London conference
[http://blogs.nature.com/ofschemesandmemes/2014/10/09/how-to-get-a-ticket-for-this-years-spoton-london] 
takes place November 14-15 and the registration has opened this Monday. I am
helping organize this conference since 2009, and I again look forward to the
sessions, and - more importantly - the discussions with people in and between
sessions this year.

The name (ScienceBlogging London, ScienceOnline London, SpotOn London), the
location (Royal Institution, ]]></description>
            <content:encoded><![CDATA[<p>This year’s <a href="http://blogs.nature.com/ofschemesandmemes/2014/10/09/how-to-get-a-ticket-for-this-years-spoton-london">SpotOn London conference</a> takes place November 14-15 and the registration has opened this Monday. I am helping organize this conference since 2009, and I again look forward to the sessions, and - more importantly - the discussions with people in and between sessions this year.</p><p>The name (ScienceBlogging London, ScienceOnline London, SpotOn London), the location (Royal Institution, British Library, Wellcome Conference Center), the people organizing (too many to mention, but Nature Publishing Group always at the core), and the fringe events (lots of cool things from <a href="http://blog.mendeley.com/academic-life/science-blogging-2008-part-i/">science tours</a> to <a href="http://www.nature.com/spoton/event/spoton-london-2012-fringe-event-the-story-collider-2/">Story Collider</a>) and the format have always changed slightly over the years, and this year again is a bit different. The biggest change is obviously that <a href="https://twitter.com/louwoodley">Lou Woodley</a> is no longer an organizer (as she announced at last year’s conference), but this is also the first SpotOn conference with a theme:</p><blockquote>The challenges of balancing the public and the private in the digital age</blockquote><p>This is obviously a very broad topic, but nicely encompasses many important issues that we are dealing with in scholarly communication today. The draft program is posted <a href="http://blogs.nature.com/ofschemesandmemes/2014/10/13/spoton-london-2014-draft-programme">here</a>, and I’m helping organize the sessions on <strong><strong>sharing sensitive data</strong></strong> and <strong><strong>open peer review</strong></strong>. More details will follow for all these sessions.</p><p>The second day of the conference will be in unconference (or barcamp) format and the program drafted by the delegates in the morning. This format is popular in the science communications community (I first heard about the project that became my current job at <a href="http://blogs.plos.org/mfenner/2009/07/10/i_was_at_scibarcamp_palo_alto/">SciBarCamp in 2009</a>), and SpotOn London has used this format in the first conference in 2008 (and again in 2009):</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/2817131778_336979a571_z.jpg" class="kg-image" alt srcset="https://martinfenner.ghost.io/content/images/size/w600/2021/01/2817131778_336979a571_z.jpg 600w, https://martinfenner.ghost.io/content/images/2021/01/2817131778_336979a571_z.jpg 640w"><figcaption><a href="https://www.flickr.com/photos/dullhunk/2817131778/">Flickr photo by Duncan Hull</a></figcaption></figure><p>For people not familiar with this format the idea of a conference (day) without predetermined topics or speakers sounds scary. As it turns out, the problem is usually not the lack of ideas or people wanting to talk, but rather how to coordinate this in a way that everyone who wants to get involved can do so, and it doesn’t become a discussion among those with the loudest voices (and biggest egos). My experience with SpotOn London and other conferences I enjoyed is that the best sessions are usually those that allow for a good discussion, and not those with the most polished PowerPoint slides. Some suggestions for when you attend an unconference for the first time:</p><ul><li>go to sessions with topics you know little about, but want to learn more</li><li>when suggesting a session, do this together with others</li><li>suggest topics that are focussed and unusual, not the obvious ones we always talk about</li><li>don’t even think about doing a PowerPoint presentation</li><li>when moderating a session, be a good moderator, not a good speaker</li></ul><h2 id="further-reading">Further reading</h2><ul><li><a href="http://en.wikipedia.org/wiki/Science_Foo_Camp">Wikipedia: SciFoo</a></li><li><a href="http://blogs.nature.com/nascent/2007/08/barcamb_cambridge.html">Ian Mulvany: BarCamp Cambridge 2007</a></li><li><a href="http://science.easternblot.net/?p=613">Eva Amsen: SciBarCamp Toronto 2008</a></li><li><a href="http://blogs.plos.org/mfenner/2010/05/11/action_points/">Me: BibCamp Hannover 2010</a></li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Please keep it simple: citations, links and references]]></title>
            <link>https://blog.martinfenner.org/posts/please-keep-it-simple-citations-links-and-references</link>
            <guid>83c6aac6-270c-4c70-97bc-93f1969127c1</guid>
            <pubDate>Wed, 01 Oct 2014 15:06:00 GMT</pubDate>
            <description><![CDATA[In my last post [/2014/09/16/please-keep-it-simple/] I wrote about the
importance of keeping things simple in scholarly publishing, today I want to go
into more detail with one example: citations in scholarly documents.

LEGO scientists discuss how they can cite their dataCitations are an essential
part of scholarly documents, and they are summarized in the references section
at the end of the article or book chapter. The problem is that not everything
that is cited in a scholarly document ends ]]></description>
            <content:encoded><![CDATA[<p>In my <a href="https://martinfenner.ghost.io/2014/09/16/please-keep-it-simple/">last post</a> I wrote about the importance of keeping things simple in scholarly publishing, today I want to go into more detail with one example: citations in scholarly documents.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/lego_discussion.jpg" class="kg-image" alt srcset="https://martinfenner.ghost.io/content/images/size/w600/2021/01/lego_discussion.jpg 600w, https://martinfenner.ghost.io/content/images/2021/01/lego_discussion.jpg 700w"><figcaption>LEGO scientists discuss how they can cite their data</figcaption></figure><p>Citations are an essential part of scholarly documents, and they are summarized in the references section at the end of the article or book chapter. The problem is that not everything that is cited in a scholarly document ends up in the references list. Examples of this include:</p><ul><li>web links, e.g. to reagents or other resources</li><li>identifiers for biological databases such as GenBank that are typically included in the text as identifiers or as links</li><li>footnotes with links to external resources</li></ul><p>In other words: we are not consistent in how we cite other content. And this is a problem because we are making it more difficult than necessary for authors, publishers and everyone else to handle these various citation flavors and, more importantly, we are loosing citations along the way. This is a particular problem for data citation, as the seminal 2013 paper by Kafkas et al. (Kafkas, Kim, &amp; McEntyre, 2013) has shown for citations to the three biological databases ENA (European Nucleotide Archive), PDB and Uniprot:</p><ul><li>there is a large numbers of accession numbers in the Open Access subset of PubMed Central (e.g. 160,112 ENA accession numbers for papers published up until June 2012)</li><li>text mining using the <a href="http://www.ebi.ac.uk/webservices/whatizit/">Whatizit</a> tool can retrieve most of these identifiers</li><li>there is only partial overlap between database identifiers annotated by publishers and database identifiers found by text mining</li><li>the overlap is even smaller between papers citing database identifiers, and papers cited in biological databases such as ENA</li><li>the study was limited to Open Access journals, as only for them the fulltext articles could be text mined</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/ena_overlap.png" class="kg-image" alt><figcaption>Comparison between article-to-database and database to citations (Kafkas et al., 2013).</figcaption></figure><p>In other words, even though including identifiers for biological databases has been an accepted community standard that every author and publisher is following for a long time, the proper citation of these identifiers is still often broken. The picture doesn’t seem to be any better for DOIs for datasets: while they are fairly common by now, their use in scholarly articles differs widely from appearance in the references list to links in the materials and methods section to no mention at all.</p><p>There are various ways how this can be fixed (e.g. requiring authors to use biological database identifiers in a consistent way, better text mining tools, opening up subscription content to text mining), but the best solution is the simplest one: every citation in a paper should go into the references list. As an example I have added the ENA mRNA U65091 (Shioda, Fenner, &amp; Isselbacher, 1997) - something I worked on a long time ago - to the references list of this post.</p><h2 id="technology">Technology</h2><p>For this to work, it is essential that reference managers - the software authors use to generate the references list - properly support citations to data, including biological databases. It appears that all major reference managers support datasets as reference type and there is good community agreement what a data citation should look like (<a href="https://www.force11.org/datacitation">Joint Declaration of Data Citation Principles</a>). What is missing is support for easily importing the required metadata for these datasets, and reference managers use two approaches for this:</p><ul><li>query external databases via API and pull in the required metadata (e.g. Papers, Endnote)</li><li>browse to the webpage describing the database entry and import the metadata via bookmarklet/web importer (e.g. Zotero, Mendeley)</li></ul><p>Both approaches require custom code for every database. Whereas many reference managers use Citation Style Language (<a href="http://citationstyles.org/">CSL</a>) as a standard way to format references, no such standard exists for web importers. Which means that every reference manager has to implement this separately, and most of them are not open source software so that the community could help.</p><p>PLOS Labs is holding a <a href="http://www.ploslabs.org/citation-hackathon/">Citation Hackathon</a> on October 18 in their San Francisco office. While I can’t attend in person, I want to contribute to this hackathon in three ways:</p><ul><li>do an evaluation of how the reference managers Papers, Mendeley and Zotero (the three reference managers I use) support citations to the biological databases ENA, PDB and Uniprot and what is missing</li><li>look at existing aggregators of this information (e.g. <a href="http://identifiers.org/">Identifiers.org</a>) to figure out whether the import process can be simplified</li><li>start work on Zotero <a href="https://www.zotero.org/support/dev/translators/coding#web_translators">web translators</a> for these three databases. Zotero is open source software and the web translators are written in Javascript</li></ul><p>Please contact me if you are interested in helping with this, e.g. with a joint virtual hackathon on the 18th (or in person in London or Cambridge on October 15 if that works better).</p><p>Together with <a href="https://twitter.com/IanMulvany">Ian Mulvany</a> from eLife and others from Papers and Mendeley we have also submitted a proposal for a pre-conference workshop/hackathon for the <a href="https://www.force11.org/meetings/force2015">Force2015 Conference</a> in January to work on this for a broader set of databases, which should for example also include software repositories. One question is how we properly handle the citation of large numbers of datasets (1000s to millions), we could for example allow a range of identifiers in a citation. We also need tools to convert identifiers and links in existing documents to proper references, something that we <a href="https://martinfenner.ghost.io/2013/06/24/citations-in-markdown-part-3/">have also discussed on this blog</a>, and we need to discuss how our bibliographic file formats (e.g. bibtex) support these citation types. I <a href="https://martinfenner.ghost.io/2013/07/30/citeproc-yaml-for-bibliographies/">said before</a> that I am a big fan of Citeproc YAML (or JSON, the bibliographic format used by CSL) as bibliographic exchange format, and I know that the PLOS Labs hackathon will also touch on this.</p><h2 id="community">Community</h2><p>While adding reference manager support for a wider range of citations is the first step, the bigger challenge is community support. I don’t think that it is a big mental jump for an author to use the reference manager to cite a biological database rather than typing in the identifier directly in the text (the hard work is registering the identifier in the first place), but this needs support by the community, and in particular journal editors. The important message is that citations should be done in a consistent way and authors don’t have to think about doing this differently for datasets or other relevant resources, or different publishers implementing this differently. I think the paper by Kafkas et al. (2013) clearly shows that our current recommendations for adding identifiers to biological databases is broken, and that we need to do something if we take data citation seriously.</p><p>There are several concerns about adding every citation to the references list. One of them is that we shouldn’t mix citations of scholarly articles with citations of other things, e.g. research data. I would argue that not only are we seeing an increasing number of citations to other resources in reference lists (Yang, Han, Ding, &amp; Song, 2012), but that we can of course group citations by citation type, in addition to the sorting by appearance in the text or last name of first author that is common now.</p><p>Another concern is that citations of datasets are something else that citations to scholarly articles, because the former are typically citations of content created by the same group of people at the time the journal article was also created. I would argue that again we can highlight this by how we display the references, and that I hope that this changes once data citation becomes more widespread.</p><p>What should or should not be cited in a scholarly document is of course a big discussion topic. What I am arguing is that everything that is cited should go into the references list, but that doesn’t change at all what should be cited. Personal communications are an example of something that should probably not be cited and therefore should also not go into the references list.</p><h2 id="references">References</h2><p>Kafkas, Ş., Kim, J.-H., &amp; McEntyre, J. R. (2013). Database Citation in Full Text Biomedical Articles. <em>PLoS ONE</em>. <a href="http://doi.org/10.1371/journal.pone.0063184">https://doi.org/10.1371/journal.pone.0063184</a></p><p>Shioda, T., Fenner, M. H., &amp; Isselbacher, K. J. (1997). Mus musculus melanocyte-specific gene 1 (msg1) mRNA, complete cds. ENA. Retrieved from <a href="http://www.ebi.ac.uk/ena/data/view/U65091">http://www.ebi.ac.uk/ena/data/view/U65091</a></p><p>Yang, S., Han, R., Ding, J., &amp; Song, Y. (2012). The distribution of Web citations. <em>Information Processing &amp; Management</em>, <em>48</em>(4), 779–790. <a href="http://doi.org/10.1016/j.ipm.2011.10.002">https://doi.org/10.1016/j.ipm.2011.10.002</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Please keep it simple]]></title>
            <link>https://blog.martinfenner.org/posts/please-keep-it-simple</link>
            <guid>f0e984e7-01d3-4899-b91f-10839b40d68c</guid>
            <pubDate>Tue, 16 Sep 2014 15:08:00 GMT</pubDate>
            <description><![CDATA[Doing scientific research is becoming increasingly complex, both in terms of the
tools and technologies used, and in the collaboration across disciplines and
locations that is increasingly commonplace. While the way we write up and
publish research is of course also very different from 25 years ago, I would
argue that our tools and services haven’t quite evolved at the same pace.

Of course there are important trends that enable what the Royal Institution 
calls
[https://royalsociety.org/policy/]]></description>
            <content:encoded><![CDATA[<p>Doing scientific research is becoming increasingly complex, both in terms of the tools and technologies used, and in the collaboration across disciplines and locations that is increasingly commonplace. While the way we write up and publish research is of course also very different from 25 years ago, I would argue that our tools and services haven’t quite evolved at the same pace.</p><p>Of course there are important trends that enable what the Royal Institution <a href="https://royalsociety.org/policy/projects/science-public-enterprise/Report/">calls</a> <em>Science as an Open Enterprise</em>, most importantly Open Access, which has broken down many barriers for open collaboration. But very few organizations - commercial or non-profit - see it as their primary mission to make it easier for researchers to collaborate and produce great science, in the sense that everything else is secondary and this focus is really obvious to everyone.</p><p>The following are just some examples that make you laugh hard or cry out loud:</p><ul><li>Finding relevant scholarly content. Why is still so hard?</li><li>Reading a paper. The majority of scholalry content is still not Open Access. It is embarassing how difficult it can be to get the fulltext paper from a subscription journal - too slow, too expensive, and sometimes even crippled in functionality.</li><li>Creating figures for publication. This process is still so painful that it hurts. And publishers often create artificial limitations in file type (TIFF or Postscript) and file size (10 MB??).</li><li>Licenses for scholarly content. We don’t need choice, but a few licenses that everyone understands and that don’t hinder sharing and collaboration</li><li>Secure login. I can use my Facebook or Google login almost everywhere, but as a scholar I have a different username and password at my institution, funder, the various publishers I submit too, and the scholarly services I frequently use?</li><li>Citation styles. Why do we still have at least 3,000 styles?</li></ul><p>Citation styles is a perfect example of a problem that should have been solved as soon as we made the switch to digital publishing. I can travel through half of Europe without showing my passport, and using the same currency, but I need to reformat citations every time I submit to a different journal? And I have to use the same tool for this as my coauthors, as the different reference managers don’t work with each other?</p><p>Too often there are other intentions at work in parallel. While notable, they sometimes stand in conflict with the goal of making a researcher’s life easier. A perfect example is the manuscript submission process. In parallel to the tools getting better and easier to use, the demands on the author seem to be increasing at an even greater rate, both in the data and metadata he or she should provide, and in the work submitting authors are asked to do that traditionally have been done by publishers. Another good example are peer review and evaluation. The proportion of time spent doing research vs. time spent doing administrative work seems to decreasing and not increasing.</p><p>I wish more people and organizations would stand up and state that keeping it simple is their primary goal.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[CommonMark and the Future of Scholarly Markdown]]></title>
            <link>https://blog.martinfenner.org/posts/commonmark-and-the-future-of-scholarly-markdown</link>
            <guid>3acc3fd3-5b94-4486-a9a7-55b578384046</guid>
            <pubDate>Sun, 07 Sep 2014 15:10:00 GMT</pubDate>
            <description><![CDATA[One of the important outcomes of the Markdown for Science
[https://github.com/scholmd/scholmd/wiki] workshop that took place in June 2013
was a decision on a name - Scholarly Markdown - and a brief definition
[https://github.com/scholmd/scholmd/wiki/What-is-Markdown]:

 1. Markdown that supports the requirements of scientific texts
 2. Markdown as format that glues open scientific text resources together
 3. A reference implementation with documentation and tests
 4. A community

In my eyes this]]></description>
            <content:encoded><![CDATA[<p>One of the important outcomes of the <a href="https://github.com/scholmd/scholmd/wiki">Markdown for Science</a> workshop that took place in June 2013 was a decision on a name - <em>Scholarly Markdown</em> - and a brief <a href="https://github.com/scholmd/scholmd/wiki/What-is-Markdown">definition</a>:</p><ol><li>Markdown that supports the requirements of scientific texts</li><li>Markdown as format that glues open scientific text resources together</li><li>A reference implementation with documentation and tests</li><li>A community</li></ol><p>In my eyes this is still a great definition. And this week something important happened that is very relevant for Scholarly Markdown. A small group of people deeply involved in Markdown announced <a href="http://commonmark.org/">Standard Markdown</a>:</p><blockquote>We propose a standard, unambiguous syntax specification for Markdown, along with a suite of comprehensive tests to validate Markdown implementations against this specification. We believe this is necessary, even essential, for the future of Markdown.</blockquote><p>Markdown is in widespread use, but a lack of standard syntax and set of comprehensive tests has hindered the adoption for more complex use cases, the development of cross-platform tools, and the use of markdown as a document interchange format. I am therefore 100% behind this initiative. In particular since this is not just an initiative by large commercial organizations heavily using Markdown such as Stackexchange, Github or Reddit, but that the entire spec and both reference implementations have been written by <a href="http://johnmacfarlane.net/">John MacFarlane</a>, the author of Pandoc, the universal document converter. Not only does Pandoc already support many of the features required by Scholarly Markdown (e.g. math and citations), but John is the Chair of the Department of Philosophy at UC Berkeley.</p><p>Markdown was developed in 2004 by John Gruber, and he <a href="http://daringfireball.net/projects/markdown/license">holds the rights</a> to the name Markdown. He didn’t want this initiative to use the name <strong><strong>Standard Markdown</strong></strong>, so the implementation was <a href="http://blog.codinghorror.com/standard-markdown-is-now-common-markdown/">renamed</a> to <a href="http://commonmark.org/">CommonMark</a>.</p><p>The consequences of all this for Scholarly Markdown?</p><ul><li>CommonMark focusses on the basic features of the language, but once the specification is agreed upon and implemented by a critical mass of tools, it is clear that there needs to be a standardized way to handle extensions of the language. This is both about features used by lots of people such as tables, but also functionality relevant only for scholarly content.</li><li>This brings us one gigantic step closer to a reference implementation and set of tests for Scholarly Markdown, as hopefully Scholarly Markdown can build upon the work by John and the CommonMark team.</li><li>The name Scholarly Markdown might not be a good idea going forward. We should either change the name to align with CommonMark, or we should come up with a totally different name, something that the screenwriters have done with <a href="http://fountain.io/">their</a> implementation of Markdown.</li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Microsoft Word with git]]></title>
            <link>https://blog.martinfenner.org/posts/using-microsoft-word-with-git</link>
            <guid>dcb09e05-e221-4a23-9567-3e72ce02716c</guid>
            <pubDate>Mon, 25 Aug 2014 15:12:00 GMT</pubDate>
            <description><![CDATA[One of the major challenges of writing a journal article is to keep track of
versions - both the different versions you create as the document progresses,
and to merge in the changes made by your collaborators. For most academics
Microsoft Word is the default writing tool, and it is both very good and very
bad in this. Very good because the track changes feature makes it easy to see
what has changed since the last version and who made the changes. Very bad
because this feature is built around ke]]></description>
            <content:encoded><![CDATA[<p>One of the major challenges of writing a journal article is to keep track of versions - both the different versions you create as the document progresses, and to merge in the changes made by your collaborators. For most academics Microsoft Word is the default writing tool, and it is both very good and very bad in this. Very good because the <em>track changes</em> feature makes it easy to see what has changed since the last version and who made the changes. Very bad because this feature is built around keeping everything in a single Word document, so that only one person can work on on a manuscript at a time. This usually means sending manuscripts around by email, and being very careful about not confusing different versions of the document, which requires <a href="http://www.phdcomics.com/comics/archive.php?comicid=1531">creativity</a>.</p><p>Approaches to overcome these challenges are to a) integrate the Word documents into collaboration tools such as Sharepoint and Office 365, or document sharing services such as Dropbox and Google Docs (if you use it just for that), or b) use a different authoring tool altogether. If neither of these approaches works for you, you have a third option: use the version control system <strong><strong>git</strong></strong>.</p><p><a href="http://www.mulvany.net/presentations/WikimaniaOpenScholarshipTalk.slides.html#/3">Git</a> is software that helps with <a href="http://blog.martinfenner.org/2014/08/25/using-microsoft-word-with-git/(http://git-scm.com/book/en/Getting-Started-About-Version-Control)">tracking changes to files</a> so that you can recall specific versions later. Git is typically used to track changes of software source code (and was originally developed by Linus Torvalds for Linux kernel development in 2005), but in fact git can be used for any file where we need to keep track of versions over time. Git is open source software that runs locally on your computer, so please go ahead and start tracking changes to your manuscripts (or other complex documents) with git. Any time you want to store a version, do a <code>git commit</code> with a little description and an optional tag.</p><p>This approach is not ideal, as git was written with source code in text format in mind and for example doesn’t understand what has changed between two revisions of a Word document. Some people will tell you to never store binary files in a version control system, but don’t listen to them. Instead give git a tool to convert Word documents into plain text, and git will then happily tell you what has changed between revisions. Several tools can do this, but since earlier this month Pandoc can read Word documents in <code>docx</code> format. Do the following to have Pandoc convert Word documents into markdown, and to compare the revisions by word and not by line (which makes more sense):</p><pre><code># .gitattributes file in root folder of your git project
*.docx diff=pandoc</code></pre><pre><code># .gitconfig file in your home folder
[diff "pandoc"]
  textconv=pandoc --to=markdown
  prompt = false
[alias]
  wdiff = diff --word-diff=color --unified=1</code></pre><p>You can then use <code>git wdiff important_file.docx</code> to see the changes (with deletions in red and insertions in green), or <code>git log -p --word-diff=color important_file.docx</code> to see all changes over time.</p><p>While you can now track revisions of a Word document and see the changes, you also want to be able to merge different versions of a Word document together so that you and your collaborators can work on the manuscript in parallel. Git can’t merge binary files together, so you need to first convert the Word document into a format that git understands. Just as in the previous example we can use Pandoc for that, with markdown as the textual format. This would also work with HTML or LaTeX, but the simplicity of markdown makes it better suited for version control which doesn’t know about the markup of these formats.</p><p>One of the reasons that git became so popular with software developers is that it is a <strong><strong>distributed version control system</strong></strong> instead of a centralized system such as Subversion. This means that you can track all revisions locally on your computer, but can still synchronize your revisions with another user. <strong><strong>Github</strong></strong> is a popular service that facilitates this synchronization and adds some nice features on top. One way to collaborate with your co-authors is therefore to set up a Github repository (public or private) for your manuscript, and store the master version of the manuscript in markdown format. Instead of working on the master version directly, you would use Pandoc to convert back and forth between this master version in markdown format and your Word document, and would continue to use Word as authoring tool. <a href="http://blog.martinfenner.org/2014/08/18/introducing-rakali/">Rakali</a> is a Pandoc tool that I released last week that can help automate this document conversion. Github has a a number of features to facilitate collaboration that can be used here, e.g. Github issues for discussion and task management.</p><p>There are still a few rough edges in the workflow described above (e.g. only partial support of Word track changes), but it is an interesting approach to collaborate using Microsoft Word and git. And this workflow can of course be enhanced to also include authors that write in LaTeX or one of the other formats that Pandoc supports. One nice side effect of using markdown is that Github will automatically render a webpage for the document (which it will not do for HTML without extra effort).</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Introducing Rakali]]></title>
            <link>https://blog.martinfenner.org/posts/introducing-rakali</link>
            <guid>6d23947a-61d8-40c4-b020-51ba50dac3d1</guid>
            <pubDate>Mon, 18 Aug 2014 15:16:00 GMT</pubDate>
            <description><![CDATA[In July and August I attended the Open Knowledge Festival
[http://2014.okfestival.org/] and Wikimania
[http://wikimania2014.wikimedia.org/wiki/Programme]. At both events I had many
interesting discussions around open source tools for open access scholarly
publishing, and I was part of a panel
[http://wikimania2014.wikimedia.org/wiki/Submissions/The_Full_OA_Stack_-_Open_Access_and_Open_Source] 
on that topic at Wikimania last Sunday. Some of my thoughts were summarized in a
blog post a few weeks ]]></description>
            <content:encoded><![CDATA[<p>In July and August I attended the <a href="http://2014.okfestival.org/">Open Knowledge Festival</a> and <a href="http://wikimania2014.wikimedia.org/wiki/Programme">Wikimania</a>. At both events I had many interesting discussions around open source tools for open access scholarly publishing, and I was part of a <a href="http://wikimania2014.wikimedia.org/wiki/Submissions/The_Full_OA_Stack_-_Open_Access_and_Open_Source">panel</a> on that topic at Wikimania last Sunday. Some of my thoughts were summarized in a blog post a few weeks ago (<a href="http://blog.martinfenner.org/2014/07/18/roads-not-stagecoaches/">Build Roads not Stagecoaches</a>). Today I am happy to announce the first public release of a tool that hopefully contributes to making publishing of open content a bit easier.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/rakali.jpg" class="kg-image" alt="LEGO Researchers are excited that they don’t have to use Microsoft Word for manuscript writing anymore."><figcaption>LEGO Researchers are excited that they don’t have to use Microsoft Word for manuscript writing anymore.</figcaption></figure><p><a href="https://github.com/rakali/rakali.rb">Rakali</a> is a Ruby gem that acts as a wrapper for the <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a> universal document converter. Pandoc is a wonderful tool to convert documents between file formats and supports many file formats and features important for scholarly publishing. Pandoc 1.13 was <a href="http://johnmacfarlane.net/pandoc/releases.html">released</a> last Friday, and one of the most exciting new features is a reader for Microsoft Word (<code>docx</code>) documents. Pandoc has supported the conversion to <code>docx</code> for a while, but now you can use the most popular file format for writing scholarly documents and turn your <code>docx</code> files into HTML, PDF, LateX, markdown, or a number of other formats, making it much easier to collaborate, and to use <code>docx</code> with Pandoc in scholarly publishing workflows. A good example would be arXiv, which <a href="http://arxiv.org/help/submit#text">doesn’t support</a> <code>docx</code> for text submissions. Instead of turning it into PDF the manuscript can now be converted to LaTeX - the preferred file format at arXiv - before submission.</p><p>I built <strong><strong>Rakali</strong></strong> to make it easier to use Pandoc to convert large numbers of documents in an automated way:</p><ul><li>bulk conversion of all files in a folder with a specific extension, e.g. <code>md</code>.</li><li>input via a configuration file in yaml format instead of via the command line</li><li>validation of documents via <a href="http://json-schema.org/">JSON Schema</a>, using the <a href="https://github.com/hoxworth/json-schema">json-schema</a> Ruby gem.</li><li>Logging via <code>stdout</code> and <code>stderr</code>.</li></ul><p>One interesting way to use Rakali and Pandoc is as part of a <a href="http://blog.martinfenner.org/2014/03/10/continuous-publishing/">continuous publishing</a> workflow that involves git and Github, automatically converting all files in a folder when something is pushed to the repository using a continuous integration tool, and exiting the continuous integration run when one of the files doesn’t validate. Look into the Rakali <a href="http://blog.martinfenner.org/2014/08/18/introducing-rakali/%5BRakali%5D(https://github.com/rakali/rakali.rb)">repo</a> for an example.</p><p>The most interesting aspect of Rakali is probably validation via JSON Schema. File conversion with Pandoc is a two-step process, the intermediate format is an internal representation of the document in something called the <a href="http://blog.martinfenner.org/2013/11/17/the-grammar-of-scholarly-communication/">abstract syntax tree</a> or AST. Pandoc makes the AST accessible in JSON format, making it straightforward to manipulate a document before the conversion into the target format with something called <a href="http://johnmacfarlane.net/pandoc/scripting.html">JSON filters</a>.</p><p>Validation of XML documents using <a href="https://en.wikipedia.org/wiki/Document_type_definition">DTDs</a>, <a href="http://relaxng.org/">RELAX NG</a> and other standards has of course been around for a long time, but validation of JSON documents is still relatively new. Since many Pandoc document conversion workflows don’t involve any XML I thought it would make more sense to validate against the AST, and we can use JSON Schema for that. I have started a <a href="https://github.com/rakali/pandoc-schemata">Github repository</a> with schemata for the Pandoc AST, and hope to evolve them over time using Rakali as a tool. An example log output (from the Rakali test suite, stopping file conversion because title and layout metadata are missing) looks like this:</p><pre><code>Validation Error: The property '#/0/unMeta' did not contain a required property of 'title' in schema 9b6d454d-e609-537b-b761-9599b6c01072# for file empty.md
Validation Error: The property '#/0/unMeta' did not contain a required property of 'layout' in schema 9b6d454d-e609-537b-b761-9599b6c01072# for file empty.md
Fatal: Conversion of file empty.md failed.</code></pre><p>As I had argued before, the challenge for building open source tools for science is to <a href="http://blog.martinfenner.org/2014/07/24/dont-reinvent-the-wheel/">not duplicate the work of others</a>, and to integrate well with existing tools by focussing on one aspect and doing that aspect well. It also helps to think about infrastructure (<a href="http://blog.martinfenner.org/2014/07/18/roads-not-stagecoaches/">the roads</a>) instead of only focussing on the user-facing aspects. There are obviously many document conversion tools out there, but Pandoc is certainly one of the oldest and most established ones for scholarly content. Rakali therefore builds on top of Pandoc and tries to play well with other existing tools and services, e.g. by using the UNIX <code>stdout</code> and <code>stderr</code> for reporting, and by using a file-based approach that works well with version control systems such as git. And since Rakali is a Ruby gem it can not only be used as a standalone command line tool, but can also be easily integrated into other Ruby applications.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Visualizing Scholarly Content]]></title>
            <link>https://blog.martinfenner.org/posts/visualizing-scholarly-content</link>
            <guid>cb59dc57-e557-438a-b87d-1d0a26b06fa1</guid>
            <pubDate>Sat, 09 Aug 2014 15:19:00 GMT</pubDate>
            <description><![CDATA[One topic I will cover this Sunday in a presentation on Open Scholarship Tools
[http://wikimania2014.wikimedia.org/wiki/Submissions/Open_Scholarship_Tools_-_a_whirlwind_tour.] 
at Wikimania 2014 together with Ian Mulvany [https://twitter.com/ianmulvany] is
visualization.

Data visualization is all about telling stories with data, something that is of
course not only important for scholarly content, but for example increasingly
common in journalism. This is a big and complex topic, but I hope the]]></description>
            <content:encoded><![CDATA[<p>One topic I will cover this Sunday in a presentation on <a href="http://wikimania2014.wikimedia.org/wiki/Submissions/Open_Scholarship_Tools_-_a_whirlwind_tour.">Open Scholarship Tools</a> at <em>Wikimania 2014</em> together with <a href="https://twitter.com/ianmulvany">Ian Mulvany</a> is visualization.</p><p>Data visualization is all about <em>telling stories with data</em>, something that is of course not only important for scholarly content, but for example increasingly common in journalism. This is a big and complex topic, but I hope the following will get you started.</p><h3 id="learn-the-basics">Learn the Basics</h3><p>Work on visualization of scientific data should start with a good understanding of the best practices and pitfalls of data visualization in general, as well as the specific aspects of visualizing scientific data. The following resources have helped me get started - please suggest more in the comments:</p><ul><li><a href="http://book.flowingdata.com/">Visualize this</a>. A book from Nathan Yau published in 2011. Very helpful in understanding the different ways data can be visualized (e.g. when to use a treemap or what is a <a href="https://en.wikipedia.org/wiki/Choropleth_map">chloropleth map</a>), and an introduction to some tools using practical examples. Nathan’s <a href="http://flowingdata.com/">FlowingData</a> blog is also a great resource.</li><li><a href="https://github.com/mbostock/d3/wiki/Gallery">D3 Gallery</a>. Lots of examples generated using Mike Bostock’s d3.js visualization library. A great inspiration for data visualization on the web, even if you use a different visualization tool.</li><li><a href="http://docs.ggplot2.org/current/index.html">ggplot2</a>. Not only a very popular visualization library for the R language by Hadley Wickham, but also an implementation of Leland Wilkison’s Grammar of Graphics. The <a href="http://www.springer.com/statistics/computational+statistics/book/978-0-387-98140-6">ggplot2 book</a> describes this powerful concept (p. 14):</li></ul><blockquote>In brief, the grammar tells us that a statistical graphic is a mapping from data to aesthetic attributes (colour, shape, size) of geometric objects (points, lines, bars). The plot may also contain statistical transformations of the data and is drawn on a specific coordinate system. Faceting can be used to generate the same plot for different subsets of the dataset. It is the combination of these independent components that make up a graphic.</blockquote><h3 id="learn-to-use-at-least-one-visualization-tool">Learn to use at least one visualization tool</h3><p>There are many great tools available, pick one and learn it well. Some options include:</p><ul><li><strong><strong>Excel</strong></strong>. Probably the most popular tool for data visualization. Commercial, with open source alternatives such as Libre Office.</li><li><strong><strong>R</strong></strong>. Software for statistical computing and analysis. Open source. <a href="http://www.rstudio.com/">RStudio</a> is a powerful user interface for R and a good way to get started.</li><li><a href="http://d3js.org/"><strong>d3.js</strong></a>. A visualization library for Javascript. Open source.</li><li><a href="http://www.graphpad.com/scientific-software/prism/"><strong>Prism</strong></a>. A popular visualization tool among scientists. Commercial.</li><li><a href="https://datawrapper.de/"><strong>Datawrapper</strong></a>. An open source tool and hosted service for data visualization.</li></ul><p>I do most visualizations in either R or d3.js. Both are open source tools with a large community and a rich set of libraries, examples and documentation, and both take a systematic approach to data visualization (see grammar of graphics above).</p><h3 id="learn-data-analysis">Learn data analysis</h3><p>Unless your interest is more in information design - see <a href="http://www.informationisbeautiful.net/">Information is beautiful</a> for some great examples - data visualization is tightly coupled with data analysis. You need to know at least the basics of data analysis to do proper data visualizations, e.g. how to handle wrongly formatted data (e.g. text in a number column), missing values and outliers. The most time-consuming step in my experience is data transformation, i.e. bringing data into the format that you want for the analysis and visualization.</p><p>R, Python and the relatively new <a href="http://julialang.org/">Julia</a> are popular languages for data analysis available as open source. There are many packages for these languages that help with common data analysis problems. One additional advantage of using a proper language over a set of tools cobbled together is that it is easy to automatically recreate a visualization with a new set of data - convenient when you need to analyze and visualize an ongoing experiment that repeatedly produces new data.</p><h3 id="use-a-vector-file-format">Use a vector file format</h3><p>Too many scientific data are still visualized using bitmap graphic formats such as <code>tiff</code>, <code>jpg</code> and <code>png</code>. These formats are not appropriate for charts and only make sense for images. They don’t scale to the screen resolution, and it is <a href="http://blog.f1000research.com/2014/02/20/the-importance-of-providing-data-and-not-just-images-of-data/">very hard to impossible</a> to reuse or even modify them. Use vector graphic formats such as <code>svg</code> or <code>pdf</code> instead. <code>svg</code> is my preferred format because in contrast to <code>pdf</code> it can be embedded into a larger HTML document, and R and d3.js (my preferred visualization tools) can generate this format. <a href="http://www.inkscape.org/">Inkscape</a> is an open source SVG editor, and the commercial <strong><strong>Adobe Illustrator</strong></strong> can be used to manually polish graphics in <code>svg</code> or <code>pdf</code> format, e.g. for journal publication.</p><h3 id="get-inspired-by-great-visualizations">Get inspired by great visualizations</h3><p>At the end of the day data visualization is all about telling a story with data. Unfortunately the current state of affairs for scientific visualizations is very different. In my opinion most graphs and figures used in publications don’t provide the data underlying the visualization (<a href="https://datawrapper.de/">Datawrapper</a> is a great example how this can be done), focus too much on detail rather than the overall message, don’t take advantage of the different chart types available, and are sometimes even misleading. And I’m not even talking about the fact that figures in scholarly papers are <a href="http://dx.doi.org/10.12688/f1000research.4263.1">almost never</a> interactive. It rarely happens that I read a paper and get excited by looking at a figure - if I do it is usually because the underlying data are so compelling that even the simplest visualization will convey the right message.</p><p>We should become more creative with visualizing data in scholalry documents, and one important step towards that goal is publishers accepting more reasonable file formats in manuscript submissions - instead of just <code>tiff</code> and <code>eps</code> (<a href="http://www.plosone.org/static/figureGuidelines#figures">PLOS</a>), or <code>tiff</code>, <code>eps</code> and <code>pdf</code> (<a href="http://www.sciencemag.org/site/feature/contribinfo/prep/prep_revfigs.xhtml#format">Science</a>), and often with a 10 MB file site limit.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What is a DOI?]]></title>
            <link>https://blog.martinfenner.org/posts/what-is-doi</link>
            <guid>2caa27fe-c069-41a0-bed2-931ab552c163</guid>
            <pubDate>Wed, 06 Aug 2014 15:24:00 GMT</pubDate>
            <description><![CDATA[This Sunday Ian Mulvany [https://twitter.com/ianmulvany] and I will do a
presentation on Open Scholarship Tools
[http://wikimania2014.wikimedia.org/wiki/Submissions/Open_Scholarship_Tools_-_a_whirlwind_tour.] 
at Wikimania 2014 in London. From the abstract:

> This presentation will give a broad overview of tools and standards that are
helping with Open Scholarship today.
One of the four broad topics we have picked are digital object identifiers
(DOI)s. We want to introduce them to people new to]]></description>
            <content:encoded><![CDATA[<p>This Sunday <a href="https://twitter.com/ianmulvany">Ian Mulvany</a> and I will do a presentation on <a href="http://wikimania2014.wikimedia.org/wiki/Submissions/Open_Scholarship_Tools_-_a_whirlwind_tour.">Open Scholarship Tools</a> at <em>Wikimania 2014</em> in London. From the abstract:</p><blockquote>This presentation will give a broad overview of tools and standards that are helping with Open Scholarship today.</blockquote><p>One of the four broad topics we have picked are <em>digital object identifiers (DOI)s</em>. We want to introduce them to people new to them, and we want to show some tricks and cool things to people who already now them. Along the way we will also try to debunk some myths about DOIs.</p><h3 id="what-a-doi-looks-like">What a DOI looks like</h3><p>DOIs - or better DOI names - start with a prefix in the format <code>10.x</code> where x is 4-5 digits. The suffix is determined by the organization registering the DOI, and there is no consistent pattern across organizations. The DOI name is typically expressed as a URL (see below). An example DOI would look like: <a href="http://dx.doi.org/10.5555/12345678">http://dx.doi.org/10.5555/12345678</a>. Something in the format <strong><strong>10/hvx</strong></strong> or <a href="http://doi.org/hvx">http://doi.org/hvx</a> is a <a href="http://shortdoi.org/">shortDOI</a>, and <strong><strong>1721.1/26698</strong></strong> or <a href="http://hdl.handle.net/1721.1/26698">http://hdl.handle.net/1721.1/26698</a> is a handle. BTW, all DOIs names are also handles, so <a href="http://hdl.handle.net/10/hvx">http://hdl.handle.net/10/hvx</a> for the shortDOI example above will resolve correctly.</p><h3 id="dois-are-persistent-identifiers">DOIs are persistent identifiers</h3><p>Links to resources can change, particularly over long periods of time. Persistent identifiers are needed so that readers can still find the content we reference in a scholarly work (or anything else where persistent linking is important) 10 or 50 years later. There are many kinds of persistent identifiers, one of the key concepts - and a major difference to URLs - is to separate the identifier for the resource from its location. Persistent identifiers require technical infrastructure to resolve identifiers (DOIs use the <a href="http://www.handle.net/">Handle System</a>) and to allow long-term archiving of resources. DOI registration agencies such as DataCite or CrossRef are required to provide that persistence. Other persistent identifier schemes besides DOIs include <a href="http://en.wikipedia.org/wiki/PURL">persistent uniform resource locators (PURLs)</a> and <a href="http://en.wikipedia.org/wiki/Archival_Resource_Key">Archival Resource Keys (ARKs)</a>.</p><h3 id="dois-have-attached-metadata">DOIs have attached metadata</h3><p>All DOIs have metadata attached to them. The metadata are supplied by the resource provider, e.g. publisher, and exposed in services run by registration agencies, for example metadata search and content negotiation (see below). There is a minimal set of required metadata for every DOI, but beyond that, different registration agencies will use different metadata schemata, and most metadata are optional. Metadata are important to build centralized discovery services, making it easier to describe a resource, e.g. journal article citing another article. Some of the more recent additions to metadata schemata include persistent identifiers for people (<a href="http://orcid.org/">ORCID</a>) and funding agencies (<a href="http://www.crossref.org/fundref/">FundRef</a>), and license information. The following API call will retrieve all publications registered with CrossRef that use a <a href="http://creativecommons.org/licenses/by/3.0/deed.en_US">Creative Commons Attribution license</a> (and where this information has been provided by the publisher):</p><pre><code>http://api.crossref.org/funders/10.13039/100000001/works?filter=license.url:http://creativecommons.org/licenses/by/3.0/deed.en_US</code></pre><h3 id="dois-support-link-tracking">DOIs support link tracking</h3><p>Links to other resources are an important part of the metadata, and describing all citations between a large number scholarly documents is a task that can only really be accomplished by a central resource. To solve this very problem DOIs were invented and the CrossRef organization started around 15 years ago.</p><h3 id="not-every-doi-is-the-same">Not every DOI is the same</h3><p>The DOI system <a href="http://www.doi.org/doi_handbook/1_Introduction.html">originated from an initiative by scholarly publishers</a> (first announced at the Frankfurt Book Fair in 1997), with citation linking of journal articles its first application. This citation linking system is managed by <a href="http://www.crossref.org/">CrossRef</a>, a non-profit member organization of scholarly publishers, and <a href="http://search.crossref.org/help/status">more than half</a> of the about <a href="http://www.doi.org/faq.html">100 million DOIs</a> that have been assigned to date are managed by them.</p><p>But many DOIs are assigned by one of the other 8 <a href="http://www.doi.org/RA_Coverage.html">registration agencies</a>. You probably know <a href="http://www.datacite.org/">DataCite</a>, but did you know that the <a href="http://publications.europa.eu/index_en.htm">Publications Office of the European Union (OP)</a> and the <a href="http://www.eidr.org/">Entertainment Identifier Registry (EIDR)</a> also assign DOIs? The distinction is important, because some of the functionality is a service of the registration agency - metadata search for example is offered by CrossRef (<a href="http://search.crossref.org/">http://search.crossref.org</a>) and DataCite (<a href="http://search.datacite.org/">http://search.datacite.org</a>), but you can’t search for a DataCite DOI in the CrossRef metadata search. There is an API to find out the registration agency behind a DOI so that you know what services to expect:</p><pre><code>http://api.crossref.org/works/10.6084/m9.figshare.821213/agency

{
  "status": "ok",
  "message-type": "work-agency",
  "message-version": "1.0.0",
  "message": {
    "DOI": "10.6084/m9.figshare.821213",
    "agency": {
      "id": "datacite",
      "label": "DataCite"
    }
  }
}</code></pre><h3 id="dois-are-urls">DOIs are URLs</h3><p><a href="http://www.doi.org/faq.html">DOI names may be expressed as URLs (URIs) through a HTTP proxy server</a> - e.g. <a href="http://dx.doi.org/10.5555/12345679">http://dx.doi.org/10.5555/12345679</a>, and this is how DOIs are typically resolved. For this reason the <a href="http://www.crossref.org/02publishers/doi_display_guidelines.htm">CrossRef DOI Display Guidelines</a> recommend that <em>CrossRef DOIs should always be displayed as permanent URLs in the online environment</em>. Because DOIs can be expressed as URLs, they also have their features:</p><h4 id="special-characters">Special characters</h4><p>Because DOIs can be expressed as URLs, DOIs <a href="http://www.crossref.org/02publishers/15doi_guidelines.html">should only include characters allowed in URLs</a>, something that wasn’t always true in the past and can cause problems, e.g. when using SICIs (<a href="https://en.wikipedia.org/wiki/Serial_Item_and_Contribution_Identifier">Serial Item and Contribution Identifier</a>), an extension of the ISSN for journals:</p><pre><code>10.4567/0361-9230(1997)42:&lt;OaEoSR&gt;2.0.TX;2-B</code></pre><h4 id="content-negotiation">Content negotiation</h4><p>The DOI resolver at <em>doi.org</em> (or <em>dx.doi.org</em>) normally resolves to the resource location, e.g. a landing page at a publisher website. Requests that are not for content type <code>text/html</code> are redirected to the registration agency metadata service (currently for CrossRef, DataCite and mEDRA DOIs). Using <a href="http://www.crosscite.org/cn/">content negotiation</a>, we can ask the metadata service to send us the metadata in a format we specify (e.g. Citeproc JSON, bibtex or even a formatted citation in one of thousands of citation styles) instead of getting redirected to the resource. This is a great way to collect bibliographic information, e.g. to format citations for a manuscript. In theory we could also use content negotiation to get a particular representation of a resource, e.g. <code>application/pdf</code> for a PDF of a paper or <code>text/csv</code> for a dataset in CSV format. This is not widely support and I don’t know the details of the implementation in the DOI resolver, but you can try this (content negotation is easier with the command line than with a browser):</p><pre><code>curl -LH "Accept: application/pdf" http://dx.doi.org/10.7717/peerj.500 &gt;peerj.500.pdf</code></pre><p>This will save the PDF of the 500th PeerJ paper published last week.</p><h4 id="fragment-identifiers">Fragment identifiers</h4><p>As discussed in <a href="http://blog.martinfenner.org/2014/08/02/fragment-identifiers-and-dois/">my last blog post</a>, we can use frament identifiers to subsections of a document with DOIs, e.g. <a href="http://dx.doi.org/10.1371/journal.pone.0103437#s2">http://dx.doi.org/10.1371/journal.pone.0103437#s2</a> or <a href="http://doi.org/10.5446/12780#t=00:20,00:27">http://doi.org/10.5446/12780#t=00:20,00:27</a>, just as we can with every other URL. This is a nice way to directly link to a specific document section, e.g. when discussing a paper on Twitter. Fragment identifiers are implemented by the client (typically web browser) and depend on the document type, but for DOIs that resolve to fulltext HTML documents they can add granularity to the DOI without much effort.</p><h4 id="queries">Queries</h4><p>URLs obviously support queries, but that is a feature I haven’t yet seen with DOIs. Queries would allow interesting features, partly overlapping with what is possible with fragment identifiers and content negotiation, e.g. <code><a href="http://dx.doi.org/10.7717/peerj.500?format=pdf" rel="nofollow">http://dx.doi.org/10.7717/peerj.500?format=pdf</a></code>. II hope to find out more until Sunday.</p><h3 id="outlook">Outlook</h3><p>My biggest wish? Make DOIs more machine-readable. They are primarily intended for human users, enabling them to find the content associated with a DOI. But they sometimes don’t work as well as they could with automated tools, one example are the <a href="http://blog.martinfenner.org/2013/10/13/broken-dois/">challenges automatically resolving a DOI</a> that I described in a blog post last year. Thinking about DOIs as URLs - and using them this way - is the right direction.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Fragment Identifiers and DOIs]]></title>
            <link>https://blog.martinfenner.org/posts/fragment-identifiers-and-dois</link>
            <guid>33c7abb9-7959-4fcf-9368-56882a8d5076</guid>
            <pubDate>Sat, 02 Aug 2014 15:26:00 GMT</pubDate>
            <description><![CDATA[Before all our content turned digital, we already used page numbers to describe
a specific section of a book or longer document, with older manuscripts using
the folio [https://en.wikipedia.org/wiki/Folio] before that. Page numbers have
transitioned to electronic books with readers such as the Kindle supporting
them
eventually
[http://pogue.blogs.nytimes.com/2011/02/08/page-numbers-for-kindle-books-an-imperfect-solution/?_php=true&_type=blogs&_r=0]
.

Image by Al Silonov from Wikimedia Commons
[]]></description>
            <content:encoded><![CDATA[<p>Before all our content turned digital, we already used <strong><strong>page numbers</strong></strong> to describe a specific section of a book or longer document, with older manuscripts using the <a href="https://en.wikipedia.org/wiki/Folio">folio</a> before that. Page numbers have transitioned to electronic books with readers such as the Kindle <a href="http://pogue.blogs.nytimes.com/2011/02/08/page-numbers-for-kindle-books-an-imperfect-solution/?_php=true&amp;_type=blogs&amp;_r=0">supporting them eventually</a>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/Folio_-number-.jpg" class="kg-image" alt srcset="https://martinfenner.ghost.io/content/images/size/w600/2021/01/Folio_-number-.jpg 600w, https://martinfenner.ghost.io/content/images/2021/01/Folio_-number-.jpg 746w" sizes="(min-width: 720px) 720px"><figcaption>Image by Al Silonov from <a href="http://commons.wikimedia.org/wiki/File:Folio_(number).jpg">Wikimedia Commons</a>. This file is licensed under the <a href="http://creativecommons.org/licenses/by-sa/3.0/deed.en">Creative Commons Attribution-Share Alike 3.0 Unported</a> license.</figcaption></figure><p>For content on the web we can use the <code>#</code> fragment identifier, e.g. <a href="https://en.wikipedia.org/wiki/Fragment_identifier#Proposals">https://en.wikipedia.org/wiki/Fragment_identifier#Proposals</a> to navigate to a specific section of a web page. How the linking to this fragment is handled, depends on the <strong><strong>MIME</strong></strong> type of the document, and will for example be done differently for a text page than a video - YouTube understands minutes and seconds into a video as fragment identifier, e.g. <a href="https://www.youtube.com/watch?v=0UNRZEsLxKc#t=54m52s">https://www.youtube.com/watch?v=0UNRZEsLxKc#t=54m52s</a>. Fragment identifiers are not only helpful to link to a subsection of a document, but of course also for navigation within a document.</p><p>All this is of course very relevant to scholarly content, which is usually much more structured, with most journal articles following the <a href="https://en.wikipedia.org/wiki/IMRAD">IMRAD</a> - introduction, methods, results, and discussion - format, usually with additional sections such as abstract, references, etc. One approach to link to figures and tables within a scholarly articles is using <a href="http://blogs.plos.org/mfenner/2011/03/26/direct-links-to-figures-and-tables-using-component-dois/">component DOIs</a>, e.g. specific DOIs for parts of a larger document. The publisher <strong><strong>PLOS</strong></strong> has been using them for a long time, and the <a href="https://martinfenner.ghost.io/2014/07/24/dont-reinvent-the-wheel/">number of component DOIs is rising</a>, but most scholarly journal articles don’t use component DOIs. And whereas component DOIs are a great concept for content such as figures (allowing us to describe the MIME type and other relevant metadata), they are probably not the best tool to link to a section or paragraph of a scholarly document.</p><p>As it turns out, we already have a tool for that, as the DOI proxy server gracefully forwards fragment identifiers (how did I miss this?). We can therefore use a DOI with a fragment identifier to</p><ul><li>Results section: <a href="http://dx.doi.org/10.1371/journal.pone.0103437#s2">http://dx.doi.org/10.1371/journal.pone.0103437#s2</a></li><li>Specific reference: <a href="http://dx.doi.org/10.12688/f1000research.4263.1#ref-7">http://dx.doi.org/10.12688/f1000research.4263.1#ref-7</a></li><li>Decision letter: <a href="http://dx.doi.org/10.7554/eLife.00471#decision-letter">http://dx.doi.org/10.7554/eLife.00471#decision-letter</a></li></ul><p>Obviously this only works if the DOI is resolved to the fulltext of a resource, and not a landing page. And how the fragment identifiers are named and implemented is up to the publisher, and the DOI resolver has no information about them. These specific links are particularly nice for discussions of a paper, whether it is on Twitter or in a discussion forum. It appears that at least the Twitter link shortener keeps the fragment identifier, the link to the eLife decision letter is shortened to <a href="http://t.co/URWaYmGHnY">http://t.co/URWaYmGHnY</a>. This kind of linking works particularly well if the publisher is using a fine-grained system of fragment identifiers, the publisher PeerJ for example allows links to a specific paragraph - e.g. <a href="http://dx.doi.org/10.7717/peerj.500#p-15">http://dx.doi.org/10.7717/peerj.500#p-15</a> - and allows users to <a href="http://blog.peerj.com/post/62886292466/peerj-questions-a-new-way-to-never-publish-forget">ask a question</a> right next to that section.</p><p>The examples above all use MIME type <code>text/html</code>, as this is what the example DOIs resolve to by default. I don’t if and how publishers have implemented fragment identifiers for other formats such as PDF or ePub, and what happens if you combine fragment identifiers with <a href="http://www.crosscite.org/cn/">content negotiation</a>. The shortDOI service works with fragment identifiers as well: <a href="http://doi.org/pxd#decision-letter">http://doi.org/pxd#decision-letter</a>. Another interesting question would be how fragment identifiers are handled for datasets. Typically separate DOIs are assigned for multiple related datasets, but there could also be a place for fragment identifiers as well, e.g. to specify a subset via a date range. The solution depends again on the content type, and the popular <code>text/csv</code> is unfortunately not well suited for this, whereas JSON – using <a href="http://tools.ietf.org/html/rfc6901">JSON Pointer</a> – would work well.</p><p><em>Update 8/2/14: <a href="https://twitter.com/ldodds">Leight Dodds</a> points out that handling the fragment identifier is up to the client and the fragment identifier is not sent to the server. Acrobat reader for example supports the <code>#page=</code> fragment identifier. He also mentions that there is a <a href="http://tools.ietf.org/html/rfc7111">RFC7111</a> for fragment identifiers for the text/csv media type - browsers in the future might support something like <code><a href="http://example.com/data.csv#row=5-7" rel="nofollow">http://example.com/data.csv#row=5-7</a></code>.</em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[One Ring to Rule them All]]></title>
            <link>https://blog.martinfenner.org/posts/one-ring-to-rule-them-all</link>
            <guid>57359c3c-929d-492b-957f-03e3ca0ef30b</guid>
            <pubDate>Wed, 30 Jul 2014 15:29:00 GMT</pubDate>
            <description><![CDATA[> One Ring to rule them all, One Ring to find them, One Ring to bring them all and
in the darkness bind them.
Yesterday 60 years ago the first volume of the Lord of the Rings trilogy by 
J.R.R. Tolkien was published. The quote above obviously doesn’t quiet apply to
scholarly publishing, but one recurring theme that I have often heard in the
last few years is that of a need for a canonical digital document format for
scholarly content that rules all other formats.

Document formats in scholarly P]]></description>
            <content:encoded><![CDATA[<blockquote>One Ring to rule them all, One Ring to find them, One Ring to bring them all and in the darkness bind them.</blockquote><p>Yesterday 60 years ago the first volume of the <em>Lord of the Rings</em> trilogy by <em>J.R.R. Tolkien</em> was published. The quote above obviously doesn’t quiet apply to scholarly publishing, but one recurring theme that I have often heard in the last few years is that of a need for a canonical digital document format for scholarly content that rules all other formats.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/rings.png" class="kg-image" alt="Document formats in scholarly Publishing"><figcaption>Document formats in scholarly Publishing</figcaption></figure><p>A few years ago almost everyone you would have said that <code>xml</code> is that format, with the NLM Archiving and Interchange Tag Suite - which has evolved into <a href="http://jats.nlm.nih.gov/publishing/">JATS</a> - probably the most commonly used Document Type Definition (DTD). <code>xml</code> does many things really well, but also has important shortcomings, most importantly that it is probably not a good format for authors (and don’t tell me that <code>docx</code> and <code>odt</code> are XML-based). We therefore don’t really expect authors to submit manuscripts in JATS <code>xml</code>, but rather convert documents into this format after a manuscript has been accepted for publication. This conversion step is often time-consuming and labor-intensive.</p><p>More recently <code>html</code> has become the most interesting candidate for a canonical scholarly document format. The big advantage over <code>xml</code> is that <code>html</code> - or at least <code>html5</code> which is most popular today - is an attractive format for online authoring tools (that is why <code>html</code> is listed both as input and output format) The downside of this flexibility is that it is much harder to embed structure and metadata into <code>html5</code> compared to <code>xml</code>. There are initiatives such as <a href="http://schema.org/">schema.org</a> and <a href="https://github.com/oreillymedia/HTMLBook">HTMLBook</a> that hope to change that, but we aren’t quiet there yet.</p><p>Or maybe we should learn from Tolkien and give up on the idea of a canonical document format and rather spend our energy on building tools that make it easier to transition from one format to another. <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a> is such as tool, but can’t do all the required conversions, e.g. it can’t yet use <code>docx</code> as input. The downside here is that every file conversion runs the risk of loosing important information. But the increase in flexibility hopefully outweights these shortcomings.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Don't Reinvent the Wheel]]></title>
            <link>https://blog.martinfenner.org/posts/dont-reinvent-the-wheel</link>
            <guid>f93ac598-d96f-40ec-85ec-e917098eccfe</guid>
            <pubDate>Thu, 24 Jul 2014 15:33:00 GMT</pubDate>
            <description><![CDATA[In a post last week
[http://blog.martinfenner.org/2014/07/18/roads-not-stagecoaches/] I talked about
roads and stagecoaches, and how work on scholarly infrastructure can often be
more important than building customer-facing apps. One important aspect of that
infrastructure work is to not duplicate efforts.

Image by Cocoabiscuit on Flickr
[http://www.flickr.com/photos/jfgallery/5673321593/]A good example is
information (or metadata) about scholarly publications. I am the technical lead
for the o]]></description>
            <content:encoded><![CDATA[<p>In a <a href="http://blog.martinfenner.org/2014/07/18/roads-not-stagecoaches/">post last week</a> I talked about roads and stagecoaches, and how work on scholarly infrastructure can often be more important than building customer-facing apps. One important aspect of that infrastructure work is to not duplicate efforts.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/5673321593_e6a7faa36d_z.jpg" class="kg-image" alt="Image by Cocoabiscuit on Flickr"><figcaption>Image by Cocoabiscuit <a href="http://www.flickr.com/photos/jfgallery/5673321593/" style="box-sizing: border-box; background: transparent; color: rgb(52, 152, 219); text-decoration: none;">on Flickr</a></figcaption></figure><p>A good example is information (or metadata) about scholarly publications. I am the technical lead for the open source <a href="http://articlemetrics.github.io/">article-level metrics (ALM) software</a>. This software can be used in different ways, but most people use it for tracking the metrics of scholarly articles, with articles that have DOIs issued by CrossRef. The ALM software needs three pieces of information for every article: <strong><strong>DOI</strong></strong>, <strong><strong>publication date</strong></strong>, and <strong><strong>title</strong></strong>. This information can be entered via a web interface, but that is of course not very practical for adding dozens or hundreds of articles at a time. The ALM software has therefore long supported the import of multiple articles via a text file and the command line.</p><p>This approach is working fine for the ALM software <a href="http://articlemetrics.github.io/plos/">running at PLOS since 2009</a>, but is for example a problem if the ALM software runs as a service for multiple publishers. A more flexible approach is to provide an API to upload articles, and I’ve <a href="http://articlemetrics.github.io/docs/api/">added an API</a> for creating, updating and deleting articles in January 2014.</p><p>While the API is an improvement, it still requires the integration into a number of possibly very different publisher workflows, and you have to deal with setting up the permissions, e.g. so that publisher A can’t delete an article from publisher B.</p><p>The next ALM release (3.3) will therefore add a third approach to importing articles: using the <a href="http://api.crossref.org/">CrossRef API</a> to look up article information. Article-level metrics is about tracking already published works, so we really only care about articles that have DOIs registered with CrossRef and are therefore published. ALM is now talking to a single API, and this makes it much easier to do this for a number of publishers without writing custom code. Since ALM is an open source application already used by several publishers that aspect is important. And because we are importing, we have don’t have to worry about permissions. The only requirement is that CrossRef has the correct article information, and has this information as soon as possible after publication.</p><p>At this point I have a confession to make: I regularly use other CrossRef APIs, but wasn’t aware of <strong><strong>api.crossref.org</strong></strong> until fairly recently. That is sort of understandable since the reference platform was deployed only September last year. The documentation to get you started is on <a href="https://github.com/CrossRef/rest-api-doc/blob/master/rest_api.md">Github</a> and the version history shows frequent API updates (now at v22). The API will return all kinds of information, e.g.</p><ul><li>how many articles has publisher x published in 2012</li><li>percentage of DOIs of publisher Y that include at least one ORCID identifier</li><li>list all books with a Creative Commons CC-BY license that were published this year</li></ul><p>Funder (via FundRef) information is also included, but is still incomplete. Another interesting result is the number of <a href="http://blogs.plos.org/mfenner/2011/03/26/direct-links-to-figures-and-tables-using-component-dois/">component DOIs</a> (DOIs for figures, tables or other parts of a document) per year:</p><figure class="kg-card kg-embed-card"><iframe src="http://cf.datawrapper.de/Ze7et/1/" frameborder="0" allowtransparency="true" allowfullscreen="allowfullscreen" webkitallowfullscreen="webkitallowfullscreen" mozallowfullscreen="mozallowfullscreen" oallowfullscreen="oallowfullscreen" msallowfullscreen="msallowfullscreen" width="640" height="480" style="box-sizing: border-box; color: rgb(0, 0, 0); font-family: ff-tisa-web-pro, Georgia, serif; font-size: 21px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"></iframe></figure><p>For my specific use case I wanted an API call that returns all articles published by PLOS (or any other publisher) in the last day which I can then run regularly. To get all DOIs from a specific publisher, use their CrossRef member ID - DOI prefixes don’t work, as publishers can own more than one DOI prefix. To make this task a little easier I built a CrossRef member search interface into the ALM application:</p><figure class="kg-card kg-image-card"><img src="http://blog.martinfenner.org/images/crossref_api.png" class="kg-image" alt></figure><p>We can filter API responses by publication date, but it is a better idea to use the update date, as it is possible that the metadata have changed, e.g. a correction of the title. We also want to increase the number of results per page (using the <code>rows</code> parameter). The final API call for all DOIs updated by PLOS since the beginning of the week would be</p><pre><code>http://api.crossref.org/members/340/works?filter=from-update-date:2014-07-21,until-update-date:2014-07-24&amp;rows=1000</code></pre><p>The next step is of course to parse the JSON of the API response, and you will notice that CrossRef is using <a href="http://gsl-nagoya-u.net/http/pub/citeproc-doc.html">Citeproc JSON</a>. This is a standard JSON format for bibliographic information used internally by several reference managers for citation styles, but increasingly also by APIs and other places where you encounter bibliographic information.</p><p>Citeproc JSON is helpful for one particular problem with CrossRef metadata: the exact publication date for an article is not always known, and CrossRef (and similarly DataCite) only requires the publication year. Citeproc JSON can nicely handle partial dates, e.g. year-month:</p><pre><code>issued: {
  date-parts: [
    [
      2014,
      7
    ]
  ]
},</code></pre><p>I think that a similar approach will work for many other systems that require bibliographic information about scholarly content with CrossRef DOIs. If are not already using <strong><strong>api.crossref.org</strong></strong>, consider integrating with it, I find the API fast, well documented, easy to use - and CrossRef is very responsive to feedback. As you can always wish for more, I would like to see the following: fix the problem were some journal articles are missing the publication date (a required field, even if only the year), and consider adding the canonical URL to the article metadata (which ALM currently has to look up itself, and which is needed to track social media coverage of an article).</p><p><em>Update July 24, 2014: added chart with number of component DOIs per year</em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Build Roads not Stagecoaches]]></title>
            <link>https://blog.martinfenner.org/posts/build-roads-not-stagecoaches</link>
            <guid>068cf033-cea0-469d-9292-0316cfc0f661</guid>
            <pubDate>Fri, 18 Jul 2014 15:36:00 GMT</pubDate>
            <description><![CDATA[I attended the Open Knowledge Festival [http://2014.okfestival.org/] this week
and I had a blast. For three days (I also attended the fringe event csv,conf
[http://csvconf.com/] on Tuesday) I listed to wonderful presentations and was
involved in great discussions - both within sessions, but more importantly all
the informal discussions between and after sessions.

Of all the things that were discussed I want to pick one theme that resonated in
particular with me. It surfaced in many places, but ]]></description>
            <content:encoded><![CDATA[<p>I attended the <a href="http://2014.okfestival.org/">Open Knowledge Festival</a> this week and I had a blast. For three days (I also attended the fringe event <a href="http://csvconf.com/">csv,conf</a> on Tuesday) I listed to wonderful presentations and was involved in great discussions - both within sessions, but more importantly all the informal discussions between and after sessions.</p><figure class="kg-card kg-image-card"><img src="https://martinfenner.ghost.io/images/okfest-2014-logo.png" class="kg-image" alt></figure><p>Of all the things that were discussed I want to pick one theme that resonated in particular with me. It surfaced in many places, but was articulated particularly well by <a href="https://twitter.com/erichysen">Eric Hysen</a> - who heads the <a href="http://www.google.com/elections/ed/us">Google Politics &amp; Elections Group</a> - in his keynote yesterday (starting at 54:52, but please also watch the keynote by Neelie Kroes, Vice-President of the European Commission):</p><figure class="kg-card kg-embed-card"><iframe width="356" height="200" src="https://www.youtube.com/embed/0UNRZEsLxKc?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><p>In his keynote he described how travel from Cambridge to London in the 18th and early 19th century improved mainly as a result of better roads, made possible by changes in how these roads are financed. Translated to today, he urged the audience to think more about the infrastructure and less about the end products:</p><blockquote>Ecosystems, not apps – Eric Hysen</blockquote><p>On Tuesday at <a href="http://csvconf.com/#nickstenning">csv,conf</a>, <a href="https://twitter.com/nickstenning">Nick Stenning</a> - Technical Director of the Open Knowledge Foundation - talked about <a href="http://dataprotocols.org/data-packages/">data packages</a>, an evolving standard to describe data that are passed around betwen different systems. He used the metaphor of containers, and how they have dramatically changed the transportation of goods in the last 50 years. He <a href="https://github.com/nickstenning/put-it-in-a-box">argued</a> that the cost of shipping was in large part determined by the cost of loading and unloading, and the container has dramatically changed that equation. We are in a very similar situation with datasets, where most of the time is spent translating between different formats, joining things together that use different names for the same thing, etc.</p><figure class="kg-card kg-image-card kg-width-wide kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/640px-Hafenarbeiter_bei_der_Verladung_von_Sackgut_-_MS_Rothenstein_NDL-_Port_Sudan_1960.png" class="kg-image" alt srcset="https://martinfenner.ghost.io/content/images/size/w600/2021/01/640px-Hafenarbeiter_bei_der_Verladung_von_Sackgut_-_MS_Rothenstein_NDL-_Port_Sudan_1960.png 600w, https://martinfenner.ghost.io/content/images/2021/01/640px-Hafenarbeiter_bei_der_Verladung_von_Sackgut_-_MS_Rothenstein_NDL-_Port_Sudan_1960.png 640w"><figcaption><a href="https://commons.wikimedia.org/wiki/File:Hafenarbeiter_bei_der_Verladung_von_Sackgut_-_MS_Rothenstein_NDL,_Port_Sudan_1960.png">Wikimedia Commons</a> image used in <a href="https://github.com/nickstenning/put-it-in-a-box">Nick Stelling's presentation</a></figcaption></figure><p>What the two presentations have in common is not only that they link the building of an open digital infrastructure to important transforming events in the history of transportation, but also the emphasis on the building blocks rather than the finished product. When I thought more about this I realized that these building blocks are exactly the projects I get most excited about, i.e. projects that develop standards or provide APIs or libraries. Some examples would be</p><ul><li><a href="http://orcid.org/">ORCID</a>: unique identifiers for scholarly authors</li><li><a href="http://citationstyles.org/">Citation Style Language</a>: a language to describe the formatting of citations and bibliographies</li><li><a href="https://github.com/jgm/pandoc">Pandoc</a>: a universal document converter</li><li><a href="http://ropensci.org/">rOpenSci</a>: packages for the statistical programming language R to access data repositories</li><li><a href="http://www.niso.org/topics/tl/altmetrics_initiative/">NISO Alternative Assessment Metrics</a>: standards and best practices for novel scholarly metrics</li><li><a href="http://www.re3data.org/">re3data</a>: a registry of research data repositories</li><li><a href="http://creativecommons.org/">Creative Commons</a>: copyright licenses for creative works</li><li><a href="https://github.com/articlemetrics/alm">ALM</a>: software to collect comprehensive information about the discussion of scholarly articles on the web</li></ul><p>This list doesn’t include all the generic software needed to build open science tools, with <strong><strong>git</strong></strong> being a perfect example. The last project is obviously the project I have been working on the past two years for PLOS, but I have tried to support the other projects mentioned in various ways from small code contributions to promotion via this blog and presentations, or direct work in these projects. But strangely enough, I haven’t really realized this until now.</p><p>Not surprisingly infrastructure, servers, libraries and other building blocks are exactly the areas where open source software has been most successful so far, and this is of course a core part of the UNIX philosophy of building parts that work well together rather than big monolithic programs that do everything.</p><h2 id="next">Next</h2><p>We need more <strong><strong>Open Science Infrastructure</strong></strong> and it is the stuff that I really care about. I think we need to better support those projects that build these essential building blocks via advice, cooperation, promotion, and financial support. I am willing to help with that effort, and I have started to think how I can best contribute.</p><p>On the other hand there are many great open science projects that don’t fall in this category, maybe even the majority of them. I wish them good luck, but I would advice them to think more about infrastructure, and whether there is a small area where they can focus on. It still amazes me how successful projects such as <strong><strong>Citation Style Language</strong></strong> and <strong><strong>Pandoc</strong></strong> have been with no or almost no funding and a very small core group of people doing the majority of the work. One critical ingredient is the total focus on a very specific problem that is both important and can be solved with specific actions. Too many open science projects want to solve too many problems at once, try to solve the exact same problems that many other parallel projects work on, don’t cooperate enough with those parallel projects, and require a critical mass of users to work.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Literate Blogging]]></title>
            <link>https://blog.martinfenner.org/posts/literate-blogging</link>
            <guid>17bdf428-a25e-45d8-b2a0-a9f26ec1596b</guid>
            <pubDate>Mon, 14 Apr 2014 15:39:00 GMT</pubDate>
            <description><![CDATA[> Literate programming is a methodology that combines a programming language with
a documentation language, thereby making programs more robust, more portable,
more easily maintained, and arguably more fun to write than programs that are
written only in a high-level language. The main idea is to treat a program as a
piece of literature, addressed to human beings rather than to a computer. The
program is also viewed as a hypertext document, rather like the World Wide Web.
Literatue Programming by]]></description>
            <content:encoded><![CDATA[<blockquote>Literate programming is a methodology that combines a programming language with a documentation language, thereby making programs more robust, more portable, more easily maintained, and arguably more fun to write than programs that are written only in a high-level language. The main idea is to treat a program as a piece of literature, addressed to human beings rather than to a computer. The program is also viewed as a hypertext document, rather like the World Wide Web.</blockquote><p>Literatue Programming by <a href="http://www-cs-faculty.stanford.edu/~uno/">Donald Knuth</a> (1983) is a seminal book that introduces the concept of literate programming. Using technology available in 2014 we can make a small but important change to the last sentence:</p><blockquote>The program is also viewed as a hypertext document on the World Wide Web.</blockquote><p>This blog post is an example for such a document. The page is written in <strong><strong>markdown</strong></strong> (markdown file available <a href="https://github.com/mfenner/mfenner.github.io/blob/source/_posts/2014-04-04-literate-blogging.Rmd">here</a>), and all embedded code was executed when this page was generated, i.e. when the markdown was converted to HTML and the blog post was published. To demonstrate this I have embedded code in three different languages below - the output is the second code block.</p><p>In R you have</p><pre><code>cat('Hello, R world!\n')</code></pre><pre><code>Hello, R world!</code></pre><p>Or Python</p><pre><code>print "Hello, Python world!"</code></pre><pre><code>Hello, Python world!</code></pre><p>Or Ruby</p><pre><code>puts 'Hello, Ruby world!'</code></pre><pre><code>Hello, Ruby world!</code></pre><p>You can also embed code within text blocks (inline), so that <code>3.48 * 723</code> becomes <strong><strong>2516.04</strong></strong>. Another important option is to generate figures using the embedded code, e.g. the following figure taken from a recent publication.</p><pre><code># code for figure 1: density plots for citation counts for PLOS Biology
# articles published in 2010

# load May 20, 2013 ALM report
alm &lt;- read.csv("data/alm_report_plos_biology_2013-05-20.csv", stringsAsFactors = FALSE)

# only look at research articles
alm &lt;- subset(alm, alm$article_type == "Research Article")

# only look at papers published in 2010
alm$publication_date &lt;- as.Date(alm$publication_date)
alm &lt;- subset(alm, alm$publication_date &gt; "2010-01-01" &amp; alm$publication_date &lt;=
    "2010-12-31")

# labels
colnames &lt;- dimnames(alm)[[2]]
plos.color &lt;- "#1ebd21"
plos.source &lt;- "scopus"

plos.xlab &lt;- "Scopus Citations"
plos.ylab &lt;- "Probability"

quantile &lt;- quantile(alm[, plos.source], c(0.1, 0.5, 0.9), na.rm = TRUE)

# plot the chart
opar &lt;- par(mai = c(0.5, 0.75, 0.5, 0.5), omi = c(0.25, 0.1, 0.25, 0.1), mgp = c(3,
    0.5, 0.5), fg = "black", cex.main = 2, cex.lab = 1.5, col = plos.color,
    col.main = plos.color, col.lab = plos.color, xaxs = "i", yaxs = "i")

d &lt;- density(alm[, plos.source], from = 0, to = 100)
d$x &lt;- append(d$x, 0)
d$y &lt;- append(d$y, 0)
plot(d, type = "n", main = NA, xlab = NA, ylab = NA, xlim = c(0, 100), frame.plot = FALSE)
polygon(d, col = plos.color, border = NA)
mtext(plos.xlab, side = 1, col = plos.color, cex = 1.25, outer = TRUE, adj = 1,
    at = 1)
mtext(plos.ylab, side = 2, col = plos.color, cex = 1.25, outer = TRUE, adj = 0,
    at = 1, las = 1)

par(opar)</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/figures/density_plot_example-1.svg" class="kg-image" alt="Figure 1. Citation counts for PLOS Biology articles published in 2010. Scopus citation counts plotted as a probability distribution for all 197 PLOS Biology research articles published in 2010. Data collected May 20, 2013. Median 19 citations; 10% of papers have at least 50 citations. From Fenner (2013)."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Figure 1. Citation counts for PLOS Biology articles published in 2010.</strong> Scopus citation counts plotted as a probability distribution for all 197 <em style="box-sizing: border-box;">PLOS Biology</em> research articles published in 2010. Data collected May 20, 2013. Median 19 citations; 10% of papers have at least 50 citations. From Fenner <span class="citation" data-cites="fenner2013" style="box-sizing: border-box;">(2013)</span>.</figcaption></figure><p>All this functionality is provided by <a href="http://yihui.name/knitr/">knitr</a>, a package for the R statistical programming language. knitr has been around for a while, but integration into the <a href="http://jekyllrb.com/">Jekyll</a> blogging platform is still fragile. Earlier this week at the <a href="https://github.com/ropensci/hackathon">rOpenSci hackathon</a> (more on this later) a group of us worked hard to improve this integration. We are still not completely done, but the source code is available <a href="https://github.com/ropensci/docs">here</a>. Most importantly, all the conversion happens on the server, and we are only using freely available tools. I have now enabled this functionality for this blog, so expect more code embedded examples in the future.</p><h2 id="references">References</h2><p>Fenner, M. (2013). What can article-level metrics do for you? <em>PLoS Biol</em>, <em>11</em>(10), e1001687. <a href="http://doi.org/10.1371/journal.pbio.1001687">doi:10.1371/journal.pbio.1001687</a></p><p>Knuth, D. E., Stanford University, &amp; Computer Science Department. (1983). <em>Literate programming</em>. Stanford, CA: Dept. of Computer Science, Stanford University.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Continuous Publishing]]></title>
            <link>https://blog.martinfenner.org/posts/continuous-publishing</link>
            <guid>8550cc6b-45e9-4621-aad6-fadf592f7681</guid>
            <pubDate>Mon, 10 Mar 2014 15:44:00 GMT</pubDate>
            <description><![CDATA[Earlier this week Björn Brembs wrote in a blog post (What Is The Difference
Between Text, Data And Code?
[http://bjoern.brembs.net/2014/03/what-is-the-difference-between-text-data-and-code/]
):

> To sum it up: our intellectual output today manifests itself in code, data and
text.
The post is about the importance of publication of data and software where
currently the rewards are stacked disproportionately in favor of text
publications. The intended audience is probably mainly other scientists (]]></description>
            <content:encoded><![CDATA[<p>Earlier this week Björn Brembs wrote in a blog post (<a href="http://bjoern.brembs.net/2014/03/what-is-the-difference-between-text-data-and-code/">What Is The Difference Between Text, Data And Code?</a>):</p><blockquote>To sum it up: our intellectual output today manifests itself in code, data and text.</blockquote><p>The post is about the importance of publication of data and software where currently <em>the rewards are stacked disproportionately in favor of text publications</em>. The intended audience is probably mainly other scientists (Björn is a neurobiologist) who are reluctant to publish data and/or code, but there is another interesting aspect to this.</p><p>Just as scientific publication increasingly means more than just text and includes data and software, we are also increasingly seeing tools and methodologies common in software development applied to scientific publishing. This in particular includes the ideas behind Open Source software (which shares many commonalities with Open Access and Open Science), but also tools like the git version control system (<a href="http://marciovm.com/i-want-a-github-of-science/">We Need a Github of Science</a>) or the markdown markdown language (<a href="http://blog.martinfenner.org/2012/12/13/a-call-for-scholarly-markdown/">A Call for Scholarly Markdown</a>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/Agile-vs-iterative-flow.jpg" class="kg-image" alt="Delivery, image from Wikimedia Commons."><figcaption>Delivery, image from <a href="http://commons.wikimedia.org/wiki/File:Agile-vs-iterative-flow.jpg" style="box-sizing: border-box; background: transparent; color: rgb(52, 152, 219); text-decoration: none;">Wikimedia Commons</a>.</figcaption></figure><p>Continuous Delivery is another concept increasingly popular in software development that has many implications on how research can be performed and reported. Martin Fowler describes it as:</p><blockquote>Continuous Delivery is a software development discipline where you build software in such a way that the software can be released to production at any time.</blockquote><p>The concept of frequent small releases is of course familiar to everyone practicing <a href="http://usefulchem.wikispaces.com/">Open Notebook Science</a>, writing science blogs, presenting preliminary data at conferences or publishing <a href="http://arxiv.org/">preprints</a>, and is even relevant to <a href="http://www.crossref.org/crossmark/">CrossMark</a>, a service that tracks corrections, enhancements and other changes of scholarly documents.</p><p>When you read the definition given by Martin Fowler carefully, you see that Continuous Delivery is about more than the frequency of software updates – it is in fact about improving the process of releasing software. The scientific publication is the corresponding event in science, and I think that nobody would argue with me that the experience publishing a paper is too complex, time-consuming and often frustrating. The focus here is not on the time it takes to do peer review, or the multiple revisions needed before a manuscript is accepted. I am talking about the pain submitting a manuscript, the back and forth regarding file formats, citation styles and other technical requirements, the reformatting of manuscripts, and also the time it takes from accepting a manuscript to finally publishing it online.</p><p>I would argue that the main reason publishing is so painful for everyone involved is that it is still very much a manual process. Just as software development is creative work, but still can benefit tremendously from tools such as automated tests and build tools, we can apply the same principles to scientific publishing. This means that everything that can be automated should be automated so that we can focus on those areas that need human judgement. The mistake that I think is commonly made is that automation for many publishers means automation for the publisher, with even more work for the author who submits a manuscript. A good example is that authors are increasingly asked to submit publication-ready manuscripts even though typesetting and desktop publishing is not their area of expertise and the manuscript text will be very different after one or more rounds of revision. The pain of processing manuscripts into something that can be published was summarized perfectly by typesetter and friend Kaveh Bazargan at the <a href="http://www.youtube.com/watch?feature=player_embedded&amp;v=CGkcsvofjdg">SpotOn London 2012 Conference</a> (via <a href="http://rossmounce.co.uk/2012/11/19/yet-another-solo12-recap-part2/">Ross Mounce’s blog</a>):</p><blockquote>It’s madness really. I’m here to say I shouldn’t be in business.</blockquote><p>The promise of Continuous Delivery for publishing is to develop tools and best practices that make the process of publication faster, with better quality, and less frustrating. Continuous Integration (<a href="http://martinfowler.com/articles/continuousIntegration.html">again Martin Fowler</a>) is an important part of Continuous Delivery and means frequently merging all developer working copies of a software project into a central repository, combined with running automated unit tests and software builds using an integration server.</p><p>We can apply Continuous Integration to scholarly documents - instead of automated tests and software builds we can automate the transformation of documents into <a href="http://blog.martinfenner.org/2013/12/12/from-markdown-to-jats-xml-in-one-step/">JATS XML</a> and other output formats, and we can automate the process of checking for required metadata, correct file formats for images, etc. And we can use the same software tools for this, many of which are freely available to Open Source projects.</p><p>As an example of how this can be done <a href="https://github.com/mfenner/jekyll-travis">I have integrated</a> the <a href="https://travis-ci.org/">Travis CI</a> Continuous Integration server with the book project <a href="http://book.openingscience.org/">Opening Science</a>. The recently published book is a dynamic book that hopefully is updated frequently in the coming months. Every time an editor approves a correction to the text - <a href="https://github.com/openingscience/book">hosted in markdown format on Github</a> - the Travis CI server is automatically triggered to build a new HTML version of the book and to push the new version to the book website. The Travis server is running the <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a> document converter to not only convert the changed document from markdown to HTML, but Pandoc will also insert and format references, and the <a href="http://jekyllrb.com/">Jekyll</a> site generator will build a nice website around the markdown files. Over time this build process can be extended to do other things as well, from <a href="http://blog.martinfenner.org/2013/07/02/auto-generating-links-to-data-and-resources/">auto-generating links to data and resources</a> to transforming the document into <a href="http://blog.martinfenner.org/2013/12/12/from-markdown-to-jats-xml-in-one-step/">other file formats</a> besides HTML.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Are static Websites the Future or the Past?]]></title>
            <link>https://blog.martinfenner.org/posts/are-static-websites-the-future-or-the-past</link>
            <guid>aedec2a6-2cea-4319-b72a-0ff1c0d03f5e</guid>
            <pubDate>Wed, 05 Mar 2014 15:47:00 GMT</pubDate>
            <description><![CDATA[Last week I had a little discussion on Twitter about a great blog post by Zach
Holman: Only 90s Web Developers Remember This
[http://zachholman.com/posts/only-90s-developers/]. The post is not only fun to
read, but also reminded me that it is now almost 20 years (1995) that I built my
first website - of course using some of the techniques (the one pixel gif!, the 
&nbsp; tag!) described in the post.

ScriptWeb logo 1995We started ScriptWeb back in 1995 as a central resource for
scripting on the ]]></description>
            <content:encoded><![CDATA[<p>Last week I had a little discussion on Twitter about a great blog post by Zach Holman: <a href="http://zachholman.com/posts/only-90s-developers/">Only 90s Web Developers Remember This</a>. The post is not only fun to read, but also reminded me that it is now almost 20 years (1995) that I built my first website - of course using some of the techniques (the one pixel gif!, the <code>&amp;nbsp;</code> tag!) described in the post.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/scriptwebtitle.gif" class="kg-image" alt><figcaption>ScriptWeb logo 1995</figcaption></figure><p>We started ScriptWeb back in 1995 as a central resource for scripting on the Mac (Applescript and Frontier). It was a nice collaborative effort and I was resposible for a directory of scripting additions (or osaxen), joining forces with MacScripter.net a few years later:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/osaxen.png" class="kg-image" alt><figcaption>Scripting Additions at Macscripter.net 2000</figcaption></figure><p>Since then I have built many other websites for fun and work, adapting to how technology changed over the years:</p><ul><li>1995: website running on a Mac Quadra 610 using the WebSTAR HTTP server and server side includes (<a href="https://en.wikipedia.org/wiki/Server_Side_Includes">SSI</a>)</li><li>1995: static site generation with outline navigation using Applescript. FTP to transfer files</li><li>1995: Visual HTML editors (Adobe PageMill 1.0)</li><li>1999: database server and application layer (too long ago to remember the technology)</li><li>2001: Open source database and application code with MySQL and PHP. CVS for version control</li><li>2001: web frameworks with PHP and MySQL: PostNuke and Xaraya</li><li>2005: more complex web application frameworks: Ruby on Rails. Subversion version control</li><li>2008: git for version control</li><li>2011: more complex frontend Javascript</li><li>2013: static site generator Jekyll</li></ul><p>Since last June this blog is running on Github pages and the site is generated with <a href="http://jekyllrb.com/">Jekyll</a>. Jekyll works really well to build static websites such as this blog, but I am increasingly using it for more complex projects, e.g. for the online version of a <a href="http://book.openingscience.org/">book on Open Science</a>.</p><p>What I find interesting in this timeline is that with Jekyll there is a shift in focus. Rather than building even more complex web pages that are generated dynamically by the server, we are going back to a two-stage process where the HTML pages are built first and then served as HTML, CSS and Javascript without any database or server application layer. Doesn’t sound too different from what we did in the 1990s. This approach obviously works well for content-heavy sites like this blog or book chapters, not so much for dynamically generated content that changes every few minutes, or where the page is put together from many different page fragments.</p><p>What I don’t know, and I am really interested to find out, is how well this scales to larger sites, specifically publisher websites that host thousands of scholarly journal articles - again content that is very text-heavy and doesn’t change that much. The potential benefits of replacing the paradigm of a database layer that holds all content with a paradigm that stores all content in files managed by git version control are clear: serving the content on the web becomes less complex, cheaper and faster. The tradeoff is of course that generating the static content becomes more complex and time-consuming, and it can become a challenge to mix the static content with dynamic content generated by servers as well as the user’s browser. For a now infamous example using this technology, look no further than <a href="http://www.huffingtonpost.com/john-pavley/obamacare-website-problems_b_4057618.html">Heathcare.gov</a>. I don’t know enough details to understand what went wrong, and it might have more to do with the scale of the project and the tight timeline to launch. For scholarly journal articles this might be a reasonable approach, as even when there is no longer a printed version of the journal, articles are still published on a specific date, and changing the content is a very formal process.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/3781208877_936e1a162c_z.jpg" class="kg-image" alt srcset="https://martinfenner.ghost.io/content/images/size/w600/2021/01/3781208877_936e1a162c_z.jpg 600w, https://martinfenner.ghost.io/content/images/2021/01/3781208877_936e1a162c_z.jpg 640w"><figcaption>Netscape Navigator 1. <a href="http://www.flickr.com/photos/bump/3781208877/">Flickr photo</a> by bump</figcaption></figure>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Six Misunderstandings about Scholarly Markdown]]></title>
            <link>https://blog.martinfenner.org/posts/six-misunderstandings-about-scholarly-markdown</link>
            <guid>8271bf5c-523e-4c4c-907f-a58f9fd09ae8</guid>
            <pubDate>Mon, 03 Mar 2014 15:50:00 GMT</pubDate>
            <description><![CDATA[In this post I want to talk about some of the misunderstandings I frequently
encounter when discussing markdown as a format for authoring scholarly documents
[http://blog.martinfenner.org/2013/06/17/what-is-scholarly-markdown/].

Scholars will always use Microsoft Word
Microsoft Word is of course what almost all authors use in the life sciences and
many other disciplines. One big reason for this is the file formats accepted my
manuscript submission systems. By limiting the options to Microsoft W]]></description>
            <content:encoded><![CDATA[<p>In this post I want to talk about some of the misunderstandings I frequently encounter when discussing <a href="http://blog.martinfenner.org/2013/06/17/what-is-scholarly-markdown/">markdown as a format for authoring scholarly documents</a>.</p><h2 id="scholars-will-always-use-microsoft-word">Scholars will always use Microsoft Word</h2><p>Microsoft Word is of course what almost all authors use in the life sciences and many other disciplines. One big reason for this is the file formats accepted my manuscript submission systems. By limiting the options to Microsoft Word (and maybe LaTeX), you make it impossible for authors to use other tools, even if they wanted to. Publishers should accept manuscripts in any reasonable file format, as I have <a href="http://blog.martinfenner.org/2013/11/17/the-grammar-of-scholarly-communication/">argued before</a>.</p><h2 id="my-markup-language-is-better-than-markdown">My markup language is better than markdown</h2><p>There are of course numerous alternatives to markdown, including <a href="http://txstyle.org/">Textile</a>, <a href="http://www.methods.co.nz/asciidoc/">AsciiDoc</a>, <a href="http://www.mediawiki.org/wiki/Help:Formatting">MediaWiki Markup</a> and <a href="http://docutils.sourceforge.net/docs/ref/rst/introduction.html">reStructuredText</a>. There will always be features that are better implemented in one of these languages, but I don’t think there is room for more than one major initiative for a scholarly markup language. And markdown has the right mix of features and broad support by tools and the community.</p><p>Related to this there is the argument against markdown that the format <a href="http://blog.codinghorror.com/the-future-of-markdown/">is a mess</a> and that there are too many versions (or flavors) of it. While that is certainly a big problem with markdown, I would argue that with <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a> we have a nice standard and reference implementation for Scholarly Markdown. Pandoc is constantly evolving, and the addition of support for arbitrary YAML metadata was the biggest new feature in 2013 for me.</p><h2 id="scholarly-markdown-is-too-complex-and-we-might-as-well-use-latex">Scholarly Markdown is too complex and we might as well use LaTeX</h2><p>“LaTeX is a high-quality typesetting system; it includes features designed for the production of technical and scientific documentation” (The LaTeX Team, 2014). Although LaTeX has solved many of the problems Scholarly Markdown tries to tackle a long time ago, it is still something else. LaTeX at its core is a typesetting system, which is not something Scholarly Markdown cares about for two reasons: a) the focus is on authoring documents, which are then submitted to other systems at publishers and elsewhere that are specialized in producing the final document, and b) the focus is on HTML and the web as this is where we want most of the interactions with scholarly documents to take place. This means that</p><ul><li>Markdown is a great input format to convert into other formats, including XML (see for example my <a href="https://github.com/mfenner/pandoc-jats">pandoc-jats</a>).</li><li>LaTeX will always be the best choice for some content, e.g. documents rich in mathematical formulas</li><li>If the ultimate goal was to produce high-quality PDF documents, Scholarly Markdown would be a bad choice. It is the right format for HTML and the related ePub.</li></ul><p>We have to be very careful that we keep the right balance of simplicity and features in Scholarly Markdown. This means that sometimes we should just include the LaTeX code, e.g. for math.</p><h2 id="scientists-need-a-wysiwyg-editor-and-then-the-file-format-doesn-t-matter">Scientists need a WYSIWYG Editor, and then the file format doesn’t matter</h2><p><a href="http://en.wikipedia.org/wiki/WYSIWYG">WYSIWYG</a> - What You See Is What You Get - is a user interface metaphor that is both a blessing and a curse. We desperately need better writing tools, and this of course also means user interfaces that help with that task. But the focus on creating a new authoring environment that focusses too much on WYSIWYG creates several problems:</p><ul><li>WYSIWYG is not always a good metaphor for scholarly documents. Typographic features such as fonts, line spacing, etc. are not something that belong into an authoring environment - this is done during the publishing step, as is the formatting of references according to a specific citation style.</li><li>WYSIWYG is for human interactions, but content in scholarly documents is increasingly created by computers. Two good examples are statistics and figures created in <a href="http://yihui.name/knitr/">R/knitr</a> or <a href="http://ipython.org/notebook.html">iPython Notebook</a>. Scholarly Markdown works perfectly with these workflows.</li><li>WYSIWYG authoring environments run the high risk of vendor lock-in. This is understandable if you run a startup and want to promote your tool, but is not in the best interest of the scholarly community.</li></ul><p>Version control via git is central to Scholarly Markdown, and this can also be challenging for a WYSIWYG environment. But there are many good examples of how to make this work.</p><h2 id="scientists-should-submit-their-manuscripts-in-jats-xml-the-standard-format-for-scholarly-documents">Scientists should submit their manuscripts in JATS XML, the standard format for scholarly documents</h2><p>At the end of the day most scholarly publications in the life sciences are converted into JATS XML. Unfortunately central aspects of the format (e.g. the required document structure or required attributes) are difficult to enforce in an authoring environment. Even if you build a tool that can nicely handle this, I’m not so sure we want to burden an author with this, especially since the manuscript will usually undergo a lot of changes before it is accepted and then published.</p><h2 id="the-future-is-html">The future is HTML</h2><p>Although the future for consuming scholarly documents is clearly HTML (and ePub), and there are great HTML editors, I’m not so sure that HTML will become the default for authoring environments. This is the reason why markdown and related markup languages were invented, and even with modern WYSIWYG editors working directly with HTML is not always the best choice. HTML has two problems: a) it is not as human-readable as markdown and therefore requires an additional layer for authoring, and b) it is not as structured as XML, which makes it difficult to create some of the rigid document structure required for scholarly documents. O’Reilly is trying to get more structure into HTML for print and digital books with <a href="https://github.com/oreillymedia/htmlbook">HTMLBook</a>, but with too much structure you might run into similar problems for authoring as discussed above for JATS XML. And of course you can include HTML in markdown documents.</p><h2 id="references">References</h2><p>The LaTeX Team. (2014). LaTeX - a document preparation system. <a href="http://www.latex-project.org/">http://www.latex-project.org/</a>. Retrieved from <a href="http://www.latex-project.org/">http://www.latex-project.org/</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[From Markdown to JATS XML in one Step]]></title>
            <link>https://blog.martinfenner.org/posts/from-markdown-to-jats-xml-in-one-step</link>
            <guid>bc1e02ba-67b3-444a-9dd4-f0a45ace76a6</guid>
            <pubDate>Thu, 12 Dec 2013 15:54:00 GMT</pubDate>
            <description><![CDATA[The Journal Article Tag Suite (JATS [http://jats.nlm.nih.gov/]) is a NISO
standard that defines a set of XML elements and attributes for tagging journal
articles. JATS is not only used for fulltext content at PubMed Central (and JATS
has evolved from the NLM Archiving and Interchange Tag Suite originally
developed for PubMed Central), but is also increasinly used by publishers.

For many publishers the version of record of an article is stored in XML, and
other formats (currently HTML, PDF and i]]></description>
            <content:encoded><![CDATA[<p>The Journal Article Tag Suite (<a href="http://jats.nlm.nih.gov/">JATS</a>) is a NISO standard that defines a set of XML elements and attributes for tagging journal articles. JATS is not only used for fulltext content at PubMed Central (and JATS has evolved from the NLM Archiving and Interchange Tag Suite originally developed for PubMed Central), but is also increasinly used by publishers.</p><p>For many publishers the <em>version of record</em> of an article is stored in XML, and other formats (currently HTML, PDF and increasingly ePub) are generated from this XML. Unfortunately the process of converting author-submitted manuscripts into JATS-compliant XML is time-consuming and costly, and this is a problem in particular for small publishers.</p><p>In a recent blog post (<a href="http://blog.martinfenner.org/2013/11/17/the-grammar-of-scholarly-communication/">The Grammar of Scholarly Communication</a>) I argued that publishers should accept manuscripts in any reasonable file format, including Microsoft Word, Open Office, LaTeX, Markdown, HTML and PDF. Readers of this blog know that I am a big fan of <a href="http://blog.martinfenner.org/tags.html#markdown-ref">markdown</a> for scholarly documents, but I am of course well aware that at the end of the day these documents have to be converted into JATS.</p><p>As a small step towards that goal I have today released the first public version of <a href="https://github.com/mfenner/pandoc-jats">pandoc-jats</a>, a <a href="http://johnmacfarlane.net/pandoc/README.html#custom-writers">custom writer for Pandoc</a> that converts markdown documents into JATS XML with a single command, e.g.</p><pre><code>pandoc -f example.md --filter pandoc-citeproc --bibliography=example.bib --csl=apa.csl -t JATS.lua -o example.xml</code></pre><p>Please see the <a href="https://github.com/mfenner/pandoc-jats">pandoc-jats</a> Github repository for more detailed information, but using this custom writer is as simple as downloading a single <code>JATS.lua</code>file. The big challenge is of course to make this custom writer work with as many documents as possible, and that will be my job the next few weeks. Two example JATS documents are below (both markdown versions of scholarly articles and posted on this blog as HTML):</p><ul><li>Nine simple ways to make it easier to (re)use your data (<a href="http://blog.martinfenner.org/2013/06/25/nine-simple-ways-to-make-it-easier-to-reuse-your-data/">HTML</a>, <a href="http://blog.martinfenner.org/files/10.7287.peerj.preprints.7v2.xml">JATS</a>)</li><li>What Can Article Level Metrics Do for You? (<a href="http://blog.martinfenner.org/2013/12/11/what-can-article-level-metrics-do-for-you/">HTML</a>, <a href="http://blog.martinfenner.org/files/10.1371.journal.pbio.1001687.xml">JATS</a>)</li></ul><p>Both JATS files were validated against the JATS DTD and XSD and showed no errors with the NLM XML StyleChecker - using the excellent <a href="https://github.com/PeerJ/jats-conversion">jats-conversion</a> conversion and validation tools written by Alf Eaton. Markdown is actually a nice file format to convert to XML - in contrast to HTML authors can’t for example put closing tags at the wrong places. And a Pandoc custom writer written in the Lua scripting language is an interesting alternative to XSLT transformations, the more common way to create JATS XML. The custom writer has not been tested with other Pandoc input formats besides markdown, of particular interest are of course HTML and LaTeX - Microsoft Word .docx is unfortunately only a Pandoc output format.</p><p>This is the first public release and there is of course a lot of room for improvement. Many elements and attributes are not yet supported - although <a href="http://orcid.org/blog/2013/03/22/orcid-how-more-specifying-orcid-ids-document-metadata">ORCID author identifiers</a> are of course included. Please help me improve this tool using the Github <a href="https://github.com/mfenner/pandoc-jats/issues">Issue Tracker</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Example article with embedded code and data]]></title>
            <link>https://blog.martinfenner.org/posts/example-article-with-embedded-code-and-data</link>
            <guid>0d59a9aa-6ec9-47c5-9a8b-77c0f1719443</guid>
            <pubDate>Wed, 11 Dec 2013 16:04:00 GMT</pubDate>
            <description><![CDATA[In October I published an essay on Article-Level Metrics (ALM) in PLOS Biology
(Fenner, 2013). The essay is a good introduction into Article-Level Metrics, and
I am proud that it is part of the Tenth Anniversary PLOS Biology Collection
[http://dx.doi.org/10.1371/issue.pcol.v06.i03]. Like all PLOS content, the
article was published with a Creative Commons attribution license
[http://blogs.plos.org/tech/creative-commons-for-science-interview-with-puneet-kishor/]
, allowing me to republish the arti]]></description>
            <content:encoded><![CDATA[<p>In October I published an essay on Article-Level Metrics (ALM) in PLOS Biology (Fenner, 2013). The essay is a good introduction into Article-Level Metrics, and I am proud that it is part of the <a href="http://dx.doi.org/10.1371/issue.pcol.v06.i03">Tenth Anniversary PLOS Biology Collection</a>. Like all PLOS content, the article was published with a <a href="http://blogs.plos.org/tech/creative-commons-for-science-interview-with-puneet-kishor/">Creative Commons attribution license</a>, allowing me to republish the article on this blog. I have now done so and the article is available <a href="http://blog.martinfenner.org/2013/12/11/what-can-article-level-metrics-do-for-you/">here</a>.</p><p>Of course I didn’t want to simply republish the article, but I wanted to publish an improved version. The article has five figures, four of them show visualizations of ALM data that were generated using R (the fifth figure is a table reproduced from another article). The PLOS article includes the ALM dataset and the R scripts used to generate the figures as <a href="http://dx.doi.org/10.1371/journal.pbio.1001687.s001">supplementary information</a>. What I have done now is to recreate the article as a single markdown file (available <a href="https://github.com/mfenner/blog/blob/master/_posts/2013-12-11-what-can-article-level-metrics-do-for-you.Rmd">here</a>) that has all R code embedded. Using R and <a href="http://yihui.name/knitr/">knitr</a> - and the <a href="http://blog.martinfenner.org/data/alm_report_plos_biology_2013-05-20.csv">CSV file with the ALM data</a> - everyone can now reproduce the figures from the paper by simply running the embedded code, and can dig deeper into the data.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/2013-12-11_figure_3.svg" class="kg-image" alt="Figure 3. Views vs. citations for PLOS Biology articles published in 2010."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Figure 3.</strong> Views vs.&nbsp;citations for PLOS Biology articles published in 2010.</figcaption></figure><p>This was a good opportunity to improve the accessibility of the article in other ways. Instead of the raster image formats PNG, JPEG and TIFF used by PLOS and almost every other publisher, I generated the figures in the vector format SVG. Not only does SVG produce images independent of device resolution and screen size (try to zoom in on the figure above), but SVG can also easily be manipulated in the browser since it is XML. This is beyond the scope of this blog post, but look at the <a href="http://d3js.org/">d3.js</a> Javascript library for great examples of how SVG can be dynamically generated and changed in the browser. <strong><strong>Figure 3</strong></strong> above could for example be enhanced so that the article title is displayed when you hover over one of the bubbles, or we could enable zooming to show more detail.</p><p>Like all content on this blog, the article was created using <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a>, and the bibliography was dynamically generated. This makes it easy to change the citation style, and I decided to use the <a href="http://www.apastyle.org/">APA Style</a> that shows the citations in the text as author-date rather than numbered as with the PLOS style (see the example citation in the first paragraph). The combined bibliography for all blog posts including the article can be downloaded in bibtex format <a href="http://blog.martinfenner.org/bibliography/references.bib">here</a>.</p><p>Lastly, I wanted to generate nicer HTML for a better online reading experience. I haven’t done anything fancy, but most publishers seem to focus on navigation around an article, so that very little screen real estate is left for the actual content of the article. I’ve tried to improve readability by reducing the navigation areas to a minimum, by using readable fonts in larger sizes: <a href="https://typekit.com/fonts/minion-pro">Adobe Minion Pro</a> for the body text and <a href="https://typekit.com/fonts/myriad-pro">Adobe Myriad Pro</a> for headings, tables and figure legends.</p><h2 id="references">References</h2><p>Fenner, M. (2013). What can article-level metrics do for you? <em>PLoS Biol</em>, <em>11</em>(10), e1001687. <a href="http://doi.org/10.1371/journal.pbio.1001687">doi:10.1371/journal.pbio.1001687</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What Can Article-Level Metrics Do for You?]]></title>
            <link>https://blog.martinfenner.org/posts/what-can-article-level-metrics-do-for-you</link>
            <guid>9cae933c-062d-4572-8d47-8a6c31f539a7</guid>
            <pubDate>Wed, 11 Dec 2013 15:58:00 GMT</pubDate>
            <description><![CDATA[Article-level metrics (ALMs) provide a wide range of metrics about the uptake of
an individual journal article by the scientific community after publication.
They include citations, usage statistics, discussions in online comments and
social media, social bookmarking, and recommendations. In this essay, we
describe why article-level metrics are an important extension of traditional
citation-based journal metrics and provide a number of example from ALM data
collected for PLOS Biology.

This is a]]></description>
            <content:encoded><![CDATA[<p><em>Article-level metrics (ALMs) provide a wide range of metrics about the uptake of an individual journal article by the scientific community after publication. They include citations, usage statistics, discussions in online comments and social media, social bookmarking, and recommendations. In this essay, we describe why article-level metrics are an important extension of traditional citation-based journal metrics and provide a number of example from ALM data collected for PLOS Biology.</em></p><p><em>This is an open-access article distributed under the terms of the Creative Commons Attribution License, authored by me and <a href="http://dx.doi.org/10.1371/journal.pbio.1001687">originally published Oct 22, 2013 in PLOS Biology</a>.</em></p><p>The scientific impact of a particular piece of research is reflected in how this work is taken up by the scientific community. The first systematic approach that was used to assess impact, based on the technology available at the time, was to track citations and aggregate them by journal. This strategy is not only no longer necessary â€” since now we can easily track citations for individual articles â€” but also, and more importantly, journal-based metrics are now considered a poor performance measure for individual articles (Campbell, 2008; Glänzel &amp; Wouters, 2013). One major problem with journal-based metrics is the variation in citations per article, which means that a small percentage of articles can skew, and are responsible for, the majority of the journal-based citation impact factor, as shown by Campbell (2008) for the 2004 <em>Nature</em> Journal Impact Factor. <strong><strong>Figure 1</strong></strong> further illustrates this point, showing the wide distribution of citation counts between <em>PLOS Biology</em> research articles published in 2010. <em>PLOS Biology</em> research articles published in 2010 have been cited a median 19 times to date in Scopus, but 10% of them have been cited 50 or more times, and two articles (Dickson, Wang, Krantz, Hakonarson, &amp; Goldstein, 2010; Narendra et al., 2010) more than 300 times. <em>PLOS Biology</em> metrics are used as examples throughout this essay, and the dataset is available in the supporting information (<strong><strong>Data S1</strong></strong>). Similar data are available for an increasing number of other publications and organizations.</p><pre><code># code for figure 1: density plots for citation counts for PLOS Biology
# articles published in 2010

# load May 20, 2013 ALM report
alm &lt;- read.csv("data/alm_report_plos_biology_2013-05-20.csv", stringsAsFactors = FALSE)

# only look at research articles
alm &lt;- subset(alm, alm$article_type == "Research Article")

# only look at papers published in 2010
alm$publication_date &lt;- as.Date(alm$publication_date)
alm &lt;- subset(alm, alm$publication_date &gt; "2010-01-01" &amp; alm$publication_date &lt;=
    "2010-12-31")

# labels
colnames &lt;- dimnames(alm)[[2]]
plos.color &lt;- "#1ebd21"
plos.source &lt;- "scopus"

plos.xlab &lt;- "Scopus Citations"
plos.ylab &lt;- "Probability"

quantile &lt;- quantile(alm[, plos.source], c(0.1, 0.5, 0.9), na.rm = TRUE)

# plot the chart
opar &lt;- par(mai = c(0.5, 0.75, 0.5, 0.5), omi = c(0.25, 0.1, 0.25, 0.1), mgp = c(3,
    0.5, 0.5), fg = "black", cex.main = 2, cex.lab = 1.5, col = plos.color,
    col.main = plos.color, col.lab = plos.color, xaxs = "i", yaxs = "i")

d &lt;- density(alm[, plos.source], from = 0, to = 100)
d$x &lt;- append(d$x, 0)
d$y &lt;- append(d$y, 0)
plot(d, type = "n", main = NA, xlab = NA, ylab = NA, xlim = c(0, 100), frame.plot = FALSE)
polygon(d, col = plos.color, border = NA)
mtext(plos.xlab, side = 1, col = plos.color, cex = 1.25, outer = TRUE, adj = 1,
    at = 1)
mtext(plos.ylab, side = 2, col = plos.color, cex = 1.25, outer = TRUE, adj = 0,
    at = 1, las = 1)

par(opar)</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/2013-12-11_figure_1.svg" class="kg-image" alt="Figure 1. Citation counts for PLOS Biology articles published in 2010. Scopus citation counts plotted as a probability distribution for all 197 PLOS Biology research articles published in 2010. Data collected May 20, 2013. Median 19 citations; 10% of papers have at least 50 citations."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Figure 1. Citation counts for PLOS Biology articles published in 2010.</strong> Scopus citation counts plotted as a probability distribution for all 197 <em style="box-sizing: border-box;">PLOS Biology</em> research articles published in 2010. Data collected May 20, 2013. Median 19 citations; 10% of papers have at least 50 citations.</figcaption></figure><p>Scientific impact is a multi-dimensional construct that can not be adequately measured by any single indicator (Bollen, Sompel, Hagberg, &amp; Chute, 2009; Glänzel &amp; Wouters, 2013; Schekman &amp; Patterson, 2013). To this end, PLOS has collected and displayed a variety of metrics for all its articles since 2009. The array of different categorised article-level metrics (ALMs) used and provided by PLOS as of August 2013 are shown in <strong><strong>Figure 2</strong></strong>. In addition to citations and usage statistics, i.e., how often an article has been viewed and downloaded, PLOS also collects metrics about: how often an article has been saved in online reference managers, such as Mendeley; how often an article has been discussed in its comments section online, and also in science blogs or in social media; and how often an article has been recommended by other scientists. These additional metrics provide valuable information that we would miss if we only consider citations. Two important shortcomings of citation-based metrics are that (1) they take years to accumulate and (2) citation analysis is not always the best indicator of impact in more practical fields, such as clinical medicine (Eck, Waltman, Raan, Klautz, &amp; Peul, 2013). Usage statistics often better reflect the impact of work in more practical fields, and they also sometimes better highlight articles of general interest (for example, the 2006 <em>PLOS Biology</em> article on the citation advantage of Open Access articles (Eysenbach, 2006), one of the 10 most-viewed articles published in <em>PLOS Biology</em>).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/2013-12-11_figure_2.png" class="kg-image" alt="Figure 2. Article-level metrics used by PLOS in August 2013 and their categories. Taken from (Lin &amp; Fenner, 2013) with permission by the authors."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Figure 2. Article-level metrics used by PLOS in August 2013 and their categories.</strong> Taken from <span class="citation" data-cites="Lin2013" style="box-sizing: border-box;">(Lin &amp; Fenner, 2013)</span> with permission by the authors.</figcaption></figure><p>A bubble chart showing all 2010 <em>PLOS Biology</em> articles (<strong><strong>Figure 3</strong></strong>) gives a good overview of the year’s views and citations, plus it shows the influence that the article type (as indicated by dot color) has on an article’s performance as measured by these metrics. The weekly <em>PLOS Biology</em> publication schedule is reflected in this figure, with articles published on the same day present in a vertical line. <strong><strong>Figure 3</strong></strong> also shows that the two most highly cited 2010 <em>PLOS Biology</em> research articles are also among the most viewed (indicated by the red arrows), but overall there isn’t a strong correlation between citations and views. The most-viewed article published in 2010 in <em>PLOS Biology</em> is an essay on Darwinian selection in robots (Floreano &amp; Keller, 2010). Detailed usage statistics also allow speculatulation about the different ways that readers access and make use of published literature; some articles are browsed or read online due to general interest while others that are downloaded (and perhaps also printed) may reflect the reader’s intention to look at the data and results in detail and to return to the article more than once.</p><pre><code># code for figure 3: Bubblechart views vs. citations for PLOS Biology
# articles published in 2010.

# Load required libraries
library(plyr)

# load May 20, 2013 ALM report
alm &lt;- read.csv("../data/alm_report_plos_biology_2013-05-20.csv", stringsAsFactors = FALSE,
    na.strings = c("0"))

# only look at papers published in 2010
alm$publication_date &lt;- as.Date(alm$publication_date)
alm &lt;- subset(alm, alm$publication_date &gt; "2010-01-01" &amp; alm$publication_date &lt;=
    "2010-12-31")

# make sure counter values are numbers
alm$counter_html &lt;- as.numeric(alm$counter_html)

# lump all papers together that are not research articles
reassignType &lt;- function(x) if (x == "Research Article") 1 else 0
alm$article_group &lt;- aaply(alm$article_type, 1, reassignType)

# calculate article age in months
alm$age_in_months &lt;- (Sys.Date() - alm$publication_date)/365.25 * 12
start_age_in_months &lt;- floor(as.numeric(Sys.Date() - as.Date(strptime("2010-12-31",
    format = "%Y-%m-%d")))/365.25 * 12)

# chart variables
x &lt;- alm$age_in_months
y &lt;- alm$counter
z &lt;- alm$scopus

xlab &lt;- "Age in Months"
ylab &lt;- "Total Views"

labels &lt;- alm$article_group
col.main &lt;- "#1ebd21"
col &lt;- "#666358"

# calculate bubble diameter
z &lt;- sqrt(z/pi)

# calculate bubble color
getColor &lt;- function(x) c("#c9c9c7", "#1ebd21")[x + 1]
colors &lt;- aaply(labels, 1, getColor)

# plot the chart
opar &lt;- par(mai = c(0.5, 0.75, 0.5, 0.5), omi = c(0.25, 0.1, 0.25, 0.1), mgp = c(3,
    0.5, 0.5), fg = "black", cex = 1, cex.main = 2, cex.lab = 1.5, col = "white",
    col.main = col.main, col.lab = col)

plot(x, y, type = "n", xlim = c(start_age_in_months, start_age_in_months + 13),
    ylim = c(0, 60000), xlab = NA, ylab = NA, las = 1)
symbols(x, y, circles = z, inches = exp(1.3)/15, bg = colors, xlim = c(start_age_in_months,
    start_age_in_months + 13), ylim = c(0, ymax), xlab = NA, ylab = NA, las = 1,
    add = TRUE)
mtext(xlab, side = 1, col = col.main, cex = 1.25, outer = TRUE, adj = 1, at = 1)
mtext(ylab, side = 2, col = col.main, cex = 1.25, outer = TRUE, adj = 0, at = 1,
    las = 1)

par(opar)</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/2013-12-11_figure_3.svg" class="kg-image" alt="Figure 3. Views vs. citations for PLOS Biology articles published in 2010. All 304 PLOS Biology articles published in 2010. Bubble size correlates with number of Scopus citations. Research articles are labeled green; all other articles are grey. Red arrows indicate the two most highly cited papers. Data collected May 20, 2013."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Figure 3. Views vs.&nbsp;citations for PLOS Biology articles published in 2010.</strong> All 304 <em style="box-sizing: border-box;">PLOS Biology</em> articles published in 2010. Bubble size correlates with number of Scopus citations. Research articles are labeled green; all other articles are grey. Red arrows indicate the two most highly cited papers. Data collected May 20, 2013.</figcaption></figure><p>When readers first see an interesting article, their response is often to view or download it. By contrast, a citation may be one of the last outcomes of their interest, occuring only about 1 in 300 times a PLOS paper is viewed online. A lot of things happen in between these potential responses, ranging from discussions in comments, social media, and blogs, to bookmarking, to linking from websites. These activities are usually subsumed under the term â€œaltmetrics,â€ and their variety can be overwhelming. Therefore, it helps to group them together into categories, and several organizations, including PLOS, are using the category labels of Viewed, Cited, Saved, Discussed, and Recommended (<strong><strong>Figures 2 and 4</strong></strong>, see also (Lin &amp; Fenner, 2013)).</p><pre><code># code for figure 4: bar plot for Article-level metrics for PLOS Biology

# Load required libraries
library(reshape2)

# load May 20, 2013 ALM report
alm &lt;- read.csv("../data/alm_report_plos_biology_2013-05-20.csv", stringsAsFactors = FALSE,
    na.strings = c(0, "0"))

# only look at research articles
alm &lt;- subset(alm, alm$article_type == "Research Article")

# make sure columns are in the right format
alm$counter_html &lt;- as.numeric(alm$counter_html)
alm$mendeley &lt;- as.numeric(alm$mendeley)

# options
plos.color &lt;- "#1ebd21"
plos.colors &lt;- c("#a17f78", "#ad9a27", "#ad9a27", "#ad9a27", "#ad9a27", "#ad9a27",
    "#dcebdd", "#dcebdd", "#789aa1", "#789aa1", "#789aa1", "#304345", "#304345")

# use subset of columns
alm &lt;- subset(alm, select = c("f1000", "wikipedia", "researchblogging", "comments",
    "facebook", "twitter", "citeulike", "mendeley", "pubmed", "crossref", "scopus",
    "pmc_html", "counter_html"))

# calculate percentage of values that are not missing (i.e. have a count of
# at least 1)
colSums &lt;- colSums(!is.na(alm)) * 100/length(alm$counter_html)
exactSums &lt;- sum(as.numeric(alm$pmc_html), na.rm = TRUE)

# plot the chart
opar &lt;- par(mar = c(0.1, 7.25, 0.1, 0.1) + 0.1, omi = c(0.1, 0.25, 0.1, 0.1),
    col.main = plos.color)

plos.names &lt;- c("F1000Prime", "Wikipedia", "Research Blogging", "PLOS Comments",
    "Facebook", "Twitter", "CiteULike", "Mendeley", "PubMed Citations", "CrossRef",
    "Scopus", "PMC HTML Views", "PLOS HTML Views")
y &lt;- barplot(colSums, horiz = TRUE, col = plos.colors, border = NA, xlab = plos.names,
    xlim = c(0, 120), axes = FALSE, names.arg = plos.names, las = 1, adj = 0)
text(colSums + 6, y, labels = sprintf("%1.0f%%", colSums))

par(opar)</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/2013-12-11_figure_4.svg" class="kg-image" alt="Figure 4. Article-level metrics for PLOS Biology. Proportion of all 1,706 PLOS Biology research articles published up to May 20, 2013 mentioned by particular article-level metrics source. Colors indicate categories (Viewed, Cited, Saved, Discussed, Recommended), as used on the PLOS website."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Figure 4. Article-level metrics for PLOS Biology.</strong> Proportion of all 1,706 <em style="box-sizing: border-box;">PLOS Biology</em> research articles published up to May 20, 2013 mentioned by particular article-level metrics source. Colors indicate categories (Viewed, Cited, Saved, Discussed, Recommended), as used on the PLOS website.</figcaption></figure><p>All <em>PLOS Biology</em> articles are viewed and downloaded, and almost all of them (all research articles and nearly all front matter) will be cited sooner or later. Almost all of them will also be bookmarked in online reference managers, such as Mendeley, but the percentage of articles that are discussed online is much smaller. Some of these percentages are time dependent; the use of social media discussion platforms, such as Twitter and Facebook for example, has increased in recent years (93% of <em>PLOS Biology</em> research articles published since June 2012 have been discussed on Twitter, and 63% mentioned on Facebook). These are the locations where most of the online discussion around published articles currently seems to take place; the percentage of papers with comments on the PLOS website or that have science blog posts written about them is much smaller. Not all of this online discussion is about research articles, and perhaps, not surprisingly, the most-tweeted PLOS article overall (with more than 1,100 tweets) is a <em>PLOS Biology</em> perspective on the use of social media for scientists (Bik &amp; Goldstein, 2013).</p><p>Some metrics are not so much indicators of a broad online discussion, but rather focus on highlighting articles of particular interest. For example, science blogs allow a more detailed discussion of an article as compared to comments or tweets, and journals themselves sometimes choose to highlight a paper on their own blogs, allowing for a more digestible explanation of the science for the non-expert reader (Fausto et al., 2012). Coverage by other bloggers also serves the same purpose; a good example of this is one recent post on the OpenHelix Blog (“Video Tip of the Week: Turkeys and their genomes,” 2012) that contains video footage of the second author of a 2010 <em>PLOS Biology</em> article (Dalloul et al., 2010) discussing the turkey genome.</p><p>F1000Prime, a commercial service of recommendations by expert scientists, was added to the PLOS Article-Level Metrics in August 2013. We now highlight on the PLOS website when any articles have received at least one recommendation within F1000Prime. We also monitor when an article has been cited within the widely used modern-day online encyclopedia, Wikipedia. A good example of the latter is the Tasmanian devil Wikipedia page (“Tasmanian devil,” 2013) that links to a <em>PLOS Biology</em> research article published in 2010 (Nilsson et al., 2010). While a F1000Prime recommendation is a strong endorsement from peer(s) in the scientific community, being included in a Wikipedia page is akin to making it into a textbook about the subject area and being read by a much wider audience that goes beyond the scientific community.</p><p><em>PLOS Biology</em> is the PLOS journal with the highest percentage of articles recommended in F1000Prime and mentioned in Wikipedia, but there is only partial overlap between the two groups of articles because they focus on different audiences (<strong><strong>Figure 5</strong></strong>). These recommendations and mentions in turn show correlations with other metrics, but not simple ones; you can’t assume, for example, that highly cited articles are more likely to be recommended by F1000Prime, so it will be interesting to monitor these trends now that we include this information.</p><pre><code># code for figure 5: Venn diagram F1000 vs. Wikipedia for PLOS Biology
# articles

# load required libraries
library("plyr")
library("VennDiagram")

# load May 20, 2013 ALM report
alm &lt;- read.csv("../data/alm_report_plos_biology_2013-05-20.csv", stringsAsFactors = FALSE)

# only look at research articles
alm &lt;- subset(alm, alm$article_type == "Research Article")

# group articles based on values in Wikipedia and F1000
reassignWikipedia &lt;- function(x) if (x &gt; 0) 1 else 0
alm$wikipedia_bin &lt;- aaply(alm$wikipedia, 1, reassignWikipedia)
reassignF1000 &lt;- function(x) if (x &gt; 0) 2 else 0
alm$f1000_bin &lt;- aaply(alm$f1000, 1, reassignF1000)
alm$article_group = alm$wikipedia_bin + alm$f1000_bin
reassignCombined &lt;- function(x) if (x == 3) 1 else 0
alm$combined_bin &lt;- aaply(alm$article_group, 1, reassignCombined)
reassignNo &lt;- function(x) if (x == 0) 1 else 0
alm$no_bin &lt;- aaply(alm$article_group, 1, reassignNo)

# remember to divide f1000_bin by 2, as this is the default value
summary &lt;- colSums(subset(alm, select = c("wikipedia_bin", "f1000_bin", "combined_bin",
    "no_bin")), na.rm = TRUE)
rows &lt;- nrow(alm)

# options
plos.colors &lt;- c("#c9c9c7", "#0000ff", "#ff0000")

# plot the chart
opar &lt;- par(mai = c(0.5, 0.75, 3.5, 0.5), omi = c(0.5, 0.5, 1.5, 0.5), mgp = c(3,
    0.5, 0.5), fg = "black", cex.main = 2, cex.lab = 1.5, col = plos.color,
    col.main = plos.color, col.lab = plos.color, xaxs = "i", yaxs = "i")

venn.plot &lt;- draw.triple.venn(area1 = rows, area2 = summary[1], area3 = summary[2]/2,
    n12 = summary[1], n23 = summary[3], n13 = summary[2]/2, n123 = summary[3],
    euler.d = TRUE, scaled = TRUE, fill = plos.colors, cex = 2, fontfamily = rep("sans",
        7))

par(opar)</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/2013-12-11_figure_5.svg" class="kg-image" alt="Figure 5. PLOS Biology articles: sites of recommendation and discussion. Number of PLOS Biology research articles published up to May 20, 2013 that have been recommended by F1000Prime (red) or mentioned in Wikipedia (blue)."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Figure 5. PLOS Biology articles: sites of recommendation and discussion.</strong> Number of <em style="box-sizing: border-box;">PLOS Biology</em> research articles published up to May 20, 2013 that have been recommended by F1000Prime (red) or mentioned in Wikipedia (blue).</figcaption></figure><p>With the increasing availability of ALM data, there comes a growing need to provide tools that will allow the community to interrogate them. A good first step for researchers, research administrators, and others interested in looking at the metrics of a larger set of PLOS articles is the recently launched ALM Reports tool (“ALM Reports,” 2013). There are also a growing number of service providers, including <a href="http://altmetric.com/" rel="nofollow">Altmetric.com</a> (“<a href="http://altmetric.com/" rel="nofollow">Altmetric.com</a>,” 2013), ImpactStory (“ImpactStory,” 2013), and Plum Analytics (“Plum Analytics,” 2013) that provide similar services for articles from other publishers.</p><p>As article-level metrics become increasingly used by publishers, funders, universities, and researchers, one of the major challenges to overcome is ensuring that standards and best practices are widely adopted and understood. The National Information Standards Organization (NISO) was recently awarded a grant by the Alfred P. Sloan Foundation to work on this (“NISO Alternative Assessment Metrics (Altmetrics) Project,” 2013), and PLOS is actively involved in this project. We look forward to further developing our article-level metrics and to having them adopted by other publishers, which hopefully will pave the way to their wide incorporation into research and researcher assessments.</p><h3 id="supporting-information">Supporting Information</h3><p><strong><strong><a href="http://dx.doi.org/10.1371/journal.pbio.1001687.s001">Data S1</a>. Dataset of ALM for PLOS Biology articles used in the text, and R scripts that were used to produce figures.</strong></strong> The data were collected on May 20, 2013 and include all <em>PLOS Biology</em> articles published up to that day. Data for F1000Prime were collected on August 15, 2013. All charts were produced with R version 3.0.0.</p><h2 id="references">References</h2><p>ALM Reports. (2013). Retrieved from <a href="http://almreports.plos.org/">http://almreports.plos.org</a></p><p><a href="http://altmetric.com/" rel="nofollow">Altmetric.com</a>. (2013). Retrieved from <a href="http://www.altmetric.com/">http://www.altmetric.com/</a></p><p>Bik, H. M., &amp; Goldstein, M. C. (2013). An introduction to social media for scientists. <em>PLOS Biology</em>, <em>11</em>(4), e1001535. <a href="http://doi.org/10.1371/journal.pbio.1001535">doi:10.1371/journal.pbio.1001535</a></p><p>Bollen, J., Sompel, H. de, Hagberg, A., &amp; Chute, R. (2009). A Principal Component Analysis of 39 Scientific Impact Measures. <em>PLoS ONE</em>, <em>4</em>(6), e6022. <a href="http://doi.org/10.1371/journal.pone.0006022">doi:10.1371/journal.pone.0006022</a></p><p>Campbell, P. (2008). Escape from the impact factor. <em>Ethics in Science and Environmental Politics</em>, <em>8</em>, 5–7. Journal article. <a href="http://doi.org/10.3354/esep00078">doi:10.3354/esep00078</a></p><p>Dalloul, R. A., Long, J. A., Zimin, A. V., Aslam, L., Beal, K., Blomberg, L. A., … Reed, K. M. (2010). Multi-platform next-generation sequencing of the domestic turkey (Meleagris gallopavo): genome assembly and analysis. <em>PLOS Biology</em>, <em>8</em>(9). <a href="http://doi.org/10.1371/journal.pbio.1000475">doi:10.1371/journal.pbio.1000475</a></p><p>Dickson, S. P., Wang, K., Krantz, I., Hakonarson, H., &amp; Goldstein, D. B. (2010). Rare variants create synthetic genome-wide associations. <em>PLOS Biology</em>, <em>8</em>(1), e1000294. <a href="http://doi.org/10.1371/journal.pbio.1000294">doi:10.1371/journal.pbio.1000294</a></p><p>Eck, N. J. van, Waltman, L., Raan, A. F. J. van, Klautz, R. J. M., &amp; Peul, W. C. (2013). Citation analysis may severely underestimate the impact of clinical research as compared to basic research. <em>PLOS ONE</em>, <em>8</em>(4), e62395. <a href="http://doi.org/10.1371/journal.pone.0062395">doi:10.1371/journal.pone.0062395</a></p><p>Eysenbach, G. (2006). Citation advantage of open access articles. <em>PLOS Biology</em>, <em>4</em>(5), e157. <a href="http://doi.org/10.1371/journal.pbio.0040157">doi:10.1371/journal.pbio.0040157</a></p><p>Fausto, S., Machado, F. A., Bento, L. F. J., Iamarino, A., Nahas, T. R., &amp; Munger, D. S. (2012). Research blogging: indexing and registering the change in science 2.0. <em>PLOS ONE</em>, <em>7</em>(12), e50109. <a href="http://doi.org/10.1371/journal.pone.0050109">doi:10.1371/journal.pone.0050109</a></p><p>Floreano, D., &amp; Keller, L. (2010). Evolution of adaptive behaviour in robots by means of Darwinian selection. <em>PLOS Biology</em>, <em>8</em>(1), e1000292. <a href="http://doi.org/10.1371/journal.pbio.1000292">doi:10.1371/journal.pbio.1000292</a></p><p>Glänzel, W., &amp; Wouters, P. (2013). The dos and don’ts in individudal level bibliometrics. Retrieved from <a href="http://de.slideshare.net/paulwouters1/issi2013-wg-pw">http://de.slideshare.net/paulwouters1/issi2013-wg-pw</a></p><p>ImpactStory. (2013). Retrieved from <a href="http://impactstory.org/">http://impactstory.org/</a></p><p>Lin, J., &amp; Fenner, M. (2013). Altmetrics in Evolution: Defining and Redefining the Ontology of Article-Level Metrics. <em>Information Standards Quarterly</em>, <em>25</em>(2), 20. <a href="http://doi.org/10.3789/isqv25no2.2013.04">doi:10.3789/isqv25no2.2013.04</a></p><p>Narendra, D. P., Jin, S. M., Tanaka, A., Suen, D.-F., Gautier, C. A., Shen, J., … Youle, R. J. (2010). PINK1 is selectively stabilized on impaired mitochondria to activate Parkin. <em>PLOS Biology</em>, <em>8</em>(1), e1000298. <a href="http://doi.org/10.1371/journal.pbio.1000298">doi:10.1371/journal.pbio.1000298</a></p><p>Nilsson, M. A., Churakov, G., Sommer, M., Tran, N. V., Zemann, A., Brosius, J., &amp; Schmitz, J. (2010). Tracking marsupial evolution using archaic genomic retroposon insertions. <em>PLOS Biology</em>, <em>8</em>(7), e1000436. <a href="http://doi.org/10.1371/journal.pbio.1000436">doi:10.1371/journal.pbio.1000436</a></p><p>NISO Alternative Assessment Metrics (Altmetrics) Project. (2013). Retrieved from <a href="http://www.niso.org/topics/tl/altmetrics/initiative">http://www.niso.org/topics/tl/altmetrics/initiative</a></p><p>Plum Analytics. (2013). Retrieved from <a href="http://www.plumanalytics.com/">http://www.plumanalytics.com/</a></p><p>Schekman, R., &amp; Patterson, M. (2013). Reforming research assessment. <em>eLife</em>, <em>2</em>, e00855. <a href="http://doi.org/10.7554/eLife.00855">doi:10.7554/eLife.00855</a></p><p>Tasmanian devil. (2013). Retrieved from <a href="http://en.wikipedia.org/wiki/Tasmanian%5Cdevil">http://en.wikipedia.org/wiki/Tasmanian\devil</a></p><p>Video Tip of the Week: Turkeys and their genomes. (2012). Retrieved from <a href="http://blog.openhelix.eu/?p=14388">http://blog.openhelix.eu/?p=14388</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Opening Science - the Book]]></title>
            <link>https://blog.martinfenner.org/posts/opening-science-the-book</link>
            <guid>98d1309e-cc0d-4f2d-8d50-66be35f7d9ad</guid>
            <pubDate>Thu, 05 Dec 2013 16:06:00 GMT</pubDate>
            <description><![CDATA[Opening Science: The Evolving Guide on How the Internet is Changing Research,
Collaboration and Scholarly Publishing
[http://www.openingscience.org/get-the-book/] is a SpringerOpen book (using a 
Creative Commons Attribution-NonCommercial license
[http://book.openingscience.org/cases_recipes_howtos/creative_commons_licences])
that will be published in a few weeks. If you can’t wait for the book to be
published and/or you want to make comments or suggestions, go to the dynamic
book online version]]></description>
            <content:encoded><![CDATA[<p><a href="http://www.openingscience.org/get-the-book/">Opening Science: The Evolving Guide on How the Internet is Changing Research, Collaboration and Scholarly Publishing</a> is a SpringerOpen book (using a <a href="http://book.openingscience.org/cases_recipes_howtos/creative_commons_licences">Creative Commons Attribution-NonCommercial license</a>) that will be published in a few weeks. If you can’t wait for the book to be published and/or you want to make comments or suggestions, go to the dynamic book online version at <a href="http://book.openingscience.org/">http://book.openingscience.org</a>. I am an author or co-author of three chapters (<a href="http://book.openingscience.org/tools/reference_management">Reference Management</a>, <a href="http://book.openingscience.org/vision/altmetrics">Altmetrics and Other Novel Measures for Scientific Impact</a>, <a href="http://book.openingscience.org/cases_recipes_howtos/unique_identifiers_for_researchers">Unique Identifiers for Researchers</a>) and have helped put the dynamic book together. The book is generated from markdown files hosted in a <a href="https://github.com/openingscience/book/">public Github repo</a> using <a href="http://jekyllrb.com/">Jekyll</a> and <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a>, and we use <a href="http://prose.io/">Prose</a> to enable online editing of the content.</p><p>Using markdown, github, jekyll and pandoc is nothing new for blogs, but this is probably one of the first scholarly books using this workflow. The dynamic book is therefore still very much work in progress and feedback is greatly appreciated.</p><p>Another great example using a very similar workflow is the upcoming book <a href="http://adv-r.had.co.nz/">Advanced R Programming</a> by Hadley Wickham, but he is of course using R and <a href="http://yihui.name/knitr/">knitr</a> to create most of the markdown. In contrast to Hadley we stored the individual chapters as Jekyll posts rather than pages, as this better integrates with other Jekyll functionality, e.g. tags.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Grammar of Scholarly Communication]]></title>
            <link>https://blog.martinfenner.org/posts/the-grammar-of-scholarly-communication</link>
            <guid>31f71610-289f-4a49-891e-b49ed9112d4d</guid>
            <pubDate>Sun, 17 Nov 2013 16:10:00 GMT</pubDate>
            <description><![CDATA[Authoring of scholarly articles is a recurring theme in this blog since it
started in 2008. Authoring is still in desperate need for improvement, and
nobody has convincingly figured out how to solve this problem. Authoring
involves several steps, and it helps to think about them separately:

 * Writing. Manuscript writing, including formatting, collaborative authoring
 * Submission. Formatting a manuscript according to a publisher’s author
   guidelines, and handing it over to a publishing platf]]></description>
            <content:encoded><![CDATA[<p>Authoring of scholarly articles is a recurring theme in this blog since it started in 2008. Authoring is still in desperate need for improvement, and nobody has convincingly figured out how to solve this problem. Authoring involves several steps, and it helps to think about them separately:</p><ul><li><strong><strong>Writing</strong></strong>. Manuscript writing, including formatting, collaborative authoring</li><li><strong><strong>Submission</strong></strong>. Formatting a manuscript according to a publisher’s author guidelines, and handing it over to a publishing platform</li><li><strong><strong>Revision</strong></strong>. Changes made to a manuscript in the peer review process, or after publication</li></ul><p>Although authoring typically involves text, similar issues arise for other research outputs, e.g research data. And these considerations are also relevant for other forms of publishing, whether it is self-publication on a blog or website, or publishing of preprints and white papers.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/grammar.jpg" class="kg-image" alt="Flickr photo by citnaj."><figcaption>Flickr photo by <a href="http://www.flickr.com/photos/citnaj/1278021067/" style="box-sizing: border-box; background: transparent; color: rgb(52, 152, 219); text-decoration: none;">citnaj</a>.</figcaption></figure><p>For me the main challenge in authoring is to go from human-readable unstructured content to highly structured machine-readable content. We could make authoring simpler by either forgoing any structure and just publishing in any format we want, or we can force authors to structure their manuscripts according to a very specific set of rules. The former doesn’t seem to be an option, not only do we have a set of community standards that have evolved for a very long time (research articles for example have title, authors, results, references, etc.), but it also makes it hard to find and reuse scholarly research by others.</p><p>The latter option is also not really viable since most researchers haven’t learned to produce their research outputs in machine-readable highly standardized formats. There are some exceptions, e.g. <a href="http://www.consort-statement.org/">CONSORT</a> and other reporting standards in clinical medicine or the <a href="http://blogs.ch.cam.ac.uk/pmr/2012/01/23/brian-mcmahon-publishing-semantic-crystallography-every-science-data-publisher-should-watch-this-all-the-way-through/">semantic publishing in Crystallography</a>, but for the most part research outputs are too diverse to easily find a format that works for all of them. The current trend is certainly towards machine-readable rather than towards human-readable, but there is still a significant gap - scholarly articles are transformed from documents in Microsoft Word (or sometimes LaTeX) format into XML (for most biomedical research that means <a href="http://jats.nlm.nih.gov/publishing/">JATS</a>) using kludgy tools and lots of manual labor.</p><p>What solutions have been tried to overcome the limitations of our current authoring tools, and to make the process more enjoyable for authors and more productive for publishers?</p><ol><li>Do the conversion manually, still a common workflow.</li><li>Tools for publishers such as <a href="http://blogs.plos.org/mfenner/2009/05/01/extyles_interview_with_elizabeth_blake_and_bruce_rosenblum/">eXtyles</a>, <a href="http://www.shabash.net/merops/">Merops</a> - both commercial - or the evolving Open Source <a href="http://www.lib.umich.edu/mpach/modules">mPach</a> that convert Microsoft Word documents into JATS XML and do a lot of automated checks along the way.</li><li>Tools for authors that directly generate JATS XML, either as a Microsoft Word plugin (the <a href="http://blogs.nature.com/mfenner/2008/11/07/interview-with-pablo-fernicola">Article Authoring Add-In</a>, not actively maintained) in the browser (e.g. <a href="http://blogs.plos.org/mfenner/2009/02/27/lemon8_xml_interview_with_mj_suhonos/">Lemon8-XML</a>, not actively maintained), or directly in a publishing platform such as Wordpress (<a href="http://annotum.org/">Annotum</a>).</li><li>Forget about XML and use HTML5 has the canonical file format, e.g. as <a href="http://blogs.plos.org/mfenner/2011/03/19/a-very-brief-history-of-scholarly-html/">Scholarly HTML</a> or HTML5 specifications such as <a href="https://github.com/oreillymedia/HTMLBook/blob/master/specification.asciidoc">HTMLBook</a>. Please read Molly Sharp’s <a href="http://blogs.plos.org/tech/structured-documents-for-science-jats-xml-as-canonical-content-format/">blog post</a> for background information about HTML as an alternative to XML.</li><li>Use file formats for authoring that are a better fit for the requirements of scholarly authors, in particular <a href="http://blog.martinfenner.org/2012/12/13/a-call-for-scholarly-markdown/">Scholarly Markdown</a>.</li><li>Build online editors for scientific content that hide the underlying file format, and guide users towards a structured format, e.g. by not allowing input that doesn’t conform to specifications.</li></ol><p><strong><strong>Solution 1.</strong></strong> isn’t really an option, as it makes scholarly publishing unnecessarily slow and expensive. Typesetter Kaveh Bazergan has gone on record at the <a href="http://www.nature.com/spoton/2012/11/spoton-london-2012-a-global-conference/">SpotOn London Conference 2012</a> by saying that the current process is insane and that he wants to be “put out of business”.</p><p><strong><strong>Solution 2.</strong></strong> is probably the most commonly used workflow used by larger publishers today, but is very much centered around a Microsoft Word to XML workflow. LaTeX is a popular authoring environment in some disciplines, but still requires work to convert documents into web-friendly formats such as HTML and XML.</p><p><strong><strong>Solutions 3. to 5.</strong></strong> have never picked up any significant traction. Overall the progress in this area has been modest at best, and the mainstream of authoring today isn’t too different from 20 years ago. Although I have gone on record for saying that <a href="http://blog.martinfenner.org/tags.html#markdown-ref">Scholarly Markdown</a> has a lot of potential, the problem is much bigger than finding a single file format, and markdown will never be the solution for all authoring needs.</p><p><strong><strong>Solution 6.</strong></strong> is an area where a lot of exciting development is currently happening, examples include <a href="https://www.authorea.com/">Authorea</a>, <a href="https://www.writelatex.com/">WriteLateX</a>, <a href="https://www.sharelatex.com/">ShareLaTeX</a>. Although the future of scholarly authoring will certainly include online authoring tools (making it much easier to collaborate, one of the authoring pain points), we run the risk of locking in users into one particular authoring environment.</p><h3 id="going-forward">Going Forward</h3><p>How can we move forward? I would suggest the following:</p><ol><li>Publishers should accept manuscripts in any reasonable file format, which means at least Microsoft Word, Open Office, LaTeX, Markdown, HTML and PDF, but possibly more. This will create a lot of extra work for publishers, but will open the doors for innovation, both in the academic and commercial sector. We will never see significant progress in scholarly authoring tools if the submission step requires manuscripts to be in a single file format (Microsoft Word) - in particular since this file format is a general purpose word processsing format and not something designed specifically for scholarly content. And we want researchers to spend their time doing research and writing up their research, not formatting documents.</li><li>To handle this avalanche of unstructured documents, publishers need conversion tools that can transform all these documents into a format that can feed into their editorial and publishing workflows. A limited number of these tools exist already, but this will require a significant development effort. Again, opening up submissions to a variety of file formats will not only foster innovation in authoring tools, but also in document conversion tools.</li><li>We should think beyond XML. Many of the workflows designed today center around conversions from one XML format to another, e.g. Microsoft Word to JATS or <a href="http://www.tei-c.org/index.xml">TEI</a> (popular in the humanities), often using XLST transforms. Not only is XML difficult for humans to read or edit, but the web and many of the technologies built around it are moving away from XML towards HTML5 and JSON. XML is fine as an important output format for publishing, but maybe not the best format to hold everything together.</li><li>As we haven’t come up with a canoical file format for scholarly documents by now, we should give up that idea. XML is great for publisher workflows, but is not something humans can easily edit or read. PDF is still the most widely read format by humans, but is not a good intermediary format. LaTeX is too complex for authors outside of mathematics, physics and related fields, and is not built with web standards in mind. Markdown is promising, but doesn’t easily support highly structured content. And HTML5 and the related ePub are widely popular, but can be hard to edit without a visual editor, and currently don’t include enough standard metadata to support scholarly content out of the box.</li><li>The focus should not be on canonical file formats for scholarly documents, but on tools that understand the manuscripts created by researchers and can transform them into something more structured. As we have learned from document conversion tools such as <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a>, we can’t do this with a simple find and replace using regular expressions, but need a more structured approach. Pandoc is taking the input document (markdown, LaTeX or HTML) apart and is constructing an abstract syntax tree (<a href="http://en.wikipedia.org/wiki/Abstract_syntax_tree">AST</a>) of the document, using parsing expression grammar (<a href="http://en.wikipedia.org/wiki/Parsing_expression_grammar">PEG</a>), which includes a set of parsing rules. Parsing expression grammars are fairly new, <a href="http://bford.info/pub/lang/peg">first described by Bryan Ford</a> about 10 years ago, but in my mind are a very good fit for the formal grammar of scientific documents. It should be fairly straightforward to generate a variety of output formats from the AST (Pandoc can convert into more than 30 document formats), the hard part is the parsing of the input.</li></ol><p>All this requires a lot of work. Pandoc is a good model to start, but is written in Haskell, a functional programming language that not many people are familar with. For small changes Pandoc allows you to directly manipulate the AST (represented as JSON) using <a href="http://johnmacfarlane.net/pandoc/scripting.html">filters</a> written in Haskell or Python. And <a href="https://github.com/jgm/pandoc">custom writers</a> for other document formats can be written using <a href="http://www.lua.org/">Lua</a>, another interesting programming language that not many people know about. Lua is a fast and relatively easy to learn scripting language that can be easily embedded into other languages, and for similar reasons is also used to <a href="http://en.wikipedia.org/wiki/Wikipedia:Lua">extend the functionality of Wikipedia</a>. PEG parsers in other languages include <a href="http://treetop.rubyforge.org/">Treetop</a> (Ruby), <a href="http://pegjs.majda.cz/">PEG.js</a> (Javascript), and <a href="http://www.antlr.org/">ANTLR</a>, a popular parser generator that also includes PEG features.</p><p>But I think the effort to build a solid open source conversion tool for scholarly documents is worth it, in particular for smaller publishers and publishing platforms who can’t afford the commercial Microsoft Word to JATS conversion tools. We shouldn’t take any shortcuts - e.g. by focussing on XML and XLST transforms - and we can improve this tool over time, e.g. by starting with a few input and output formats. This tool will be valuable beyond authoring, as it can also be very helpful to convert published scholarly content into other formats such as ePub, and in text mining, which in many ways tries to solve many of the same problems. The <a href="http://johnmacfarlane.net/pandoc/scripting.html">Pandoc documentation</a> includes an example of extracting all URLs out of a document, and this can be modified to extract other content. In case you wonder whether I gave up on the idea of <a href="http://blog.martinfenner.org/tags.html#markdown-ref">Scholarly Markdown</a> - not at all. To me this is a logical next step, opening up journal submission systems to Scholarly Markdown and other evolving file formats. And Pandoc, one of the most interesting tools in this space, is a markdown conversion tool at its heart. The next steps could be the following:</p><ul><li>write a custom writer in Lua that generates JATS output from Pandoc</li><li>explore how difficult it would be to add Microsoft Word .docx as Pandoc input format</li><li>develop Pandoc filters relevant for scholarly documents (e.g. <a href="http://blog.martinfenner.org/2013/07/02/auto-generating-links-to-data-and-resources/">auto-linking accession numbers of biomedical databases</a>)</li></ul><hr>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What is holding us back?]]></title>
            <link>https://blog.martinfenner.org/posts/what-is-holding-us-back</link>
            <guid>d603c9b3-2089-4425-a680-e3c04fcbf5ce</guid>
            <pubDate>Mon, 11 Nov 2013 16:14:00 GMT</pubDate>
            <description><![CDATA[Last Friday and Saturday the 6th SpotOn London conference
[http://www.nature.com/spoton/event/spoton-london2013/] tool place at the
British Library. I had a great time with many interesting sessions and good
conversations both in and between sessions. But I might be biased, since I
helped organize the event, and in particular did help put the sessions for the
Tools strand [http://www.nature.com/spoton/?cat=11] together.

SpotOn London name tags. Flickr photo by keatl
[http://www.flickr.com/photo]]></description>
            <content:encoded><![CDATA[<p>Last Friday and Saturday the 6th <a href="http://www.nature.com/spoton/event/spoton-london2013/">SpotOn London conference</a> tool place at the British Library. I had a great time with many interesting sessions and good conversations both in and between sessions. But I might be biased, since I helped organize the event, and in particular did help put the <a href="http://www.nature.com/spoton/?cat=11">sessions for the Tools strand</a> together.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/blendology.jpg" class="kg-image" alt="SpotOn London name tags. Flickr photo by keatl."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">SpotOn London name tags</strong>. Flickr photo by <a href="http://www.flickr.com/photos/keatl/10739125344/in/photolist-hmYPPw-9CVkfd/" style="box-sizing: border-box; background: transparent; color: rgb(52, 152, 219); text-decoration: none;">keatl</a>.</figcaption></figure><p>The following blog post summarizes some of my thoughts before, during and after the conference, and I want to focus on innovation in scholarly publishing, or rather: what is holding us back?</p><h2 id="reason-1">Reason #1</h2><p>The <a href="http://www.nature.com/spoton/event/spoton-london-2013-whats-your-number-altmetrics-session/">#solo13alt</a> session on Saturday looked at <em>the role of altmetrics in the evaluation of scientific research</em>. I was one of the panelists and had summarized my ideas prior to the session in a <a href="http://blogs.plos.org/tech/evaluating-impact-whats-your-number/">blog post</a> written together with Jennifer Lin. It was an interesting session, although a bit too controversial for my taste. But it became obvious to me in this and a few other sessions that other obsession with quantitative assessment of science is increasingly dangerous. Other people have said this more eloquently:</p><ul><li>The mania for measurement - Stephen Curry in the <a href="http://www.nature.com/spoton/event/spoton-london-2013-whats-your-number-altmetrics-session/">#solo13alt</a> session</li><li>Why research assessment is out of control - <a href="http://www.theguardian.com/education/2013/nov/04/peter-scott-research-excellence-framework">Peter Scott</a></li><li>Universities are becoming metrics factories, driven by large corporates - <a href="http://blogs.ch.cam.ac.uk/pmr/2013/11/10/spoton2013-yet-another-wonderful-meeting/">Peter Murray-Rust</a></li><li>The ‘real’ revolution in science will come when the scientific egosystem gets rid of the credit-imperative - <a href="https://twitter.com/Villavelius/status/399157271793762304">Jan Velterop</a></li><li>Excellence by Nonsense: The Competition for Publications in Modern Science - <a href="http://book.openingscience.org/basics_background/excellence_by_nonsense/">Mathias Binswanger</a></li></ul><p>My job title is <em>Technical Lead Article-Level Metrics</em> so it might sound surprising that I say this. But we have to differentiate of what we do now and in the next few years - which is mainly to get away from the Journal Impact Factor to more reasonable metrics that look at individual articles and include other metrics besides citations - to where we want to be in 10 or more years. And for the latter it is essential that journal articles and other research outputs are valued for the research they contain, rather than serving as a currency for <em>merit</em> that can be exchanged into grants and acadmic advancement. This is a very difficult problem to solve and I have no answers yet. Going back to how science was conducted until about 50 years ago - as a small elite club that worked based on closed personal networks - is definitely not the answer.</p><h2 id="reason-2">Reason #2</h2><p>In <a href="http://www.nature.com/spoton/event/spoton-london-2013-keynote-1-boson-50-years-50003-scientists-understanding-our-universe-through-global-scientific-collaboration-and-open-access/">his keynote</a> Salvatore Mele from CERN explained to us that Open Access in High Energy Phsics is 50 years old, and that the culture of sharing preprints preceeded the <a href="http://arxiv.org/">ArXiv</a> e-prints service - scientists were mailing their manuscripts to each other at least 20 years before ArXiV launched in 1991. A similar culture doesn’t exist in the life sciences and therefore the preprint services for biologists launched this year (e.g. <a href="https://peerj.com/preprints/">PeerJ Preprints</a> and <a href="http://biorxiv.org/">bioRxiv</a>) will have a hard time gaining traction.</p><p>Email is one of those services that every researcher uses, and we should think much more about how we can create innovative services around email rather than only considerung new tools and services that are still used only by early adopters. AJ Cann had coordinated a workshop around email at SpotOn London that he called <a href="http://www.nature.com/spoton/event/spoton-london-2013-the-dark-art-of-dark-social-email-the-antisocial-medium-which-will-not-die-workshop/">the dark art of dark social: email, the antisocial medium that will not die</a>. I am still puzzled why most researchers prefer to receive tables of content by email rather than as a RSS feed, but we shouldn’t confuse what we get excited about as software developers and early adopters of online tools with what the mainstream scientist would be likely to use.</p><p>Another good example is <a href="http://royalsociety.org/policy/projects/science-public-enterprise/report/">data sharing</a>, a topic that was discussed in at least three SpotOn sessions. Even though most attendees at SpotOn London agreed that sharing of research data is important, it is obvious that this is currently not common practice in most scientific disciplines. Funders have created data sharing policies (e.g. <a href="http://www.nsf.gov/bfa/dias/policy/dmp.jsp">NSF</a> or the <a href="http://www.wellcome.ac.uk/About-us/Policy/Spotlight-issues/Data-sharing/">Wellcome Trust</a>), as <a href="http://dx.doi.org/10.1371/journal.pone.0067111">have publishers</a>, and many organizations are thinking about incentives for data sharing, including data journals such as <a href="http://www.nature.com/scientificdata/">Scientific Data</a> that will launch in 2014 and was presented by Ruth Wilson in the <a href="http://www.nature.com/spoton/event/spoton-london-2013-how-can-we-encourage-data-sharing-discussion/">motivations for data sharing</a> session. Even though incentives can help promote changes, I am pessimistic that something as central to the conduct of science as data sharing can be changed without more scientists being intrinsically motivated to do so. This is a much slower process that should start as early as possible during training, as pointed out by Kaitlin Thaney in the <a href="http://www.nature.com/spoton/event/spoton-london-2013-how-can-we-encourage-data-sharing-discussion/">#solo13carrot</a> session.</p><h2 id="reason-3">Reason #3</h2><p>In terms of the technology that is holding us back, I increasingly think that publisher manuscript submission systems may be the single most important place that is slowing down innovation. I participated in the first <a href="https://sites.google.com/site/beyondthepdf/">Beyond the PDF</a> workshop in 2011, and I think now that <strong><strong>Beyond the MTS (or manuscript tracking system)</strong></strong> might have been a better motto than <strong><strong>Beyond the PDF</strong></strong>, as many of the problems we discussed relate to typical editorial workflows we use today. These systems need to implement many of the ideas discussed at SpotOn London and other places, from opening up peer review (<a href="http://www.nature.com/spoton/event/spoton-london-2013-how-should-peer-review-evolve/">#solo13peer</a>) to making it easier to integrate research data into manuscripts (<a href="http://www.nature.com/spoton/event/spoton-london-2013-how-should-peer-review-evolve/">#solo13carrot</a>) and to ideas of how the scientific record should like in the digital age (<a href="http://www.nature.com/spoton/event/spoton-london-2013-what-should-the-scientific-record-look-like-in-the-digital-age-discussion/">#solo13digital</a>). In the latter panel we discussed both new authoring tools such as <a href="https://www.writelatex.com/">WriteLaTeX</a>, and new ideas of what a research object should look like and how the different parts are linked to each other. A major theme here was reproducibility highlighted both by Carol Goble (also see her <a href="http://www.slideshare.net/carolegoble/ismb2013-keynotecleangoble">ISMB/ECCB 2013 Keynote</a>) and Peter Kraker (see also his <a href="http://science.okfn.org/2013/10/18/its-not-only-peer-reviewed-its-reproducible/">Open Knowledge Foundation blog post</a>).</p><p>The problem with today’s manuscript submission systems is that they have grown so big and complex that any change is slow and cumbersome, rather than iterative and part of an ongoing dialogue. I don’t want to blame any single vendor of these systems, but rather suggest that we carefully re-evaluate the workflow from the manuscript written by one or more authors to the accepted manuscript. My personal interest is mainly in authoring tools, and I have recently written about and experimented with <a href="http://localhost:4000/tags.html#markdown-ref">Markdown</a>. This process of re-evaluating manuscript tracking systems is not simply about technology, but is rather about how we approach this problem as author, publisher, tool vendor and as a community.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What is the Value of Hack Days?]]></title>
            <link>https://blog.martinfenner.org/posts/what-is-the-value-of-hack-days</link>
            <guid>eb96ef52-422c-49cd-86eb-9c2d49967461</guid>
            <pubDate>Mon, 04 Nov 2013 16:18:00 GMT</pubDate>
            <description><![CDATA[This Friday and Saturday the SpotOn London Conference
[http://www.nature.com/spoton/event/spoton-london2013/] will take place at the
British Library in London. I am very excited, as I have come to this conference
since the first one in 2008
[https://twitter.com/McDawg/status/397068628102610945], and have helped organize
the event since 2009. The conference is about science communication in the
broadest sense, and has three strands that focus on science communication,
science policy and tools. Eq]]></description>
            <content:encoded><![CDATA[<p>This Friday and Saturday the <a href="http://www.nature.com/spoton/event/spoton-london2013/">SpotOn London Conference</a> will take place at the British Library in London. I am very excited, as I have come to this conference since the <a href="https://twitter.com/McDawg/status/397068628102610945">first one in 2008</a>, and have helped organize the event since 2009. The conference is about science communication in the broadest sense, and has three strands that focus on <em>science communication, science policy and tools</em>. Equally important as the sessions are of course the many highly engaging informal discussions of the 250 participants that take place between and after the sessions.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/spoton12_hack.jpg" class="kg-image" alt="Presenting from SpotOn London 2012 hackathon. One of the projects in 2012 was a collaborative commenting system. Picture from Flickr, taken by Lou Woodley."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Presenting from SpotOn London 2012 hackathon</strong>. One of the projects in 2012 was a collaborative commenting system. Picture from <a href="http://www.flickr.com/photos/25467658@N00/8252989528/" style="box-sizing: border-box; background: transparent; color: rgb(52, 152, 219); text-decoration: none;">Flickr</a>, taken by Lou Woodley.</figcaption></figure><p>SpotOn London sessions are also more conversations than presentations, as they usually have 2-4 panelists with ample time for discussion with the audience. I will take part in two panels:</p><ul><li><a href="http://www.nature.com/spoton/event/spoton-london-2013-what-the-hack-part-one-hackdays-session/">What the hack?!</a> (Friday 10:30 AM, hashtag <a href="https://twitter.com/search?q=%23solo13hack">#solo13hack</a>), with Peter Murray-Rust, Ross Mounce and Helen Jackson</li><li><a href="http://www.nature.com/spoton/event/spoton-london-2013-whats-your-number-altmetrics-session/">What’s your number? - Altmetrics session</a> (Saturday 10:30 AM, hashtag <a href="https://twitter.com/search?q=%23solo13alt">#solo13alt</a>), with Marie Boran, David Colquhoun, Jean Liu and Stephen Curry</li></ul><p>I will summarize my thoughts regarding the altmetrics session in another post, but want to talk about the first session in more detail. According to the <a href="http://en.wikipedia.org/wiki/Hackathon">English Wikipedia</a></p><blockquote>A <strong><strong>hackathon</strong></strong> (also known as a <strong><strong>hack day</strong></strong>, <strong><strong>hackfest</strong></strong> or <strong><strong>codefest</strong></strong>) is an event in which computer programmers and others involved in software development, including graphic designers, interface designers and project managers, collaborate intensively on software projects.</blockquote><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/Wikimedia_hackathon_020_-_Berlin_2012_03.jpg" class="kg-image" alt="Wikimedia Hackathon Berlin June 2012. Largest hackathon I have attended so far with 100 people. Photo by Gulliaume Paumier, CC-BY license."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Wikimedia Hackathon Berlin June 2012</strong>. Largest hackathon I have attended so far with 100 people. Photo by Gulliaume Paumier, CC-BY license.</figcaption></figure><p>It is too bad that we will have no hackathon at year’s SpotOn London for logistical reasons, but the session is a great opportunity to reflect on the value of science hackdays. It is clear that hackdays for scientific software have become popular, with almost too many opportunities to participate.</p><h3 id="what-i-like">What I like</h3><ul><li>Do stuff. And have plenty of time to do stuff instead sessions in short intervals</li><li>Hackdays let you do great team work</li><li>Learn about other interesting projects and meet people doing cool work</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/alm12_anti_gaming.png" class="kg-image" alt="ALM 2012 hackathon. Brainstorming board from anti-gaming group."><figcaption><a href="http://article-level-metrics.plos.org/alm-workshop-2012/hackathon/" style="box-sizing: border-box; background: transparent; color: rgb(52, 152, 219); text-decoration: none;"><strong style="box-sizing: border-box; font-weight: bold;">ALM 2012 hackathon</strong></a>. Brainstorming board from anti-gaming group.</figcaption></figure><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/group_foto.jpg" class="kg-image" alt="#hack4ac. Our team working on PLOS Author Contributions."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">#hack4ac</strong>. Our team working on <a href="http://hack4ac.com/plos-author-contributions/" style="box-sizing: border-box; background: transparent; color: rgb(52, 152, 219); text-decoration: none;">PLOS Author Contributions</a>.</figcaption></figure><h3 id="what-i-don-t-like">What I don’t like</h3><ul><li>Hackdays are very much targeted at intermediate to advanced software developers, and it is sometimes not easy for beginners to participate</li><li>Too much time spent setting up stuff</li><li>Some of the work done at hackdays can be better done in virtual collaborations over weeks or months</li><li>Not many projects make it beyond the hackday and actually turn into a useable product. One example where this is not true are the visualizations started at the ALM 2012 hackathon that were implemented by OJS in 2013 (<a href="http://dx.doi.org/10.3402/gha.v6i0.19283">see article for more</a>), and of course <a href="http://impactstory.org/">ImpactStory</a> that started at a hackathon at the <a href="http://beyond-impact.org/">Beyond Impact</a> conference in May 2011.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/alm_d3.png" class="kg-image" alt="ALM 2012 hackathon. Sparkline visualization implemented by OJS based on work at the workshop."><figcaption><a href="http://article-level-metrics.plos.org/alm-workshop-2012/hackathon/" style="box-sizing: border-box; background: transparent; color: rgb(52, 152, 219); text-decoration: none;"><strong style="box-sizing: border-box; font-weight: bold;">ALM 2012 hackathon</strong></a>. Sparkline visualization implemented by OJS based on work at the workshop.</figcaption></figure><h3 id="some-of-the-challenges">Some of the challenges</h3><ul><li>Coming up with projects where progress can be made in a day or two</li><li>Technology: WiFi access, access to servers for code deployment, collaboration tools, etc.</li><li>Come up with a good unifying theme, so that the various projects during the hackday relate to each other. The theme at <a href="http://hack4ac.com/">#hack4ac</a> was to demonstrate the value of the CC-BY license within academia.</li></ul><h3 id="some-ideas-to-improve-science-hackdays">Some ideas to improve science hackdays</h3><ul><li>Go beyond software development. We <a href="http://blogs.plos.org/tech/alm-data-challenge-metrics-for-a-standard-set-of-dois/">recently tried a data challenge using Altmetrics data</a>, and at a <a href="http://blog.martinfenner.org/2013/07/02/auto-generating-links-to-data-and-resources/">hackathon between IGSN, DataCite, PANGAEA and ORCID in July</a> we focussed on a high-level discussion of technical issues. There is a continuum towards the <a href="http://en.wikipedia.org/wiki/BarCamp">BarCamp</a> format, although I don’t like to drift too much from <em>doing</em> to <em>talking</em>. A good example of a workshop open to everyone and not just software developers is the SpotOn London workshop this Saturday on <a href="http://www.nature.com/spoton/event/spoton-london-2013-wikipedia-editing-workshop/">Wikipedia Editing</a> run by Brian Kelly and Toni Sant.</li><li>Meet before and after the hackathon. This can be done online, but it helps to focus on what can be achieved in the limited time available for a hackathon, and to follow up on projects that have just been started. But a hackathon is also a great opportunity to meet new people and new ideas, so meeting afterwards is more important than before.</li><li>Involve remote people. A lot of the fun of hackdays comes from sitting around a table and doing something together. But sometimes this is not possible for everyone, so think about remote participation where it makes sense.</li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Commenting on scientific papers]]></title>
            <link>https://blog.martinfenner.org/posts/commenting-on-scientific-papers</link>
            <guid>efacbd9d-fcd9-4ec5-a23d-075f0c1f88d6</guid>
            <pubDate>Fri, 25 Oct 2013 16:21:00 GMT</pubDate>
            <description><![CDATA[I think it is fair to say that commenting on scientific papers is broken. And
with commenting I mean online comments that are publicly available, not informal
discussions in journal clubs or at meetings. This definition would include
discussions of papers on social media such as Twitter or Facebook. Why do I
think that commenting is broken?

 * the number of papers with online comments is low. For PLOS Biology we have
   comments on the journal platform for 11% of articles, tweets for 14% of
   ]]></description>
            <content:encoded><![CDATA[<p>I think it is fair to say that commenting on scientific papers is broken. And with commenting I mean online comments that are publicly available, not informal discussions in journal clubs or at meetings. This definition would include discussions of papers on social media such as Twitter or Facebook. Why do I think that commenting is broken?</p><ul><li>the number of papers with online comments is low. For PLOS Biology we have comments on the journal platform for 11% of articles, tweets for 14% of articles and Facebook activity for 22% of articles (Fenner, 2013). The numbers for Twitter and Facebook are much higher for more recently published articles, but are nowhere close to every article having at least one comment.</li><li>even though there is a fair amount of social media activity around articles, the quality of the discussion is varied. Twitter for example seems to work mostly as an alerting service for interesting articles with little more than the title of the article in the tweet text and not much discussion.</li><li>when comments are made, they are really hard to find coming from the article. Unless they are made on the journal platform, or the publisher tracks article-level metrics and links out to these comments.</li></ul><p>What can be done to address these issues, i.e. increase the number of comments, increase the depth of the discussion, and make it easier to link comments to articles? Some of the thoughts that I and others have had include the following:</p><ul><li>lower the technical barriers for commenting by providing a common and familiar commenting platform with an attractive user interface. Many blogs (including this one) and <a href="http://elife.elifesciences.org/">some publishers</a> use Disqus, which is arguably the most popular third-party commenting platform.</li><li>develop new features that make commenting more attractive, including comments linked to specific sections of the text and notes that can be public, semi-public or private. See for example <a href="https://medium.com/about/5972c72b18f2">what Medium is doing</a>, check out <a href="http://hypothes.is/">Hypothes.is</a> and <a href="http://blog.peerj.com/post/62886292466/peerj-questions-a-new-way-to-never-publish-forget">PeerJ Questions</a>, or study what services such as <a href="http://stackoverflow.com/">Stackoverflow</a> are doing.</li><li>Link comments made in different places about the same object together, e.g. through Article-Level Metrics services.</li><li>create incentives for scientists to comment, e.g. through <a href="http://openbadges.org/">Mozilla Open Badges</a> or by making them part of a community.</li></ul><p>On Tuesday the US National Library of Medicine launched <a href="http://ncbiinsights.ncbi.nlm.nih.gov/2013/10/22/pubmed-commons-a-new-forum-for-scientific-discourse/">PubMed Commons</a> as a <em>New Forum for Scientific Discourse</em>:</p><blockquote>We hope that PubMed Commons will leverage the social power of the internet to encourage constructive criticism and high quality discussions of scientific issues that will both enhance understanding and provide new avenues of collaboration within the community.</blockquote><p>PubMed Commons is still a pilot project and in order to read or write comments you have to be a PubMed Commons participant and be signed in with your My NCBI account. PubMed Commons has some important features:</p><ul><li>PubMed is probably the place where most life sciences researchers search for literature. Having comments and discussion there makes perfect sense, and is probably a better place than a publisher platform that only targets particular journals. PubMed also has a reputation that is very different from social media tools that are popular, but not really familiar to most scientists.</li><li>Access to PubMed Commons is restricted to researchers, and this is one strategy to have the comments focus on scientific discourse. It has to be seen whether the process of registering for PubMed Commons (which currently is a bit more involved than most commenting systems) is a barrier for scientists to take part in the discussion, or whether it generates an audience that makes it more likely that scientists contribute.</li><li>For people signing in with their My NCBI account (I don’t know the percentage of PubMed users that do that on a regular basis), commenting is really easy and the interface is straightforward. The comment editor uses markdown, which makes it easy to format comments and to include links.</li></ul><p><em>10/26/13: added link to the recently launched <a href="http://blog.peerj.com/post/62886292466/peerj-questions-a-new-way-to-never-publish-forget">PeerJ Questions</a> which uses a question and answer format (thanks to Jason Hoyt for reminding me).</em></p><h2 id="references">References</h2><p>Fenner, M. (2013). What can article-level metrics do for you? <em>PLoS Biol</em>, <em>11</em>(10), e1001687. <a href="http://doi.org/10.1371/journal.pbio.1001687">doi:10.1371/journal.pbio.1001687</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What Can Article-Level Metrics Do for You?]]></title>
            <link>https://blog.martinfenner.org/posts/what-can-article-level-metrics-do-for-you-2</link>
            <guid>9fd59d1c-8d24-4879-93c3-cd37c9f60ecf</guid>
            <pubDate>Wed, 23 Oct 2013 16:23:00 GMT</pubDate>
            <description><![CDATA[Yesterday PLOS Biology published an essay by me: What Can Article Level Metrics
Do for You? [http://dx.doi.org/10.1371/journal.pbio.1001687] (Fenner, 2013). I
had help from many others in writing the essay, in particular PLOS Biology
editor Emma Ganley. I hope that the essay can help researchers get introduced to
article-level metrics, and I am honored that the essay is part of the PLOS
Biology 10th anniversary collection
[http://dx.doi.org/10.1371/journal.pbio.1001688].

The essay is an Open Ac]]></description>
            <content:encoded><![CDATA[<p>Yesterday PLOS Biology published an essay by me: <a href="http://dx.doi.org/10.1371/journal.pbio.1001687">What Can Article Level Metrics Do for You?</a> (Fenner, 2013). I had help from many others in writing the essay, in particular PLOS Biology editor Emma Ganley. I hope that the essay can help researchers get introduced to article-level metrics, and I am honored that the essay is part of the <a href="http://dx.doi.org/10.1371/journal.pbio.1001688">PLOS Biology 10th anniversary collection</a>.</p><p>The essay is an Open Access article published under a CC-BY license, so not only can everyone read it, but the text and figures can be freely reused, as long as proper attribution is provided, e.g. Fig. 5:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/venndiagram_plos_biology.png" class="kg-image" alt="PLOS Biology articles: sites of recommendation and discussion. Number of PLOS Biology research articles published until May 20, 2013 that have been recommended by F1000Prime (red) and/or mentioned in Wikipedia (blue). Taken from doi:10.1371/journal.pbio.1001687.g005"><figcaption><strong style="box-sizing: border-box; font-weight: bold;">PLOS Biology articles: sites of recommendation and discussion</strong>. Number of PLOS Biology research articles published until May 20, 2013 that have been recommended by F1000Prime (red) and/or mentioned in Wikipedia (blue). Taken from <a href="http://dx.doi.org/10.1371/journal.pbio.1001687.g005" style="box-sizing: border-box; background: transparent; color: rgb(52, 152, 219); text-decoration: none;">doi:10.1371/journal.pbio.1001687.g005</a></figcaption></figure><p>Although this is an essay and not a research article, I’ve added the data and R scripts used to generate the figures (1, 3-5) as <a href="http://dx.doi.org/10.1371/journal.pbio.1001687.s001">supporting information</a>. As I <a href="http://blog.martinfenner.org/2013/10/20/the-complete-article/">have said earlier</a>, I think it is important that an article contains more than the text. With the open source software <a href="http://www.r-project.org/">R</a> or <a href="http://www.rstudio.com/">RStudio</a> everyone can recreate the figures, and can look at the data underlying the figures in the essay. One can for example look into the data behind Fig. 5 to better understand how articles with F1000Prime recommendations <strong><strong>and</strong></strong> Wikipedia mentions differ from those <strong><strong>only</strong></strong> recommended in F1000Prime. Feel free to ask for help getting started in the comments.</p><p>Incidentally this is also my first PLOS article (my wife is way ahead of me with 5 research articles), so that I can finally look at PLOS article-level metrics as an author - after being the technical lead for this project since May 2012.</p><h2 id="references">References</h2><p>Fenner, M. (2013). What can article-level metrics do for you? <em>PLoS Biol</em>, <em>11</em>(10), e1001687. <a href="http://doi.org/10.1371/journal.pbio.1001687">doi:10.1371/journal.pbio.1001687</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Complete Article]]></title>
            <link>https://blog.martinfenner.org/posts/the-complete-article</link>
            <guid>b3200195-69bd-4488-ba19-6706a1ae9db9</guid>
            <pubDate>Sun, 20 Oct 2013 16:26:00 GMT</pubDate>
            <description><![CDATA[Open access to research data is becoming increasingly important, as manifested
by memos or press releases from the Wellcome Trust
[http://www.wellcome.ac.uk/About-us/Policy/Policy-and-position-statements/WTX035043.htm]
, the European Commission
[http://europa.eu/rapid/press-release_IP-12-790_en.htm], and the the Office of
Science and Technology Policy
[http://www.whitehouse.gov/blog/2013/02/22/expanding-public-access-results-federally-funded-research] 
(OSTP) from the White House.

Open access t]]></description>
            <content:encoded><![CDATA[<p>Open access to research data is becoming increasingly important, as manifested by memos or press releases from the <a href="http://www.wellcome.ac.uk/About-us/Policy/Policy-and-position-statements/WTX035043.htm">Wellcome Trust</a>, the <a href="http://europa.eu/rapid/press-release_IP-12-790_en.htm">European Commission</a>, and the <a href="http://www.whitehouse.gov/blog/2013/02/22/expanding-public-access-results-federally-funded-research">the Office of Science and Technology Policy</a> (OSTP) from the White House.</p><p>Open access to research data is important as this makes it easier for other researchers to reproduce the research, and to build upon the research by others by re-analysis of data or combination with other research data. In other words, <a href="http://royalsociety.org/policy/projects/science-public-enterprise/report/">Science as an open enterprise</a>.</p><p>The major challenge to open access to research data is that data sharing is not a widespread practice. Several strategies have been developed to create incentives for researchers to share research data, including services that make it easier to share research data (e.g. <a href="http://figshare.com/">figshare</a>, <a href="http://dataup.cdlib.org/">DataUp</a> and <a href="http://www.zenodo.org/">Zenodo</a>), <a href="http://www.knowledge-exchange.info/Default.aspx?ID=586">metrics for research data</a>, and data journals such as <a href="http://www.earth-system-science-data.net/">Earth System Science Data</a>, <a href="http://www.gigasciencejournal.com/">GigaScience</a> or the <a href="http://openarchaeologydata.metajnl.com/">Journal of open archaeology data</a>. Some of the sticks that have been tried in addition to the carrots above include data management plan requirements such as those <a href="http://www.nsf.gov/bfa/dias/policy/dmp.jsp">set forth by the National Science Foundation (NSF)</a> in 2011.</p><p>I would argue that all these carrots and sticks will eventually fall short, unless we redefine what the journal article (and similarly monograph) in the digital age should be about. Research data should become a required part of any research article, rather than an optional afterthought, or taking on a life on their own in a separate data journal.</p><p>The <em>complete article</em> - as I would like to call this journal article made fit for the digital age - should not only include the research data used to create figures and tables and reportes as results. Equally important are descriptions of reagents, workflows and software tools that go into much more detail compared to what is common practice today.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/complete_paper.png" class="kg-image" alt="Ingredients of the complete article"><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Ingredients of the complete article</strong></figcaption></figure><p>The <em>complete article</em> does not have to come as one big file. More likely the research data will be hosted at one or more data centers elsewhere. Authorship will turn into contributorship and will include all roles required to put the <em>complete article</em> together, including for example data collection and -analysis, and writing software needed to analyze the data. The <em>complete article</em> can be shorter or longer than the typical article today, important is not article length, but the combination of text, data, and description of reagents and analysis tools.</p><p>The <em>complete article</em> should also include (or link to) the text of the peer reviews and previous article versions, including preprints. This makes it much easier to understand the article (and the data) in context. The <em>complete article</em> should also link to article-level metrics post-publication for similar reasons.</p><p>This idea of a <em>complete article</em> is not too far away from the best practices used today, but it is important to make it the default for scientific publication. Too much of what we publish today is still centered around the concept of what can be printed on paper, and telling exciting stories that have impact counts more than telling complete stories that can be reproduced.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Challenges in automated DOI resolution]]></title>
            <link>https://blog.martinfenner.org/posts/challenges-in-automated-doi-resolution</link>
            <guid>629bdd07-70e5-4e32-bc76-3bf31ec5c061</guid>
            <pubDate>Sun, 13 Oct 2013 16:29:00 GMT</pubDate>
            <description><![CDATA[Yesterday we created a set of roughly 10,000 DOIs for journal articles published
in 2011 or 2012. We used these DOIs as a reference set in a data hackathon
[http://almdatachallenge.eventbrite.com/] around article-level
metrics/altmetrics - material for another blog post.

The random DOis were generated using the CrossRef RanDOIm service
[http://random.labs.crossref.org/], with article titles fetched from the 
CrossRef OpenURL API [http://labs.crossref.org/openurl/]. We didn’t have time to
proper]]></description>
            <content:encoded><![CDATA[<p>Yesterday we created a set of roughly 10,000 DOIs for journal articles published in 2011 or 2012. We used these DOIs as a reference set in a <a href="http://almdatachallenge.eventbrite.com/">data hackathon</a> around article-level metrics/altmetrics - material for another blog post.</p><p>The random DOis were generated using the <a href="http://random.labs.crossref.org/">CrossRef RanDOIm service</a>, with article titles fetched from the <a href="http://labs.crossref.org/openurl/">CrossRef OpenURL API</a>. We didn’t have time to properly parse the publication date and only used the publication year. We used the <code>crossref_r</code> and <code>crossref</code> functions from the rOpenSci <a href="http://ropensci.github.io/rplos/">rplos package</a> (and some extra help from Scott Chamberlain) to achieve this, the datasets were deposited to figshare and can be found <a href="http://dx.doi.org/10.6084/m9.figshare.821209">here</a> (2011) and <a href="http://dx.doi.org/10.6084/m9.figshare.821213">here</a> (2012).</p><p>The basic idea behind DOI names is summarized well in the <a href="http://en.wikipedia.org/wiki/Digital_object_identifier">Wikipedia entry</a>:</p><blockquote>A digital object identifier (DOI) is a character string (a “digital identifier”) used to uniquely identify an object such as an electronic document. Metadata about the object is stored in association with the DOI name and this metadata may include a location, such as a URL, where the object can be found. The DOI for a document is permanent, whereas its location and other metadata may change. Referring to an online document by its DOI provides more stable linking than simply referring to it by its URL, because if its URL changes, the publisher need only update the metadata for the DOI to link to the new URL.</blockquote><p>DOIs for journal articles should provide users with a URL specific for that journal article. This URL could point to a digital copy of the journal article in HTML or PDF format, or could point to a landing page (with an abstract or other basic metadata) for journal articles that require a subscription. This should work not only for humans using a web browser, but also for automated services using command line tools such as <a href="http://curl.haxx.se/">curl</a> as scientific infrastructure depends heavily on automation and computers talking to each other. In our use case we want to find content linking to a specific article, and as some services (e.g. social media) will use the URL and not DOI of an article, we need to find out that URL.</p><p>Unfortunately it was difficult to find a URL for many DOIs in our reference set using automated tools. All these DOIs resolve to URLs for human users using a web browser, but for automated tools there are a number of challenges:</p><h3 id="requiring-a-cookie">Requiring a cookie</h3><p>Some publishers require a cookie, and that can cause problems for automated tools. We can use the popular command line tool <code>curl</code> with the options <code>-L</code> to follow redirects and <code>-I</code> to only send the header (as we care about the location and not the content of the page).</p><pre><code>curl -I -L "http://dx.doi.org/10.1080/13658816.2010.531020"</code></pre><p>This command will lead us not to a page specific for that article, but to a “Cookie absent” page. You can work around this by having curl accept cookies:</p><pre><code>curl -I -L --cookie "tmp" "http://dx.doi.org/10.1080/13658816.2010.531020"</code></pre><p>Unfortunately not all tools do this. The way Facebook tracks likes, shares, comments, etc. is a prominent example.</p><h3 id="too-many-redirects">Too many redirects</h3><p>Some DOIs never resolve using a HEAD request, and curl stops after 50 redirects:</p><pre><code>curl -I -L "http://dx.doi.org/10.1097/SLA.0b013e318235e525"</code></pre><p>This error may relate to the “requiring a cookie” error above.</p><h3 id="method-not-allowed">Method not allowed</h3><p>Some DOis HEAD requests result in a “405 Method Not Allowed” error. The reason is that the journal platform doesn’t accept the HEAD request, but wants a GET instead.</p><pre><code>curl -I -L "http://dx.doi.org/10.1002/sam.10120"</code></pre><p>The <a href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html">HTTP 1.1 protocol</a> says about HEAD:</p><blockquote>The HEAD method is identical to GET except that the server MUST NOT return a message-body in the response. … This method is often used for testing hypertext links for validity, accessibility, and recent modification.</blockquote><p>We can work around this error by using a GET request, which unfortunately creates extra overhead and is not the recommended way to obtain this kind of information.</p><h3 id="empty-reply-from-server">Empty reply from server</h3><p>Some DOIs never resolve using a HEAD because curl reports “Empty reply from server” and we don’t get a HTTP 200 status code.</p><pre><code>curl -I -L "http://dx.doi.org/10.1016/j.cca.2011.04.012"</code></pre><p>You can again work around this by using the location information before the last redirect, but maybe resolving a DOI should not result in curl routinely throwing an error. It looks as if this error is related to “method not allowed”, as a GET request resolves to a landing page.</p><p>This problem is not specific to the <code>curl</code> tool, we get exactly the same error with <code>wget</code>:</p><pre><code>wget -S --spider "http://dx.doi.org/10.1016/j.cca.2011.04.012"</code></pre><h3 id="timeout-errors">Timeout errors</h3><p>Some DOI resolutions resulted in timeout errors, but this was temporary and much less frequent than the errors above.</p><h3 id="resource-not-found">Resource not found</h3><p>We didn’t specifically look into this error, which is a well-known problem with URLs. The DOI names we used were from 2011 and 2012, and it is known that <a href="http://en.wikipedia.org/wiki/Link_rot">link rot</a> is more common the older the resource is.</p><h3 id="content-negotiation">Content negotiation</h3><p>As Karl Ward has pointed out in the comments there are other ways to get to the URL from the DOI name, e.g. using content negotiation:</p><pre><code>curl -LH "Accept: application/vnd.crossref.unixref+xml" "http://dx.doi.org/10.1016/j.cca.2011.04.012"</code></pre><p>The URL is stored in the <code>doi_data/resource</code> attribute. The URL stored there is unfortunately not always the final landing page for the article, e.g. for the DOI name used in the example above.</p><h3 id="conclusions">Conclusions</h3><p>We created a reference set of 10,000 DOIs to collect metrics around them. The first conclusion from this exercise is that getting the URL for these articles is a challenge in many cases. This does not seem to relate to a permission problem for subscription content, but rather how the HTTP HEAD request is handled. Content negotiation is one alternative, but sometimes leads to different URLs for the landing page than where the user would get via the browser. We therefore have to rewrite our code to use GET requests and to better handle the scenarios above.</p><p><em>Update 10/13/13: Updated the title and the text to make it clear that I am not talking about DOIs that don’t resolve for human users, but rather about the problems automating this process using command-line tools.</em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Altmetrics coming of age? Not for Wikipedia]]></title>
            <link>https://blog.martinfenner.org/posts/altmetrics-coming-of-age-not-for-wikipedia</link>
            <guid>2f4b11bb-2bb2-4998-8dfa-dad98d706ecd</guid>
            <pubDate>Sat, 10 Aug 2013 16:31:00 GMT</pubDate>
            <description><![CDATA[Ten days ago Information Standards Quarterly (ISQ) published a special issue on
altmetrics [http://www.niso.org/publications/isq/2013/v25no2/]. I was the guest
editor for the five altmetrics articles, and in the editorial
[http://dx.doi.org/10.3789/isqv25no2.2013.01] that I titled Altmetrics have come
of age I argued that

> We no longer need to talk about whether it is possible to reliably collect
altmetrics, or whether this is valuable information that can complement
citations and usage statis]]></description>
            <content:encoded><![CDATA[<p>Ten days ago Information Standards Quarterly (ISQ) published a <a href="http://www.niso.org/publications/isq/2013/v25no2/">special issue on altmetrics</a>. I was the guest editor for the five altmetrics articles, and in the <a href="http://dx.doi.org/10.3789/isqv25no2.2013.01">editorial</a> that I titled <strong><strong>Altmetrics have come of age</strong></strong> I argued that</p><blockquote>We no longer need to talk about whether it is possible to reliably collect altmetrics, or whether this is valuable information that can complement citations and usage statistics.</blockquote><p>In June we have seen that the National Information Standards Organization (<a href="http://www.niso.org/home/">NISO</a>) was <a href="http://dx.doi.org/10.3789/isqv25no2.2013.07">awarded a grant</a> by the <a href="http://www.sloan.org/">Sloan Foundation</a> to develop standards and recommended best practices for altmetrics.</p><p>Unfortunately Wikipedia - which is of course an important source of altmetrics information and was also mentioned in the editorial - doesn’t think so. When you try to go to the <a href="https://en.wikipedia.org/w/index.php?title=Altmetrics&amp;redirect=no">Altmetrics</a> page on the English Wikipedia, you get this:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/wikipedia_redirect.png" class="kg-image" alt="Wikipedia doesn’t think Altmetrics need their own page"><figcaption>Wikipedia doesn’t think Altmetrics need their own page</figcaption></figure><p>In other words, you are redicted to a short section on the <a href="https://en.wikipedia.org/wiki/Impact_factor#Article_level_metrics_and_altmetrics">Impact Factor</a> page. I would go and start an altmetrics (and article-level metrics) page, but with my professional involvement in altmetrics it is difficult to write from a <a href="http://en.wikipedia.org/wiki/Wikipedia:Neutral_point_of_view">neutral point of view</a>, one of the core Wikipedia policies.</p><p><em>Update August 13, 2013: We now have a nice <a href="http://en.wikipedia.org/wiki/Altmetrics">altmetrics</a> Wikipedia page thanks to the hard work of <a href="http://en.wikipedia.org/wiki/User:Egonw">Egon Willighagen</a> and others.</em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[CSL is more than citation styles]]></title>
            <link>https://blog.martinfenner.org/posts/csl-is-more-than-citation-styles</link>
            <guid>7a66bcf9-958d-494a-b4c5-ae875db5231a</guid>
            <pubDate>Thu, 08 Aug 2013 16:33:00 GMT</pubDate>
            <description><![CDATA[According to the description [http://citationstyles.org/] on the Citation Style
Language (CSL) website, CSL is an open XML-based language to describe the
formatting of citations and bibliographies. We use reference managers such as 
Zotero, Mendeley, or Papers to format our references in manuscripts we submit
for publication, and underneath a CSL processor such as Citeproc-js
[https://bitbucket.org/fbennett/citeproc-js/wiki/Home] - together with a CSL
file for a particular citation style - is do]]></description>
            <content:encoded><![CDATA[<p>According to the <a href="http://citationstyles.org/">description</a> on the Citation Style Language (CSL) website, CSL <em>is an open XML-based language to describe the formatting of citations and bibliographies</em>. We use reference managers such as <strong><strong>Zotero</strong></strong>, <strong><strong>Mendeley</strong></strong>, or <strong><strong>Papers</strong></strong> to format our references in manuscripts we submit for publication, and underneath a CSL processor such as <a href="https://bitbucket.org/fbennett/citeproc-js/wiki/Home">Citeproc-js</a> - together with a CSL file for a particular citation style - is doing the work:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/csl.png" class="kg-image" alt="Citation processing during manuscript writing"><figcaption>Citation processing during manuscript writing</figcaption></figure><p>When the journal article is accepted the publisher takes the text with the formatted text citation and turns it into XML, a process that is error-prone and takes time:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/csl2.png" class="kg-image" alt="Citation processing by the publisher"><figcaption>Citation processing by the publisher</figcaption></figure><p>It is not hard to see that something is very wrong here:</p><ul><li>Authors are required to use a specific citation style (there are probably about 1,000 different citation styles and many more dependent styles) even though the publisher doesn’t directly use the formatted text. The publisher eLife <a href="http://www.elifesciences.org/elife-references/">accepts references in any format</a>.</li><li>Turning structured information into plain text and back into structured XML is always a bad idea. <a href="http://twitter.com/kaveh1000">Kaveh Bazargan</a> is a typesetter who has gone on record for saying that we should stop this nonsense and put him out of business.</li></ul><p>It is also obvious how the ideal workflow should look like:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/csl3.png" class="kg-image" alt="Ideal workflow of citation processing"><figcaption>Ideal workflow of citation processing</figcaption></figure><p>We go from structured content to structured content, and never use citations formatted as text as intermediary steps in the workflow.</p><p>What is surprising is that this is an ideal workflow and not something that publishers actually do. Most journal author instructions don’t even mention CSL styles (I work for PLOS and they are no exception). There are some issues to be solved, but they are all minor:</p><ul><li>The Citeproc JSON citation format isn’t really an official standard, but rather something invented for the most popular CSL processor, Citeproc-js.</li><li>People like to fight over standards, and there are always people you prefer bibtex, RIS, MODS or BibJSON over Citeproc JSON, or want authors to to use JATS XML.</li></ul><p>I would really like to push Citeproc JSON as a standard bibliographic exchange format for authors. There are several things I like about Citeproc JSON:</p><ul><li>It is the native format to format citations, so it is used internally by many reference managers anyway.</li><li>Citeproc JSON is really good in handling all the possible variations of author names. Putting all authors into a single text field as in bibtex requires a lot of trickery to get it right.</li><li>JSON is a standard serialization format and there are a kinds of libraries in different programming languages to do things like searching, sorting or finding of duplicates. And JSON is easily extensible, e.g. if we would want to add ORCID identifiers for authors.</li></ul><p>I have five suggestions to move forward:</p><ul><li>Make a specification for Citeproc JSON that is as clear as the CSL specification.</li><li>Consider extending the specification to include content other than citations. Ideally we should be able to add arbitrary <a href="http://blog.martinfenner.org/2013/06/29/metadata-in-scholarly-markdown/">metadata about a manuscript</a>.</li><li>Consider other serialization formats besides JSON. I particularly <a href="http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/">like YAML</a> as it is very similar to JSON, but human-readable, but other people might prefer XML. It is relatively easy to transform data between these serialization formats, in particular between JSON and YAML. In my <a href="http://blog.martinfenner.org/about.html">About page</a> I only need the <a href="https://github.com/nodeca/js-yaml">js-yaml</a> library and one extra line of code to use Citeproc YAML instead of Citeproc JSON (in the d3.js visualization).</li><li>Add Citeproc JSON (and YAML) support to reference managers. Zotero is already doing this, but it should be an easy to add feature if the reference manager is already using CSL internally (Mendeley and Papers).</li><li>Push publishers to accept Citeproc JSON with manuscript submissions.</li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What a publication timeline can tell you]]></title>
            <link>https://blog.martinfenner.org/posts/what-a-publication-timeline-can-tell-you</link>
            <guid>2fda06c5-a583-4d45-8376-3e580db9c488</guid>
            <pubDate>Tue, 06 Aug 2013 16:35:00 GMT</pubDate>
            <description><![CDATA[Now that I can automatically import my publications from my ORCID profile and
display them
[http://blog.martinfenner.org/2013/08/04/automatically-list-all-your-publications-in-your-blog/] 
in this blog, I also want to visualize them. I have started with d3.js code
[https://github.com/mfenner/blog/blob/master/_includes/by_year.js] that displays
the number of publications per year - using the list of my publications in
Citeproc JSON format. The chart is displayed on my About page
[http://blog.mart]]></description>
            <content:encoded><![CDATA[<p>Now that I can <a href="http://blog.martinfenner.org/2013/08/04/automatically-list-all-your-publications-in-your-blog/">automatically import my publications from my ORCID profile and display them</a> in this blog, I also want to visualize them. I have started with <a href="https://github.com/mfenner/blog/blob/master/_includes/by_year.js">d3.js code</a> that displays the number of publications per year - using the list of my publications in Citeproc JSON format. The chart is displayed on my <a href="http://blog.martinfenner.org/about.html">About page</a>, but I have also embedded the Javascript here:</p><p>I am a big fan of data visualizations because they can highlight something that you would otherwise miss. In this case I was really surprised to see how well my different academic jobs over the years (1991-1993, 1994-1998, 1998-2000, 2000-2005, 2005-2012) are reflected in my publication pattern. You clearly see the gaps between the jobs, indicating that I not only switched jobs, but also changed the research focus every time. The publications are listed chronologically on the <a href="http://blog.martinfenner.org/about.html">About page</a> page and you can look at the papers I wrote since my first publication in 1993. My publication pattern seems to indicate that I was never really on track for a typical academic career, so it should not be a surprise that I left academia in 2012.</p><p>There are at least two other visualizations I want to do: publications by type (journal article, book chapter, dataset, etc.), and author position with number of co-authors. You can reuse the Javascript code with small modifications (CSS and the JSON query) even if you are not running a Jekyll blog.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Automatically list all your publications in your blog]]></title>
            <link>https://blog.martinfenner.org/posts/automatically-list-all-your-publications-in-your-blog</link>
            <guid>d6eaf4b6-f664-433a-964d-e1e9b6441bab</guid>
            <pubDate>Sun, 04 Aug 2013 16:37:00 GMT</pubDate>
            <description><![CDATA[A common feature of blogs written by scientists is a listing of all their
publications. Publication lists are a great way to provide background
information about your research. Publication lists should provide links to the
fulltext versions of these publications, should be nicely formatted - e.g. using
a common citation style such as APA - and should be easy to maintain. A number
of tools for a variety of blogging platforms (including Wordpress and Jekyll)
are available to help with this task, b]]></description>
            <content:encoded><![CDATA[<p>A common feature of blogs written by scientists is a listing of all their publications. Publication lists are a great way to provide background information about your research. Publication lists should provide links to the fulltext versions of these publications, should be nicely formatted - e.g. using a common citation style such as APA - and should be easy to maintain. A number of tools for a variety of blogging platforms (including Wordpress and Jekyll) are available to help with this task, but maintaining the list of publications has remained difficult.</p><p>Publication lists are best maintained in a system built for this purpose. This could be either a reference manager, or a profile page in a social network for scientists. Even better suited for this task is your Open Researcher &amp; Contributor ID (<a href="http://orcid.org/">ORCID</a>) profile, as this service (<strong><strong>???</strong></strong>) directly integrates with a number of bibliographic databases and makes the profile information available via an open API.</p><h3 id="orcid-feed">ORCID Feed</h3><p>Last week I have started work on <a href="http://feed.labs.orcid-eu.org/">ORCID Feed</a>, a service that reformats the API response from ORCID into RSS, bibtex and formattted citations, making it easier for scientists to reuse the content stored in their ORCID profile. This service is still experimental, so please report any issues <a href="https://github.com/orcid-eu-labs/orcid-feed/issues">here</a>.</p><h3 id="jekyll-orcid">jekyll-orcid</h3><p>I have now added the final piece to automatically import my publications into this blog. <a href="https://github.com/mfenner/jekyll-orcid">jekyll-orcid</a> is a Jekyll plugin that automatically downloads all my publications from my ORCID profile via <strong><strong>ORCID Feed</strong></strong> and stores them in a subfolder of this blog, both in bibtex and Citeproc JSON format. It does this every time you regenerate your blog, so that the publication list will be automatically updated with new content. I can then use <a href="https://github.com/inukshuk/jekyll-scholar">jekyll-scholar</a>, a popular Jekyll plugin written by Sylvester Keil to generate a bibliography (<code>jekyll-orcid</code> automatically adds a YAML frontmatter section to the files so that jekyll-scholar can process it). I can format this auto-generated bibliography in a variety of ways - you can see the result in my <a href="http://blog.martinfenner.org/about.html">About</a> page where I also provide a download link of the bibtex file.</p><p>My publications are of course also available if I want to cite them in the text, e.g. our recent publication summarizing the main findings from the 2011 European Consensus Conference on germ-cell cancer (<strong><strong>???</strong></strong>), or last year’s case report on liver toxicity induced by the cancer drug imatinib (<strong><strong>???</strong></strong>).</p><p>Similar tools also exist for Wordpress, e.g. <a href="http://wordpress.org/plugins/papercite/">Papercite</a>, which can import the bibtex file directly from ORCID Feed.</p><h3 id="next">Next</h3><p>Now there is only one step missing to have your paper that was just published automatically appear in your publication list. Assuming you have provided your ORCID identifier when you submitted the paper, and the publisher has included your ORCID identifier in the metadata sent to CrossRef (both are already common practices), we only need CrossRef to automatically push that paper into your ORCID profile.</p><p>And once we have this workflow in place, we can automatically add additional information, including links to the fulltext paper in the institutional repository, copyright information, and metrics.</p><h2 id="references">References</h2>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Citeproc YAML for bibliographies]]></title>
            <link>https://blog.martinfenner.org/posts/citeproc-yaml-for-bibliographies</link>
            <guid>34df496a-1835-41c8-bb0a-f0d96365c130</guid>
            <pubDate>Tue, 30 Jul 2013 16:39:00 GMT</pubDate>
            <description><![CDATA[The standard local file formats for bibliographic data are probably bibtex and
RIS. They have been around for a long time, and are supported by all reference
managers and many other tools and services. Unfortunately these formats are far
from perfect:

 * neither bibtex nor RIS use a web-friendly data interchange format such as XML
   or JSON, which makes it harder to work with these formats
 * bibtex - and to a lesser extend RIS - don’t support all entry types that we
   need, e.g. datasets, or]]></description>
            <content:encoded><![CDATA[<p>The standard local file formats for bibliographic data are probably bibtex and RIS. They have been around for a long time, and are supported by all reference managers and many other tools and services. Unfortunately these formats are far from perfect:</p><ul><li>neither bibtex nor RIS use a web-friendly data interchange format such as XML or JSON, which makes it harder to work with these formats</li><li>bibtex - and to a lesser extend RIS - don’t support all entry types that we need, e.g. datasets, or new standards such as ORCID author identifiers</li><li>bibtex stores all authors in a single field, which makes author names hard to parse</li></ul><h3 id="bibtex">bibtex</h3><pre><code>@article{fenner2012a,
  title = {One-click science marketing},
  volume = {11},
  url = {http://dx.doi.org/10.1038/nmat3283},
  doi = {10.1038/nmat3283},
  number = {4},
  journal = {Nature Materials},
  publisher = {Nature Publishing Group},
  author = {Fenner, Martin},
  year = {2012},
  month = {mar},
  pages = {261-263}
}</code></pre><p>One obvious solution would be to store bibliographic data in XML or JSON. These formats have very good support in all programming languages, and they are the formats used by APIs on the web. There have been some efforts to standardize these formats for bibliographic data, e.g. <a href="http://www.bibjson.org/">BibJSON</a>, <a href="http://www.loc.gov/standards/mods/">MODS</a>, <a href="http://bibtexml.sourceforge.net/">BibTeX XML</a> or Endnote XML.</p><h3 id="bibtex-xml">BibTeX XML</h3><pre><code>&lt;bibtex:entry id='fenner2012a'&gt;
  &lt;bibtex:article&gt;
    &lt;bibtex:title&gt;One-click science marketing&lt;/bibtex:title&gt;
    &lt;bibtex:volume&gt;11&lt;/bibtex:volume&gt;
    &lt;bibtex:url&gt;http://dx.doi.org/10.1038/nmat3283&lt;/bibtex:url&gt;
    &lt;bibtex:doi&gt;10.1038/nmat3283&lt;/bibtex:doi&gt;
    &lt;bibtex:number&gt;4&lt;/bibtex:number&gt;
    &lt;bibtex:journal&gt;Nature Materials&lt;/bibtex:journal&gt;
    &lt;bibtex:publisher&gt;Nature Publishing Group&lt;/bibtex:publisher&gt;
    &lt;bibtex:person&gt;
      &lt;bibtex:first&gt;Martin&lt;/bibtex:first&gt;
      &lt;bibtex:last&gt;Fenner&lt;/bibtex:last&gt;
    &lt;bibtex:person&gt;&lt;bibtex:author/&gt;
    &lt;bibtex:year&gt;2012&lt;/bibtex:year&gt;
    &lt;bibtex:month&gt;mar&lt;/bibtex:month&gt;
    &lt;bibtex:pages&gt;261-263&lt;/bibtex:pages&gt;
  &lt;/bibtex:article&gt;
&lt;/bibtex:entry&gt;</code></pre><p>My problem with these formats is that they are made for computers talking to each other and not humans. I personally think that a file with bibliographic data should be human-readable, similar to why <a href="http://blog.martinfenner.org/2012/12/13/a-call-for-scholarly-markdown/">I like markdown</a> for writing scientific documents.</p><p>When you have too many standards and are not happy with any of them, you of course create a new standard.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://imgs.xkcd.com/comics/standards.png" class="kg-image" alt="How Standards Proliferate. Taken from http://xkcd.com/927/"><figcaption><strong style="box-sizing: border-box; font-weight: bold;">How Standards Proliferate</strong>. Taken from <a href="http://xkcd.com/927/" class="uri" style="box-sizing: border-box; background: transparent; color: rgb(52, 152, 219); text-decoration: none;">http://xkcd.com/927/</a></figcaption></figure><p>My suggestion for a new bibliographic file format is twofold: a) use YAML for data serialization and b) use CSL as data format. <a href="http://www.yaml.org/spec/1.2/spec.html">YAML</a> is a data format popular with Ruby Developers and is described on the <a href="http://yaml.org/">YAML website</a> as</p><blockquote>YAML is a human friendly data serialization standard for all programming languages.</blockquote><p>Something that not may people seem to know is that YAML is a superset of JSON and that <a href="http://yaml.org/spec/1.2/spec.html#id2759572">every JSON file is also a valid YAML file</a>. The main difference is the better human readability of YAML.</p><p><strong><strong>Citation Style Language</strong></strong> is described on the <a href="http://citationstyles.org/">CSL website</a> as</p><blockquote>CSL is an open XML-based language to describe the formatting of citations and bibliographies.</blockquote><p>Although some commercial applications still use proprietary citation styles, CSL has become the de facto standard, and is used by the reference managers <strong><strong>Zotero</strong></strong>, <strong><strong>Mendeley</strong></strong>, <strong><strong>Papers</strong></strong>, and others. This blog uses CSL via Pandoc and the <a href="http://code.google.com/p/citeproc-hs/">citeproc-hs</a> library. CSL processors need bibliographic data in a standard format. The popular <a href="https://bitbucket.org/fbennett/citeproc-js/wiki/Home">Citeproc-js</a> Javascript CSL processor by Frank Bennett for example uses JSON, but we might as well use YAML:</p><h3 id="citeproc-yaml">Citeproc YAML</h3><pre><code>- title: One-click science marketing
  volume: '11'
  URL: http://dx.doi.org/10.1038/nmat3283
  DOI: 10.1038/nmat3283
  issue: '4'
  container-title: Nature Materials
  publisher: Nature Publishing Group
  author:
  - family: Fenner
    given: Martin
    orcid: 0000-0003-1419-2405
  page: 261-263
  id: fenner2012a
  type: article-journal
  issued:
    date-parts:
      - 2012
      - 3</code></pre><p>I hope you agree that this format is not only structured and can be understood by computers, but is also very readable by humans. You may have noticed that I have inserted my ORCID, something that is very difficult to do with bibtex where all authors are stored in one text string (see above).</p><p>Careful readers of this blog will of course remember that <a href="http://blog.martinfenner.org/2013/06/29/metadata-in-scholarly-markdown/">I have written about</a> using YAML to store metadata about a blog post. We could now add bibliographic information to these metadata, either in the YAML frontmatter (if it is a Jekyll blog), or in a separate file. It should be straightforward to adapt the existing CSL processors to understand YAML since YAML and JSON are so similar. To get started with some Citeproc YAML, use the new (and still experimental) <strong><strong>ORCID Feed</strong></strong> Webservice with your ORCID and specify the <code>yml</code> format, e.g. <a href="http://feed.labs.orcid-eu.org/0000-0003-1419-2405.yml">http://feed.labs.orcid-eu.org/0000-0003-1419-2405.yml</a> for my publications.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[RSS Feeds for Scholarly Authors]]></title>
            <link>https://blog.martinfenner.org/posts/rss-feeds-for-scholarly-authors</link>
            <guid>502c138f-de35-4147-ae94-a226f16c51e3</guid>
            <pubDate>Fri, 26 Jul 2013 16:48:00 GMT</pubDate>
            <description><![CDATA[Open Researcher & Contributor ID (ORCID
[https://speakerdeck.com/mfenner/orcid-connecting-research-and-researchers-1])
provides a persistent identifier for researchers and lets them claim their
research outputs in the ORCID Registry. I have been involved with ORCID since
early 2010 and I am happy to see that nine months after launch 200,000
researchers have signed up for the service, and the organisation has more than 
70 member organizations [http://orcid.org/about/community/members].

Register]]></description>
            <content:encoded><![CDATA[<p>Open Researcher &amp; Contributor ID (<a href="https://speakerdeck.com/mfenner/orcid-connecting-research-and-researchers-1">ORCID</a>) provides a persistent identifier for researchers and lets them claim their research outputs in the ORCID Registry. I have been involved with ORCID since early 2010 and I am happy to see that nine months after launch 200,000 researchers have signed up for the service, and the organisation has more than <a href="http://orcid.org/about/community/members">70 member organizations</a>.</p><figure class="kg-card kg-embed-card"><iframe src="http://s3.datawrapper.de/BZBSQ/" frameborder="0" allowtransparency="true" allowfullscreen="allowfullscreen" webkitallowfullscreen="webkitallowfullscreen" mozallowfullscreen="mozallowfullscreen" oallowfullscreen="oallowfullscreen" msallowfullscreen="msallowfullscreen" width="600" height="400" style="box-sizing: border-box; color: rgb(0, 0, 0); font-family: ff-tisa-web-pro, Georgia, serif; font-size: 21px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"></iframe></figure><p><a href="https://orcid.org/register">Registering for an ORCID identifier</a> is easy, and can be done in a few minutes. Claiming works in the profile is also straighforward, and works by integration with CrossRef Search, Scopus, Web of Science, DataCite Metadata Search, and other services. Even though about 1.5 million works have been claimed by now, many users have still not claimed any works or added profile information in other ways.</p><p>These numbers should go up as more academic institutions sign up for ORCID and help their researchers create ORCIDs and claim works. In the meantime we need more incentives for researchers to add publications to their ORCID profile. Publication lists are a very good reason to add your papers and other research outputs to your ORCID profile.</p><h3 id="publication-lists">Publication Lists</h3><p>Every researcher maintains a list of his publications in some form. These publication lists are used for grant and job applications, for academic websites to attract collaborators and students, and more. Publication lists can be generated in many different ways, but I have never heard that someone finds this process fun or easy. The challenge is multiplied when the publication list is not generated for an individual, but for a research group, department or institution (my university goes through this process every year uisng RefWorks and produces an <a href="http://www.refworks.com/RefShare2?site=047931198213200000/RWWS6A619751/2013%20Hochschulbibliografie">annual institutional bibliography</a>).</p><p>Although the library usually takes care of the larger publication lists and can help researchers setting up their own lists, there still is much that needs to be done by individual researchers, and the process needs to be easier. Some recommendations are:</p><ul><li>don’t reinvent the wheel</li><li>use persistent identifiers</li><li>use standards</li><li>don’t worry about citation styles</li><li>keep everything upstream, not locally</li></ul><p>Don’t try to invent a new way of managing publication lists. Other people have worked on this problem before, and there are many tools available. This doesn’t mean you shouldn’t try something new, but please build it on top of all the infrastructure and services we have already.</p><p>Managing publication lists becomes much easier when you use persistent identifiers such as DOIs. They make it much easier to obtain metadata (e.g. authors, title, journal) and the fulltext version. Some disciplines use other identifiers, but a local identifier such as a URL is usually a bad idea.</p><p>Use standard protocols, standard file formats and standard metadata. BibTex and RIS are file formats for references that almost every piece of software handling references understands.</p><p>Citation styles come from a time when publications were printed on paper. They make no real sense anymore, and as a researcher you shouldn’t bother which one of 3000+ styles is the appropriate one.</p><p>The last recommendation is the most important one. Don’t try to manage publication lists in your local system, or your department, but rather do this as much upstream as possible. ORCID is an ideal service for this. But don’t try to manually add or edit publications in the ORCID registry, but rather claim them from CrossRef, DataCite or similar services, because these are the places that have authoritative information about publication. If you try to “fix” information (because all metadata can contain mistakes), nobody will notice. If something is wrong with your works, notify the publisher so that the CrossRef metadata can be updated.</p><h3 id="orcid-profiles-as-rss-feeds">ORCID Profiles as RSS Feeds</h3><p>ORCID is a good place to manage publication lists, but it is often not easy to get the information out of the system. The standard way is via a REST API (XML or JSON). This might work really well for a software developer who wants to connect his system to ORCID, but most researchers have other things to do.</p><p>RSS was invented to publish information about frequently updated works, and a good example are Tables of Content (TOC) for journals. RSS is also a great tool to manage publication lists, as it can be easily integrated into content management systems such as Wordpress or Drupal. There is a <a href="http://oxford.crossref.org/best_practice/rss/">Recommendation on RSS Feeds for Scholarly Publishers</a>, and we can apply the same guidelines to <strong><strong>RSS Feeds for Scholarly Authors</strong></strong>. With <a href="http://en.wikipedia.org/wiki/OPML">OPML</a> we also have a standard format to aggregate multiple RSS feeds, and this is true not only for journal RSS feeds, but also author RSS feeds.</p><p>Unfortunately there is one missing piece in this workflow: turning ORCID profiles into RSS feeds. At the <a href="http://occamstypewriter.org/trading-knowledge/2012/11/13/solo-hackday/">SpotOn London hackathon</a> last November I worked with <a href="http://twitter.com/easternblot">Eva Amsen</a> and <a href="http://twitter.com/graemedmoffat">Graeme Moffat</a> to hack this workflow together using available tools. But we really need a more mature solution. Until RSS feeds are provided by the core ORCID service - and there is so much other stuff to do right now that this will take time - the best solution might be a web service that turns ORCID profiles into scholarly RSS as described above for journal articles.</p><p>Today I finally came around implementing a first version of this - hacking together a Ruby Sinatra application hosted on Amazon Web Services (<a href="http://hack4ac.com/">#hack4ac</a> attendees know why). The application takes an ORCID ID (e.g. mine: <a href="http://feed.labs.orcid-eu.org/0000-0003-1419-2405.rss">http://feed.labs.orcid-eu.org/0000-0003-1419-2405.rss</a>) and returns an RSS feed. The first version just returns just the name and biography from the profile, but I only started working on this today. ORCID Feed can be found at <a href="http://feed.labs.orcid-eu.org/">http://feed.labs.orcid-eu.org</a> and the source code is available at <a href="https://github.com/mfenner/orcid-feed">Github</a>. Please add suggestions and comments to the Github issue tracker <a href="https://github.com/mfenner/orcid-feed/issues">here</a>.</p><p><strong><strong>Update 7/28/13</strong></strong>: <em>I’ve added publications to the output, and additional content types. Use them as extension (e.g. <code>.json</code>), as format parameter (e.g. <code>?format=rss</code>), or use an accept-header, e.g. <code>Accept: application/x-bibtex</code>. I’ve also added basic error checking with cleanup of names and removal of duplicates.</em></p><ul><li>html (the default): forward to profile on the ORCID website</li><li>rss - RSS feed</li><li>bib - bibtex file</li><li>json - Citeproc JSON</li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The trouble with keynotes]]></title>
            <link>https://blog.martinfenner.org/posts/the-trouble-with-keynotes</link>
            <guid>87320192-6f0b-42e4-b0b6-03907d8466ff</guid>
            <pubDate>Tue, 23 Jul 2013 16:50:00 GMT</pubDate>
            <description><![CDATA[A keynote is a presentation typically given at a start of a conference that sets
the central theme for the event. A keynote speaker usually has more time (45-60
min) than other presenters, and has the full attention of everyone attending the
conference. The keynotes at the conferences I attended the last several years
(mostly scholarly communication conferences) seem to work like this:

 * find a prominent speaker, ideally not a core member of the community
   attending the conference
 * tell hi]]></description>
            <content:encoded><![CDATA[<p>A keynote is a presentation typically given at a start of a conference that sets the central theme for the event. A keynote speaker usually has more time (45-60 min) than other presenters, and has the full attention of everyone attending the conference. The keynotes at the conferences I attended the last several years (mostly scholarly communication conferences) seem to work like this:</p><ul><li>find a prominent speaker, ideally not a core member of the community attending the conference</li><li>tell him to talk about something he knows a lot about, not necessarily a central theme of the conference</li><li>the keynote should be inspiring and eye-opening, instead of focussing on the conference</li></ul><p>The problem with this approach is that it focusses too much on the <em>prominent speaker</em> and it runs the risk of the keynote speaker talking about what he always talks about. Meaning that we don’t learn much if we have heard the keynote speaker before. Which is too bad, because keynotes should contain things that are unexpected and exciting.</p><p>One of the best keynotes I had the pleasure of listening to in the last several years was the one given by <a href="http://michaelnielsen.org/blog/michael-a-nielsen/">Michael Nielsen</a> at <a href="https://martinfenner.ghost.io/2013/07/23/the-trouble-with-keynotes/(http://www.nature.com/spoton/)">Science Online London 2011</a> (disclaimer: I was one of the conference organizers). Not only is Michael a very good speaker, but his presentation about <strong><strong>Open Science</strong></strong> fit perfectly into the conference, and it was clear that he had made the presentation specifically for this conference (with an audience that knows a lot about Open Science). One of the main themes of his presentation - the <em>collective action problem</em>, or to get started with something that benefits everyone, but where there is a cost doing the first step - is something I later picked up in a publication about the Open Researcher &amp; Contributor ID (Fenner, Gomez, &amp; Thorisson, 2011).</p><figure class="kg-card kg-embed-card kg-card-hascaption"><iframe src="http://player.vimeo.com/video/29784152" width="720" height="480" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen="" style="box-sizing: border-box; color: rgb(0, 0, 0); font-family: ff-tisa-web-pro, Georgia, serif; font-size: 21px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"></iframe><figcaption><em>Keynote by Michael Nielsen at the <a href="http://www.nature.com/spoton/">Science Online London 2011 Conference</a>, video recording and editing by <a href="http://river-valley.tv/keynote-solo2011/">River Valley TV</a>.</em></figcaption></figure><p>Luckily we increasingly have video recordings of keynote presentations available online, making it easier to listen to the good presentations. <a href="http://www.ted.com/tedx">TED and TEDx</a> have of course made the format of recordings of carefully prepared talks popular. For large scholarly and academic conferences the best starting point is <a href="http://river-valley.tv/">River Valley TV</a>. The <a href="http://www.mediatheque.lindau-nobel.org/">Lindau Nobel Laureate Meeting</a> has hundreds of presentations by Nobel laureates. And as video recording and streaming has become easier technically (e.g. with <a href="http://googleblog.blogspot.de/2011/09/google-92-93-94-95-96-97-98-99-100.html">Google Hangouts on Air</a>), recording good keynotes should become the norm and not the exception.</p><p><em>After several hundred blog posts here and elsewhere, this may well be my first blog post with embedded video.</em></p><h2 id="references">References</h2><p>Fenner, M., Gomez, C. G., &amp; Thorisson, G. A. (2011). Key issue: Collective action for the open researcher &amp; contributor iD (oRCID). <em>Serials: The Journal for the Serials Community</em>, <em>24</em>(3), 277–279. Retrieved from <a href="http://dx.doi.org/10.1629/24277">http://doi.org/10.1629/24277</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Creating charts with Datawrapper]]></title>
            <link>https://blog.martinfenner.org/posts/creating-charts-with-datawrapper</link>
            <guid>3db0ce74-7053-4f2a-a11c-1b779494c341</guid>
            <pubDate>Fri, 19 Jul 2013 16:54:00 GMT</pubDate>
            <description><![CDATA[Figures are an important part of any scientific document. While the kind of
figure commonly used obviously varies between disciplines, charts are an
important part of many publications. There are two problems in how charts are
currently used:

 * the data used to draw the chart are not available or difficult to obtain
 * charts are drawn as static images with no interactivity, e.g. to see the
   values of individual data points

Ross Mounce and others did a Figures → Data project at the recent h]]></description>
            <content:encoded><![CDATA[<p>Figures are an important part of any scientific document. While the kind of figure commonly used obviously varies between disciplines, charts are an important part of many publications. There are two problems in how charts are currently used:</p><ul><li>the data used to draw the chart are not available or difficult to obtain</li><li>charts are drawn as static images with no interactivity, e.g. to see the values of individual data points</li></ul><p>Ross Mounce and others did a <strong><strong>Figures → Data</strong></strong> project at the recent <a href="http://hacka4ac.com/">hack4ac</a> to extract data from figures, described in a <a href="http://rossmounce.co.uk/2013/07/09/hack4ac-recap/">blog post</a>. The experience was painful, even though they started with a <em>really</em> simple chart.</p><p>While we should of course <a href="http://datadryad.org/">publish all data associated with a paper</a>, the smarter strategy to overcome the two limitations above would be to embed the data used for a chart directly into the document. We have many tools that can accomplish this, and I have given an example using R in an <a href="http://blog.martinfenner.org/2013/06/17/what-is-scholarly-markdown/">earlier blog post</a>. The problem is the sometimes steep learning curve.</p><p>One approach is to build an easy-to use online tool, and <a href="http://datawrapper.de/">Datawrapper</a> is exactly that:</p><blockquote>An open source tool helping anyone to create simple, correct and embeddable charts in minutes.</blockquote><p>Datawrapper uses the <strong><strong>d3.js</strong></strong> and <strong><strong>Highcharts</strong></strong> Javascript libraries for data visualizations, and the service is easy to use. It took me for example about 15 min to generate the chart below. The data used for the chart are embedded (click <strong><strong>Get the data</strong></strong>) and you can hover over the chart to see the actual numbers by month.</p><figure class="kg-card kg-embed-card"><iframe src="http://s3.datawrapper.de/7PqqU/" frameborder="0" allowtransparency="true" allowfullscreen="allowfullscreen" webkitallowfullscreen="webkitallowfullscreen" mozallowfullscreen="mozallowfullscreen" oallowfullscreen="oallowfullscreen" msallowfullscreen="msallowfullscreen" width="720" height="480" style="box-sizing: border-box;"></iframe></figure><p>Most journal articles see the highest usage immediately after publication, and the light purple line shows this pattern for Darcy et al. (2009). The dark purple line for Moher et al. (2009) – published on the same day – on the other hand shows a highly unusual usage pattern, as the usage actually increases over time, starting about 1 1/2 years after publication. The article is a guideline for reporting systematic reviews and meta-analyses, and is now viewed more often than directly after publication four years ago.</p><p>Datawrapper does three things: it makes it easy to generate charts, it allows you to embed them directly into your webpage (using an <code>&lt;iframe&gt;</code> tag), and it is Open Source software (MIT license, Github repo <a href="https://github.com/datawrapper/datawrapper">here</a>) so that you can help improve the code and host this service on your own. DataWrapper was written in Javascript and PHP by a group of German journalists, and the main focus is data journalism where the service has become really <a href="http://blog.datawrapper.de/2013/datawrapper-crosses-mark-of-10-million-visits/">popular</a> with more than 3.5 million views of embedded charts in May 2013 alone.</p><p>Datawrapper is a perfect tool for science blogs and websites with scientific content, but it can also enhance the charts in scientific articles. We need a few additional chart types, error bars and more flexible labeling. And we might want to add a license picker, making it easy to add a Creative Commons license so that it is clear how the chart can be reused. Datawrapper is intended for online use, but the service can also save the charts as PNG or PDF. We would want to add saving to SVG (already used for online rendering) for easier embedding into the XML and epub versions of articles.</p><h2 id="references">References</h2><p>D’Arcy, E., &amp; Moynihan, R. (2009). Can the relationship between doctors and drug companies ever be a healthy one? <em>PLoS Medicine</em>, <em>6</em>(7), e1000075. Retrieved from <a href="http://dx.doi.org/10.1371/journal.pmed.1000075">http://dx.doi.org/10.1371/journal.pmed.1000075</a></p><p>Moher, D., Liberati, A., Tetzlaff, J., &amp; Altman, D. G. (2009). Preferred reporting items for systematic reviews and meta-analyses: The pRISMA statement. <em>PLoS Medicine</em>, <em>6</em>(7), e1000097. Retrieved from <a href="http://dx.doi.org/10.1371/journal.pmed.1000097">http://dx.doi.org/10.1371/journal.pmed.1000097</a></p><hr>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Auto generating links to data and resources]]></title>
            <link>https://blog.martinfenner.org/posts/auto-generating-links-to-data-and-resources</link>
            <guid>84de2484-f805-4df2-b842-f9803c8cd8ea</guid>
            <pubDate>Tue, 02 Jul 2013 16:58:00 GMT</pubDate>
            <description><![CDATA[A few weeks ago Kafkas et al. (2013) published a paper looking at current
patterns of how datasets o biological databases are cited in research articles,
based on an analysis of the full text Open Access articles available from Europe
PMC. They identified data ctiations by:

 1. Accession numbers available in articles as publisher-supplied, structured
    content;
 2. Accession numbers identified in articles by text mining;
 3. References to articles from the ENA, UniProt and PDBe records.

They]]></description>
            <content:encoded><![CDATA[<p>A few weeks ago Kafkas et al. (2013) published a paper looking at current patterns of how datasets o biological databases are cited in research articles, based on an analysis of the full text Open Access articles available from Europe PMC. They identified data ctiations by:</p><ol><li>Accession numbers available in articles as publisher-supplied, structured content;</li><li>Accession numbers identified in articles by text mining;</li><li>References to articles from the ENA, UniProt and PDBe records.</li></ol><p>They could show that text mining doubles the number of structured annotations available in journal articles (from 2.26% to 5.15%), and that these structured annotations should be extended beyond the ENA, UniProt and PDB identifiers that their analysis focused on. ENA identifiers (for nucleotide sequences in GenBank, EMBL or DDBJ) make up the largest group, with 160,112 identifiers found in the 410,364 articles that were analyzed.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/journal.pone.0063184.g003.png" class="kg-image" alt="Database Citation in Full Text Biomedical Articles. Fig. 3 from (Kafkas et al., 2013)."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Database Citation in Full Text Biomedical Articles</strong>. Fig. 3 from <span class="citation" data-cites="Kafkas:2013fp" style="box-sizing: border-box;">(Kafkas et al., 2013)</span>.</figcaption></figure><p>Another result in the paper is that references to articles in these databases show little overlap with database links found in articles. One of the conclusions drawn by the author is that</p><blockquote>Text-mining can be used to extend structured data citation, and could be a basis for the development of services to help authors or editors to add structured content at the beginning of the publication process, rather than after the fact.</blockquote><p>Adding structured data citations during the authoring phase of a manuscript requires tools that make this process easier, providing auto-linking and verification of the without requiring extra input from the author. Scholarly Markdown is an ideal platform for these tools, as it is easier to extend than traditional word processors such as Microsoft Word. During a small workshop around persistent identifiers for data (<a href="http://datacite.org/">DataCite</a>), people (<a href="http://orcid.org/">ORCID</a>) and geological samples (<a href="http://www.geosamples.org/igsnabout">IGSN</a>) that took place yesterday and today at the <a href="http://www.gfz-potsdam.de/portal/gfz/cegit">GFZ Potsdam</a> I worked on a tool that does auto-linking for these identifiers:</p><ul><li>IGSN. <a href="http://www.geosamples.org/igsnabout">International Geosample Number</a></li><li>MGI identifiers for genetically modified mouse strains in the <a href="http://www.findmice.org/about">Internal Mouse Strain Resource</a></li><li>ENA. <a href="http://www.ebi.ac.uk/ena/about/about">Genbank / ENA / DDBJ nucleotide sequences</a></li><li>UniProt protein sequences from the <a href="http://www.uniprot.org/help/about">UniProt database</a></li><li>PDB. <a href="http://www.rcsb.org/pdb/static.do?p=home/faq.html">Protein Data Bank protein structure information</a></li></ul><p>The list includes the IGSN, the database identifiers studied by Kafkas et al (2013), and the MGI identifier for genetically altered mice. In the life sciences there is a long tradition - and requirement by journals - to use database identifiers for data, but identifiers for resources such as genetically modified mice are unfortunately not in common use.</p><p>This blog uses the Pandoc markdown processor and the Jekyll static website generator. The easiest way to implement this functionality was by writing a filter for the liquid templating engine used by Jekyll, and provide this filter as a Jekyll plugin. The Jekyll plugin can be found at <a href="https://github.com/mfenner/jekyll-scholmd">mfenner/jekyll-scholmd</a>. The plugin expects the name of the identifier, followed by a colon and optional space, followed by the identifier:</p><pre><code>GenBank:  M10090
IGSN:  JRH964436
MGI:  96922
UniProt:  P02144
PDB:  1mbn</code></pre><p>This input is automatically translated into <a href="http://www.ebi.ac.uk/ena/data/view/M10090">GenBank:M10090</a>, <a href="http://hdl.handle.net/10273/JRH964436">IGSN:JRH964436</a>, <a href="http://www.findmice.org/summary?gaccid/96922">MGI:96922</a>, and information about the human myoglobin protein (<a href="http://www.uniprot.org/uniprot/P02144">UniProt:P02144</a>, <a href="http://www.rcsb.org/pdb/explore/explore.do?structureId=1mbn">PDB:1mbn</a>) is generated in a similar fashion.</p><p>The plugin was written in a few hours today, and is my first Jekyll plugin. There is room for improvement, e.g. support for more identifiers, better regex matching, validation of the resulting links, and automated tag generation if an identifier is found. Ideally the auto-linking should happen in the markdown and not the HTML output, so that these structured database links are also available in other markdown outputs such as PDF. But this is another example how Scholarly Markdown can make it easier for researchers to author documents without requiring a fancy web-based user interface.</p><h2 id="references">References</h2><p>Kafkas, Ş., Kim, J.-H., &amp; McEntyre, J. R. (2013). Database Citation in Full Text Biomedical Articles. <em>PLoS ONE</em>. <a href="http://doi.org/10.1371/journal.pone.0063184">doi:10.1371/journal.pone.0063184</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Metadata in Scholarly Markdown]]></title>
            <link>https://blog.martinfenner.org/posts/metadata-in-scholarly-markdown</link>
            <guid>e902cfc5-f699-4a65-b974-2d549adb467f</guid>
            <pubDate>Sat, 29 Jun 2013 17:01:00 GMT</pubDate>
            <description><![CDATA[Scholarly documents often need metadata that describe them: typically author(s),
title and location (DOI or URL), but possibly many other things. For some
metadata it makes sense to store them in the document text, e.g. as is typically
done for citations. The problem is that this can make it hard to make the
metadata machine-readable. The worst place for metadata is of course outside of
the document, and unfortunately that it is the most common way of doing this.
Two examples:

 * Manuscript sub]]></description>
            <content:encoded><![CDATA[<p>Scholarly documents often need metadata that describe them: typically author(s), title and location (DOI or URL), but possibly many other things. For some metadata it makes sense to store them in the document text, e.g. as is typically done for citations. The problem is that this can make it hard to make the metadata machine-readable. The worst place for metadata is of course outside of the document, and unfortunately that it is the most common way of doing this. Two examples:</p><ul><li>Manuscript submission. Papers submitted to scholarly journals contain the metadata in the text, but authors are required to enter the information again into a webform. You can add metadata (<a href="http://office.microsoft.com/en-001/word-help/add-property-information-to-a-document-HA010163766.aspx">property information</a>) to Microsoft Word documents, but it seems that nobody is doing it.</li><li>PDFs and image files. Even though we have at least one good standard with <a href="http://blogs.plos.org/mfenner/2008/12/22/just_doi_it/">XMP</a> to store metadata in these documents, it is not a common practice. Information about these documents is therefore stored somewhere else and doesn’t automatically travel with them.</li></ul><p>The best place for metadata is the document itself, and the metadata should be stored in machine-readable format. Another requirement is flexibility in what we can store, and we shouldn’t limit ourselves to a predefined list. Pandoc for example allows only three attributes in the <a href="http://johnmacfarlane.net/pandoc/README.html">title block</a>:</p><pre><code>% title
% author(s) (separated by semicolons)
% date</code></pre><p>For Scholarly Markdown we have another requirement: the metadata should be writeable and readable by humans. <a href="http://en.wikipedia.org/wiki/YAML">YAML</a> is the perfect format for this. JSON is closely related to YAML (and is in fact a subset of YAML 1.2), but YAML can also be written with whitespace instead of curly braces. The static website generator Jekyll - which I use to parse the markdown for this blog into HTML - uses YAML at the beginning of markdown documents to store metadata, and we can easily extend this functionality. Carl Boettinger posted a <a href="http://blog.martinfenner.org/2013/06/21/what-flavor-is-scholarly-markdown/#comment-945513935">comment</a> yesterday saying that YAML support is on the Pandoc development roadmap.</p><p>Below is the YAML for (Ethan P. White, 2013), where I reposted a paper written in markdown:</p><pre><code>---
layout: post
title: "Nine simple ways to make it easier to (re)use your data"
tags: [example, citation]
authors:
 - name: Ethan P. White
   orcid: 0000-0001-6728-7745
   affiliation: Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341
 - name: Elita Baldrige
   orcid: 0000-0003-1639-5951
   affiliation: Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341
 - name: Zachary T. Brym
   affiliation: Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341
 - name: Kenneth J. Locey
   affiliation: Dept. of Biology, Utah State University, Logan, UT, USA, 84341
 - name: Daniel J. McGlinn
   affiliation: Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341
 - name: Sarah R. Supp
   affiliation: Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341
---</code></pre><p>In JSON the same information would look like this (and Jekyll is able to parse it, since JSON is a subset of YAML 1.2):</p><pre><code>---
{
  "layout": "post",
  "title": "Nine simple ways to make it easier to (re)use your data",
  "tags": [
    "example",
    "citation"
  ],
  "authors": [
    {
      "name": "Ethan P. White",
      "orcid": "0000-0001-6728-7745",
      "affiliation": "Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341"
    },
    {
      "name": "Elita Baldrige",
      "orcid": "0000-0003-1639-5951",
      "affiliation": "Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341"
    },
    {
      "name": "Zachary T. Brym",
      "affiliation": "Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341"
    },
    {
      "name": "Kenneth J. Locey",
      "affiliation": "Dept. of Biology, Utah State University, Logan, UT, USA, 84341"
    },
    {
      "name": "Daniel J. McGlinn",
      "affiliation": "Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341"
    },
    {
      "name": "Sarah R. Supp",
      "affiliation": "Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341"
    }
  ]
}
---</code></pre><p>You can see that the author information required for manuscript submission can easily be written in YAML (email addresses were removed to protect privacy). JSON is also possible for people where this is a better fit into their workflow, but it is more difficult to write for humans because of the curly braces, and because all strings need to be in double quotes.</p><p>Once the ORCID Registry <a href="http://orcid.org/blog/2013/06/27/orcid-plans-launch-affiliation-module-using-isni-and-ringgold-organization">adds affiliation</a> information, we no longer need to provide email and affiliation when submitting manuscripts. I have stored my own name, orcid, email and affiliation in my site configuration file so that I don’t have to provide this info for every blog post.</p><p>In this blog markdown files are currently only processed to HTML, and I store the metadata in HTML <code>meta</code> tags in a <a href="http://www.monperrus.net/martin/accurate+bibliographic+metadata+and+google+scholar">format</a> used by many sites and services, including Google Scholar - look at the source code of Ethan P. White et al. (2013) for an example. These metadata are also understood by the Greycite service built by Phil Lord and Lindsay Marshall (2012) that generates citation information for weblinks, adding important metadata such as title, authors and publication_date so that we can properly cite our blog post (Ethan P. White, 2013).</p><p>And I use the metadata to link the author names to their ORCID profile (if they have an ORCID) or email address, with the affiliation visible when you hover over the name. My own name is linked to the <a href="http://blog.martinfenner.org/about.html">About</a> page of this site, but with a little development effort I could automatically add all my publications (and other works) in my ORCID profile to that page.</p><p>Metadata are important, and Scholarly Markdown makes it easy to embed them.</p><p><em>Update 06/30/13: added JSON example to demonstrate the differences to YAML, and to show that Jekyll also works with JSON (used in this blog post, and tested with the examples above which produce identical HTML output). Also added two references, using the embedded HTML metadata and the Greycite service to generate citations in bibtex.</em></p><h2 id="references">References</h2><p>Ethan P. White, Z. T. B., Elita Baldrige. (2013). Nine simple ways to make it easier to (re)use your data. <em>Gobbledygook</em>. Retrieved from <a href="http://blog.martinfenner.org/2013/06/25/nine-simple-ways-to-make-it-easier-to-reuse-your-data">http://blog.martinfenner.org/2013/06/25/nine-simple-ways-to-make-it-easier-to-reuse-your-data</a></p><p>Lord, P., &amp; Marshall, L. (2012). Greycite: Citing the web. <em>An Exercise in Irrelevance</em>. Retrieved from <a href="http://www.russet.org.uk/blog/2071">http://www.russet.org.uk/blog/2071</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Nine simple ways to make it easier to (re)use your data]]></title>
            <link>https://blog.martinfenner.org/posts/nine-simple-ways-to-make-it-easier-to-re-use-your-data</link>
            <guid>94df384d-0c20-40ac-945c-5ede4042763a</guid>
            <pubDate>Tue, 25 Jun 2013 17:04:00 GMT</pubDate>
            <description><![CDATA[> This paper in markdown format was written by Ethan White et al. The markdown
file and the associated bibliogaphy and figure files are available from the 
Github repository of the paper [https://github.com/weecology/data-sharing-paper]
.
> I used this
[https://github.com/weecology/data-sharing-paper/commit/b5a73eb0942a18bb29810025a528aea48a8465e7] 
version, an earlier version was published as PeerJ Preprint
[http://dx.doi.org/10.7287/peerj.preprints.7v1]. Special thanks to Ethan White
for allow]]></description>
            <content:encoded><![CDATA[<blockquote>This paper in markdown format was written by Ethan White et al. The markdown file and the associated bibliogaphy and figure files are available from the <a href="https://github.com/weecology/data-sharing-paper">Github repository of the paper</a>.</blockquote><blockquote>I used <a href="https://github.com/weecology/data-sharing-paper/commit/b5a73eb0942a18bb29810025a528aea48a8465e7">this</a> version, an earlier version was published as <a href="http://dx.doi.org/10.7287/peerj.preprints.7v1">PeerJ Preprint</a>. Special thanks to Ethan White for allowing me to reuse this paper. The paper is used here as an example document to show how markdown can handle scholarly documents, in particular tables, figures and citations. The document was slightly modified from the orginal: added YAML frontmatter (needed by jekyll, author names are also stored there), and changed the anchor text for some links. This post is using the APA citation style. Please restrict your comments to issues related to Scholarly Markdown, for the content of the article contact Ethan directly.</blockquote><h2 id="abstract">Abstract</h2><p>Sharing data is increasingly considered to be an important part of the scientific process. Making your data publicly available allows original results to be reproduced and new analyses to be conducted. While sharing your data is the first step in allowing reuse, it is also important that the data be easy to understand and use. We describe nine simple ways to make it easy to reuse the data that you share and also make it easier to work with it yourself. Our recommendations focus on making your data understandable, easy to analyze, and readily available to the wider community of scientists.</p><h2 id="introduction">Introduction</h2><p>Sharing data is increasingly recognized as an important component of the scientific process (Whitlock, McPeek, Rausher, Rieseberg, &amp; Moore, 2010). The sharing of scientific data is beneficial because it allows replication of research results and reuse in meta-analyses and projects not originally intended by the data collectors (Poisot, Mounce, &amp; Gravel, 2013). In ecology and evolutionary biology, sharing occurs through a combination of formal data repositories like <a href="http://www.ncbi.nlm.nih.gov/genbank/">GenBank</a> and <a href="http://datadryad.org/">Dryad</a>, and through individual and institutional websites.</p><p>While data sharing is increasingly common and straightforward, much of the shared data in ecology and evolutionary biology are not easily reused because they do not follow best practices in terms of data structure, metadata, and licensing (M. B. Jones, Schildhauer, Reichman, &amp; Bowers, 2006). This makes it more difficult to work with existing data and therefore makes the data less useful than it could be (M. B. Jones et al., 2006; O. J. Reichman, Jones, &amp; Schildhauer, 2011). Here we provide a list of 9 simple ways to make it easier to reuse the data that you share.</p><p>Our recommendations focus on making your data understandable, easy to work with, and available to the wider community of scientists. They are designed to be simple and straightforward to implement, and as such represent an introduction to good data practices rather than a comprehensive treatment. We contextualize our recommendations with examples from ecology and evolutionary biology, though many of the recommendations apply broadly across scientific disciplines. Following these recommendations makes it easier for anyone to reuse your data including other members of your lab and even yourself.</p><h2 id="1-share-your-data">1. Share your data</h2><p>The first and most important step in sharing your data is to share your data. The recommendations below will help make your data more useful, but sharing it in any form is a big step forward. So, why should you share your data?</p><p>Data sharing provides substantial benefits to the scientific community (Fienberg &amp; Martin, 1985). It allows</p><ol><li>the results of existing analyses to be reproduced and improved upon (Fienberg &amp; Martin, 1985; Poisot et al., 2013),</li><li>data to be combined in meta-analyses to reach general conclusions (Fienberg &amp; Martin, 1985),</li><li>new approaches to be applied to the data and new questions asked using it (Fienberg &amp; Martin, 1985), and</li><li>approaches to scientific inquiry that couldn’t even be considered without broad scale data sharing (Hampton et al., 2013).</li></ol><p>As a result, data sharing is increasingly required by funding agencies (Poisot et al. (2013); e.g., <a href="http://www.nsf.gov/bfa/dias/policy/dmp.jsp">NSF</a>, <a href="http://grants.nih.gov/grants/guide/notice-files/NOT-OD-03-032.html">NIH</a>, <a href="http://www.nserc-crsng.gc.ca/Professors-Professeurs/FinancialAdminGuide-GuideAdminFinancier/Responsibilities-Responsabilites_eng.asp">NSERC</a>, <a href="http://www.fwf.ac.at/en/public_relations/oai/index.html">FWF</a>), journals (Whitlock et al., 2010), and potentially by law (e.g. <a href="http://doyle.house.gov/sites/doyle.house.gov/files/documents/2013%2002%2014%20DOYLE%20FASTR%20FINAL.pdf">FASTR</a>).</p><p>Despite these potential benefits to the community, many scientists are still reluctant to share data. This reluctance is largely due to perceived fears of 1) competition for publications based on the shared data, 2) technical barriers, and 3) a lack of recognition for sharing data (Hampton et al., 2013; Palmer et al., 2004). These concerns are often not as serious as they first appear, and the minimal costs associated with data sharing are frequently offset by individual benefits to the data sharer (Hampton et al., 2013; Parr &amp; Cummings, 2005). Many data sharing initiatives allow for data embargoes or limitations on direct competition that can last for several years while the authors develop their publications and thus avoid competition for deriving publications from the data. Also, logistical barriers to data sharing are diminishing as data archives become increasingly common and easy to use (Hampton et al., 2013; Parr &amp; Cummings, 2005). Datasets are now considered citable entities and data providers receive recognition in the form of increased citation metrics and credit on CVs and grant applications (Heather A Piwowar &amp; Vision, 2013; Heather A. Piwowar, Day, &amp; Fridsma, 2007; Poisot et al., 2013). In addition to increased citation rates, shared datasets that are documented and standardized are also more easily reused in the future by the original investigator. As a result, it is increasingly beneficial to the individual researcher to share data in the most useful manner possible.</p><h2 id="2-provide-metadata">2. Provide metadata</h2><p>The first key to using data is understanding it. Metadata is information about the data including how it was collected, what the units of measurement are, and descriptions of how to best use the data. Clear metadata makes it easier to figure out if a dataset is appropriate for a project. It also makes data easier to use by both the original investigators and by other scientists by making it easy to figure out how to work with the data. Without clear metadata, datasets can be overlooked or not used due to the difficulty of understanding the data (Fraser &amp; Gluck, 1999; A. S. Zimmerman, 2003), and the data becomes less useful over time (Michener, Brunt, Helly, Kirchner, &amp; Stafford, 1997).</p><p>Metadata can take several forms, including descriptive file and column names, a written description of the data, images (<em>i.e.,</em> maps, photographs), and specially structured information that can be read by computers. Good metadata should provide 1) the what, when, where, and how of data collection, 2) how to find and access the data, 3) suggestions on the suitability of the data for answering specific questions, 4) warnings about known problems or inconsistencies in the data, and 5) information to check that the data are properly imported, such as the number of rows and columns in the dataset and the total sum of numerical columns (Michener et al., 1997; Strasser, Cook, Michener, &amp; Budden, 2012; A. S. Zimmerman, 2003).</p><p>Just like any other scientific publication, metadata should be logically organized, complete, and clear enough to enable interpretation and use of the data (A. Zimmerman, 2007). Specific metadata standards exist (<em>e.g.,</em> Ecological Metadata Language <a href="http://knb.ecoinformatics.org/software/eml/">EML</a>, Directory Interchange Format <a href="http://gcmd.gsfc.nasa.gov/add/difguide/index.html">DIF</a>, Darwin Core <a href="http://rs.tdwg.org/dwc/">DWC</a> (Wieczorek et al., 2012), Dublin Core Metadata Initiative <a href="http://dublincore.org/metadata-basics/">DCMI</a>, Federal Geographic Data Committee <a href="http://www.fgdc.gov/metadata/geospatial-metadata-standards">FGDC</a> (O. J. Reichman et al., 2011; Whitlock, 2011). These standards are designed to provide consistency in metadata across different datasets and also to allow computers to interpret the metadata automatically. This allows broader and more efficient use of shared data (Brunt, McCartney, Baker, &amp; Stafford, 2002; M. B. Jones et al., 2006). While following these standards is valuable, the most important thing is to have metadata at all.</p><p>You don’t need to spend a lot of extra time to write good metadata. The easiest way to develop metadata is to start describing your data during the planning and data collection stages. This will help you stay organized, make it easier to work with your data after it has been collected, and make eventual publication of the data easier. If you decide to take the extra step and follow metadata standards, there are tools designed to make this easier including: <a href="http://knb.ecoinformatics.org/morpho%20portal.jsp">KNB Morpho</a>, <a href="http://geology.usgs.gov/tools/metadata/tools/doc/xtme.html">USGS xtme</a>, and <a href="http://www.fgdc.gov/metadata/documents/workbook_0501_bmk.pdf">FGDC workbook</a>.</p><h2 id="3-provide-an-unprocessed-form-of-the-data">3. Provide an unprocessed form of the data</h2><p>Often, the data used in scientific analyses are modified in some way from the original form in which they were collected. This is done to address the questions of interest in the best manner possible and to address common limitations associated with the raw data. However, the best way to process data depends on the question being asked and corrections for common data limitations often change as better approaches are developed. It can also be very difficult to combine data from multiple sources that have each been processed in different ways. Therefore, to make your data as useful as possible it is best to share the data in as raw a form as possible.</p><p>This is not to say that your data are best suited for analysis in the raw form, but providing it in the raw form gives data users the most flexibility. Of course, your work to develop and process the data is also very important and can be quite valuable for other scientists using your data. This is particularly true when correcting data for common limitations. Providing both the raw and processed forms of the data, and clearly explaining the differences between them in the metadata, is an easy way to include the benefits of both data forms. An alternate approach is to share the unprocessed data along with the code that process the data to the form you used for analysis. This allows other scientists to assess and potentially modify the process by which you arrived at the values used in your analysis.</p><h2 id="4-use-standard-data-formats">4. Use standard data formats</h2><p>Everyone has their own favorite tools for storing and analyzing data. To make it easy to use your data it is best to store it in a standard format that can be used by many different kinds of software. Good standard formats include the type of file, the overall structure of the data, and the specific contents of the file.</p><h3 id="use-standard-file-formats">Use standard file formats</h3><p>You should use file formats that are readable by most software and, when possible, are non-proprietary (Borer, Seabloom, Jones, &amp; Schildhauer, 2009; Strasser, Cook, Michener, Budden, &amp; Koskela, 2011; Strasser et al., 2012). Certain kinds of data in ecology and evolution have well established standard formats such as <a href="http://zhanglab.ccmb.med.umich.edu/FASTA/">FASTA</a> files for nucleotide or peptide sequences and the <a href="http://evolution.genetics.washington.edu/phylip/newicktree.html">Newick files</a> for phylogenetic trees. Use these well defined formats when they exist, because that is what other scientists and most existing software will be able to work with most easily.</p><p>Data that does not have a well defined standard format is often stored in tables. Tabular data should be stored in a format that can be opened by any type of software to increase reuseability of the data, i.e. text files. These text files use delimiters to indicate different columns. Commas are the most commonly used delimiter (i.e., comma-delimited text files with the .csv extension). Tabs can also be used as a delimiter, although problems can occur in displaying the data correctly when importing data from one program to another. In contrast to plain text files, proprietary formats such as those used by Microsoft Excel (e.g, .xls, .xlsx) can be difficult to load into other programs. In addition, these types of files can become obsolete, eventually making it difficult to open the data files at all if the newer versions of the software no longer support the original format (Borer et al., 2009; Strasser et al., 2011, 2012).</p><p>When naming files you should use descriptive names so that it is easy to keep track of what data they contain (Borer et al., 2009; Strasser et al., 2011, 2012). If there are multiple files in a dataset, name them in a consistent manner to make it easier to automate working with them. You should also avoid spaces in file names, which can cause problems for some software (Borer et al., 2009). Spaces in file names can be avoided by using camel case (e.g, RainAvg) or by separating the words with underscores (e.g., rain_avg).</p><h3 id="use-standard-table-formats">Use standard table formats</h3><p>Data tables are ubiquitous in ecology and evolution. Tabular data provides a great deal of flexibility in how to structure the data, which makes it easy to structure the data in a way that is difficult to (re)use. We provide three simple recommendations to help ensure that tabular data are properly structured to allow the data to be easily imported and analyzed by most data management systems and common analysis software, such as R and Python.</p><ul><li>Each row should represent a single observation (i.e., a record) and each column should represent a single variable or type of measurement (i.e., a field) (Borer et al., 2009; Strasser et al., 2011, 2012). This is the standard format for tables in the most commonly used database management systems and analysis packages and makes the data easy to work with in the most general way.</li><li>Every cell should contain only a single value (Strasser et al., 2012). For example, do not include units in the cell with the values (Figure 1) or include multiple measurements in a single cell, and break taxonomic information up into single components with one column each for family, genus, species, subspecies, etc. Violating this rule makes it difficult to process or analyze your data using standard tools, because there is no easy way for the software to treat the items within a cell as separate pieces of information.</li><li>There should only be one column for each type of information (Borer et al., 2009; Strasser et al., 2011, 2012). The most common violation of this rule is <a href="http://en.wikipedia.org/wiki/Cross_tabulation">cross-tab structured data</a>, where different columns contain measurements of the same variable (e.g., in different sites, treatments, etc.; Figure 1).</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/Data_formatting.jpg" class="kg-image" alt="Figure 1. Examples of how to restructure two common issues with tabular data. (a) Each cell should only contain a single value. If more than one value is present then the data should be split into multiple columns. (b) There should be only one column for each type of information. If there are multiple columns then the column header should be stored in one column and the values from each column should be stored in a single column."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Figure 1. Examples of how to restructure two common issues with tabular data</strong>. (a) Each cell should only contain a single value. If more than one value is present then the data should be split into multiple columns. (b) There should be only one column for each type of information. If there are multiple columns then the column header should be stored in one column and the values from each column should be stored in a single column.</figcaption></figure><p>While cross-tab data can be useful for its readability, and may be appropriate for data collection, this format makes it difficult to link the records with additional data (e.g., the location and environmental conditions at a site) and it cannot be properly used by most common database management and analysis tools (e.g., relational databases, dataframes in R and Python, etc.). If tabular data are currently in a cross-tab structure, there are tools to help restructure the data including functions in Excel, R (e.g., melt() function in the R package reshape; Wickham (2007)), and Python (e.g., melt() function in the <a href="http://pandas.pydata.org/">Pandas</a> Python module.</p><p>In addition to following these basic rules you should also make sure to use descriptive column names (Borer et al., 2009). Descriptive column names make the data easier to understand and therefore make data interpretation errors less likely. As with file names, spaces can cause problems for some software and should be avoided.</p><h3 id="use-standard-formats-within-cells">Use standard formats within cells</h3><p>In addition to using standard table structures it is also important to ensure that the contents of each cell don’t cause problems for data management and analysis software. Specifically, we recommend:</p><ul><li>Be consistent. For example, be consistent in your capitalization of words, choice of delimiters, and naming conventions for variables.</li><li>Avoid special characters. Most software for storing and analyzing data works best on plain text, and accents and other special characters can make it difficult to import your data (Borer et al., 2009; Strasser et al., 2012).</li><li>Avoid using your delimiter in the data itself (e.g., commas in the notes filed of a comma-delimited file). This can make it difficult to import your data properly. This means that if you are using commas as the decimal separator (as is often done in continental Europe) then you should use a non-comma delimiter (e.g., a tab).</li><li>When working with dates use the YYYY-MM-DD format (i.e., follow the <a href="http://www.iso.org/iso/support/faqs/faqs_widely_used_standards/widely_used_standards_other/iso8601">ISO 8601</a> data standard).</li></ul><h2 id="5-use-good-null-values">5. Use good null values</h2><p>Most ecological and evolutionary datasets contain missing or empty data values. Working with this kind of “null” data can be difficult, especially when the null values are indicated in problematic ways. Unfortunately, there are many different ways to indicate a missing/empty value, and very little agreement on which approach to use.</p><p>We recommend choosing a null value that is both compatible with most software and unlikely to cause errors in analyses (Table 1). The null value that is most compatible with the software commonly used by biologists is the blank (i.e., nothing; Table 1). Blanks are automatically treated as null values by R, Python, SQL, and Excel. They are also easily spotted in a visual examination of the data. Note that a blank involves entering nothing, it is not a space, so if you use this option make sure there aren’t any hidden spaces. There are two potential issues with blanks that should be considered:</p><ol><li>It can be difficult to know if a value is missing or was overlooked during data entry.</li><li>They can be confusing when spaces or tabs are used as delimiters in text files.</li></ol><p>NA and NULL are reasonable null values, but they are only handled automatically by a subset of commonly used software (Table 1). NA can also be problematic if it is also used as an abbreviation (e.g., North America, Namibia, <em>Neotoma albigula</em>, sodium, etc.). We recommend against using numerical values to indicate nulls (e.g., 999, -999, etc.) because they typically require an extra step to remove from analyses and can be accidentally included in calculations. We also recommend against using non-standard text indications (e.g., No data, ND, missing, —) because they can cause issues with software that requires consistent data types within columns). Whichever null value that you use, only use one, use it consistently throughout the data set, and indicate it clearly in the metadata.</p><!--kg-card-begin: html--><table style="box-sizing: border-box; border-collapse: collapse; border-spacing: 0px; max-width: 100%; background-color: rgb(255, 255, 255); font-size: 19px; font-family: ff-tisa-sans-web-pro, Arial, sans-serif; width: 750px; margin-bottom: 21px; color: rgb(0, 0, 0); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"><caption style="box-sizing: border-box; text-align: left; vertical-align: top; font-size: 19px; font-style: italic; line-height: 1.33; margin-bottom: 0.75em;"><strong style="box-sizing: border-box; font-weight: bold;">Tabel 1. Commonly used null values, limitations, compatibility with common software and a recommendation regarding whether or not it is a good option</strong>. Null values are indicated as being a null value for specific software if they work consistently and correctly with that software. For example, the null value “NULL” works correctly for certain applications in R, but does not work in others, so it is not presented as part of the table.</caption><colgroup style="box-sizing: border-box;"><col style="box-sizing: border-box; width: 0px;"><col style="box-sizing: border-box; width: 0px;"><col style="box-sizing: border-box; width: 0px;"><col style="box-sizing: border-box; width: 0px;"></colgroup><thead style="box-sizing: border-box;"><tr class="header" style="box-sizing: border-box;"><th style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: bottom; border-top: 0px; font-weight: bold;">Null values</th><th style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: bottom; border-top: 0px; font-weight: bold;">Problems</th><th style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: bottom; border-top: 0px; font-weight: bold;">Compatibility</th><th style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: bottom; border-top: 0px; font-weight: bold;">Recommendation</th></tr></thead><tbody style="box-sizing: border-box;"><tr class="odd" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">0</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Indistinguishable from a true zero</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Never use</p></td></tr><tr class="even" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">blank</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Hard to distinguish values that are missing from those overlooked on entry. Hard to distinguish blanks from spaces, which behave differently.</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">R, Python, SQL</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Best option</p></td></tr><tr class="odd" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">999, -999</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Not recognized as null by many programs without user input. Can be inadvertently entered into calculations.</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Avoid</p></td></tr><tr class="even" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">NA, na</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Can also be an abbreviation (e.g., North America), can cause problems with data type (turn a numerical column into a text column). NA is more commonly recognized than na.</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">R</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Good option</p></td></tr><tr class="odd" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">N/A</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">An alternate form of NA, but often not compatible with software</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Avoid</p></td></tr><tr class="even" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">NULL</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Can cause problems with data type</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">SQL</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Good option</p></td></tr><tr class="odd" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">None</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Can cause problems with data type</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Python</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Avoid</p></td></tr><tr class="even" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">No data</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Can cause problems with data type, contains a space</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Avoid</p></td></tr><tr class="odd" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Missing</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Can cause problems with data type</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Avoid</p></td></tr><tr class="even" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">-,+,.</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Can cause problems with data type</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Avoid</p></td></tr></tbody></table><!--kg-card-end: html--><h2 id="6-make-it-easy-to-combine-your-data-with-other-datasets">6. Make it easy to combine your data with other datasets</h2><p>Ecological and evolutionary data are often most valuable when combined with other kinds of data (e.g., taxonomic, environmental). You can make it easier to combine your data with other data sources by including the data that is common across many data sources (e.g., Latin binomials, latitudes and longitudes) It is common for data to include codes or abbreviations. For example, in ecology and evolution codes often appear in place of site locations or taxonomy. This is useful because it reduces data entry (e.g., DS instead of <em>Dipodomys spectabilis</em>) and redundancy (a single column for a species ID rather than separate columns for family, genus, and species). However, without clear definitions these codes can be difficult to understand and make it more difficult to connect your data with external sources. The easiest way to link your data to other datasets is to include additional tables that contain a column for the code and additional columns that describe the item in the standard way. For example, you might include a table with the species codes followed by their most current family, genus, and specific epithet. For site location, you could include a table with the site code followed by latitude and longitude. Linked tables can also be used to include additional information about your data, such as spatial extent, temporal duration, and other appropriate details.</p><h2 id="7-perform-basic-quality-control">7. Perform basic quality control</h2><p>Data, just like any other scientific product, should undergo some level of quality control (O. J. Reichman et al., 2011). This is true regardless of whether you plan to share the data because quality control will make it easier to analyze your own data and decrease the chance of making mistakes. However, it is particularly important for data that will be shared because scientists using the data won’t be familiar with quirks in the data and how to work around them.</p><p>At its most basic, quality control can consist of a few quick sanity checks of the data. More advanced quality control can include automated checks on data as it is entered and double-entry of data (Lampe &amp; Weiler, 1998; Paulsen, Overgaard, &amp; Lauritsen, 2012). This additional effort can be time consuming, but is valuable because it increases data accuracy by catching typographical errors, reader/recorder error, out-of-range values, and questionable data in general (Lampe &amp; Weiler, 1998; Paulsen et al., 2012).</p><p>Before sharing your data we recommend performing a quick “data review”. Start by performing some basic sanity checks on your data. For example:</p><ul><li>If a column should contain numeric values, check that there are no non-numeric values in the data.</li><li>Check that empty cells actually represent missing data, and not mistakes in data entry, and indicate that they are empty using the appropriate null values (see recommendation 6).</li><li>Check for consistency in unit of measurement, data type (e.g., numeric, character), naming scheme (e.g., taxonomy, location), etc.</li></ul><p>These checks can be performed by carefully looking at the data or can be automated using common programming and analysis tools like R or Python.</p><p>Then ask someone else to look over your metadata and data and provide you with feedback about anything they didn’t understand. In the same way that friendly reviews of papers can help catch mistakes and identify confusing sections of papers, a friendly review of data can help identify problems and things that are unclear in the data and metadata.</p><h2 id="8-use-an-established-repository">8. Use an established repository</h2><p>For data sharing to be effective, data should be easy to find, accessible, and stored where it will be preserved for a long time (Kowalczyk &amp; Shankar, 2011). To make your data (and associated code) visible and easily accessible, and to ensure a permanent link to a well maintained website, we suggest depositing your data in one of the major well-established repositories. This guarantees that the data will be available in the same location for a long time, in contrast to personal and institutional websites that do not guarantee the long-term persistence of the data. There are repositories available for sharing almost any type of biological or environmental data. Repositories that host specific data types, such as molecular sequences (e.g., DDBJ, GenBank, MG-RAST), are often highly standardized in data type, format, and quality control approaches. Other repositories host a wide array of data types and are less standardized (e.g., Dryad, KNB, PANGAEA). In addition to the repositories focused on the natural sciences there are also all purpose repositories where data of any kind can be shared (e.g., figshare).</p><p>When choosing a repository you should consider where other researchers in your discipline are sharing their data. This helps you quickly identify the community’s standard approach to sharing and increases the likelihood that other scientists will discover your data. In particular, if there is a centralized repository for a specific kind of data (e.g., GenBank for sequence data) then you should use that repository.</p><p>In cases where there is no <em>de facto</em> standard it is worth considering differences among repositories in terms of use, data rights, and licensing (Table 2) and whether your funding agency or journal has explicit requirements or restrictions related to repositories. We also recommend that you use a repository that allows your dataset to be easily cited. Most repositories will describe how this works, but an easy way to guarantee that your data are citable is to confirm that the repository associates it with a persistent identifier, the most popular of which is the digital object identifier (DOI). DOIs are permanent unique identifiers that are independent of physical location and site ownership. There are also online tools for finding good repositories for your data including <a href="http://databib.org/">Databib</a> and <a href="http://re3data.org/">re3data</a>.</p><!--kg-card-begin: html--><table style="box-sizing: border-box; border-collapse: collapse; border-spacing: 0px; max-width: 100%; background-color: rgb(255, 255, 255); font-size: 19px; font-family: ff-tisa-sans-web-pro, Arial, sans-serif; width: 750px; margin-bottom: 21px; color: rgb(0, 0, 0); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"><caption style="box-sizing: border-box; text-align: left; vertical-align: top; font-size: 19px; font-style: italic; line-height: 1.33; margin-bottom: 0.75em;"><strong style="box-sizing: border-box; font-weight: bold;">Table 2. Popular repositories for scientific datasets</strong>. This table does not include well-known molecular repositories (e.g.&nbsp;GenBank, EMBL, MG-RAST) that have become<span>&nbsp;</span><em style="box-sizing: border-box;">de facto</em><span>&nbsp;</span>standards in molecular and evolutionary biology. Consequently, several of these primarily serve the ecological community. These repositories are not exclusively used by members of specific institutions or museums, but accept data from the general scientific community.</caption><colgroup style="box-sizing: border-box;"><col style="box-sizing: border-box; width: 0px;"><col style="box-sizing: border-box; width: 0px;"><col style="box-sizing: border-box; width: 0px;"><col style="box-sizing: border-box; width: 0px;"><col style="box-sizing: border-box; width: 0px;"><col style="box-sizing: border-box; width: 0px;"></colgroup><thead style="box-sizing: border-box;"><tr class="header" style="box-sizing: border-box;"><th style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: bottom; border-top: 0px; font-weight: bold;">Repository</th><th style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: bottom; border-top: 0px; font-weight: bold;">License</th><th style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: bottom; border-top: 0px; font-weight: bold;">DOI</th><th style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: bottom; border-top: 0px; font-weight: bold;">Metadata</th><th style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: bottom; border-top: 0px; font-weight: bold;">Access</th><th style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: bottom; border-top: 0px; font-weight: bold;">Notes</th></tr></thead><tbody style="box-sizing: border-box;"><tr class="odd" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Dryad</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">CC0</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Yes</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Suggested</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Open</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Ecology &amp; evolution data associated with publications</p></td></tr><tr class="even" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Ecological Archives</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">No</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Yes</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Required</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Open</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Publishes supplemental data for ESA journals and stand alone data papers</p></td></tr><tr class="odd" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Knowledge Network for Biocomplexity</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">No</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Yes</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Required</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Variable</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Partners with ESA, NCEAS, DataONE</p></td></tr><tr class="even" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Paleobiology Database</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Various CC</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">No</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Optional</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Variable</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Paleontology specific</p></td></tr><tr class="odd" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Data Basin</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Various CC</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">No</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Optional</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Open</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">GIS data in ESRI files, limited free space</p></td></tr><tr class="even" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Pangaea</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Various CC</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Yes</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Required</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Variable</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Editors participate in QA/QC</p></td></tr><tr class="odd" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">figshare</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">CC0</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Yes</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Optional</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Open</p></td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);"><p style="box-sizing: border-box; margin: 0px 0px 21px;">Also allows deposition of other research outputs and private datasets</p></td></tr></tbody></table><!--kg-card-end: html--><h2 id="9-use-an-established-and-liberal-license">9. Use an established and liberal license</h2><p>Including an explicit license with your data is the best way to let others know exactly what they can and cannot do with the data you shared. Following the <a href="http://pantonprinciples.org/">Panton Principles</a> we recommend:</p><ol><li>Using well established licenses in order to clearly communicate the rights and responsibilities of both the people providing the data and the people using it.</li><li>Using the most open license possible, because even minor restrictions on data use can have unintended consequences for the reuse of the data (Poisot et al., 2013; Schofield et al., 2009).</li></ol><p>The Creative Commons Zero license (CC0) places no restrictions on data use and is considered by many to be one of the best license for sharing data (e.g., (Poisot et al., 2013; Schofield et al., 2009), <a href="http://blog.datadryad.org/2011/10/05/why-does-dryad-use-cc0/">Why does Dryad use CC0</a>). Having a clear and open license will increase the chance that other scientists will be comfortable using your data.</p><h2 id="concluding-remarks">Concluding remarks</h2><p>Data sharing has the potential to transform the way we conduct ecological and evolutionary research (Fienberg &amp; Martin, 1985; Poisot et al., 2013; Whitlock et al., 2010). As a result, there are an increasing number of initiatives at the federal, funding agency, and journal levels to encourage or require the sharing of the data associated with scientific research (Heather A Piwowar &amp; Chapman, 2008; Poisot et al., 2013; Whitlock et al., 2010). However, making the data available is only the first step. To make data sharing as useful as possible it is necessary to make the data usable with as little effort as possible (M. B. Jones et al., 2006; O. J. Reichman et al., 2011). This allows scientists to spend their time doing science rather than cleaning up data.</p><p>We have provided a list of 9 practices that require only a small additional time investment but substantially improve the usability of data. These practices can be broken down into three major groups.</p><ol><li>Well documented data are easier to understand.</li><li>Properly formatted data are easier to use in a variety of software.</li><li>Data that is shared in established repositories with open licenses is easier for others to find and use.</li></ol><p>Most of these recommendations are simply good practice for working with data regardless of whether that data are shared or not. This means that following these recommendations (2-7) make the data easier to work with for anyone, including you. This is particularly true when returning to your own data for further analysis months or years after you originally collected or analyzed it. In addition, data sharing often occurs within a lab or research group. Good data sharing practices make these in-house collaborations faster, easier, and less dependent on lab members who may have graduated or moved on to other things.</p><p>By following these practices we can assure that the data collected in ecology and evolution can be used to its full potential to improve our understanding of biological systems.</p><h2 id="acknowledgments">Acknowledgments</h2><p>Thanks to Karthik Ram for organizing this special section and inviting us to contribute. Carly Strasser and Kara Woo recommended important references and David Harris and Carly Strasser provided valuable feedback on null values, all via Twitter. Carl Boettiger, Matt Davis, Daniel Hocking, Heinz Pampel, Karthik Ram, Thiago Silva, Carly Strasser, Tom Webb, and beroe (Twitter handle) provided value comments on the manuscript. Many of these comments were part of the informal review process facilitated by posting this manuscript as a preprint. The writing of this paper was supported by a CAREER grant from the U.S. National Science Foundation (DEB 0953694) to EPW.</p><h2 id="references">References</h2><p>Borer, E. T., Seabloom, E. W., Jones, M. B., &amp; Schildhauer, M. (2009). Some simple guidelines for effective data management. <em>Bulletin of the Ecological Society of America</em>, <em>90</em>(2), 205–214. Retrieved from <a href="http://dx.doi.org/10.1890/0012-9623-90.2.205">http://dx.doi.org/10.1890/0012-9623-90.2.205</a></p><p>Brunt, J. W., McCartney, P., Baker, K., &amp; Stafford, S. G. (2002). The future of ecoinformatics in long term ecological research. In <em>Proceedings of the 6th world multiconference on systemics, cybernetics and informatics: SCI</em> (pp. 14–18).</p><p>Fienberg, S. E., &amp; Martin, M. E. (1985). <em>Sharing research data</em>. Natl Academy Pr.</p><p>Fraser, B., &amp; Gluck, M. (1999). Usability of geospatial metadata or space-time matters. <em>Bulletin of the American Society for Information Science and Technology</em>, <em>25</em>(6), 24–28. Retrieved from <a href="http://dx.doi.org/10.1002/bult.134">http://dx.doi.org/10.1002/bult.134</a></p><p>Hampton, S. E., Strasser, C. A., Tewksbury, J. J., Gram, W. K., Budden, A. E., Batcheller, A. L., … Porter, J. H. (2013). Big data and the future of ecology. <em>Frontiers in Ecology and the Environment</em>, <em>11</em>(3), 156–162. Retrieved from <a href="http://dx.doi.org/10.1890/120103">http://dx.doi.org/10.1890/120103</a></p><p>Jones, M. B., Schildhauer, M. P., Reichman, O., &amp; Bowers, S. (2006). The new bioinformatics: Integrating ecological data from the gene to the biosphere. <em>Annual Review of Ecology, Evolution, and Systematics</em>, <em>37</em>(1), 519–54. Retrieved from <a href="http://dx.doi.org/10.1146/annurev.ecolsys.37.091305.110031">http://dx.doi.org/10.1146/annurev.ecolsys.37.091305.110031</a></p><p>Kowalczyk, S., &amp; Shankar, K. (2011). Data sharing in the sciences. <em>Annual Review of Information Science and Technology</em>, <em>45</em>(1), 247–294. Retrieved from <a href="http://dx.doi.org/10.1002/aris.2011.1440450113">http://dx.doi.org/10.1002/aris.2011.1440450113</a></p><p>Lampe, A., &amp; Weiler, J. (1998). Data capture from the sponsors’ and investigators’ perspectives: Balancing quality, speed, and cost. <em>Drug Information Journal</em>, <em>32</em>(4), 871–886.</p><p>Michener, W. K., Brunt, J. W., Helly, J. J., Kirchner, T. B., &amp; Stafford, S. G. (1997). Nongeospatial metadata for the ecological sciences. <em>Ecological Applications</em>, <em>7</em>(1), 330–342. Retrieved from <a href="http://dx.doi.org/10.1890/1051-0761(1997)007%5B0330:nmftes%5D2.0.co;2">http://dx.doi.org/10.1890/1051-0761(1997)007[0330:nmftes]2.0.co;2</a></p><p>Palmer, M. A., Bernhardt, E. S., Chornesky, E. A., Collins, S. L., Dobson, A. P., Duke, C. S., … Turner, M. G. (2004). Ecological science and sustainability for a crowded planet. Retrieved from <a href="http://www.esa.org/ecovisions/ppfiles/EcologicalVisionsReport.pdf">http://www.esa.org/ecovisions/ppfiles/EcologicalVisionsReport.pdf</a></p><p>Parr, C., &amp; Cummings, M. (2005). Data sharing in ecology and evolution. <em>Trends in Ecology &amp; Evolution</em>, <em>20</em>(7), 362–363. Retrieved from <a href="http://dx.doi.org/10.1016/j.tree.2005.04.023">http://dx.doi.org/10.1016/j.tree.2005.04.023</a></p><p>Paulsen, A., Overgaard, S., &amp; Lauritsen, J. M. (2012). Quality of data entry using single entry, double entry and automated forms processing–An example based on a study of patient-reported outcomes. <em>PloS ONE</em>, <em>7</em>(4), e35087. Retrieved from <a href="http://dx.doi.org/10.1371/journal.pone.0035087">http://dx.doi.org/10.1371/journal.pone.0035087</a></p><p>Piwowar, H. A., &amp; Chapman, W. W. (2008). A review of journal policies for sharing research data. In <em>ELPUB2008</em>.</p><p>Piwowar, H. A., &amp; Vision, T. J. (2013). Data reuse and the open data citation advantage. <em>PeerJ PrePrints</em>, <em>1</em>, e1. Retrieved from <a href="http://dx.doi.org/10.7287/peerj.preprints.1">http://dx.doi.org/10.7287/peerj.preprints.1</a></p><p>Piwowar, H. A., Day, R. S., &amp; Fridsma, D. B. (2007). Sharing detailed research data is associated with increased citation rate. <em>PLoS ONE</em>, <em>2</em>(3), e308. Retrieved from <a href="http://dx.doi.org/10.1371/journal.pone.0000308">http://dx.doi.org/10.1371/journal.pone.0000308</a></p><p>Poisot, T., Mounce, R., &amp; Gravel, D. (2013). Moving toward a sustainable ecological science: Don’t let data go to waste! Retrieved from <a href="https://github.com/tpoisot/DataSharingPaper/blob/master/DataSharing-MS.md">https://github.com/tpoisot/DataSharingPaper/blob/master/DataSharing-MS.md</a></p><p>Reichman, O. J., Jones, M. B., &amp; Schildhauer, M. P. (2011). Challenges and opportunities of open data in ecology. <em>Science</em>, <em>331</em>(6018), 703–705. Retrieved from <a href="http://dx.doi.org/10.1126/science.1197962">http://dx.doi.org/10.1126/science.1197962</a></p><p>Schofield, P. N., Bubela, T., Weaver, T., Portilla, L., Brown, S. D., Hancock, J. M., … Rosenthal, N. (2009). Post-publication sharing of data and tools. <em>Nature</em>, <em>461</em>(7261), 171–173. Retrieved from <a href="http://dx.doi.org/10.1038/461171a">http://dx.doi.org/10.1038/461171a</a></p><p>Strasser, C. A., Cook, R. B., Michener, W. K., Budden, A., &amp; Koskela, R. (2011). Promoting data stewardship through best practices. In <em>Proceedings of the environmental information management conference 2011 (eIM 2011)</em>. Oak Ridge National Laboratory (ORNL).</p><p>Strasser, C. A., Cook, R., Michener, W. K., &amp; Budden, A. (2012). Primer on data management: What you always wanted to know. DataONE. Retrieved from <a href="http://dx.doi.org/10.5060/D2251G48">http://dx.doi.org/10.5060/D2251G48</a></p><p>Whitlock, M. C. (2011). Data archiving in ecology and evolution: Best practices. <em>Trends in Ecology &amp; Evolution</em>, <em>26</em>(2), 61–65. Retrieved from <a href="http://dx.doi.org/10.1016/j.tree.2010.11.006">http://dx.doi.org/10.1016/j.tree.2010.11.006</a></p><p>Whitlock, M. C., McPeek, M. A., Rausher, M. D., Rieseberg, L., &amp; Moore, A. J. (2010). Data archiving. <em>The American Naturalist</em>, <em>175</em>(2), 145–146. <a href="http://doi.org/10.1086/650340">doi:10.1086/650340</a></p><p>Wickham, H. (2007). Reshaping data with the reshape package. <em>Journal of Statistical Software</em>, <em>21</em>(12). Retrieved from <a href="http://www.jstatsoft.org/v21/i12/paper">http://www.jstatsoft.org/v21/i12/paper</a></p><p>Wieczorek, J., Bloom, D., Guralnick, R., Blum, S., Döring, M., Giovanni, R., … Vieglais, D. (2012). Darwin core: An evolving community-developed biodiversity data standard. <em>PLoS ONE</em>, <em>7</em>(1), e29715. <a href="http://doi.org/10.1371/journal.pone.0029715">doi:10.1371/journal.pone.0029715</a></p><p>Zimmerman, A. (2007). Not by metadata alone: The use of diverse forms of knowledge to locate data for reuse. <em>International Journal on Digital Libraries</em>, <em>7</em>(1-2), 5–16. <a href="http://doi.org/10.1007/s00799-007-0015-8">doi:10.1007/s00799-007-0015-8</a></p><p>Zimmerman, A. S. (2003). <em>Data sharing and secondary use of scientific data: Experiences of ecologists</em> (PhD thesis). The University of Michigan.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Citations in Markdown Part 3]]></title>
            <link>https://blog.martinfenner.org/posts/citations-in-markdown-part-3</link>
            <guid>dd7d89a7-84c2-4e59-bdda-517e976f11d8</guid>
            <pubDate>Mon, 24 Jun 2013 17:08:00 GMT</pubDate>
            <description><![CDATA[After the post last week
[http://blog.martinfenner.org/2013/06/19/citations-in-scholarly-markdown/] and
the crazy discussion that followed I would understand that you feel you have
heard enough about citations in markdown. But I had the feeling last week that
something was still missing, and I have done some more thinking. What we have so
far:

 * Pandoc has nice support for citations, including Citation Style Language
   support (i.e. it is using the same 5000+ citation styles as Zotero, Mendel]]></description>
            <content:encoded><![CDATA[<p>After the <a href="http://blog.martinfenner.org/2013/06/19/citations-in-scholarly-markdown/">post last week</a> and the crazy discussion that followed I would understand that you feel you have heard enough about citations in markdown. But I had the feeling last week that something was still missing, and I have done some more thinking. What we have so far:</p><ul><li>Pandoc has nice support for citations, including Citation Style Language support (i.e. it is using the same 5000+ citation styles as Zotero, Mendeley and Papers).</li><li>Pandoc requires a separate file to store the citations, typically in bibtex format. This is fine for some people, but can make the workflow complicated for short documents or when several people work on the bibliography at the same time.</li><li>Citations are similar to links, and we can use links for almost all the functionality we need, making it much easier to add citations to a text. The problem is a) citations that don’t include a weblink, b) being able to do this offline, and c) where in the HTML to store the citation metadata.</li></ul><p>And I looked at how Wikipedia is <a href="http://en.wikipedia.org/wiki/Wikipedia:Citing_sources">doing this</a>, and they use a) links, b) citations and c) footnotes. If Wikipedia thinks that it can’t do without citations and do everything as links, then maybe we also shouldn’t enforce this for scholarly texts.</p><p>I think what we need is the best of both worlds. We should use the Pandoc citation workflow, as it is similar to what we are used to from other authoring environments, and we get good citation style support, including more complex formatting of references. Some reference managers already support copy/paste of Pandoc citation keys. The inclusion of a bibtex file with a scholarly markdown text is also a bonus, as it allows the automated extraction of citations, e.g. by manuscript submission systems.</p><p>We also want to support a simpler solution for shorter texts or when people don’t want to use a separate bibtex file. Here we would add the citations as links, ideally in a syntax very similar to Pandoc citation keys:</p><pre><code>Johnson [@Johnson2006] didn't agree with ...

[@Johnson2006]: http://dx.doi.org/10.1002/aris.201 "Data sharing in the sciences"</code></pre><p>We need to write a tool that parses the markdown before Pandoc, fetches the citation metadata for these links in bibtex format (e.g. using CrossRef Content Negotiation), and adds them to the existing bibtex file (or creates a new bibtex file). The next time the markdown is parsed, the citation is already “cached” in the bibtex file. Those people who don’t have such a tool would see the citation as link (<strong><strong>???</strong></strong>), with the essential information (DOI or URL) preserved so that a downstream tool can fetch the bibliographic information. Some people were worried about typos in DOIs and URLs. They can add additional information - e.g. the title of the paper - in double quotes to allow checking of the correct DOI.</p><p>This workflow now makes a lot of sense to me, as it uses existing solutions, but also allows for easy entering of citation information in a way similar to the <a href="http://carlboettiger.info/2012/05/30/knitcitations.html">knitcitations</a> and <a href="http://wordpress.org/plugins/kcite/">kcite</a> tools. As I use jekyll and am a Ruby developer, I will implement the citation parsing as a jekyll plugin.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What Flavor is Scholarly Markdown?]]></title>
            <link>https://blog.martinfenner.org/posts/what-flavor-is-scholarly-markdown</link>
            <guid>97197639-565a-4090-82da-63f3c8a8507e</guid>
            <pubDate>Fri, 21 Jun 2013 17:11:00 GMT</pubDate>
            <description><![CDATA[One important outcome of the recent Markdown for Science
[https://github.com/scholmd/scholmd/wiki/workshop] workshop was an overall
agreement that all the different implementations (or flavors) of markdown that
currently exist are a big problem for the adoption of Scholarly Markdown and
that we need [https://github.com/scholmd/scholmd/wiki/What-is-Markdown]:

> A reference implementation with documentation and tests
As described by Karthik Ram (31 flavors is great for ice cream but not markdown
]]></description>
            <content:encoded><![CDATA[<p>One important outcome of the recent <a href="https://github.com/scholmd/scholmd/wiki/workshop">Markdown for Science</a> workshop was an overall agreement that all the different implementations (or flavors) of markdown that currently exist are a big problem for the adoption of Scholarly Markdown and that we <a href="https://github.com/scholmd/scholmd/wiki/What-is-Markdown">need</a>:</p><blockquote>A reference implementation with documentation and tests</blockquote><p>As described by Karthik Ram (<a href="https://github.com/scholmd/scholmd/wiki/workshop">31 flavors is great for ice cream but not markdown</a>), <a href="http://blog.martinfenner.org/2012/12/13/a-call-for-scholarly-markdown/">me</a> and <a href="http://www.codinghorror.com/blog/2012/10/the-future-of-markdown.html">others</a>, there is really a large number of markdown implementations to choose from, including</p><ul><li>John Gruber’s <a href="http://daringfireball.net/projects/markdown/">original Markdown</a></li><li><a href="https://help.github.com/articles/github-flavored-markdown">Github-flavored Markdown</a></li><li><a href="http://michelf.ca/projects/php-markdown/extra/">PHP Markdown Extra</a></li><li><a href="http://johnmacfarlane.net/pandoc/">Pandoc</a></li><li><a href="http://fletcherpenney.net/multimarkdown/">MultiMarkdown</a></li></ul><p>These different flavors all serve their needs, but for Markdown to take off in the relatively small scholarly community it would be very helpful to come up with a reference implementation. But how do we get to that point?</p><ol><li>Think about the features we need for Scholarly Markdown and make this the reference implementation?</li><li>Organize a working group or committee that decides what is Scholarly Markdown?</li><li>Pick the Markdown flavor with the best developer support?</li><li>Figure out what markdown flavor has the widest support by tools relevant for scholars?</li><li>See what markdown flavor most scholars are currently using?</li></ol><p>I think as a starting point, and until we come up with something better, #5 makes the most sense. The number of markdown users among scholars is still small, but my guess would be that Pandoc is currently the most popular Markdown flavor among scholars. This blog uses Pandoc and the static site generator <a href="http://jekyllrb.com/">Jekyll</a>, and is hosted on <a href="http://pages.github.com/">Github Pages</a> - for the source code use the link in the footer. Please tell me in the comments what you are using (Markdown flavor and tools), and whether I am correct with my wild guess regarding Pandoc. And make sure your preferred tool is listed in the <a href="https://github.com/scholmd/scholmd/wiki/Tools-to-support-your-markdown-authoring">Tools to spport your markdown authoring</a> wiki page.</p><p>A reference Markdown document is also very helpful to move forward, as we can see what outputs in HTML, PDF (or other formats) our specific Markdown tools produce, and how they differ. This reference document should include citations, tables, figures, and other features typical for scholarly content. Ideally this is a paper written in Markdown and accepted for publication - proving the concept -, or it can be a published paper transformed into markdown, e.g. a paper by <a href="http://www.elifesciences.org/elife-now-supports-content-negotiation/">eLife</a>. Feel free to suggest a paper in the comments.</p><p>The idea of tests that came up in the Markdown workshop is also great. Ideally we have a set of tests that we (or someone else, e.g. a publisher) can run to make sure that the markdown in the document conforms with the reference implementation. This could also include basic checks for required metadata (title, author, publication date, etc.), and could optionally validate the citations as well.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Citations in Scholarly Markdown]]></title>
            <link>https://blog.martinfenner.org/posts/citations-in-scholarly-markdown</link>
            <guid>54b4a024-a0d7-4726-8130-5dd7c0e7a63a</guid>
            <pubDate>Wed, 19 Jun 2013 17:13:00 GMT</pubDate>
            <description><![CDATA[In the comments on Monday’s blog post
[http://blog.martinfenner.org/2013/06/17/what-is-scholarly-markdown/] about the
Markdown for Science workshop, Carl Boettiger [http://carlboettiger.info/] had
some good arguments against the proposal for how to do citations
[https://github.com/scholmd/scholmd/wiki/citations] that we came up with during
the workshop. As this is a complex topic, I decided to write this blog post.

Citations of the scholarly literature are an essential part of scholarly texts
a]]></description>
            <content:encoded><![CDATA[<p>In the comments on <a href="http://blog.martinfenner.org/2013/06/17/what-is-scholarly-markdown/">Monday’s blog post</a> about the Markdown for Science workshop, <a href="http://carlboettiger.info/">Carl Boettiger</a> had some good arguments against the proposal for how to do <a href="https://github.com/scholmd/scholmd/wiki/citations">citations</a> that we came up with during the workshop. As this is a complex topic, I decided to write this blog post.</p><p>Citations of the scholarly literature are an essential part of scholarly texts and therefore have to be supported by scholarly markdown. Both the <a href="http://johnmacfarlane.net/pandoc/README.html">Pandoc</a> and <a href="https://github.com/fletcher/MultiMarkdown/wiki/MultiMarkdown-Syntax-Guide">Multimarkdown</a> flavors of markdown support citations, using a bibtex file that contains citations, placeholders for citekeys – <code>[@smith04]</code> for Pandoc and <code>[#smith04]</code> for Multimarkdown – and the <a href="http://citationstyles.org/">Citation Style Language</a> for citation formatting (Pandoc). A very reasonable approach would therefore be to use this functionality, with a preference for Pandoc because of the Citation Style Language support. All reference managers can export to the bibtex format, and some of them (e.g. <a href="http://www.papersapp.com/papers/">Papers</a>) make it very easy to copy and paste citekeys.</p><p>Ten days after the workshop I’m not so sure anymore this is the best approach. For four reasons:</p><ol><li><strong><strong>YFNS</strong></strong>. This approach failed the YFNS (your friendly neighborhood scientist) test. We came up with this term during the workshop and it means that our ideas about authoring should make sense to the workflow of the average scientist. I thought that using citekeys is a good idea, but my wife (my YFNS) tells me that she never uses citekeys because there are just too many <code>[@smith04]</code>, and it is too easy get out of sync with the reference manager. She therefore prefers to put the complete reference information into the text while writing.</li><li><strong><strong>Snippets</strong></strong>. As I said <a href="http://blog.martinfenner.org/2013/06/17/what-is-scholarly-markdown/">previously</a>, I think that scholarly markdown has great potential not so much for writing full papers, but for all the little scientific documents we write on a daily basis. For this reason the citation information should ideally be embedded in the document if it is short, and that is difficult with bibtex (which is not human-readable).</li><li><strong><strong>Citations as links</strong></strong>. Carl Boettiger reminded me that I wrote a <a href="http://blogs.plos.org/mfenner/2010/12/11/citations-are-links-so-where-is-the-problem/">blog post in 2010</a> stating that citations are nothing else than links, and that we should treat them accordingly. He has written a tool (<a href="http://carlboettiger.info/2012/05/30/knitcitations.html">knitcitations</a>) for R that does just that, and Phil Lord and colleagues have written a similar tool (<a href="http://wordpress.org/plugins/kcite/">kcite</a>) for Wordpress. In 2010 I wrote a tool for Wordpress (<a href="http://wordpress.org/plugins/link-to-link/">Link to Link</a>) that takes a different approach but also treats citations as links. All that we need is the DOI (or URL) for the article.</li><li><strong><strong>Vendor lock-in</strong></strong>. Although a number of excellent reference managers are available now, users are still limited in their choices because everyone has to use the same reference manager when multiple authors work on the same document. This has always annoyed me. It would no longer be the case if we embed the citation information in the document in a standard format.</li></ol><p>Part of the motivation for using scholarly markdown is that we can come up with best practices that make sense for digital content and don’t need to support conventions from an era when articles were still printed on paper. Reference information in the form of volumes and pages, and 1000s of citation styles certainly have outlived their purpose. Citation styles are a particular pain point, as they are nothing more than a visual representation of a citation - we should care much more about the machine-readable metadata, in particular the DOI or other identifier.</p><p>The best practice for scholarly markdown could therefore be to treat citations as links, using DOIs or other standard identifiers (PMID, ArXiV, etc.) where possible. Because we typically want to list the citations as references at the end of the document, reference-style links should be preferred over inline links. From the <a href="http://daringfireball.net/projects/markdown/syntax#link">markdown syntax documentation</a>:</p><pre><code>This is [an example][id] reference-style link.

This is [an example](http://example.com/ "Title") inline link.
[id]: http://example.com/  "Optional Title Here"</code></pre><p>It might be tempting to use sequential numbers as id for the reference-style links, but the order of links can of course change during writing. It may make sense to think of the id in reference-style links as a citekey, and people should be free use that functionality of their reference manager. The citekey is used to link to the reference list at the bottom of the document, different from linking to the citekey in a separate bibtex file.</p><p>All of the above can be done in any text editor. This also includes the text editor that scholars spend most of their time with - their email program. Reference-style citations in an email are very readable, and also actionable since they are links and not text with bibliographic information.</p><p>One problem with this approach is of course that all links are inline in the resulting HTML, without a references section at the end of the document. This may be fine, as we can provide citation information in the title attribute, available upon hovering over the link (try hovering over <a href="http://dx.doi.org/10.1371/journal.pmed.0020124">this link</a>, the journal eLife is doing <a href="http://dx.doi.org/10.7554/eLife.00633">something similar</a>). The markdown could look like this (using the <em>Vancouver</em> citation style):</p><pre><code>[@Ioannidis2005]: http://dx.doi.org/10.1371/journal.pmed.0020124 "Ioannidis JPA. Why Most Published Research Findings Are False. PLoS Medicine. Public Library of Science; 2005;2(8):e124. Available from: http://dx.doi.org/10.1371/journal.pmed.0020124"</code></pre><p>The title attribute now of course uses a citation style, but this is optional information and can easily be reformatted as we have the DOI.</p><p>Or we break away from standard markdown and display reference-style links at the end of the document - similar to <a href="http://rephrase.net/box/word/footnotes/syntax/">footnotes</a>, which are also not part of standard markdown. But this is just a display issue that can be solved, and the solution might look different depending on whether the output is HTML, PDF or XML. This document for example contains 14 reference-style citations.</p><p>There is obviously a need for tools that make adding citations to scholarly markdown easier. This could be accomplished by relatively small changes to existing reference managers (enabling copy/paste of citations in reference-style markdown format), or by tools similar to the <a href="http://carlboettiger.info/2012/05/30/knitcitations.html">knitcitations</a> and <a href="http://wordpress.org/plugins/kcite/">kcite</a> mentioned above.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What is Scholarly Markdown?]]></title>
            <link>https://blog.martinfenner.org/posts/what-is-scholarly-markdown</link>
            <guid>c5c6f033-8b9a-4f40-a6e7-3d5ad0f5bc30</guid>
            <pubDate>Mon, 17 Jun 2013 17:16:00 GMT</pubDate>
            <description><![CDATA[One of the important discussions taking place at the Markdown for Science
[https://github.com/scholmd/scholmd/wiki] workshop last weekend was about the
definition of Scholarly Markdown. We came up with this
[https://github.com/scholmd/scholmd/wiki/What-is-Markdown]:

 1. Markdown that supports the requirements of scientific texts
 2. Markdown as format that glues open scientific text resources together
 3. A reference implementation with documentation and tests
 4. A community

We also agreed th]]></description>
            <content:encoded><![CDATA[<p>One of the important discussions taking place at the <a href="https://github.com/scholmd/scholmd/wiki">Markdown for Science</a> workshop last weekend was about the definition of Scholarly Markdown. We came up with <a href="https://github.com/scholmd/scholmd/wiki/What-is-Markdown">this</a>:</p><ol><li>Markdown that supports the requirements of scientific texts</li><li>Markdown as format that glues open scientific text resources together</li><li>A reference implementation with documentation and tests</li><li>A community</li></ol><p>We also agreed that <strong><strong>Scholarly Markdown</strong></strong> is a better term than <strong><strong>Markdown for Science</strong></strong>, as it also includes the Social Sciences and Humanities. And we agreed on a hashtag, <a href="https://twitter.com/search?q=%23scholmd&amp;src=typd">#scholmd</a>.</p><p>I like #3 and #4, and I was not surprised to see #1. #2 is one of the most important outcomes of the workshop for me personally, and was reflected in the discussion we had in the breakout session on <em>What is needed for Markdown to be adopted by the scientific community?</em> One important strategy is the following:</p><blockquote>We need an online tool that makes it easy for scholars to write scholarly markdown in a collaborative manner.</blockquote><p>We called this the <strong><strong>Google Docs for Scientists</strong></strong> (Google Docs is a good collaborative tool, but is lacking some important features required for scientific documents, e.g. integrated citation management). <a href="https://www.authorea.com/">Authorea</a> was mentioned as a promising example of this concept. It was also noted that some previous efforts failed, because the tool looked too different from Microsoft Word. But building such a tool wouldn’t really require markdown as a file format, and could for example also be done directly in HTML5. This would be a reasonable strategy, but in my mind is falling short because I think the problem we need to solve is more complicated than making collaboration easier with tools that look like Microsoft Word. We therefore also discussed a different strategy:</p><blockquote>We need multiple tools that make it easy for scholars to create scholarly markdown documents and openly share them. This collaborative work is not limited to authoring scholarly papers, but also includes shorter scholarly texts, e.g. experimental results, lab notebooks, lecture notes, blog posts and working papers. Ease of use is not only defined by the writing experience, but also how easy it is to share documents with others.</blockquote><p>This definition almost sounds like a definition for Open Science, and assumes that data - and increasingly software - are an integral part of reporting science. This makes <a href="https://www.scienceexchange.com/reproducibility">reproducibility</a> of scientific results much easier, and one nice example how this can be done is the integration of markdown into the R statistical software, using the <a href="http://yihui.name/knitr/">knitr package</a>. Using the <a href="http://article-level-metrics.plos.org/plos-alm-data/">May 2013 PLOS article-level metrics data</a> which are freely available for download, the R code below can be embedded into a markdown file and will produce the bar plot below when the markdown file is run in R (to try this yourself, download the ALM data and <a href="https://github.com/articlemetrics/plosOpenR/blob/master/barPlotSummary.Rmd">markdown file for this article</a>).</p><pre><code># Load required libraries
library(reshape2)

# Load the data from the bulk download, filter out DOIs that are not from PLoS journals
alm &lt;- read.csv("data-alm/alm_report_2013-05-20.csv", encoding = "UTF8", sep = ",", stringsAsFactors=FALSE, na.strings=c("0"))
alm &lt;- subset(alm, (substr(alm$doi,1,15) == "10.1371/journal"))
alm$publication_date &lt;- as.Date(alm$publication_date)
alm$counter_html &lt;- as.numeric(alm$counter_html)

# Options
plos.start_date &lt;- NA
plos.end_date &lt;- NA
plos.colors &lt;- c("#304345","#304345","#789aa1","#789aa1","#789aa1","#ad9a27","#ad9a27","#ad9a27","#ad9a27","#ad9a27","#ad9a27","#ad9a27")

# Aggregate notes and comments
alm$comments &lt;- as.numeric(alm$comments)

# Aggregate Mendeley
alm$mendeley &lt;- rowSums(subset(alm, select=c("mendeley_readers","mendeley_groups")), na.rm=TRUE)
alm$mendeley[alm$mendeley == 0] &lt;- NA

# Use subset of columns
alm &lt;- subset(alm, select=c("counter_html","pmc_html","crossref","scopus","pubmed","mendeley","citeulike","comments","researchblogging","facebook","twitter","wikipedia"))

# Calculate percentage of values that are not missing (i.e. have a count of at least 1)
colSums &lt;- colSums(!is.na(alm)) * 100 / length(alm$counter_html)
exactSums &lt;- sum(as.numeric(alm$pmc_html),na.rm =TRUE)

# Plot the chart.
opar &lt;- par(mar=c(1,7,2,1)+0.1,omi=c(1,0.3,1,1))
plos.names &lt;- c("PLoS HTML Views", "PMC HTML Views","CrossRef","Scopus","PubMed Citations", "Mendeley","CiteULike","PLoS Comments","Research Blogging","Facebook","Twitter","Wikipedia")
y &lt;- barplot(colSums,horiz=TRUE,col=plos.colors, border = NA, xlab=plos.names, xlim=c(0,120), axes=FALSE, names.arg=plos.names,las=1, adj=0)
text(colSums+6,y,labels=sprintf("%1.0f%%", colSums))</code></pre><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://blog.martinfenner.org/images/barplot-2013-06-17.svg" class="kg-image" alt="Proportion of articles covered by source. Article-level metrics for all 80,602 PLOS journal articles published until May 20, 2013."><figcaption><strong style="box-sizing: border-box; font-weight: bold;">Proportion of articles covered by source</strong>. Article-level metrics for all 80,602 PLOS journal articles published until May 20, 2013.</figcaption></figure><p>In a way this approach to scholarly markdown is much more difficult than building a nice online collaborative writing tool. But for me scholarly markdown is not about competing with Microsoft Word, it is about building something new that scholars want to use because it allows them to do something that is impossible with the existing tools. For the same reason my todo item at the end of the workshop was <em>think about document type where markdown shines</em>. The R example above is a great example where markdown shines. If you can think of additional examples, please add them to the comments.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Goodbye PLOS Blogs, Welcome Github Pages]]></title>
            <link>https://blog.martinfenner.org/posts/goodbye-plos-blogs-welcome-github-pages</link>
            <guid>f5d9c6e3-8b57-429a-b4d1-fbdc1197fdc4</guid>
            <pubDate>Sat, 15 Jun 2013 17:18:00 GMT</pubDate>
            <description><![CDATA[This is the last Gobbledygook [http://blogs.plos.org/mfenner] post on PLOS
Blogs, and at the same time the first post at the new Github blog location
[http://blog.martinfenner.org/]. I have been blogging at PLOS Blogs since the 
PLOS Blogs Network [http://blogs.plos.org/blogosphere/] was launched in
September 2010, so this step wasn’t easy. But I have two good reasons.

In May 2012 I started to work as technical lead for the PLOS Article-Level
Metrics [http://article-level-metrics.plos.org/] pro]]></description>
            <content:encoded><![CDATA[<p>This is the last <a href="http://blogs.plos.org/mfenner">Gobbledygook</a> post on PLOS Blogs, and at the same time the first post at the <a href="http://blog.martinfenner.org/">new Github blog location</a>. I have been blogging at PLOS Blogs since the <a href="http://blogs.plos.org/blogosphere/">PLOS Blogs Network</a> was launched in September 2010, so this step wasn’t easy. But I have two good reasons.</p><p>In May 2012 I started to work as technical lead for the <a href="http://article-level-metrics.plos.org/">PLOS Article-Level Metrics</a> project. Although this is contract work, and I also do other things - including spending 5% of my time as clinical researcher at Hannover Medical School - this created the awkward situation that I was never quite sure whether I was blogging as Martin Fenner or as someone working for PLOS. This was all in my head, as I never had any restrictions in my blogging from PLOS. With the recent launch of the <a href="http://blogs.plos.org/tech/">PLOS Tech Blog</a> there is now a good venue for the kind of topics I like to write about, and I have started to work on two posts for this new blog.</p><p>There will always be topics for which the PLOS Tech Blog is not a good fit, and for these posts I have launched the new personal blog at Github. But the main reason for this new blog is a technical one: I’m moving away from blogging on Wordpress to writing my posts in <a href="http://daringfireball.net/projects/markdown/">markdown</a> (a lightweight markup language), that are then transformed into static HTML pages using <a href="http://jekyllrb.com/">Jekyll</a> and <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a>. Last weekend I co-organized the workshop <a href="https://github.com/scholmd/markdown_science/wiki"><strong><strong>Scholarly Markdown</strong></strong></a> together with <a href="http://twitter.com/houshuang">Stian Håklev</a>. A full workshop report will follow in another post, but the discussions before, at and after the workshop convinced me that <strong><strong>Scholarly Markdown</strong></strong> has a bright future and that it is time to move more of my writing to markdown. At the end of the workshop each participant suggested a <a href="https://github.com/scholmd/markdown_science/wiki/Todo-list-from-workshop">todo item</a> that he/she would be working on, and my todo item was “Think about document type where MD shines”. Markdown might be good for writing scientific papers, but I think it really shines in shorter scientific documents that can easily be shared with others. And blog posts are a perfect fit.</p><p>The new site is work in progress. Over time I will copy over all old blog posts from PLOS Blogs, and will work on the layout as well as additional features. Special thanks to <a href="http://carlboettiger.info/">Carl Boettiger</a> for helping me to get started with Jekyll and Github pages.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Announcing Markdown for Science Workshop on June 8th]]></title>
            <link>https://blog.martinfenner.org/posts/announcing-markdown-for-science-workshop-on-june-8th</link>
            <guid>189d5fd2-1c4e-4d0f-8fae-179c2631a602</guid>
            <pubDate>Wed, 08 May 2013 17:20:00 GMT</pubDate>
            <description><![CDATA[On Saturday June 8th – exactly a month from today – the PLOS San Francisco
offices will host a workshop/hackathon about using markdown for science. A lot
of people are experimenting with markdown for authoring scientific articles –
see blog posts here [http://blog.yoavram.com/markx/], here
[http://inundata.org/2012/06/01/markdown-and-the-future-of-collaborative-manuscript-writing/] 
or my post here
[http://blog.martinfenner.org/2012/12/13/a-call-for-scholarly-markdown/], and
the scientific manus]]></description>
            <content:encoded><![CDATA[<p>On Saturday June 8th – exactly a month from today – the PLOS San Francisco offices will host a workshop/hackathon about using markdown for science. A lot of people are experimenting with markdown for authoring scientific articles – see blog posts <a href="http://blog.yoavram.com/markx/">here</a>, <a href="http://inundata.org/2012/06/01/markdown-and-the-future-of-collaborative-manuscript-writing/">here</a> or my post <a href="http://blog.martinfenner.org/2012/12/13/a-call-for-scholarly-markdown/">here</a>, and the scientific manuscript <a href="https://github.com/weecology/data-sharing-paper/">here</a>.</p><p>Markdown is a simple markup language for text, and is primarily used for HTML content on the web, but can also be converted to PDF, LaTeX and others. One challenge with markdown is that there are a number of slightly different “flavors” out there, from the original markdown to multimarkdown, github-flavored markdown and pandoc. Some of the advanced formatting of scientific documents – tables, citations, math – is still a challenge for markdown.</p><p>Will markdown become our next authoring format for scientific content? Will there be yet another flavor, scholarly markdown? How will markdown writing tools be different from LaTeX tools or Microsoft Word? If you care about any of these questions and are in or near San Francisco, join us on for all full day on June 8th. Free registration is open at <a href="http://mdsci13.eventbrite.com/">http://mdsci13.eventbrite.com</a>. We are collecting workshop ideas at <a href="https://github.com/karthikram/markdown_science/wiki/workshop">https://github.com/karthikram/markdown_science/wiki/workshop</a>, the Twitter hashtag is #mdsci13.</p><p>This event is organized by <a href="http://twitter.com/houshuang">Stian Håklev</a> and myself, with generous support by a <a href="http://www.force11.org/node/4358">1K Challenge prize from Force11</a>, and hosting provided by PLOS.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Additional Markdown we need in Scholarly Texts]]></title>
            <link>https://blog.martinfenner.org/posts/additional-markdown-we-need-in-scholarly-texts</link>
            <guid>8ec4bfc6-eb75-4484-ae36-dcd8fbe76592</guid>
            <pubDate>Tue, 18 Dec 2012 17:34:00 GMT</pubDate>
            <description><![CDATA[Following up from my post last week [/2012/12/13/a-call-for-scholarly-markdown/]
, below is a suggested list of features that should be supported in documents
written in scholarly markdown. Please provide feedback via the comments, or by
editing the Wiki version I have set up here
[https://github.com/mfenner/scholarly-markdown/wiki]. Listed are features that
go beyond the standard markdown syntax
[http://daringfireball.net/projects/markdown/syntax].

The goals of scholarly markdown are

 1. to s]]></description>
            <content:encoded><![CDATA[<p>Following up from <a href="https://martinfenner.ghost.io/2012/12/13/a-call-for-scholarly-markdown/">my post last week</a>, below is a suggested list of features that should be supported in documents written in scholarly markdown. Please provide feedback via the comments, or by editing the Wiki version I have set up <a href="https://github.com/mfenner/scholarly-markdown/wiki">here</a>. Listed are features that go beyond the <a href="http://daringfireball.net/projects/markdown/syntax">standard markdown syntax</a>.</p><p>The goals of scholarly markdown are</p><ol><li>to support writing of complete scholarly articles,</li><li>don’t make the syntax more complicated than it is today, and</li><li>don’t rely on HTML as the fallback mechanism.</li></ol><p>In practice this means that scholarly markdown should support most, but not all scholarly texts – documents that are heavy in math formulas, have complicated tables, etc. may be better written with LaTeX or Microsoft Word. It also means that scholarly markdown will probably contain only limited semantic markup, as this is difficult to do with a lightweight markup language and much easier with XML or a binary file format.</p><h2 id="cover-page">Cover Page</h2><p>Optional metadata about a document. Typically used for title, authors (including affiliation), and publication date, but should be flexible enough to handle any kind of metadata (keywords, copyright, etc.).</p><pre><code>---
layout: post
title: "Additional Markdown we need in Scholarly Texts"
tags: [markdown]
authors:
 - name: Martin Fenner
   orcid: 0000-0003-1419-2405
copyright: http://creativecommons.org/licenses/by/3.0/deed.en
---</code></pre><h2 id="typography">Typography</h2><p>Scholarly markdown should support superscript and subscript text, and should provide an easy way to enter greek ζ letters.</p><h2 id="tables">Tables</h2><p>Tables should work as anchors (i.e. you can link to them) and table captions should support styled text. Unless the table is very simple, tables are probably better written as CSV files with another tool, and then imported into the scholarly markdown document similar to figures.</p><!--kg-card-begin: html--><table style="box-sizing: border-box; border-collapse: collapse; border-spacing: 0px; max-width: 100%; background-color: rgb(255, 255, 255); font-size: 19px; font-family: ff-tisa-sans-web-pro, Arial, sans-serif; width: 750px; margin-bottom: 21px; color: rgb(0, 0, 0); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"><caption style="box-sizing: border-box; text-align: left; vertical-align: top; font-size: 19px; font-style: italic; line-height: 1.33; margin-bottom: 0.75em;"><strong style="box-sizing: border-box; font-weight: bold;">This is the table caption</strong>. We can explain the table here.</caption><colgroup style="box-sizing: border-box;"><col style="box-sizing: border-box; width: 0px;"><col style="box-sizing: border-box; width: 0px;"><col style="box-sizing: border-box; width: 0px;"></colgroup><thead style="box-sizing: border-box;"><tr class="header" style="box-sizing: border-box;"><th style="box-sizing: border-box; padding: 8px; text-align: center; vertical-align: bottom; border-top: 0px; font-weight: bold;">Centered Header</th><th style="box-sizing: border-box; padding: 8px; text-align: right; vertical-align: bottom; border-top: 0px; font-weight: bold;">Right Aligned</th><th style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: bottom; border-top: 0px; font-weight: bold;">Left Aligned</th></tr></thead><tbody style="box-sizing: border-box;"><tr class="odd" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: center; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);">First</td><td style="box-sizing: border-box; padding: 8px; text-align: right; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);">12.0</td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);">Example of a row that spans multiple lines.</td></tr><tr class="even" style="box-sizing: border-box;"><td style="box-sizing: border-box; padding: 8px; text-align: center; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);">Second</td><td style="box-sizing: border-box; padding: 8px; text-align: right; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);">5.0</td><td style="box-sizing: border-box; padding: 8px; text-align: left; vertical-align: top; border-top: 1px solid rgb(221, 221, 221);">Here’s another one. Note the blank line between rows.</td></tr></tbody></table><!--kg-card-end: html--><h2 id="figures">Figures</h2><p>Figures in scholarly works are separated from the text, and have a figure caption (which can contain styled text). Figures should work as anchors (i.e. you can link to them). Figures can be in different file formats, including TIFF and PDF, and those formats have to be converted into web-friendly formats when exporting to HTML (e.g. PNG and SVG).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://martinfenner.ghost.io/content/images/2021/01/set-operations-illustrated-with-venn-diagrams.png" class="kg-image" alt><figcaption>Set operations illustrated with Venn diagrams. Example taken <a href="https://texample.net/tikz/examples/set-operations-illustrated-with-venn-diagrams/">TeXample.net</a>.</figcaption></figure><h2 id="citations-and-links">Citations and Links</h2><p>Scholarly articles typically don’t have inline links, but rather citations. The external links (both scholarly identifiers such as DOIs and regular web URLs) are collected in a bibliography at the end of the document, and the citations in the text link to this bibliography. This functionality is similar to footnotes.</p><p>Citations should include a citation key in the text, e.g. <code>[@kowalczyk2011]</code>, parsed as (Kowalczyk &amp; Shankar, 2011), and a separate bibliography file in BibTeX (or RIS) format that contains references for all citations. Inserting citations and creating the bibliography can best be done with a reference manager.</p><p>Cross-links – i.e. links within a document – are important for scholarly texts. It should be possible to link to section headers (e.g. the beginning of the discussion section), figures and tables.</p><h2 id="math">Math</h2><p>Complicated math is probably best done in a different authoring environment, but simple formulas, both inline 2‾√x and block elements</p><p>ddxarctan(sin(x2))=−2cos(x2)x−2+(cos(x2))2</p><p>should be supported by scholarly markdown.</p><h2 id="comments">Comments</h2><p>Comments are important for multi-author documents and if reviewer feedback should be included. Comments should be linked to a particular part of a document to provide context, or attached at the end of a document for general comments. It would also be helpful to “comment out” parts of a document, e.g. to indicate parts that are incomplete and need more work. Revisions of a markdown document are best handled using a version control system such as git.</p><h2 id="references">References</h2><p>Kowalczyk, S., &amp; Shankar, K. (2011). Data sharing in the sciences. <em>Annual Review of Information Science and Technology</em>, <em>45</em>(1), 247–294. Retrieved from <a href="http://doi.org/10.1002/aris.2011.1440450113">http://doi.org/10.1002/aris.2011.1440450113</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Call for Scholarly Markdown]]></title>
            <link>https://blog.martinfenner.org/posts/a-call-for-scholarly-markdown</link>
            <guid>fd7b416a-1cbd-4ca0-8932-03ab2e144464</guid>
            <pubDate>Thu, 13 Dec 2012 17:37:00 GMT</pubDate>
            <description><![CDATA[Markdown is a lightweight markup language, originally created by John Gruber for
writing content for the web. Other popular lightweight markup languages are
Textile and Mediawiki. Whereas Mediawiki markup is of course popular thanks to
the ubiquitous Wikipedia, Markdown seems to have gained momentum among scholars.
Markdown really focuses on writing content, many of the features of today’s word
processors are just a distraction (e.g. fonts, line spacing or style sheets).
Adding markup for docume]]></description>
            <content:encoded><![CDATA[<p>Markdown is a lightweight markup language, originally created by John Gruber for writing content for the web. Other popular lightweight markup languages are Textile and Mediawiki. Whereas Mediawiki markup is of course popular thanks to the ubiquitous Wikipedia, Markdown seems to have gained momentum among scholars. Markdown really focuses on writing content, many of the features of today’s word processors are just a distraction (e.g. fonts, line spacing or style sheets). Adding markup for document structure (e.g. title, authors or abstract) on the other hand is overly complicated with tools such as Microsft Word.</p><p>Fortunately or unfortunately there are several versions (or flavors) of Markdown. The original specification by John Gruber hasn’t been updated for years. Github uses Markdown with some minor modifications. Multimarkdown and Pandoc provide features important for scholarly content, e.g. citations, superscript and tables.</p><ul><li>Markdown</li><li>Github-flavored Markdown</li><li>Multimarkdown</li><li>Pandoc</li></ul><p>The Pandoc flavor of Markdown probably comes closest to the requirements of a scholar, but still has limitations, e.g. support for metadata and tables isn’t very flexible. I propose that we as a community create a new Scholarly Markdown flavor, which takes into account most of the use cases important for scholarly content.</p><p>One of the big advantages of Markdown is that the format can not only be translated to HTML, but also to other formats, and Pandoc is particularly good in translating to and from many different formats. We want to make sure that Scholarly Markdown not only translates into nice Scholarly HTML (with good support for HTML5 tags relevant for scholars), but also into Microsot Word, LaTeX and PDF, as these are the formats typically required by manuscript tracking systems.</p><p>Some of the features required for Scholarly Markdown include:</p><ul><li>Superscript and subscript</li><li>Highlighting text (supporting the HTML tag <code>&lt;mark&gt;</code>)</li><li>Captions for tables and figures (with support for the HTML tags <code>&lt;caption&gt;</code> and <code>&lt;figcaption&gt;</code>)</li><li>Support for document sections (the HTML5 tags <code>&lt;article&gt;</code>, <code>&lt;header&gt;</code>, <code>&lt;footer&gt;</code>, <code>&lt;section&gt;</code>)</li><li>Good table support</li><li>Math support</li><li>Good citation support</li><li>Support for comments and annotations</li></ul><p>Multimarkdown and Pandoc of course already support many of these features. Tables and citations are two examples where it is important to not only support them, but support them in a non-intrusive way that doesn’t get in the way of the flow of writing.</p><p>BTW, this wouldn’t be the first community flavor for Markdown. The screenwriting community has done this already with <a href="http://fountain.io/">Fountain</a>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Interview with Geoffrey Bilder]]></title>
            <link>https://blog.martinfenner.org/posts/interview-with-geoffrey-bilder</link>
            <guid>9226f9e4-0604-4faa-a58d-466f7900d1d1</guid>
            <pubDate>Tue, 17 Feb 2009 20:27:00 GMT</pubDate>
            <description><![CDATA[Almost exactly two years ago, CrossRef
[https://web.archive.org/web/20090221213233/http://www.crossref.org/] invited a
number of people to discuss unique identifiers for researchers (CrossRef Author
ID meeting
[https://web.archive.org/web/20090221213233/http://www.crossref.org/CrossTech/2007/02/crossref_author_id_meeting.html]
). One year ago Thomson Reuters launched ResearcherID (Thomson Scientific
launches ResearcherID to uniquely identify authors
[https://web.archive.org/web/20090221213233/ht]]></description>
            <content:encoded><![CDATA[<p>Almost exactly two years ago, <a href="https://web.archive.org/web/20090221213233/http://www.crossref.org/"><strong>CrossRef</strong></a> invited a number of people to discuss unique identifiers for researchers (<a href="https://web.archive.org/web/20090221213233/http://www.crossref.org/CrossTech/2007/02/crossref_author_id_meeting.html"><strong>CrossRef Author ID meeting</strong></a>). One year ago Thomson Reuters launched <strong>ResearcherID</strong> (<a href="https://web.archive.org/web/20090221213233/http://network.nature.com/people/mfenner/blog/2008/01/21/thomson-scientific-launches-researcherid-to-uniquely-identify-authors"><strong>Thomson Scientific launches ResearcherID to uniquely identify authors</strong></a>). And two months ago Phil Bourne and Lynn Fink wrote about this topic in a  <strong>PLoS Computational Biology</strong> paper (<a href="https://web.archive.org/web/20090221213233/http://dx.doi.org/10.1371/journal.pcbi.1000247"><strong>I Am Not a Scientist, I Am a Number</strong></a>).</p><p>So it comes as no surprise that we also talked about author identifiers at the recent <a href="https://web.archive.org/web/20090221213233/http://www.scienceonline09.com/index.php/wiki/"><strong>ScienceOnline09</strong></a> meeting in North Carolina (both in the session <a href="https://web.archive.org/web/20090221213233/http://www.scienceonline09.com/index.php/wiki/Reputation_authority_and_incentives/"><strong>Impact Factors and researcher incentives</strong></a> and over drinks afterwards). <a href="https://web.archive.org/web/20090221213233/http://network.nature.com/people/U42E63119/profile"><strong>Cameron Neylon</strong></a> wrote down his thoughts after the meeting in a blog post (<a href="https://web.archive.org/web/20090221213233/http://blog.openwetware.org/scienceintheopen/2009/01/20/a-specialist-openid-service-to-provide-unique-researcher-ids/"><strong>A specialist OpenID service to provide unique researcher IDs?</strong></a>), and this resulted in <a href="https://web.archive.org/web/20090221213233/http://friendfeed.com/e/c1fd00ec-15f9-d894-4ea9-4ffeaac5ae28/A-specialist-OpenID-service-to-provide-unique"><strong>a very interesting</strong></a> discussion on <strong>FriendFeed</strong>. And at about the same time both <a href="https://web.archive.org/web/20090221213233/http://network.nature.com/people/jandot/profile"><strong>Jan Aerts</strong></a> (<a href="https://web.archive.org/web/20090221213233/http://saaientist.blogspot.com/2009/01/who-o-o-are-you-who-who-who-who.html"><strong>Who-o-o are you? Who who? Who who?</strong></a>) and Christopher Leonard (<a href="https://web.archive.org/web/20090221213233/http://blogs.openaccesscentral.com/blogs/pmcblog/entry/some_thoughts_on_unique_author"><strong>Some thoughts on unique author IDs</strong></a>) independently wrote blog posts about the same topic. This week <a href="https://web.archive.org/web/20090221213233/http://network.nature.com/people/U42E63119/profile"><strong>Cameron Neylon</strong></a> summarized the discussion in another blog post (<a href="https://web.archive.org/web/20090221213233/http://blog.openwetware.org/scienceintheopen/2009/02/15/contributor-ids-an-attempt-to-aggregate-and-integrate/"><strong>Contributor IDs – an attempt to aggregate and integrate</strong></a>).</p><p>Science bloggers have put a lot of thought into the idea of a unique author identifier and I’ve collected more reading material about author ID at <strong>Connotea</strong> using the tag <a href="https://web.archive.org/web/20090221213233/http://www.connotea.org/tag/authorid"><strong>authorid</strong></a>. But I was also very curious to learn more about the work that has already been done. That’s why I asked <a href="https://web.archive.org/web/20090221213233/http://network.nature.com/people/gbilder/profile"><strong>Geoffrey Bilder</strong></a> from <strong>CrossRef</strong> a few questions. In the end Geoffrey talked not only about author identifiers, but also about CrossRef, DOIs and many other aspects of scholarly publishing.</p><h3 id="1-can-you-describe-what-crossref-is-and-does">1. Can you describe what CrossRef is and does?</h3><p>Let me start with what it does because this is a little less likely to make your eyes glaze-over.</p><p>CrossRef was originally founded by scholarly publishers to fight <a href="https://web.archive.org/web/20090221213233/http://en.wikipedia.org/wiki/Link_rot"><strong>link-rot</strong></a>.</p><p>Web links have a half-life of about six years. That is, after six years a link is likely to break because the content that it pointed to has been moved. To a lay-audience this might be a mere annoyance, but to scholarly and professional publishers broken links are anathema. The scholarly record is built on a foundation of links in the form of citations. If these online citation links break, the online scholarly citation record breaks.<br><br>But surely fighting link-rot should be simple, right? After all, the glory of the web is its decentralized architecture, one in which the domain name that you own can be used as a namespace for identifiers. <a href="https://web.archive.org/web/20090221213233/http://en.wikipedia.org/wiki/Tim_Berners-Lee"><strong>Tim Berners-Lee</strong></a> has said that “cool URIs never die”. Aren’t “persistent URIs” merely a matter of being disciplined in the way that you mint URIs and in being conscientious about sensibly redirecting URIs when things change location on your web server?</p><p>Well, certainly the majority of broken links on the web are the result of careless web administrators not taking the time to structure and redirect their web site’s URIs properly, but there are a significant percentage of links that will break despite the best efforts of webmasters. This is because in some cases the domain name in the link will change, and in these cases the whole “domain name as URI minting name-space” starts to crumble. When otherwise sensible technorati refer to “owning” a domain name, it makes me want to stick forks in my eyeballs. We do not “own” domain names. At best, we only lease them and there are manifold ways in which we could lose control of a domain name – through litigation, through forgetfulness, through poverty, through voluntary transfer, etc. Once you don’t control a domain name anymore, then you can’t control your domain-name-based persistent identifiers either.</p><p>Incidentally, another technorati meme that makes me want to self-harm with cutlery is the notion that “persistent identifiers don’t matter in the age of the search engine. If a link breaks, we can just find the content again wherever it has moved.” This, naturally, is the self-serving argument often used by Google and it’s starry-eyed acolytes. Even just few minutes’ reflection reveals the gaping hole in this approach.</p><p>The hole is this – how do you cite a specific copy of something if there are multiple <strong>almost identical</strong> copies of it located in different places? For instance, lets say there are 2 copies (X and Y) of an article which only differ in a few paragraphs, but those few paragraphs are crucial and likely to change the reader’s interpretation of the work. How, in a world where persistent linking is maintained by search engines, do you create a citation link to article X instead of article Y? To create a search-based link that is more likely to resolve to article X, you would essentially have to encode the entire article in the search URI! Even this wouldn’t guarantee you that the link directed a future reader to article X first; it is still possible that article Y might end up getting a higher ranking because more people have linked to it and it therefore has a higher page-rank (or equivalent). Believe me, even variations of the search engine scenario (using document hashes for citations, pingbacks, etc.) quickly unravel after a little reflection.</p><p>This is all a long-winded and ranty way of saying that the issue of persistent identifiers on the web is just a wee bit more complex than most people think. So how does CrossRef address the persistent identifier issue?</p><p>From our perspective, the persistent identifier problem is much more a social problem than a technical one.</p><p>In fact, the technical part of our service is relatively straightforward. CrossRef provides a level of indirection (i.e. a pointer) between an identifier and a URL. When publishers put something online, they assign a CrossRef <a href="https://web.archive.org/web/20090221213233/http://www.doi.org/"><strong>Digital Object Identifier</strong></a> (DOI) to it and submit a record for that item with CrossRef. The record includes the CrossRef DOI, basic bibliographic metadata for the content and a URL that points to the current location of the content. People citing the publisher’s content are encouraged to use the CrossRef DOI for the citation instead of the publisher’s URL. When a researcher clicks on a CrossRef DOI, the CrossRef service redirects the URL to whatever URL the publisher has currently registered for that CrossRef DOI. This means that the publisher can update their CrossRef DOI record to point to a completely new URI (including a new domain name) and any CrossRef DOI citations will continue to work. We provide a few other services based on this infrastructure too. So, for instance, we can resolve an <a href="https://web.archive.org/web/20090221213233/http://www.oclc.org/research/projects/openurl/default.htm"><strong>OpenURL</strong></a> to a CrossRef DOI (by querying the publisher-submitted bibliographic metadata), resolve a free-text query to a CrossRef DOI and we can also return bibliographic metadata instead of redirecting if that is what the user wants.</p><p>So-far, so good, but this isn’t anything that couldn’t be accomplished using other redirection tools such as <a href="https://web.archive.org/web/20090221213233/http://www.purl.org/"><strong>PURLs</strong></a>, <a href="https://web.archive.org/web/20090221213233/http://www.handle.net/"><strong>CNRI Handles</strong></a>, <a href="https://web.archive.org/web/20090221213233/http://www.oasis-open.org/committees/tc_home.php?wg_abbrev=xri"><strong>XRIs</strong></a>, <a href="https://web.archive.org/web/20090221213233/http://numly.com/numly/default.asp"><strong>NUmly Numbers</strong></a>, etc.? The crucial question to ask of any such service is, “what guarantees that the publisher will actually update their URL pointers?” If the publisher doesn’t update these pointers, then the links will break anyway. It isn’t enough that a publisher decides to use PURLs, if they then don’t update their PURLs- in perpetuity.</p><p>This is where it is important to explain the organizational structure and the social effect that this has on the service.</p><p>CrossRef was founded as a non-profit, membership organization for publishers. Note that we are entirely catholic in our definition of what a publisher is, so our membership includes commercial publishers, non-profit publishers, open access publishers, institutional repositories, NGOs and IGOs, Video publishers, Wiki-based publishers, etc. We are also open to publishers of all disciplines (humanities, social sciences, sciences, professional), geographies and content types (journals, books, database records, videos, etc.)</p><p>In practice, what unites our membership is a concern that their content should be considered worthy of trust by professional researchers. One way in which researchers assess the trustworthiness of content is by determining how it sits within the scholarly record. Does it provide evidence for its assertions in citations? Do other people cite it?</p><p>When a publisher joins CrossRef, they agree to a <a href="https://web.archive.org/web/20090221213233/http://www.crossref.org/02publishers/59pub_rules.html"><strong>set of enforceable terms and conditions</strong></a> that govern the way in which they use CrossRef’s persistent citation infrastructure. Specifically, they agree to:</p><ul><li>Register DOIs within a week of something being published online</li><li>Update the URLs associated with a DOI when said URLs change</li><li>Link citations in their content via the DOI</li></ul><p>In joining CrossRef they also agree that CrossRef can fine them or throw them out of the service if they do not meet the terms and conditions of the service. Note that the penalty of being thrown out can be quite severe as it effectively means that the publisher would become invisible in the online scholarly citation record. In short, the system has a built-in social feedback loop that strongly enforces good citizenship.</p><p>As I said, the technical infrastructure of CrossRef is pretty mundane, and it is the social aspect of the service that does the most to guarantee the persistency of CrossRef citation links.</p><h3 id="2-what-are-your-responsibilities-within-crossref">2. What are your responsibilities within CrossRef?</h3><p>Thinking of, gathering the requirements for, designing and (most importantly) launching new services.</p><p>Last year we launched a plagiarism detection service called <a href="https://web.archive.org/web/20090221213233/http://www.crossref.org/crosscheck.html"><strong>CrossCheck</strong></a>. This year I am working on Contributor ID, another project tentatively named CrossMark and a bunch of smaller projects designed to encourage the use of DOIs in citations.</p><h3 id="3-what-did-you-do-before-starting-to-work-for-crossref">3. What did you do before starting to work for CrossRef?</h3><p>In the early nineties Allen Renear and I co-founded Brown University’s Scholarly Technology Group, where we were charged with providing advanced consulting and support to Brown’s research community. In the mid-nineties I grew tired of the politics, resource constraints and institutional paralysis that seems to grip so many universities and I decided to do something as far away from the academic sphere as possible. In short, I worked at a management consultancy doing R&amp;D for their IT group. In 2000 I was lured into managing the web development efforts for an Information Architecture firm called Dynamic Diagrams. In 2001 we were bought by <a href="https://web.archive.org/web/20090221213233/http://www.ingentaconnect.com/"><strong>Ingenta</strong></a> in the UK. I became Ingenta’s CTO and I moved to Oxford in 2002. I left Ingenta in 2005, did a brief spell of consulting for publishers in 2006 and joined CrossRef in 2007.</p><h3 id="4-what-are-your-thoughts-on-how-an-author-identifier-should-look-like">4. What are your thoughts on how an author identifier should look like? </h3><p>First of all, I think we need to stop talking about “author” identifiers. One of the first requirements we found when interviewing publishers, researchers and librarians is that we would ideally like to be able to identify any party who contributes to the scholarly literature in any way. That is, we would also like to be able to identify reviewers, editors, correspondents, bloggers, commenters, etc. This is why we have taken to calling our project the “CrossRef Contributor ID” project. This isn’t just playing with words either. For instance, as soon as you start thinking about things like “how do you accommodate reviewers” in this system you need to think of things like pseudo-anonymity. That is, you want somebody to be able to get credit for doing reviews in a way that doesn’t necessarily reveal who reviewed what. In turn, the pay-off for designing a system whereby anonymous reviewers might be credited with reviews could be profound. It might ultimately result in researchers having much more incentive to review if reviewing were something that could be counted and rated in the same way that authorship is.</p><p>Second, I think that people conflate a lot of issues when they talk about “author identifiers” [sic]. Are they talking about the simple token used (e.g. a unique string or a number assigned to an individual like a social security number), are they talking about an authentication mechanism (e.g. <a href="https://web.archive.org/web/20090221213233/http://openid.net/what/"><strong>OpenID</strong></a>, <a href="https://web.archive.org/web/20090221213233/http://shibboleth.internet2.edu/"><strong>Shibboleth</strong></a>) or are they talking about the profile information associated with an identifier (e.g. publications, affiliation, contact info, etc.)? Obviously, these all overlap in some ways, but how they relate and what you choose to focus on depends largely on your use cases.</p><p>Third, speaking of use cases, our requirements gathering has identified two broad categories of use cases that, though related, have profoundly different implementation implications. One category of use cases identified revolves around “knowledge discovery” and the other category of cases revolves around “authentication.”</p><p>The “knowledge discovery” use cases are probably the most obvious things that people would like to be able to do with a contributor ID such as:</p><ul><li>Determine what IDs authored/edited/reviewed document X</li><li>What documents where authored/edited/reviewed by ID Y</li><li>What IDs are related to ID Z and what is the nature of that relationship (e.g. co-authored, edited, reviewed)</li><li>What (subject to privacy settings) is the profile information for ID Z (e.g. institutional affiliation, email address, etc.)</li><li>All the author IDs and their respective publications where the institutional affiliation recorded by the author is X</li><li>Etc.</li></ul><p>At this point I feel obliged to point out that the bulk of our requirements gathering has been focused on trying to understand the needs of our member publishers. The reason I mention this here is that the bulk of the “authentication” use cases that we identified are all focused around making publisher back-office systems less cumbersome. So, for instance, publishers are interested in using a “contributor id” for:</p><ul><li><a href="https://web.archive.org/web/20090221213233/http://en.wikipedia.org/wiki/Single_sign-on"><strong>single sign-on</strong></a> (SSO) for manuscript tracking systems</li><li>Disambiguating contact information for use by editorial offices, royalty payments systems, copyright clearances, etc.</li><li>Automatic updating of email addresses for table of contents (TOC) alerts and other automated email communications</li><li>Automated tools for detecting potential reviewers, including tools for detecting potential conflicts of interest</li><li>Synchronization with publisher web site user profiles and granting researchers customized, privileged access to content based on profiles</li><li>Understanding all of the manifold ways in which an individual “contributes” to a publisher or a field (e.g. As an editor, reviewer, letter writer, conference chair, etc.).</li><li>Etc.</li></ul><p>As I said, these are very publisher-focused use cases, but this is not to say that we are not interested in the use cases posed by librarians, researchers and funding agencies. We have actively been talking to people from each of these constituencies and we are trying to understand if there are ways in which we can help them. For instance, we have recently been speaking to a group of researchers who are interested in using some sort of authenticated contributor ID as a mechanism for controlling who gets trusted access to sensitive genome-wide aggregate genotype data.</p><p>The interesting thing to note about these “authentication” use cases is that they have far more stringent requirements than the “knowledge discovery” use cases. In other words if you are only trying to address the knowledge discovery problem, it might be fine to use automated techniques to disambiguate authors and assign IDs to them. State-of-the-art mechanisms for automatic disambiguation of authors from a defined corpus can be 96-97% accurate, which sounds pretty good. At least until you realize that CrossRef has ~200K new article DOIs deposited each month, each of which on average has about 3 authors. This could potentially leave you with upwards of 20K in mis/un-identified authors. This error rate might be an acceptable tradeoff for knowledge discovery type applications, but it certainly isn’t suitable for authentication type applications.</p><p>Speaking of authentication, I think the fourth thing to note is that, though I think <strong>OpenID</strong> will probably play an important role in any service we provide, by itself it makes a pretty bad identity token and would provide little utility on its own. This all gets back to some of the issues that I raised above when discussing persistent identifiers: URI-based identifiers are fragile because they depend on the domain name. What happens if your OpenID is tied to a domain that you don’t control (e.g. a company, an institution a country)? How can you guarantee that, should you leave that company/institution/country that they will do the right thing and let you maintain or redirect that identifying credential?</p><p>The traditional geeky response to this scenario is “don’t get yourself into that situation. Only tie your <strong>OpenID </strong>to a domain that you own.” (Insert forks in eyeballs). Again, you do not “own” a domain name. You lease it. What happens if you lose control of it due to litigation, forgetfulness, poverty, divorce, death? Death? Yes, what happens when somebody dies? When I die does the not-yet-born Georgia Bilder get to buy “my” domain “gbilder.com” and make it the basis of her identity? Mmm… Gets kind of complicated doesn’t it?</p><p>Of course, lots of the same issues can be raised with CrossRef, right? What guarantees that CrossRef won’t become evil and co-opt all of our identities? This, of course is the big fear underlining the knee-jerk reaction against “centralized systems” in favor of “distributed systems”. The problem with this, as I mentioned in the <a href="https://web.archive.org/web/20090221213233/https://friendfeed.com/e/c1fd00ec-15f9-d894-4ea9-4ffeaac5ae28/A-specialist-OpenID-service-to-provide-unique/"><strong>FriendFeed thread</strong></a> is that my personal and unfashionable observation is that “distributed” begets “centralized.” For every distributed service created, we’ve then had to create a centralized service to make it useable again (ICANN, Google, Pirate Bay, CrossRef, DOAJ, ticTocs, WorldCat, etc.). This gets us back to square one and makes me think the real issue is- how do you make the centralized system that eventually emerges accountable? This is, of course, a social issue more than a technical issue and involves making sure that whatever entity emerges has clearly defined data portability policies and a “living will” that attempts to guarantee that the service can be run in perpetuity- even if by another organization. For the record, I don’t think adopting the slogan “don’t be evil” is enough ;).</p><p>Anyway- I could go on talking about what the contributor ID “should look like” for a very, very long time, but I think that the above probably addresses some of the major points that are raised when the topic is discussed.</p><h3 id="5-what-are-the-benefits-and-maybe-disadvantages-if-crossref-manages-the-author-identifier">5. What are the benefits (and maybe disadvantages) if CrossRef manages the author identifier?</h3><p>I think the biggest potential disadvantage that CrossRef has is that it is a consensus-based organization that is governed by sometimes fierce competitors. This aspect of the organization can sometimes slow things down. On the other hand, this can also be a huge strength for us. Once a consensus is agreed, we can move very quickly and push uptake across the industry.</p><p>Research increasingly transcends institutional, geographic and discipline boundaries, so I think another advantage that we have is that we are well positioned to provide a service that is similarly unconstrained.</p><p>Finally, I think that we have a very interesting advantage by virtue of the fact that our infrastructure is already integrated upstream in the publication process. There is a useful property of the system that we are designing in that, as researchers used the CrossRef identifier in their interactions with publishers and this data is fed back into our system via DOI deposits, you could start to develop a trust-metric based on the types of claims attached to an author’s profile. For instance, an author profile that consisted of nothing but self-claims (e.g. I claim I wrote paper X) might not be very worthy of trust whereas an author profile that consisted of publications that had been verified by the publisher (by virtue of those publications having been processed along with the CrossRef contributor ID) would have far more credibility. You can start to see an interesting hierarchy of publication claims emerging such as:</p><ul><li>Proxy claims (Leigh claims Geoffrey wrote article X)</li><li>Self Claims (Geoffrey claims Geoffrey wrote article X)</li><li>Verified claims (Geoffrey claims Geoffrey wrote article X <strong>and</strong> the “Journal of Psychoceramics” confirms this claim)</li><li>Verified Proxy Claims (Geoffrey (who has already been verified as an author of article X) claims that Kirsty was also an author of article X)</li></ul><h3 id="6-how-does-your-author-identifier-relate-to-other-identifiers-e-g-researcherid-scopus-author-idor-openid">6. How does your author identifier relate to other identifiers, e.g. <a href="https://web.archive.org/web/20090221213233/http://www.researcherid.com/">ResearcherID</a>, <a href="https://web.archive.org/web/20090221213233/http://help.scopus.com/robo/projects/schelp/h_autsrch_intro.htm">Scopus Author ID</a>or <a href="https://web.archive.org/web/20090221213233/http://openid.net/what/">OpenID</a>?</h3><p>OpenID is a different kettle of fish, and I discussed it already above. As for the others (I’d add <a href="https://web.archive.org/web/20090221213233/http://www.authorresolver.com/"><strong>Author Resolver</strong></a>, <a href="https://web.archive.org/web/20090221213233/http://repec.org/"><strong>RePEC</strong></a>, <a href="https://web.archive.org/web/20090221213233/http://www.scilink.com/start.action"><strong>SciLink</strong></a>, <a href="https://web.archive.org/web/20090221213233/http://bibserver.berkeley.edu/cgi-bin/mathweb/index.py"><strong>MathPeople</strong></a>, <a href="https://web.archive.org/web/20090221213233/http://network.nature.com/"><strong>Nature Network</strong></a>, etc.), we’ve actually been talking to some of these parties in order to understand how they might relate to a CrossRef Contributor ID. One obvious difference is in the use-cases being addressed. All of the above are focused on “knowledge discovery” use-cases. None of them pretends to provide any sort of authentication services. It is also interesting to note that in a lot of the above cases, the parties see their author identification functionality as a means to an end. For instance, their primary application is “creating better metrics” or “running a social network” or “expert identification” for recruiting purposes. In these cases they don’t necessarily see a CrossRef system as being competitive and, in fact, they think that such a service might even improve their primary application.</p><h3 id="7-can-you-talk-about-the-current-status-and-next-planned-steps-of-the-contributorid-project">7. Can you talk about the current status and next planned steps of the ContributorID project?</h3><p>We just ended lengthy period of investigation and requirements gathering. In the process we went down a few blind alleys. Now we are working on a prototype that we will test with a few publishers. It is hard to say how long this will take as we are just in the process of planning this phase.</p><h3 id="8-satisfying-many-different-interests-is-one-of-the-biggest-challenges-in-creating-an-author-identifier-what-are-the-lessons-learned-from-implementing-the-digital-object-identifier-doi-">8. Satisfying many different interests is one of the biggest challenges in creating an author identifier. What are the lessons learned from implementing the digital object identifier (<a href="https://web.archive.org/web/20090221213233/http://www.doi.org/">DOI</a>)?</h3><p>I’ll give you one tactical lesson and one strategic lesson.</p><p>The tactical lesson is foremost in my mind because I have recently been trying to build tools to encourage researchers to use DOIs in their citations. The problem arrises when a researcher occasionally encounters a DOI that is 80 characters long. There is just no way that a researcher is going to insert <strong>that</strong> in a citation. The tactical lesson here is that it is sometimes better to make an identifier opaque and short. This is also a tremendously unfashionable position to take, but I think that one of Clay Shirky’s observations about hierarchical categorization systems also applies to identifiers. If you make the identifier human-interpretable and add semantics, then people will be extremely tempted to start hard-coding ontologies into their identifiers. This makes said identifiers both long and inherently brittle. The ontologies will inevitably evolve, and then people will want to change the identifiers- at which point they will either break or you have a giant identifier mapping subsystem to create.</p><p>We see a manifestation of this syndrome already with the DOI. Each DOI has a four-digit “prefix” which is effectively a namespace for the assigning publisher. Note that I said the “assigning” publisher- this is not necessarily the publisher who currently “owns” the DOI with that prefix. What this often means is that, when publisher A acquires publisher B, publisher A will ask CrossRef if we can create new DOIs for all of publisher B’s backfiles so that they all have the same prefix! The answer to their request is “no”, but you wouldn’t believe how stroppy publishers can get about this. They somehow imbue this ridiculous four-digit prefix with branding significance. This, of course, is absolutely mental, but it is a predictable form of mental. The French went mad when they had to replace their region-encoded license plates with opaque EU ones. People in the US go mad when they are given new area codes. In short, when people associate semantic significance in identifiers, you will face problems.</p><p>The strategic lesson is basically a recapitulation of the "technical vs “social” theme I’ve been banging on about. I think that, at first, even our membership thought of the CrossRef DOI as being a technical solution to a problem, not a social one. It has become much clearer to us over the years that CrossRef DOIs are only as persistent as CrossRef staff. That is, we sometimes have to bang on lots of heads and threaten members with fines and worse in order to make sure that they are meeting their terms &amp; conditions. The good news is that CrossRef has become essential infrastructure for a wide variety of publishers who are often at each other’s throats in any other circumstances. In many ways these “different interests” are our strength. Everybody wants it to work better, nobody wants to see it die and nobody wants it to be co-opted. We are working hard to put the social structures into place that will guarantee its longevity. Part of this is making sure that we are fiscally sound (which we are) and part of this is making sure that, even if we do disappear, other stakeholders can run the system if need be.</p><h3 id="9-what-can-researchers-interested-in-author-identifiers-do-to-help">9. What can researchers interested in author identifiers do to help?</h3><ul><li>Feed CrossRef more use cases.</li><li>Let CrossRef know what you think will/won’t work.</li><li>Make sure you let your publishers know if you think this is a good idea. Naturally, I expect you will also let them know if you think it is a bad idea ;-)</li></ul><p>I can be reached at <strong>gbilder at crossref dot org</strong>.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[New ways to look at your presentation]]></title>
            <link>https://blog.martinfenner.org/posts/new-ways-to-look-at-your-presentation</link>
            <guid>ca9ef7fb-2ec8-4a89-a358-e399962e7dc6</guid>
            <pubDate>Fri, 26 Sep 2008 04:45:00 GMT</pubDate>
            <description><![CDATA[This blog post is about presentations. And this usually means PowerPoint
presentations, although some people do well without it1
[https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn1]
. Edward Tufte argues that PowerPoint can be a really bad tool to create slides2
[https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn2]
. ]]></description>
            <content:encoded><![CDATA[<p>This blog post is about presentations. And this usually means PowerPoint presentations, although some people do well without it<sup><a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn1"><strong>1</strong></a></sup>. Edward Tufte argues that PowerPoint can be a really bad tool to create slides<sup><a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn2"><strong>2</strong></a></sup>. But it is probably not the software, but rather the people that produce these slides that are responsible for the quality<sup><a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn3"><strong>3</strong></a></sup>. The Neurotic Physiology blog published a list of things you shouldn’t do during a Powerpoint presentation<sup><a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn4"><strong>4</strong></a></sup>. But there are also many tips to create better presentations. A May 2008 <em>Nature Methods</em> editorial<sup><a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn5"><strong>5</strong></a></sup> gives ten such suggestions. Links to some more Powerpoint tips were collected in a Nautilus blog post<sup><a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn6"><strong>6</strong></a></sup> by <a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/maxine/profile"><strong>Maxine Clarke</strong></a>. One positive example is this presentation by <a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mza/profile"><strong>Matt Wood</strong></a>from the Science Blogging London conference:</p><p><a href="https://web.archive.org/web/20080929085351/http://www.slideshare.net/mza/how-to-make-friendfeeds-and-influence-people-presentation?type=powerpoint">How to make Friendfeeds and influence people</a></p><p>View SlideShare <a href="https://web.archive.org/web/20080929085351/http://www.slideshare.net/mza/how-to-make-friendfeeds-and-influence-people-presentation?type=powerpoint"><strong>presentation</strong></a> or <a href="https://web.archive.org/web/20080929085351/http://www.slideshare.net/upload?type=powerpoint"><strong>Upload</strong></a> your own. (tags: <a href="https://web.archive.org/web/20080929085351/http://slideshare.net/tag/science"><strong>science</strong></a> <a href="https://web.archive.org/web/20080929085351/http://slideshare.net/tag/blogs"><strong>blogs</strong></a>)</p><p>The Nature Network <a href="https://web.archive.org/web/20080929085351/http://network.nature.com/groups/scivis/forum/topics"><strong>Visualization &amp; Science Forum</strong></a> is a great place for further discussions.</p><p>Presentations can also be created online. Google Docs and Zoho Show have been around for a while now, but 280Slides<sup><a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn7"><strong>7</strong></a></sup> is a fairly new offering with a very slick interface. The advantages of these programs: slides can be created by several authors working together and slides can be easily shared. But presentations created with Powerpoint can also be shared online. Slideshare and Scribd are the most popular tools for this, and since last week these presentations can be embedded into Nature Network blog posts. By default, these presentations are public and can be seen by everybody. But they can also be uploaded as private presentations and only those that know the secret URL can see them. Presentations in the life sciences can also be uploaded to Nature Preceedings<sup><a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn8"><strong>8</strong></a></sup>. This way the scientific presentation receives a DOI and becomes citable. But Nature Preceedings has still a long way to go with currently only about 50 presentations available. Which is a bit suprising, since it looks like the perfect platform to host conference presentations.</p><p>YouTube videos or podcasts are probably the preferred format to share presentations that also include the recorded audio. Having the audio available is especially important for those presentations that have little text on their slides. Many presentations from the TED (Technology, Entertainment, Design) conference<sup><a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn9"><strong>9</strong></a></sup> have been made available as TEDTalks, including this one by Neuroanatomist Jill Bolte Taylor:</p><p>If you want to give a presentation remotely (i.e. to one or more people in a different location), you could use that feature in Google Docs<sup><a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn10"><strong>10</strong></a></sup>. Or use a full-fledged web conferencing solution such as Dimdin<sup><a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/mfenner/blog/2008/09/26/new-ways-to-look-at-your-presentation#fn11"><strong>11</strong></a></sup>, which is free for up to 20 users and also is available as Open Source community edition.</p><p><sup>1</sup> <a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/henrygee/blog/2008/04/18/power-point-to-the-people"><strong>Powerpoint to the People</strong></a></p><p><sup>2</sup> <strong>Kemp M.</strong> PowerPoint presentations and the culture of pitch. <em>Nature</em>2006; <a href="https://web.archive.org/web/20080929085351/http://dx.doi.org/10.1038/442140a"><strong>doi:10.1038/442140a</strong></a></p><p><sup>3</sup> <a href="https://web.archive.org/web/20080929085351/http://freakonomics.blogs.nytimes.com/2007/06/20/dont-hate-powerpoint-hate-the-powerpointers/"><strong>Don’t hate Powerpoint; Hate the Powerpointers</strong></a></p><p><sup>4</sup> <a href="https://web.archive.org/web/20080929085351/http://scicurious.wordpress.com/2008/08/12/and-now-a-powerpoint-presentation/"><strong>And Now, a Powerpoint Presentation</strong></a></p><p><sup>5</sup> Talking points. <em>Nature Methods</em> 2008; <a href="https://web.archive.org/web/20080929085351/http://dx.doi.org/10.1038/nmeth0508-371"><strong>doi:10.1038/nmeth0508-371</strong></a></p><p><sup>6</sup> <a href="https://web.archive.org/web/20080929085351/http://blogs.nature.com/nautilus/2008/05/how_to_give_a_good_presentatio.html"><strong>How to give a good presentation</strong></a></p><p><sup>7</sup> <a href="https://web.archive.org/web/20080929085351/http://280slides.com/"><strong>280Slides</strong></a></p><p><sup>8</sup> <a href="https://web.archive.org/web/20080929085351/http://precedings.nature.com/"><strong>Nature Preceedings</strong></a></p><p><sup>9</sup> <a href="https://web.archive.org/web/20080929085351/http://www.ted.com/"><strong>TED</strong></a></p><p><sup>10</sup> <a href="https://web.archive.org/web/20080929085351/http://network.nature.com/people/andrewsun/blog/2007/10/01/google-docs-now-with-presentation"><strong>Google Docs – Now with Presentation</strong></a></p><p><sup>11</sup> <a href="https://web.archive.org/web/20080929085351/http://www.dimdim.com/"><strong>Dimdim</strong></a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How to lure (German) researchers back to Germany]]></title>
            <link>https://blog.martinfenner.org/posts/how-to-lure-german-researchers-back-to-germany</link>
            <guid>8c66b4ed-f989-4481-8c26-8a6cd508191e</guid>
            <pubDate>Fri, 19 Sep 2008 19:21:00 GMT</pubDate>
            <description><![CDATA[The german academic international network (GAIN)1
[https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn1] 
informs German researchers working in North America about research opportunities
in Germany. The implied intention is to lure German researchers back to Germany.
GAIN project director Katja Simons explains:

Many german researchers abroad are highly interested in returning but they need
support ]]></description>
            <content:encoded><![CDATA[<p>The <strong>german academic international network (GAIN)</strong><sup><a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn1"><strong>1</strong></a></sup> informs German researchers working in North America about research opportunities in Germany. The implied intention is to lure German researchers back to Germany. GAIN project director Katja Simons explains:</p><p><em>Many german researchers abroad are highly interested in returning but they need support creating networks and receiving information on career opportunities in Germany. Germany has invested a great deal in their education and is in need of these bright minds and their experience they gained abroad.</em></p><p>GAIN is a joint initiative by the <strong>Alexander von Humboldt Foundation (AvH)</strong>, the <strong>German Academic Exchange Service (DAAD)</strong> and the <strong>German Research Foundation (DFG)</strong>. Their 8th annual meeting took place two weeks ago in Boston<sup><a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn2"><strong>2</strong></a></sup>. More than 200 researchers working in North America participated, together with representatives from many German research organizations, including Matthias Kleiner, president of the German Research Foundation (DFG)<sup><a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn3"><strong>3</strong></a></sup> and Margeret Wintermantel, president of the <strong>German Recotors’ Conference</strong> (HRK, the association of all higher education institutions in Germany)<sup><a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn4"><strong>4</strong></a></sup>. Representatives from business and politics (members of the parliament) were also present.</p><p>To get a more personal perspective, I talked to two researchers that attented the meeting. <a href="https://web.archive.org/web/20080921133726/http://network.nature.com/profile/avmaier"><strong>Alexander Maier</strong></a>, a research fellow at the <a href="https://web.archive.org/web/20080921133726/http://network.nature.com/affiliations/735"><strong>NIH</strong></a>, thinks that the GAIN meeting was a success. Sceptical at the beginning of the conference, he acknowledges that things have changed and that doing reseach in Germany has become much more attractive. The <strong>excellence initiative</strong> by the German Government is one big reason for that change<sup><a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn5"><strong>5</strong></a></sup>. The most rewarding part of the program for him was the workshop on how to apply for professorhips.</p><p>Florian Jaeger, an assistant professor from the University of Rochester (his blog is here<sup><a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn6"><strong>6</strong></a></sup>) is on the GAIN advisory board. He also got the impression that the German reseach system is changing, and that</p><p><em>Many institutions in Germany seem to be inspired to learn from the positive aspects of the American system (and maybe even to improve on it).</em></p><p>But both Alexander and Florian felt that the research environment in Germany is still far from perfect, and the German research organizations should not think that all problems have been solved. Startup grants are often relatively low and junior research groups are usually not as independent as in the United States. And Florian thinks that the research atmosphere – the way people interact and approach problems – is still more stimulating in the United States.</p><p>Most reports about the GAIN meeting are in the German media, including the newspapers Hamburger Abendblatt<sup><a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn7"><strong>7</strong></a></sup> and Süddeutsche Zeitung<sup><a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn8"><strong>8</strong></a></sup>. I found one blog post from a German postdoc attending the meeting<sup><a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn9"><strong>9</strong></a></sup>. Nature Network is a good place to have a discussion not only about the research environment in different countries, but also to learn more about similar strategies carried out by other countries, e.g. France, Italy, Japan or China. Feel free to leave your comments about why you left your home country to do research somewhere else<sup><a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn10"><strong>10</strong></a></sup>, or why you returned after finishing your PhD or postdoc<sup><a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/mfenner/blog/2008/09/19/how-to-lure-german-researchers-back-to-germany#fn11"><strong>11</strong></a></sup>.</p><p><sup>1</sup> <a href="https://web.archive.org/web/20080921133726/http://www.gain-network.org/"><strong>GAIN Homepage</strong></a></p><p><sup>2</sup> <a href="https://web.archive.org/web/20080921133726/http://www.eurekalert.org/pub_releases/2008-09/df-wc091008.php"><strong>DFG news report of the meeting</strong></a></p><p><sup>3</sup> <a href="https://web.archive.org/web/20080921133726/http://www.dfg.de/en/index.html"><strong>DFG Homepage</strong></a></p><p><sup>4</sup> <a href="https://web.archive.org/web/20080921133726/http://www.hrk.de/index_eng.php"><strong>HRK Homepage</strong></a></p><p><sup>5</sup> <a href="https://web.archive.org/web/20080921133726/http://www.dfg.de/en/research_funding/coordinated_programmes/excellence_initiative/general_information.html"><strong>Excellence initiative</strong></a></p><p><sup>6</sup> <a href="https://web.archive.org/web/20080921133726/http://hlplab.wordpress.com/"><strong>HLP/Jaeger lab blog</strong></a></p><p><sup>7</sup> <a href="https://web.archive.org/web/20080921133726/http://www.abendblatt.de/daten/2008/09/15/937232.html"><strong>Lockrufe der deutschen Forschung</strong></a></p><p><sup>8</sup> <a href="https://web.archive.org/web/20080921133726/http://jetzt.sueddeutsche.de/texte/anzeigen/446385"><strong>In Boston werben Politiker für deutsche Unis</strong></a></p><p><sup>9</sup> <a href="https://web.archive.org/web/20080921133726/http://sonjatoots.blogspot.com/2008/09/one-with-boston.html"><strong>Sonja in the City</strong></a></p><p><sup>10</sup> <a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/etchevers/blog"><strong>A Developing Passion</strong></a></p><p><sup>11</sup> <a href="https://web.archive.org/web/20080921133726/http://network.nature.com/people/massimopinto/blog"><strong>Science in the Bel Paese</strong></a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[It's time for Conference 2.0]]></title>
            <link>https://blog.martinfenner.org/posts/its-time-for-conference-2-0</link>
            <guid>50b818e8-281d-46f7-9924-1583e94d8b28</guid>
            <pubDate>Fri, 12 Sep 2008 16:16:00 GMT</pubDate>
            <description><![CDATA[Conference 2.0 – A scheduled meeting of people sharing a common interest that
takes advantage of Web 2.01
[https://web.archive.org/web/20080929012015/http://network.nature.com/people/mfenner/blog/2008/09/12/its-time-for-conference-2-0#fn1] 
concepts.

Scientific conferences are essential both for the exchange of ideas and for
networking. But they don’t have to be organized the same way as 10-20 years ago.
Web 2.0 tools now allow much broader user participation before, during and after
the confer]]></description>
            <content:encoded><![CDATA[<p><em><strong>Conference 2.0</strong> – A scheduled meeting of people sharing a common interest that takes advantage of Web 2.0<sup><a href="https://web.archive.org/web/20080929012015/http://network.nature.com/people/mfenner/blog/2008/09/12/its-time-for-conference-2-0#fn1"><strong>1</strong></a></sup> concepts.</em></p><p>Scientific conferences are essential both for the exchange of ideas and for networking. But they don’t have to be organized the same way as 10-20 years ago. Web 2.0 tools now allow much broader user participation before, during and after the conference. Technology conferences have seen this change already<sup><a href="https://web.archive.org/web/20080929012015/http://network.nature.com/people/mfenner/blog/2008/09/12/its-time-for-conference-2-0#fn2"><strong>2</strong></a></sup>. We also already have open source software to organize conferences<sup><a href="https://web.archive.org/web/20080929012015/http://network.nature.com/people/mfenner/blog/2008/09/12/its-time-for-conference-2-0#fn3"><strong>3</strong></a></sup>. I’ve collected a few of those ideas and concepts below.</p><p><strong>1. Keep the conference small</strong><br>Active user participation works better in smaller conferences, e.g. not more than maybe 150 participants (derived from Dunbar’s number<sup><a href="https://web.archive.org/web/20080929012015/http://network.nature.com/people/mfenner/blog/2008/09/12/its-time-for-conference-2-0#fn4"><strong>4</strong></a></sup> introduced by <a href="https://web.archive.org/web/20080929012015/http://network.nature.com/profile/duncan"><strong>Duncan Hull</strong></a>). This will exclude large or very large conferences – the largest scientific conferences today have more than 10.000 participants. But those larger conferences can still adopt some of the principles discussed below.</p><p><strong>2. Allow for user input to the conference program</strong><br>Even though most scientific conferences ask for abstract submissions well before the conference, the conference schedule is ultimately decided my a small program committee. But conference organizers could well ask for user input about session topics. In the BarCamp or unconference format, the conference program is even decided on the first day of the conference<sup><a href="https://web.archive.org/web/20080929012015/http://network.nature.com/people/mfenner/blog/2008/09/12/its-time-for-conference-2-0#fn5"><strong>5</strong></a></sup>.</p><p><strong>3. Provide free WiFi</strong><br>This is essential for liveblogging about the conference. And free WiFi in combination with a good conference website with detailed schedule, message boards and practical information would greatly reduce the amount of printed material that needs to be handed out at the conference.</p><p><strong>4. Set aside time for networking</strong><br>There should be enough time (and space) between sessions to talk to the other conference participants. After all, this is one main reason for many people to attend a conference. And the conference organizers can facilitate networking in other ways. Poster sessions (see below) are one way, a very short introduction by every participant (either in person or on the conference website) is another idea.</p><p><strong>5. Pay attention to poster sessions and discussions</strong><br>Conferences can have other session formats than oral presentations. Poster sessions are an often neglected part of many conferences. But they are a great tool for networking, especially with younger scientists. The conference organizers should set aside enough time and avoid parallel oral sessions. Providing drinks and food also helps. Round-table discussions are another underused format with a lot of potential.</p><p><strong>6. Encourage blogging</strong><br>There should be a clear policy regarding blogging stated at the conference website. And this policy should make it easy for conference participants to blog. This means no preregistration and no required affiliation with a news service or journal. Conference organizers should provide a tag for the conference so that blog entries can be tracked.</p><p><em>Blogging about the conference is encouraged by the conference organizers. Please use the tag *conference</em>name* for all your blog posts. Please don’t blog about sessions marked <strong>non_public</strong> in the conference program. They contain information that should not become become public at this time, e.g. because they discuss unpublished results. For further questions regarding blogging at the conference, please contact …_</p><p>There are many different ways to blog about a conference, microblogging via FriendFeed is currently a very popular option<sup><a href="https://web.archive.org/web/20080929012015/http://network.nature.com/people/mfenner/blog/2008/09/12/its-time-for-conference-2-0#fn6"><strong>6</strong></a></sup>.</p><p><strong>7. Produce podcasts</strong><br>Podcasts with audio and video of the slides are a great way to capture oral sessions at a conference. They are espcially valuable for those unable to attend. This week’s <strong>Science in the 21st Century</strong> conference is a good example of how this can be done<sup><a href="https://web.archive.org/web/20080929012015/http://network.nature.com/people/mfenner/blog/2008/09/12/its-time-for-conference-2-0#fn7"><strong>7</strong></a></sup>.</p><p><strong>8. Organize parallel local conferences</strong><br>The costs and annoyances of traveling, combined with concerns about the carbon footprint<sup><a href="https://web.archive.org/web/20080929012015/http://network.nature.com/people/mfenner/blog/2008/09/12/its-time-for-conference-2-0#fn8"><strong>8</strong></a></sup> have led to new concepts. Instead of following the conference from the distance via live-streaming or live-blogging, why not organize several parallel local conferences<sup><a href="https://web.archive.org/web/20080929012015/http://network.nature.com/people/mfenner/blog/2008/09/12/its-time-for-conference-2-0#fn9"><strong>9</strong></a></sup>? The Singularity web conference next month is using this concept<sup><a href="https://web.archive.org/web/20080929012015/http://network.nature.com/people/mfenner/blog/2008/09/12/its-time-for-conference-2-0#fn10"><strong>10</strong></a></sup>. Will the next science blogging conference happen in parallel in several locations?</p><p><sup>1</sup> <a href="https://web.archive.org/web/20080929012015/http://oreilly.com/pub/a/oreilly/tim/news/2005/09/30/what-is-web-20.html"><strong>What is Web 2.0</strong></a></p><p><sup>2</sup> <a href="https://web.archive.org/web/20080929012015/http://money.cnn.com/2008/03/11/technology/fost_conference.fortune/"><strong>Welcome to Conference 2.0</strong></a></p><p><sup>3</sup> <a href="https://web.archive.org/web/20080929012015/http://pkp.sfu.ca/?q=ocs"><strong>Open Conference Systems</strong></a></p><p><sup>4</sup> <a href="https://web.archive.org/web/20080929012015/http://en.wikipedia.org/wiki/Dunbar%27s_number"><strong>Dunbar’s number</strong></a></p><p><sup>5</sup> <a href="https://web.archive.org/web/20080929012015/http://network.nature.com/blogs/user/UE19877E8/2008/08/11/in-which-i-am-utterly-fooed"><strong>In which I am utterly Fooed</strong></a></p><p><sup>6</sup> <a href="https://web.archive.org/web/20080929012015/http://scienceblogs.com/principles/2008/09/microblogging_conference_talks.php"><strong>Micro-Blogging Conference Talks</strong></a></p><p><sup>7</sup> <a href="https://web.archive.org/web/20080929012015/http://pirsa.org/C08021"><strong>Perimeter Institute Recorded Seminar Archive</strong></a></p><p><sup>8</sup> <a href="https://web.archive.org/web/20080929012015/http://www.carbonfootprint.com/"><strong>Carbon Footprint</strong></a></p><p><sup>9</sup> <a href="https://web.archive.org/web/20080929012015/http://www.insideria.com/2008/06/building-conference-20.html"><strong>Building Conference 2.0</strong></a></p><p><sup>10</sup> <a href="https://web.archive.org/web/20080929012015/http://www.headconference.com/"><strong>Head – the global web conference</strong></a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Interview with Victor Henning from Mendeley]]></title>
            <link>https://blog.martinfenner.org/posts/interview-with-victor-henning-from-mendeley</link>
            <guid>5d76d882-6ebb-4021-b4b4-3175696f0e6e</guid>
            <pubDate>Fri, 05 Sep 2008 06:18:00 GMT</pubDate>
            <description><![CDATA[In the last few months we have seen an ever increasing number of new social
networking (Web 2.0) sites for scientists. Good Web 2.0 tools for scientists
primarily try to solve a problem. But by adding a social aspect, they will gain
useful features that would otherwise not be possible. Eva Amsen
[https://web.archive.org/web/20080920001317/http://network.nature.com/profile/U27CE62BB] 
has recently written a great blog post about this1
[https://web.archive.org/web/20080920001317/http://network.nat]]></description>
            <content:encoded><![CDATA[<figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20080920001317im_/http://farm4.static.flickr.com/3120/2827515376_ac2f0d81a1_o.jpg" class="kg-image" alt></figure><p>In the last few months we have seen an ever increasing number of new social networking (Web 2.0) sites for scientists. Good Web 2.0 tools for scientists primarily try to solve a problem. But by adding a social aspect, they will gain useful features that would otherwise not be possible. <a href="https://web.archive.org/web/20080920001317/http://network.nature.com/profile/U27CE62BB"><strong>Eva Amsen</strong></a> has recently written a great blog post about this<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn1"><strong>1</strong></a></sup>. Many of these new services have overlapping functions, e.g. almost all of them allow the user to maintain a list of contacts. This raises two questions:</p><ol><li>Which of these sites has (or have) the features that I’m most interested in?</li><li>Do any of these sites work with the commonly used desktop tools and with each other, so that I don’t have to maintain duplicate information, e.g. the list of my publications?</li></ol><p><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/profile/U42E63119"><strong>Cameron Neylon</strong></a> in August posted an open letter to the developers of these sites<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn2"><strong>2</strong></a></sup> and also started a comprehensive list<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn3"><strong>3</strong></a></sup>. <a href="https://web.archive.org/web/20080920001317/http://network.nature.com/profile/http-network-nature-comprofilegerrymck"><strong>Gerry McKiernan</strong></a><strong> </strong>collects information about social networking sites for scientists on his SciTechNet blog<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn4"><strong>4</strong></a></sup>. Mendeley<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn5"><strong>5</strong></a></sup> is one of these new Web 2.0 sites for scientists (they launched in August), and I spoke with <a href="https://web.archive.org/web/20080920001317/http://network.nature.com/profile/U4C4A58A2[6"><strong>Victor Henning</strong></a>, one of the founders of Mendeley, about it.</p><p><strong>1. Can you describe what Mendeley is and does?</strong><br>Mendeley is actually two things: Mendeley Desktop and Mendeley Web. Mendeley Desktop is free academic software (available for Windows, Mac and Linux) for managing &amp; sharing research papers. Mendeley Web lets you back up your research papers online, shows you research trends in your academic discipline, and connects you to like-minded researchers.</p><p><strong>2. What is the connection to Last.fm?</strong><br>There is a conceptual as well as a personal connection. Conceptually, in the long run Mendeley aims to do for research what Last.fm did for music. For those of your readers who don’t know Last.fm, this is how it works: When you install Last.fm’s desktop software on your computer, it will monitor which music you listen to and automatically build a profile of your musical taste on the Last.fm website. The website then recommends you music that you might like, shows you statistics about the most popular songs and artists in your favourite genre, and lets you discover people with a similar taste in music. By aggregating the listening habits and tags of its 20 million users, Last.fm has managed to create the largest ontological classification (and the largest open database) of music in the world – it would be great if we could achieve the same for research papers.</p><p>So, if you install Mendeley Desktop on your computer, you can manage and share research papers on your machine, but you can also upload your papers to your private account on Mendeley Web to access them online. Mendeley Web anonymously aggregates the metadata of these papers to generate statistics about the most popular authors and papers in your research discipline, and – in the future – generates recommendations for papers which you might like. One thing that we handle very differently from Last.fm is privacy: The Last.fm website lets everyone know which music you listen to, whereas Mendeley Web keeps all your research data in your private account which can’t be accessed by anyone else. The conceptual parallels between Last.fm and Mendeley are outlined in more detail in a talk I gave both at the EuroScience Open Forum 2008 in Barcelona and at the Southampton Open Science Workshop<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn7"><strong>7</strong></a></sup>.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20080920001317im_/http://farm4.static.flickr.com/3129/2822784112_f01ed9673b_o.jpg" class="kg-image" alt></figure><p>Besides the conceptual similarities, the personal connection to Last.fm is Dr. Stefan Glänzer. He was Last.fm’s first investor and executive chairman, and now has the same role at Mendeley. My co-founder Jan and I first met him back in 2003, when he was a guest lecturer in Entrepreneurship at our university, the WHU Vallendar<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn8"><strong>8</strong></a></sup>, and we contributed a case study to the book he published together with our professors. So when we were looking for funding, we approached him again in June 2007, he was fascinated by the idea of Mendeley and luckily decided to join us. He also brought us in touch with the former founding engineers of Skype, who became investors as well.</p><p><strong>3. What are your responsibilities within Mendeley?</strong><br>I do most of the conceptual work behind Mendeley and write the specifications for our developers: What is the vision of Mendeley Desktop and Mendeley Web in the long term, which features should we develop next to get there, how does each feature work in detail, right down to questions like “where do we place this button and how do we label it?”. So you can blame me for all the usability problems you might encounter.</p><p>I’m also responsible for staying in touch with the academic community and incorporating its wishes into the Mendeley development roadmap, which has the enjoyable side effect that I get to travel to all these wonderful academic conferences. Next week, I’ll be at the Science in the 21st Century conference at the Perimeter Institute for Theoretical Physics, Waterloo/Ontario<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn9"><strong>9</strong></a></sup>.</p><p><strong>4. What did you do before starting to work for Mendeley?</strong><br>Until a little more than a year ago, I thought I’d pursue an academic career – I’m currently finishing my Ph.D. on decision-making and choice at the Bauhaus-University of Weimar. I’ve been saying “currently finishing” for almost a year now, but I’m hopeful that I’ll manage to submit my thesis by the end of this year :-) Prior to that, I’ve worked in film production and the music industry a lot, and I opened a café/bar<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn10"><strong>10</strong></a></sup> opposite the WHU in parallel to writing my master’s thesis. I actually left Vallendar the day after the opening night to pursue my Ph.D. in Weimar, so I had all of the work and little of the fun of owning a café/bar!</p><p><strong>5. What is your policy regarding users sharing their PDF files of publications with others?</strong><br>I think it’s important to point out that we’re not a “Napster for research papers” – i.e. no free-for-all peer-to-peer filesharing. Quite a lot of people are disappointed when I tell them that! Sharing is currently limited to “Shared Document Groups” of max. 10 people (e.g. a lab, or collaborators on a research paper), although you can create and join as many Shared Document Groups as you like. Also, as we state in our terms of use<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn11"><strong>11</strong></a></sup>, you may only share PDFs with the permission of the copyright owner – e.g. your own articles or working papers, articles from Open Access databases, articles under a Creative Commons/Scientific Commons license, or perhaps when you and the person you are sharing the PDF with both have licensed access to the database the PDF was taken from.</p><p>We also encourage users to post their own papers and working papers on their Mendeley profiles – depending on whether they have permission to do so from their publishers. As you can imagine, we’re big fans of Open Access!</p><p><strong>6. How is Mendeley different from other desktop reference managers such as Endnote?</strong><br>There are a number of things that make Mendeley Desktop unique, but I’d probably highlight the collaboration aspect, the “automatic metadata extraction”, the online back-up/multi-machine support, and the cross-platform support:</p><ul><li>As far as I know, Mendeley Desktop is the only desktop reference management software that lets you share and collaboratively annotate research papers. We’re also working on a “groups” feature in Mendeley Web which labs can use for discussions, sharing files, setting up a lab blog/wiki etc. – all of this will tie into the reference management seamlessly.</li><li>The “automatic metadata extraction” is quite unique as well: When you drop your PDFs into Mendeley Desktop, it will automatically extract the full-text to make it searchable, try to guess the correct metadata from the full-text (author, title, journal, volume, issue etc.) so you don’t have to type it in manually, and also parse each documents’ cited references, so you can add them to your library as well.</li><li>Due to the integration with Mendeley Web, you can back-up your entire library for online access through simple drag &amp; drop in Mendeley Desktop. Also, this means that you can install Mendeley Desktop on multiple computers and easily synchronize your PDF library across them via the Mendeley Online Library.</li><li>Last but not least, Mendeley Desktop is the only desktop reference manager that is available on all three major platforms (Windows, Mac, and Linux).</li></ul><p>Not to mention that, in comparison to solutions such as EndNote, RefMan, RefWorks etc., which cost hundreds of Euros per license, Mendeley Desktop is completely free.</p><p><strong>7. How is Mendeley different from other social networking sites for scientists?</strong><br>While social networking is an aspect of Mendeley Web, we don’t see ourselves primarily as a social network. We don’t believe that social networking in itself is the killer feature that researchers are looking for – rather we’re using a social network to enable researchers to share their data. I think that <a href="https://web.archive.org/web/20080920001317/http://network.nature.com/profile/neilfws"><strong>Neil Saunders</strong></a>, who is also here on Nature Network, said it best: “Really, it’s our data that needs to be social, not ourselves”. So we’ve tried to develop a research tool which is useful without any network effects, and which uses networking as a means rather than an end.</p><p>To invoke the analogy to Last.fm again: Even though people have profiles on Last.fm and can connect to each other, Last.fm is not primarily a social network. Last.fm connects the music first, and networks of people form around the music as a second step.</p><p><strong>8. Does Mendeley integrate with other social networking sites and services for scientists, e.g. Connotea or CiteULike? Does it integrate with desktop reference managers?</strong><br>At the moment we’re focusing on increasing the speed and stability of Mendeley, as well as introducing more features which make Mendeley useful as a standalone software. However, compatibility with Connotea or CiteULike is something we’ll be working on in the near future.</p><p>Regarding integration with other desktop reference managers: At the Science Blogging Conference<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn11"><strong>11</strong></a></sup> I briefly spoke to <a href="https://web.archive.org/web/20080920001317/http://network.nature.com/profile/mekentosj"><strong>Alexander Griekspoor</strong></a>, the developer of the Mac software Papers<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn12"><strong>12</strong></a></sup>, whether we couldn’t make our software compatible so that Papers users and Mendeley Desktop users could share and synchronize their collections online. As for EndNote: It’s already possible to import/export data to/from Mendeley in the EndNote XML format – but, sadly, I don’t think that EndNote’s developers would be inclined to enable online sharing between EndNote and Mendeley.</p><p><strong>9. Do you want to talk about future plans for Mendeley?</strong><br>Sure! Besides speed and stability, which I already mentioned, we’ll be working on two main issues to better integrate Mendeley into the research workflow. First, there will be a “cite-while-you-write” plugin for Microsoft Word (or Open Office, if our users would prefer that), so that you can generate reference lists from your Mendeley library automatically. Similarly, we’ll improve the integration with LaTeX by automating the BibTeX file export from Mendeley Desktop. Second, we’ll introduce a “bookmarklet” like the ones CiteULike or Connotea have, which lets you import metadata/papers from websites into your Mendeley Online Library with a single click. This metadata will then be automatically synchronized with the Mendeley Desktop library on your computer.</p><p>The Microsoft Word/LaTeX integration and the online bookmarklet will be available very soon. Over the coming months, we’ll also introduce OCR to Mendeley Desktop, so that you can extract metadata, full-text and references from older scanned-image documents; we’ll integrate Mendeley Desktop with external databases such as PubMed, Scopus, and Web of Science; we’ll implement the groups/lab management feature on Mendeley Web that I already mentioned; there will be much more detailed research trend statistics on Mendeley Web; we’ll introduce the recommendation engine for academic papers, and many more little goodies. For example, a frequent user request has been automatic PDF file renaming based on the metadata – so that’s going to be in one of the next versions.</p><p><strong>10. If a user is interested to learn more about Mendeley or give feedback, who should he contact?</strong><br>You can always contact me directly at <a href="https://web.archive.org/web/20080920001317/mailto:victor.henning@mendeley.com"><strong>victor.henning@mendeley.com</strong></a>. There is also a Mendeley team blog on which there is plenty of behind-the-scenes information<sup><a href="https://web.archive.org/web/20080920001317/http://network.nature.com/people/mfenner/blog/2008/09/05/interview-with-victor-henning-from-mendeley#fn13"><strong>13</strong></a></sup>. You can also submit feature requests or bug reports on our homepage (the buttons on the top left).</p><p>Victor, thank you very much for giving me this interview.</p><figure class="kg-card kg-image-card"><img src="https://web.archive.org/web/20080920001317im_/http://farm4.static.flickr.com/3212/2822759480_c330822084_o.jpg" class="kg-image" alt></figure><p><em>The Mendeley founders Stefan Glänzer, Victor Henning, Paul Föckler and Jan Reichelt</em></p><p><sup>1</sup> <a href="https://web.archive.org/web/20080920001317/http://network.nature.com/blogs/user/U27CE62BB/2008/08/19/how-to-get-scientists-to-adopt-web-2-0-technologies"><strong>How to get scientists to adopt web 2.0 technologies</strong></a></p><p><sup>2</sup> <a href="https://web.archive.org/web/20080920001317/http://blog.openwetware.org/scienceintheopen/2008/08/06/an-open-letter-to-the-developers-of-social-network-and-%E2%80%98web-20%E2%80%99-tools-for-scientists"><strong>An open letter to the developers of social network and web 2.0 tools for scientists</strong></a></p><p><sup>3</sup> <a href="https://web.archive.org/web/20080920001317/http://docs.google.com/View?docid=dhs5x5kr_572hccgvcct"><strong>A critical analysis Google Docs</strong></a></p><p><sup>4</sup> <a href="https://web.archive.org/web/20080920001317/http://scitechnet.blogspot.com/"><strong>SciTechNet</strong></a></p><p><sup>5</sup> <a href="https://web.archive.org/web/20080920001317/http://www.mendeley.com/"><strong>Mendeley</strong></a></p><p><sup>6</sup> <a href="https://web.archive.org/web/20080920001317/http://www.mendeley.com/profiles/victor-henning"><strong>Victor Henning’s profile on Mendeley</strong></a></p><p><sup>7</sup> <a href="https://web.archive.org/web/20080920001317/http://www.youtube.com/watch?v=UzJbrA9EY7A"><strong>A Last.fm for Research</strong></a></p><p><sup>8</sup> <a href="https://web.archive.org/web/20080920001317/http://www.whu.edu/cms/index.php?id=1959&amp;amp;L=1&amp;amp;1354"><strong>WHU Otto Beisheim School of Management</strong></a></p><p><sup>9</sup> <a href="https://web.archive.org/web/20080920001317/http://www.science21stcentury.org/"><strong>Science in the 21st Century</strong></a></p><p><sup>10</sup> <a href="https://web.archive.org/web/20080920001317/http://www.korova-bar.de/korova/"><strong>Korova Bar</strong></a></p><p><sup>11</sup> <a href="https://web.archive.org/web/20080920001317/http://www.mendeley.com/terms/"><strong>Mendeley terms of use</strong></a></p><p><sup>12</sup> <a href="https://web.archive.org/web/20080920001317/http://www.nature.com/natureconferences/sciblog2008/index.html"><strong>Science Blogging 2008: London</strong></a></p><p><sup>13</sup> <a href="https://web.archive.org/web/20080920001317/http://mekentosj.com/papers/"><strong>Papers</strong></a></p><p><sup>14</sup> <a href="https://web.archive.org/web/20080920001317/http://www.mendeley.com/blog"><strong>Mendeley blog</strong></a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Looking back on a year of gobbledygook]]></title>
            <link>https://blog.martinfenner.org/posts/looking-back-on-a-year-of-gobbledygook</link>
            <guid>83abbe61-4297-4d75-9d57-2ef4afe5c076</guid>
            <pubDate>Sun, 03 Aug 2008 00:00:00 GMT</pubDate>
            <description><![CDATA[A year ago today I wrote my first blog post on Nature Network (Open access may
become mandatory for NIH-funded research
[https://web.archive.org/web/20080929033935/http://network.nature.com/blogs/user/mfenner/2007/08/03/open-access-may-become-mandatory-for-nih-funded-research]
). This is blog post #84 one year later and a good time to reflect on the
experience. In May of last year I started the science blog in a nutshell
[https://web.archive.org/web/20080929033935/http://blog.xartrials.com/], ho]]></description>
            <content:encoded><![CDATA[<p>A year ago today I wrote my first blog post on Nature Network (<a href="https://web.archive.org/web/20080929033935/http://network.nature.com/blogs/user/mfenner/2007/08/03/open-access-may-become-mandatory-for-nih-funded-research"><strong>Open access may become mandatory for NIH-funded research</strong></a>). This is blog post #84 one year later and a good time to reflect on the experience. In May of last year I started the science blog <a href="https://web.archive.org/web/20080929033935/http://blog.xartrials.com/"><strong>in a nutshell</strong></a>, hosted on my own server and written just for fun. I discovered Nature Network in July and started <strong>Publish or Perish 2.0</strong>. In November 2007 I <a href="https://web.archive.org/web/20080929033935/http://network.nature.com/blogs/user/mfenner/2007/11/11/a-case-for-goobledygook"><strong>changed the blog name</strong></a> to <strong>Gobbledygook</strong>.</p><p>I try to write about the paper writing process from the perspective of a researcher. I’m interested in the technical changes in paper writing thanks to Web 2.0. Open access is another important topic and the perspective of a researcher is obviously very different from a journal publisher, science library or the interested public. I am sometimes not comfortable to write about open access, as this is a very political topic and the discussion can move away from arguments and into something about doing the right thing. That’s why I would never write about Evolution vs. Intelligent Design or some of the other hotly debated topics in science blogging.</p><p>The blog post that received the most comments is <a href="https://web.archive.org/web/20080929033935/http://network.nature.com/blogs/user/mfenner/2008/06/14/my-paper-writing-dream-machine-1-0"><strong>My Paper Writing Dream Machine 1.0</strong></a>. That was also one of my favorite blog posts as I would love to see more of the potential of Web 2.0 technologies in our paper writing tools. I also enjoyed the dicussion on posters at scientific meetings (<a href="https://web.archive.org/web/20080929033935/http://network.nature.com/blogs/user/mfenner/2008/03/01/are-posters-worth-the-effort"><strong>Are posters worth the effort?</strong></a>) and on blogging from conferences (<a href="https://web.archive.org/web/20080929033935/http://network.nature.com/blogs/user/mfenner/2008/05/17/scientific-meetings-need-more-bloggers"><strong>Scientific meetings need more bloggers</strong></a>).</p><p>I participated in a wonderful SynchroBlogging effort on April Fools Day (organized by <a href="https://web.archive.org/web/20080929033935/http://phylogenomics.blogspot.com/"><strong>Jonathan Eisen</strong></a> and with “help” from the <a href="https://web.archive.org/web/20080929033935/http://homepage.mac.com/jonathan_eisen/Wabda/Wabda.html"><strong>World Anti-Brain Doping Authority</strong></a>) with <a href="https://web.archive.org/web/20080929033935/http://network.nature.com/blogs/user/mfenner/2008/04/01/what-can-erythopoetin-do-for-you"><strong>What can Erythopoetin do for you?</strong></a>. I think we should do more SynchroBlogging, and not just on April 1st. <strong>Public Access Week</strong> was another SynchroBlogging effort and I learned a lot about access to my own papers in <a href="https://web.archive.org/web/20080929033935/http://network.nature.com/blogs/user/mfenner/2008/04/11/public-access-week-who-could-read-my-papers"><strong>Public Access Week: Who could read my papers?</strong></a>.</p><p>Only two blog posts are about scientific research. <a href="https://web.archive.org/web/20080929033935/http://network.nature.com/blogs/user/mfenner/2008/02/07/using-rna-interference-to-identify-genes-that-protect-from-cancer"><strong>Using RNAinterference to identify genes that protect from cancer</strong></a> was my contribution to <a href="https://web.archive.org/web/20080929033935/http://www.justscience.net/2008/?page_id=1368"><strong>Just Science 2008</strong></a>. In <a href="https://web.archive.org/web/20080929033935/http://network.nature.com/blogs/user/mfenner/2008/07/14/mouse-models-of-human-cancer-and-the-need-for-more-translational-research"><strong>Mouse models of human cancer and the need for more translational research</strong></a> I wrote about a presentation by Mario Capecchi at the International Genetics Conference. I would love to do more <a href="https://web.archive.org/web/20080929033935/http://www.researchblogging.org/index.php"><strong>ResearchBlogging</strong></a>, but I think that we have to wait a few more years before science blogging has attracted enough people that read and comment on specific research findings.</p><p>Thanks to this blog I have met a number of very interesting and intelligent people with similar interests (see <a href="https://web.archive.org/web/20080929033935/http://scienceblogs.com/clock/2008/05/eurotrip_08_berlin_part_iii_we.php"><strong>this blog entry</strong></a> by Bora Zivkovic and <a href="https://web.archive.org/web/20080929033935/http://network.nature.com/london/news/blog/matt/2008/07/12/dinner-with-the-nobel-prize-winners"><strong>this blog entry</strong></a> by Matt Brown). That’s why I’m very much looking forward to the <a href="https://web.archive.org/web/20080929033935/http://www.nature.com/natureconferences/sciblog2008/index.html"><strong>Science Blogging 2008: London</strong></a> conference at the end of this month. My goal for the next year: help to make reading and writing science blogs part of the everyday life at more universities and reseach institutions.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My Paper Writing Dream Machine 1.0]]></title>
            <link>https://blog.martinfenner.org/posts/my-paper-writing-dream-machine-1-0</link>
            <guid>7c832e7d-2d5b-4452-89ff-7a9c3b716550</guid>
            <pubDate>Sat, 14 Jun 2008 14:41:00 GMT</pubDate>
            <description><![CDATA[I’ve written a similar post before
[https://web.archive.org/web/20081014105337/http://network.nature.com/blogs/user/mfenner/2008/03/31/pubmed-and-other-annoyances-in-the-paper-writing-process]
, put I would like to talk about some of the features that I would like to see
in an ideal paper writing application.

Intelligent Formatting
Content and formatting should be separated from each other. A manuscript should
require as little formatting as possible, and that formatting (including the
format o]]></description>
            <content:encoded><![CDATA[<p>I’ve written a similar post <a href="https://web.archive.org/web/20081014105337/http://network.nature.com/blogs/user/mfenner/2008/03/31/pubmed-and-other-annoyances-in-the-paper-writing-process"><strong>before</strong></a>, put I would like to talk about some of the features that I would like to see in an ideal paper writing application.</p><p><strong>Intelligent Formatting</strong><br>Content and formatting should be separated from each other. A manuscript should require as little formatting as possible, and that formatting (including the format of references) should be defined in a Journal style that is automatically applied to the manuscript. <a href="https://web.archive.org/web/20081014105337/http://www.wolfram.com/products/publicon/index.html"><strong>Publicon</strong></a> by Wolfram Research tried to achieve this, but unfortunately appears to be a dead product.</p><p><strong>References</strong><br>A reference database should be integrated into the paper writing application. Ideally this would be a web-based database such as <a href="https://web.archive.org/web/20081014105337/http://www.connotea.org/"><strong>Connotea</strong></a>, <a href="https://web.archive.org/web/20081014105337/http://www.citeulike.org/"><strong>CiteULike</strong></a>. Both <a href="https://web.archive.org/web/20081014105337/http://www.refworks.com/"><strong>Refworks</strong></a> and <a href="https://web.archive.org/web/20081014105337/http://www.endnoteweb.com/enwebinfo.asp"><strong>EndNote Web</strong></a> already offer some level of integration.</p><p><strong>Versioning</strong><br>Storing all versions of a manuscript is very important for obvious reasons: backup, keeping track of revisions and coordinating the input from more than one author. Version control is standard practice in software development, using tools like <a href="https://web.archive.org/web/20081014105337/http://subversion.tigris.org/"><strong>Subversion</strong></a> or the newer <a href="https://web.archive.org/web/20081014105337/http://git.or.cz/index.html"><strong>Git</strong></a>.</p><p><strong>Integration with Online Submission Systems</strong><br>Submitting a manuscript to an online manuscript submission system such as <a href="https://web.archive.org/web/20081014105337/http://www.editorialmanager.com/homepage/home.htm"><strong>EditorialManager</strong></a> or <a href="https://web.archive.org/web/20081014105337/http://www.topazproject.org/trac/"><strong>Topaz</strong></a> is too complicated. This process could and should be automated.</p><p><strong>Summary</strong><br>My Paper Writing Dream Machine will in all likelihood turn out to be a web-based application, using on one of the more advanced platforms <a href="https://web.archive.org/web/20081014105337/http://gears.google.com/"><strong>Google Gears</strong></a>, <a href="https://web.archive.org/web/20081014105337/http://silverlight.net/"><strong>Microsoft Silverlight</strong></a> or <a href="https://web.archive.org/web/20081014105337/http://www.adobe.com/de/products/flex/"><strong>Adobe Flex</strong></a>. And the data will be in XML using a standard <a href="https://web.archive.org/web/20081014105337/http://en.wikipedia.org/wiki/Document_Type_Definition"><strong>DTD</strong></a>. Some of the pieces of the puzzle already exist, but nobody has yet put them together in a way that it creates a compelling alternative to the currently used systems.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Open access may become mandatory for NIH-funded research]]></title>
            <link>https://blog.martinfenner.org/posts/open-access-may-become-mandatory-for-nih-funded-research</link>
            <guid>562258f0-2a28-4091-b827-fda27b76642e</guid>
            <pubDate>Fri, 03 Aug 2007 14:17:00 GMT</pubDate>
            <description><![CDATA[The National Institutes of Health (NIH) is currently recommending public access
of all papers from NIH-funded research. Fewer than 5% of research papers
[https://web.archive.org/web/20080517063146/http://publicaccess.nih.gov/Final_Report_20060201.pdf] 
have gone this route since the policy went into effect in 2005. On July 19, 2007
[https://web.archive.org/web/20080517063146/http://www.scientificblogging.com/news/house_of_representatives_backs_faster_public_access_to_nih_studies] 
the House of R]]></description>
            <content:encoded><![CDATA[<p>The National Institutes of Health (NIH) is currently recommending public access of all papers from NIH-funded research. <a href="https://web.archive.org/web/20080517063146/http://publicaccess.nih.gov/Final_Report_20060201.pdf"><strong>Fewer than 5% of research papers</strong></a> have gone this route since the policy went into effect in 2005. On <a href="https://web.archive.org/web/20080517063146/http://www.scientificblogging.com/news/house_of_representatives_backs_faster_public_access_to_nih_studies"><strong>July 19, 2007</strong></a> the House of Representatives passed the FY2008 Labor, HHS, and Education Appropriations Bill that will make public access within 12 months of publication a requirement for all NIH-funded work. The bill still has to be approved by the Senate.</p><p>What is the policy of other granting agencies on open access requirements for funded work?</p><p><strong>Howard Hughes Medical Institute (HHMI)</strong><br>On June 26, 2007 the <a href="https://web.archive.org/web/20080517063146/http://www.hhmi.org/news/20070626.html"><strong>HHMI</strong></a> announced that original research papers by HHMI scientists have to be publicly available within 6 months of publication. The policy goes into effect for all papers submitted on or after January 1, 2008 where a HHMI scientist is first author, last author and/or corresponding author.</p><p><strong>Welcome Trust</strong><br>All publications of research funded by the British <a href="https://web.archive.org/web/20100122231246/http://www.wellcome.ac.uk/doc_WTD002766.html"><strong>Welcome Trust</strong></a> have to be publicly available within 6 months of publication since October 1, 2006.</p><p><strong>Deutsche Forschungsgemeinschaft (DFG)</strong><br>The German DFG has issued recommendations for open access in <a href="https://web.archive.org/web/20100122231246/http://www.dfg.de/aktuelles_presse/information_fuer_die_wissenschaft/andere_verfahren/info_wissenschaft_04_06.html"><strong>January 2006</strong></a>. Although scientists receiving DFG grants are encouraged to have their papers publicly available, there is no requirement to do so.</p><p><strong>CNRS, INSERM, INRA and INRIA</strong><br>The French National Center for Scientific Research (CNRS), the National Institute of Health and Medical Research (INSERM), the National Institute for Agricultural Research (INRA) and the National Research Institute for IT and Robotics (INRIA) have created <a href="https://web.archive.org/web/20100122231246/http://www2.cnrs.fr/en/332.htm"><strong>publicly available institutional repositories</strong></a> for their researchers. Deposition of papers in these repositories is voluntary.</p><p>In summary, there is a clear trend towards making open access a requirement for funding. The taxpayer-funded research agencies – or rather the legislators – have more problems with making this a requirement than the private charities (Welcome Trust and HHMI).</p>]]></content:encoded>
        </item>
    </channel>
</rss>