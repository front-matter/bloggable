<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://blog.front-matter.io/</id>
    <title>Front Matter</title>
    <updated>2021-09-06T08:18:36.550Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <author>
        <name>Martin Fenner</name>
        <email>martin@front-matter.io</email>
    </author>
    <link rel="alternate" href="https://blog.front-matter.io/"/>
    <link rel="self" href="https://blog.front-matter.io/feed.xml"/>
    <subtitle>Front Matter is a science blogging platform.</subtitle>
    <rights>Copyright © 2021 Front Matter. Distributed under the terms of the Creative Commons Attribution 4.0 License.</rights>
    <entry>
        <title type="html"><![CDATA[Editorial by more than 200 health journals: Call for emergency action to limit global temperature increases, restore biodiversity, and protect health]]></title>
        <id>31z2jfe-fx19tr9-t8cbp4p-fbx70</id>
        <link href="https://blog.front-matter.io/mfenner/editorial-by-more-than-200-call-for-emergency-action-to-limit-global-temperature-increases-restore-biodiversity-and-protect-health"/>
        <updated>2021-09-06T07:50:05.000Z</updated>
        <summary type="html"><![CDATA[More than 200 health journals today published an editorial calling for urgent action to keep average global temperature increases below 1.5°C, halt the destruction of nature, and protect health. The editorial can be read for example here (published under a CC-BY Open Access license),...]]></summary>
        <content type="html"><![CDATA[<p>More than 200 health journals today published an editorial calling for urgent action to keep average global temperature increases below 1.5°C, halt the destruction of nature, and protect health. The editorial can be read for example <a href="https://doi.org/10.1136/bmj.n1734">here</a> (published under a CC-BY Open Access license), and the full list of participating journals can be found <a href="https://www.bmj.com/content/full-list-authors-and-signatories-climate-emergency-editorial-september-2021">here</a>.</p>
<blockquote>
The greatest threat to global public health is the continued failure of world leaders to keep the global temperature rise below 1.5°C and to restore nature. Urgent, society-wide changes must be made and will lead to a fairer and healthier world. We, as editors of health journals, call for governments and other leaders to act, marking 2021 as the year that the world finally changes course.
</blockquote>
<p>The COVID Pandemic is currently taking up all our attention. But we should not loose time addressing the environmental crisis triggered by climate change, and having a significant negative impact on global health, both via direct effects of global heating and extreme weather events, and indirectly via reduced food production.</p>
<p>This joint editorial by more than 200 health journals worldwide is yet another clear sign of the urgency of the situation, and the need for action. And we should not forget that health is just one aspect of how the climate crisis is seriously affecting all our lives, in particular those most vulnerable.</p>
<p>The editorial makes a strong point that the time for action is now, and that wealthy nations must do much more, and much faster. It is also an opportunity to think about what we as individuals can do, whether it is about re-evaluating our individual carbon footprint, thinking about what political parties we support (Germany where I live for example has a federal election in three weeks), and whether the organizations we work for can do more to slow down global warming.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A step forward for software citation: GitHub's enhanced software citation support]]></title>
        <id>a0zcvjr-909bht8-kcsq1sd-87fh</id>
        <link href="https://blog.front-matter.io/mfenner/step-forward-for-software-citation"/>
        <updated>2021-08-24T16:57:24.000Z</updated>
        <summary type="html"><![CDATA[<em>Cross-posted from the FORCE11 blog. Authors: Martin Fenner, Stephan Druskat, Neil Chue Hong, Daniel S. Katz, Morane Gruenpeter, Arfon Smith, Tom Morell and Robert Haines</em>On August 19, GitHub announced software citation support in GitHub repositories....]]></summary>
        <content type="html"><![CDATA[<p><em><a href="https://www.force11.org/blog/step-forward-software-citation-githubs-enhanced-software-citation-support">Cross-posted</a> from the FORCE11 blog. Authors: Martin Fenner, Stephan Druskat, Neil Chue Hong, Daniel S. Katz, Morane Gruenpeter, Arfon Smith, Tom Morell and Robert Haines</em></p>
<p>On August 19, <a href="https://github.blog/2021-08-19-enhanced-support-citations-github/">GitHub announced</a> software citation support in GitHub repositories. Citation information provided by users (using a CITATION.cff YAML file in the root directory of the default branch) is parsed and made available as bibtex file or formatted citation, currently supporting the APA citation style. The APA style is a good start as it is the only popular citation style that labels software with the string <code>[Computer software]</code> but we hope to see support for more citation styles – including those popular in computer science – going forward. Going forward we also hope to see support for biblatex and potentially <code>@software</code> as the bibtex entry type.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/August/24th/Bildschirmfoto%202021-08-24%20um%2018.16.55.png" class="kg-image" width="792" height="626" alt="Cite this repository GitHub example" /><figcaption aria-hidden="true">Cite this repository GitHub example</figcaption>
</figure>
<pre><code>@misc{Haines_Ruby_CFF_Library_2021,
  author = {Haines, Robert and {The Ruby Citation File
  Format Developers}},
  doi = {10.5281/zenodo.1184077},
  month = {8},
  title = {{Ruby CFF Library}},
  url = {https://github.com/citation-file-format/ruby-cff},
  year = {2021}
}</code></pre>
<p>This is an important step forward for wider software citation adoption, as it allows software authors to provide the required information necessary for citing software directly in a GitHub code repository, and for tools and workflows to integrate with this information. Within days of the initial GitHub announcement via a tweet by GitHub CEO Nat Friedman, we saw support for this new workflow by the scholarly repository <a href="https://twitter.com/ZENODO_ORG/status/1420357001490706442">Zenodo</a> and the reference manager <a href="https://twitter.com/zotero/status/1420515377390530560">Zotero</a> (see below). The <a href="https://docs.softwareheritage.org/devel/swh-indexer/metadata-workflow.html">swh-indexer</a> by <a href="https://www.softwareheritage.org/">Software Heritage</a> (SWH) already indexed and supported searching over the CITATION.cff files available on the HEAD/master branch of a repository archived in SWH.</p>
<blockquote>
<p>We’ve added support for GitHub’s new citation feature. When saving GitHub repos to your library, Zotero can now use the enhanced metadata provided by developers.<br />
<br />
If there’s no citation file, Zotero will continue to use the existing repo metadata (Company, Prog. Language, etc.). <a href="https://t.co/Q34zPBRGFj">https://t.co/Q34zPBRGFj</a></p>
<p>— Zotero (@zotero) <a href="https://twitter.com/zotero/status/1420515377390530560?ref_src=twsrc%5Etfw">July 28, 2021</a></p>
</blockquote>
<p>We also see hundreds of repositories adding CITATION.cff files every week since the initial announcement – you can track the adoption via <a href="https://github.com/search?o=desc&amp;p=1&amp;q=CITATION.cff&amp;s=committer-date&amp;type=Commits">this</a> GitHub query.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/August/24th/Bildschirmfoto%202021-08-24%20um%2018.24.19.png" class="kg-image" width="2000" height="964" alt="Tracking Citation.CFF files via GitHub query" /><figcaption aria-hidden="true">Tracking Citation.CFF files via GitHub query</figcaption>
</figure>
<p>While a citation pointing to the GitHub code repository is a great start, ideally the software author wants to perform an important additional step: archive the source code in a long-term archive either via the Software Heritage universal software archive (detailed instructions <a href="https://www.softwareheritage.org/save-and-reference-research-software/">here</a>, and automatable from the GitHub repository with a <a href="https://github.com/marketplace/actions/save-to-software-heritage">GitHub Action</a>), and/or the scholarly repository Zenodo via the the <em>Making Your Code Citable</em> workflow described <a href="https://guides.github.com/activities/citable-code/">here</a>, which will use the metadata provided in a CITATION.cff file.</p>
<p>One particular challenge with citing software is versioning, where there are multiple use cases to be supported, including the need to cite a specific version, and to aggregate the citations of all versions in a single place. There is more work needed to link <a href="https://docs.github.com/en/github/administering-a-repository/releasing-projects-on-github/managing-releases-in-a-repository">GitHub releases</a> to the version information provided in this new GitHub feature, and to support the generic citation without a specific version – what Zenodo calls a <em>concept DOI</em>, and the Functional Requirements for Bibliographic Records (FRBR) call an <a href="https://en.wikipedia.org/wiki/Functional_Requirements_for_Bibliographic_Records"><em>expression</em></a><em>.</em></p>
<p>Many software authors also want to link to a publication describing their software from the GitHub repository, and the <a href="https://citation-file-format.github.io/">Citation File Format</a> (CFF) supports this via a ‘<a href="https://github.com/citation-file-format/citation-file-format/blob/main/schema-guide.md#credit-redirection">preferred-citation</a>’ field. Additionally, authors can cite the software (and other works) their software builds on in a ‘<a href="https://github.com/citation-file-format/citation-file-format/blob/main/schema-guide.md#referencing-other-work">references</a>’ section in CFF files. Software authors are expected to provide the relevant information in a CITATION.cff file in <a href="https://en.wikipedia.org/wiki/YAML">YAML</a> format that follows the <a href="https://citation-file-format.github.io/">Citation File Format</a> specification. An example CITATION.cff file can be found <a href="https://github.com/citation-file-format/ruby-cff/blob/main/CITATION.cff">here</a>.</p>
<p>Some software authors might want to use tools to help generate the CITATION.cff file. A starting point is the CFF Initializer available <a href="https://citation-file-format.github.io/cff-initializer-javascript/">here</a>, and a list of available tools for working with CFF files <a href="https://github.com/citation-file-format/citation-file-format/blob/main/README.md#tools-to-work-with-citationcff-files-wrench">here</a>; we expect more tools to appear over time. There are many standards for describing software (we already mentioned bibtex and DOI metadata), and <a href="https://codemeta.github.io/">CodeMeta</a> also plays a particularly important role by providing <a href="https://codemeta.github.io/crosswalk/">crosswalks</a> and tools for converting between the various metadata standards for software. Going forward we expect to see more metadata conversion workflows, in particular via <a href="https://docs.github.com/en/actions">GitHub Actions</a>, adding to the already existing <a href="https://github.com/marketplace/actions/cffconvert">cffconvert</a> and <a href="https://github.com/marketplace/actions/codemeta2cff">CodeMeta2CFF</a> GitHub Actions. We also hope to see similar software citation support appear in the GitLab platform.<br />
</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Support open source software as a GitHub sponsor]]></title>
        <id>2p1pn59-jwr8c4s-1316px4-nwk85</id>
        <link href="https://blog.front-matter.io/mfenner/support-open-source"/>
        <updated>2021-08-12T14:56:42.000Z</updated>
        <summary type="html"><![CDATA[Two years ago GitHub introduced the ability to sponsor an open source contributor – person or organization. They handle (and pay for) the payment logistics for a one-time or regular contribution. A blog post from June 2019 describes the thinking of the...]]></summary>
        <content type="html"><![CDATA[<p>Two years ago GitHub introduced the ability to sponsor an open source contributor – person or organization. They handle (and pay for) the payment logistics for a one-time or regular contribution. A <a href="https://github.blog/2019-06-12-faq-with-the-github-sponsors-team/">blog post from June 2019</a> describes the thinking of the GiHub Sponsors team that went into this service, and the practicalities of using the service are documented <a href="https://docs.github.com/en/sponsors/sponsoring-open-source-contributors/sponsoring-an-open-source-contributor">here</a>.</p>
<p>In my <a href="https://blog.front-matter.io/mfenner/how-readers-can-support">last blog post</a> I talked about a similar concept, supporting this blog via a one-time donation or small monthly contribution. Similar to GitHub sponsors this is also a small voluntary contribution rather than a required payment, and it depends on a backend service that can handle small contributions of just a few dollars/euros.</p>
<p>Readers of this blog know that I am a big fan of dog food (the <a href="https://en.wikipedia.org/wiki/Eating_your_own_dog_food">saying</a> not the food). In this case it means that Front Matter should become a GitHub sponsor for open source software it depends on. As a small startup you have to start somewhere, and the pick for the first organization to support was an easy one: <a href="https://citationstyles.org/">Citation Style Language</a> (CSL). For practical reasons Front Matter sponsors Rintze Zelle, one of the main contributors to CSL:</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/August/12th/Bildschirmfoto%202021-08-12%20um%2016.06.31.png" class="kg-image" width="1070" height="555" alt="Front Matter GitHub sponsoring" /><figcaption aria-hidden="true">Front Matter GitHub sponsoring</figcaption>
</figure>
<p>The reason I picked CSL is that I never seen an open source project contributing to scholarly infrastructure so much with so little resources. CSL is everywhere (see the list of software products using CSL <a href="https://citationstyles.org/">here</a>), and has been for many years. And there is no big organization standing behind CSL that pays a salary that allows one or more developers to work on CSL, a common pattern with important open source software. So a big thank you to <a href="https://twitter.com/bdarcus">Bruce D'Arcus</a>, <a href="https://twitter.com/skornblith">Simon Kornblith</a>, <a href="https://twitter.com/fgbjr">Frank Bennett</a>, <a href="https://twitter.com/rintzezelle">Rintze Zelle</a>, <a href="https://twitter.com/adam42smith">Sebastian Karcher</a>, <a href="https://twitter.com/1nukshuk">Sylvester Keil</a>, <a href="https://twitter.com/johanneskrtek">Johannes Krtek</a>, Liam Magee, <a href="https://twitter.com/cparnot">Charles Parnot</a>, Carles Pina, Andrea Rossato, <a href="https://twitter.com/danstillman">Dan Stillman</a>, and <a href="https://twitter.com/zuphilip">Philipp Zumstein</a>, to name just a few of the many contributors to CSL. You can thank them yourself in the <a href="https://discourse.citationstyles.org/">CSL discussion forum</a>, but if your own software depends on CSL, please consider GitHub sponsorship.</p>
<p>GitHub sponsors and donations and memberships for the Front Matter blog are of course flavors of <a href="https://en.wikipedia.org/wiki/Crowdfunding">crowdfunding</a>, which has an interesting history and many success stories you can learn from, and mistakes to avoid.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How readers can support the Front Matter blog]]></title>
        <id>3kg9325-wss8khv-xaddcc6-zkg2a</id>
        <link href="https://blog.front-matter.io/mfenner/how-readers-can-support"/>
        <updated>2021-08-10T14:19:57.000Z</updated>
        <summary type="html"><![CDATA[The Front Matter blog launched last week and while the content is currently only written by me, I hope this will change in the coming months to include one-time guest posts and regular writers. One challenge is to figure out how to finance the blog in the long run....]]></summary>
        <content type="html"><![CDATA[<p>The Front Matter blog <a href="https://blog.front-matter.io/mfenner/front-matter-officially-launches-today">launched last week</a> and while the content is currently only written by me, I hope this will change in the coming months to include one-time guest posts and regular writers. One challenge is to figure out how to finance the blog in the long run. Running blog infrastructure is not overtly expensive, and in the case of the Front Matter blog amounts to about 120€/$140 a month:</p>
<ul>
<li>hosting Ghost blog editor 15€/month (<a href="https://www.digitalocean.com/">Digital Ocean</a>)</li>
<li>hosting Next.js frontend 20€/month (<a href="https://vercel.com/">Vercel</a>)</li>
<li>hosting full-text search index 35€/month (<a href="https://typesense.org/">Typesense</a>)</li>
<li>hosting Discourse commenting platform 15€/month (<a href="https://www.digitalocean.com/">Digital Ocean</a>)</li>
<li>web analytics €5/month (<a href="https://plausible.io/">Plausible</a>)</li>
<li>registration domain name 35€/year (<a href="https://dnsimple.com/">DNSimple</a>)</li>
<li>DOI registration €300/year (estimate, <a href="https://www.crossref.org/">Crossref</a>)</li>
</ul>
<p>The cost not reflected above, is of course staff time for writing blog posts, but also editing, deciding on topics, reaching out to potential writers, etc. A reasonable goal would be to pay for an editor one day a week, which would add about 1280€/1500$ to the monthly cost running the blog. How can this be supported financially?</p>
<p>While there is always the hope of finding an organization (funder, publisher, institution, infrastructure provider, etc.) that cares enough about Open Science to pay for running this blog, a more realistic alternative is to depend on reader contributions, combined with volunteer donations.</p>
<p>Advertisements are a very painful way to recover even a fraction of the costs for running a blog that caters to a relatively small audience (scholarly communication). In response several platforms, including <a href="https://medium.com/">Medium</a>, <a href="https://ghost.org/">Ghost</a> and <a href="https://substack.com/">Substack</a>, have evolved with a business model focused on providing exclusive content to paying subscribers. This is obviously not an option for an Open Science blog, where the assumption is that content is free to read and reuse under a Creative Commons license.</p>
<p>Today the Front Matter blog is starting a newsletter with a small monthly subscription fee (3€/month or 30€/year). The newsletter does not provide exclusive content, but rather is a convenience for users to directly receive content in their email inbox rather than via the <a href="https://blog.front-matter.io/feed.xml">the Front Matter RSS Feed</a>. In addition, users can give one-time donations to support the Front Matter blog. These payment options are provided by <a href="https://www.buymeacoffee.com/">Buy Me a Coffee</a>, with a <a href="https://blog.front-matter.io/support">link</a> to the membership/support page in the footer of every Front Matter page  (see screenshot below).</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/August/9th/Bildschirmfoto%202021-08-09%20um%2020.21.20.png" class="kg-image" width="2210" height="1252" alt="Buy Me a Coffee page for Front Matter" /><figcaption aria-hidden="true"><a href="https://www.buymeacoffee.com/frontmatter">Buy Me a Coffee page</a> for Front Matter</figcaption>
</figure>
<p>I hope that by the end of the year, some revenue will accrue that can support the Front Matter blog. It would definitely be a strong motivation to write regular Open Science blog posts with interesting content. A related aim of course is to nurture active and engaged readers who write comments in response to the posts, suggest topics to write about, and generally tell me via comments or email how Front Matter can do better.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[First InvenioRDM Long-Term Support (LTS) version released today – and Front Matter is joining as a participating partner]]></title>
        <id>7x0afvb-64y9spv-8x7e211-ybnjr</id>
        <link href="https://blog.front-matter.io/mfenner/inveniordm-lts-announced"/>
        <updated>2021-08-05T17:15:49.000Z</updated>
        <summary type="html"><![CDATA[The open source research data management platform InvenioRDM today announced the first Long-Term Support (LTS) release, usable on production services. And I am joining the effort as a participating partner via Front Matter,...]]></summary>
        <content type="html"><![CDATA[<p>The open source research data management platform InvenioRDM <a href="https://inveniordm.docs.cern.ch/releases/versions/version-v6.0.0/%20%20%20">today announced</a> the first Long-Term Support (LTS) release, usable on production services. And I am joining the effort as a participating partner via <a href="https://front-matter.io">Front Matter</a>, the organization I started this week.</p>
<p>InvenioRDM was <a href="https://inveniosoftware.org/blog/2019-04-29-rdm/">first announced in April 2019</a>:</p>
<blockquote>
Our vision in the next five-years, is to make InvenioRDM a world-leading extensible research data management platform used by research institutions all around the world and with businesses providing services, support and customizations on top of InvenioRDM.
</blockquote>
<p>The first concrete set of goals was defined as</p>
<ul>
<li>A stable InvenioRDM platform - A research data management platform based on <a href="https://zenodo.org/">Zenodo</a> and the <a href="https://inveniosoftware.org/">Invenio v3 Framework</a>.</li>
<li>A community of public and private institutions to sustain InvenioRDM.</li>
<li>Minimum two existing repositories migrated to InvenioRDM, with Zenodo being one of them.</li>
</ul>
<p>Today's release brings invenioRDM much closer to achieving these goals. The next major milestone for InvenioRDM is to migrate <a href="https://zenodo.org">Zenodo</a> to run on top of InvenioRDM.</p>
<p>In the coming two months I will not only try to get up to speed with the invenioRDM project and start working with the CERN team and the other participating partners, but I also have the specific task of making sure invenioRDM fully supports the data citation roadmap for scholarly data repositories, work done by the <a href="https://www.force11.org/group/dcip">Force11 DCIP project</a> with Merce Crosas and me as co-leads, and described in a 2019 <em>Scientific Data</em> paper (Fenner <em>et al</em>. 2019):</p>
<h3 id="guidelines-for-repositories-1-5-required-6-9-recommended-10-11-optional">Guidelines for Repositories (1-5 required, 6-9 recommended, 10-11 optional)</h3>
<ol>
<li>All datasets intended for citation must have a globally unique persistent identifier that can be expressed as an unambiguous URL.</li>
<li>Persistent identifiers for datasets must support multiple levels of granularity, where appropriate.</li>
<li>The persistent identifier expressed as an URL must resolve to a landing page specific for that dataset, and that landing page must contain metadata describing the dataset.</li>
<li>The persistent identifier must be embedded in the landing page in machine-readable format.</li>
<li>The repository must provide documentation and support for data citation.</li>
<li>The landing page should include metadata required for citation, and ideally also metadata facilitating discovery, in human-readable and machine-readable format.</li>
<li>The machine-readable metadata should use schema.org markup in JSON-LD format.</li>
<li>Metadata should be made available via HTML meta tags to facilitate use by reference managers.</li>
<li>Metadata should be made available for download in BibTeX and/or another standard bibliographic format.</li>
<li>Content negotiation for schema.org/JSON-LD and other content types may be supported so that the persistent identifier expressed as URL resolves directly to machine-readable metadata.</li>
<li>HTTP link headers may be supported to advertise content negotiation options</li>
</ol>
<p>Several of these recommendations are of course already addressed by invenioRDM, but there is more work needed in the details, e.g. how metadata are exposed in dataset landing pages using schema.org. And these recommendations have evolved, e.g. as described in the output of the <a href="https://www.rd-alliance.org/groups/research-metadata-schemas-wg">Research Data Alliance (RDA) Research Metadata Schemas Working Group</a> published in June (Wu <em>et al.</em> 2021).</p>
<p>Please reach out to me in the comments or via email if you have any questions or suggestions regarding this upcoming work, or more generally my new involvement in invenioRDM.</p>
<h3 id="references">References</h3>
<p>Fenner, M., Crosas, M., Grethe, J. S., Kennedy, D., Hermjakob, H., Rocca-Serra, P., Durand, G., Berjon, R., Karcher, S., Martone, M., &amp; Clark, T. (2019). A data citation roadmap for scholarly data repositories. <em>Scientific Data</em>, <em>6</em>(1). <a href="https://doi.org/10.1038/S41597-019-0031-8">https://doi.org/10.1038/S41597-019-0031-8</a></p>
<p>Wu, M., Juty, N., RDA Research Metadata Schemas WG , Collins, J., Duerr, R., Ridsdale, C., Shepherd, A., Verhey, C., &amp; Castro, L. J. (2021). <em>Guidelines for publishing structured metadata on the Web</em>. <a href="https://doi.org/10.15497/RDA00066">https://doi.org/10.15497/RDA00066</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Front Matter officially launches today]]></title>
        <id>726q3a1-fyj8d3v-jwhgha4-qxcph</id>
        <link href="https://blog.front-matter.io/mfenner/front-matter-officially-launches-today"/>
        <updated>2021-08-02T10:22:32.000Z</updated>
        <summary type="html"><![CDATA[Front Matter describes the content preceding the main text of a book or journal. In science, several research journals, including <em>PLOS Biology</em>, <em>Nature</em> and <em>Science</em>, have Front Matter sections, used for news, opinions,...]]></summary>
        <content type="html"><![CDATA[<p>Front Matter describes the content preceding the main text of a book or journal. In science, several research journals, including <em>PLOS Biology</em>, <em>Nature</em> and <em>Science</em>, have Front Matter sections, used for news, opinions, and other content that are not a research articles. Front Matter is also the name of the company that I registered last month, as I think it is a good fit for what I am trying to accomplish, after leaving the DOI registration agency DataCite in July 2021 (<a href="https://doi.org/10.5438/zx3k-3923">Farewell to DataCite</a>) having been their Technical Director for six years.</p>
<p>I sincerely believe that there is a need for more venues that talk about emerging scholarly content types such as research data, research software or preprints as scholarly outputs. The Front Matter Blog hopes to become such a venue. As a starting point I have added (almost) all my blog posts since 2007, collected from my previous blogging locations (<em>Nature Network</em>, <em>PLOS Blogs,</em> my Personal Blog, and the DataCite blog<em>)</em>, and I hope at least some of them still make an interesting read all these years later. Technically, the blog uses <a href="https://ghost.org/">Ghost</a> and the Ghost Editor as a backend, and a frontend built with <a href="https://nextjs.org/">Next.js</a>, a common <a href="https://jamstack.org/">Jamstack</a> setup:</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/August/2nd/jamstack-javascript-apis-markup.png" class="kg-image" width="850" height="394" alt="Jamstack, via Snipcart." /><figcaption aria-hidden="true">Jamstack, via <a href="https://snipcart.com/blog/jamstack">Snipcart</a>.</figcaption>
</figure>
<p>Two features of the Front Matter Blog are particularly important to me:</p>
<ul>
<li>full-text search of all blog posts (with the help of  <a href="https://typesense.org/">Typesense</a>), as tags, categories, and search by title allow only limited content discovery, and  </li>
<li>comments using a mature and open commenting system (<a href="https://www.discourse.org/">Discourse</a>).</li>
</ul>
<p>There is more work planned for the coming months, with the next milestone being the registration of DOIs for all blog posts.</p>
<p>But Front Matter is more than a blogging platform. It is also a consulting business, which will help with building and hosting scholarly infrastructure. To kick this off, I am involved with development work for the <a href="https://inveniosoftware.org/products/rdm/">invenioRDM</a> data management repository platform. More on that in the next blog post on Thursday.</p>
<p><em>Special thanks to <a href="https://orcid.org/0000-0002-9317-6819">Christine Ferguson</a> for reviewing the blog post before publishing.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Farewell to DataCite]]></title>
        <id>7ec2qd6-wd09zmr-jqcqb91-3hzaf</id>
        <link href="https://blog.front-matter.io/mfenner/farewell-to-datacite"/>
        <updated>2021-07-09T15:39:00.000Z</updated>
        <summary type="html"><![CDATA[After six years as DataCite Technical Director, I am both sad and excited to announce that I will be leaving DataCite, beginning a new adventure as an independent developer for the invenioRDM project on August 1st. My focus will remain on research data management,...]]></summary>
        <content type="html"><![CDATA[<p>After six years as DataCite Technical Director, I am both sad and excited to announce that I will be leaving DataCite, beginning a new adventure as an independent developer for the <a href="https://inveniosoftware.org/products/rdm/">invenioRDM</a> project on August 1st. My focus will remain on research data management, but with a different angle.</p>
<p>A lot has changed since 2015 at DataCite in general, and the DataCite technical architecture in particular. Rather than describe the work on DataCite infrastructure over the past six years in more detail, I want to provide a snapshot of where the DataCite infrastructure is with its core services, and what considerations the team is taking into account going forward.</p>
<h3 id="content-registration">Content registration</h3>
<p>DataCite members can register DOIs and metadata for content using the Metadata Store (MDS). The MDS API hasn’t changed much for users since 2012, although the technology powering the API has been replaced more than once, and the metadata schema is constantly evolving. We have added a JSON REST API and web frontend (<a href="https://doi.datacite.org/">Fabrica</a>) starting in 2017. Going forward we hope to see more adoption of the JSON REST API, e.g. by finalizing and promoting the JSON schema. And we may want to explore other ways to register content, namely by embedding metadata in landing pages using schema.org in combination with sitemaps files.</p>
<h3 id="discovery">Discovery</h3>
<p>In October 2020 DataCite launched <a href="https://commons.datacite.org/">DataCite Commons</a> as a new discovery platform, followed by an announcement to retire DataCite Search by the end of 2021. DataCite Commons enables the discovery of connections between content, people, and organizations. DataCite Commons uses the existing DataCite backend infrastructure with relational databases and Elasticsearch in combination with a new <a href="https://graphql.org/">GraphQL</a> API. Going forward we will see whether this approach scales appropriately, or whether a different technology is needed to power DataCite Commons. This would include the exploration of graph database technologies such as neo4j with or without GraphQL. Further, we will work to track the adoption of GraphQL and our REST API architecture.</p>
<h3 id="backend-services">Backend services</h3>
<p>DataCite as an infrastructure provider has always focussed on backend APIs and related services. As part of this work, DataCite has migrated to a Docker container-based cloud architecture. There is still work ahead, from migrating to <a href="https://kubernetes.io/">Kubernetes</a> to service meshes and better monitoring and handling of service loads.</p>
<h3 id="frontend-services">Frontend services</h3>
<p>The DataCite frontend service architecture has gradually evolved over the last six years, with the new services Fabrica and DataCite Commons using modern Javascript frameworks (Ember.js and Next.js, respectively). One side effect of this gradual evolution is a rather complex mix of technologies. Going forward, rewriting outdated services, and consolidating the various technologies are important goals. Combined with this could be the broader use of <a href="https://www.serverless.com/">serverless architectures</a> which we started using in DataCite Commons.</p>
<h3 id="looking-ahead">Looking ahead</h3>
<p>DataCite is in a good position to handle our technology projects during this transition. I have been working closely with the DataCite team to transition responsibilities and will continue to be involved in community initiatives. Matt will publish a blog post next week that will cover the future team structure.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/zx3k-3923">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The DataCite Technology Stack]]></title>
        <id>53yd7ag-a0s8zbr-h8s784p-gbvra</id>
        <link href="https://blog.front-matter.io/mfenner/the-datacite-technology-stack"/>
        <updated>2021-06-17T15:42:00.000Z</updated>
        <summary type="html"><![CDATA[DataCite is a DOI registration agency that enables the registration of scholarly content with a persistent identifier (DOI) and metadata. This content can then be searched for, reused, and connected to other scholarly resources....]]></summary>
        <content type="html"><![CDATA[<p>DataCite is a DOI registration agency that enables the registration of scholarly content with a persistent identifier (DOI) and metadata. This content can then be searched for, reused, and connected to other scholarly resources. But how does the underlying infrastructure enable this? In this blog post, we will describe what we have built to make this work. This is a fairly technical post, as I tried to go a little deeper into the details.</p>
<h3 id="cloud-hosting-and-devops">Cloud hosting and DevOps</h3>
<p>DataCite is a small nonprofit organization (<a href="https://datacite.org/staff.html">currently 12 team members</a>, including three in the development team), and the team is fully remote. All our infrastructure is running in the Cloud (most of it using Amazon Web Services (<a href="https://aws.amazon.com/">AWS</a>), with the servers that store data located in Ireland). We have automated the operation of our services as much as possible, following <a href="https://en.wikipedia.org/wiki/DevOps">DevOps</a> best practices. Because of the small team size, we have no separation of software development and system administration teams, and DevOps allows us to highly integrate these roles. Two important automation tools we use are <a href="https://docs.github.com/en/actions">GitHub Actions</a> for Continues Integration/Continues Deployment (CI/CD) and <a href="https://www.terraform.io/">Terraform</a> for managing “Infrastructure as Code”. While we are using Terraform since 2016, we have only recently migrated our CI/CD workflows from <a href="https://travis-ci.com/">Travis CI</a> (finishing the migration in the next few months), mainly because GitHub Actions come with many ready-to-use actions for some of the more complex parts of our deployment pipeline. All DataCite software is available with an open license in a public GitHub repository, and that also includes our GitHub actions and Terraform configurations. You can for example find the code for our REST API <a href="https://github.com/datacite/lupo">here</a>, and the corresponding GitHub actions <a href="https://github.com/datacite/lupo/tree/master/.github/workflows">here</a>.</p>
<h3 id="datacite-backend-services">DataCite backend services</h3>
<p>The DataCite backend uses services that store data (files or databases), and we use managed AWS services for those, e.g. <a href="https://aws.amazon.com/rds/">RDS</a> to manage our MySQL relational databases. Our APIs are all running as stateless Docker containers, and we use the Amazon Elastic Container Service (<a href="https://aws.amazon.com/ecs/">ECS</a>), in combination with <a href="https://aws.amazon.com/elasticloadbalancing/application-load-balancer/">Amazon Application Load Balancers</a> to manage those. The adoption of Docker containers was the biggest change in our infrastructure 2016-2019, and we have developed a lot of expertise in this area. Going forward we will switch to Kubernetes (<a href="https://aws.amazon.com/eks/">AWS Kubernetes Service</a>) at some point, as it has become the de-facto standard for container management in the cloud and provides additional functionalities in a widely-used open source platform. In 2015 all backend services were written in Java, over the last six years – as we upgraded our services one after another – this has changed to backend services written in Ruby and Python. This might again change going forward, we have for example started to use an <a href="https://github.com/plausible/analytics">open source software for collecting usage stats</a> that is written in Elixir. While we have to be careful as a small development team to not spread our expertise too wide, we need to be open to new technologies, and a grant-funded project that can be based on existing open source software</p>
<h3 id="datacite-frontend-services">DataCite frontend services</h3>
<p>The DataCite frontend services have over time been clearly separated from backend services, with only one service still running as a full-stack service (mainly because of the special needs for authentication). <a href="https://github.com/datacite/bracco">DataCite Fabrica</a> is our main service for account management and DOI registration and was originally launched in September 2017. Fabrica uses the Ember.js Javascript framework. Ember.js addresses the needs we have for this service but has not seen the same level of adoption as some other Javascript frameworks, namely React or Vue. When we launched <a href="https://commons.datacite.org/">DataCite Commons</a> – our replacement for the DataCite Search discovery service – in October 2020, we decided to use the <a href="https://nextjs.org/">Next.js</a> framework (based on React), together with GraphQL as the query language for our APIs. Next.js supports server-side rendering, which enabled us to provide embedded metadata (in particular in <a href="https://schema.org/">schema.org</a> format) that are picked up by Google (e.g. for Google Dataset Search) and other indexers. The initial experience has been positive and we are going to not only build out the functionality – and performance – of DataCite Commons, but over time also transition our legacy frontend services to the same platform – currently we are using six different technologies for those, as they were built over time in the last ten years. We can then also explore more the use of <a href="https://www.serverless.com/">serverless</a> as a technology to support our frontend services, as we are doing with DataCite Commons, running on the <a href="https://vercel.com/">Vercel</a> platform.</p>
<p>The following picture puts everything I talked about together into a single view (obviously omitting a lot of detail):</p>
<figure>
<img src="https://blog.datacite.org/images/uploads/reference_architecture.png" class="kg-image" alt="Reference architecture" /><figcaption aria-hidden="true">Reference architecture</figcaption>
</figure>
<p>Please feel free to reach out to <a href="mailto:mfenner@datacite.org">me</a> if you have any questions about the DataCite technology stack. If you are now interested in working for the DataCite development team, you can find more information about an open position <a href="https://doi.org/10.5438/wkc7-p624">here</a>.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/v5tc-zz53">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We need your feedback: Aligning the CodeMeta vocabulary for scientific software with schema.org]]></title>
        <id>45v8pxg-xn28vhb-vsx5age-yk5hb</id>
        <link href="https://blog.front-matter.io/mfenner/we-need-your-feedback-aligning-the-codemeta-vocabulary-for-scientific-software-with-schema-org"/>
        <updated>2021-05-11T15:44:00.000Z</updated>
        <summary type="html"><![CDATA[Metadata that describes scientific software in standard ways – in particular citation metadata such as title, authors, publication year, and venue – is essential for proper software citation implementation. The metadata should be generated by the software author,...]]></summary>
        <content type="html"><![CDATA[<p>Metadata that describes scientific software in standard ways – in particular citation metadata such as title, authors, publication year, and venue – is essential for proper software citation implementation. The metadata should be generated by the software author, stored in the code repository, included in submissions to journals, and archived with the source code in a software archive. The CodeMeta community has created such a standard vocabulary, with version 2.0 of the metadata schema released in 2017, and with mappings to common metadata standards for software source code, including DataCite metadata. Since that release, a task force of the Force11 Software Citation Implementation Working Group with co-chairs Morane Gruenpeter from Software Heritage and Martin Fenner from DataCite has worked on updating the schema to version 3.0, with a particular focus on aligning the CodeMeta schema with schema.org metadata.</p>
<p>The following terms in CodeMeta 2.0 are not (yet) part of schema.org:</p>
<ul>
<li>softwareSuggestions</li>
<li>maintainer</li>
<li>contIntegration</li>
<li>buildInstructions</li>
<li>developmentStatus</li>
<li>embargoDate</li>
<li>funding</li>
<li>issueTracker</li>
<li>referencePublication</li>
<li>Readme</li>
</ul>
<p>The CodeMeta Task Force of the Force11 Software Citation Implementation Working Group is now seeking community input via the CodeMeta GitHub issue tracker (summarized in <a href="https://github.com/codemeta/codemeta/issues/232">https://github.com/codemeta/codemeta/issues/232</a>). The task force asks for input until June 15, so that CodeMeta version 3.0 can ideally be finalized by July. After this work is done, the task force will propose the integration of all CodeMeta terms into the schema.org schema, so that CodeMeta becomes fully aligned with schema.org for better interoperability with existing documentation, tooling, and related communities. For any questions or comments that are not a good fit for the GitHub issue tracker, please reach out to the Task Force co-chairs Morane Gruenpeter and Martin Fenner via the <a href="https://www.force11.org/group/software-citation-implementation-working-group">Force11 Software Citation Implementation WG Homepage</a>.</p>
<p><em><em>This blog post was <a href="https://doi.org/10.5438/a49j-x692">originally published</a> on the DataCite</em> B<em>log.</em></em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Welcome Suzanne to the DataCite Team!]]></title>
        <id>3jz3ea3-0379p6a-spt275q-3aq0f</id>
        <link href="https://blog.front-matter.io/mfenner/welcome-suzanne-to-the-datacite-team"/>
        <updated>2020-11-12T15:48:00.000Z</updated>
        <summary type="html"><![CDATA[DataCite is pleased to welcome Suzanne Vogt to our team. Suzanne joined DataCite as an application developer in November 2020.Can you tell us a little bit about what you did before you started working for DataCite?I have been a software developer for a number of years,...]]></summary>
        <content type="html"><![CDATA[<p>DataCite is pleased to welcome Suzanne Vogt to our team. Suzanne joined DataCite as an application developer in November 2020.</p>
<figure>
<img src="https://blog.datacite.org/images/uploads/svogt.png" class="kg-image" />
</figure>
<h3 id="can-you-tell-us-a-little-bit-about-what-you-did-before-you-started-working-for-datacite">Can you tell us a little bit about what you did before you started working for DataCite?</h3>
<p>I have been a software developer for a number of years, my very early experience is in operating systems and networking. Since 2013 I have moved to web development and application development in higher education for the University of New Hampshire. Most recently I have worked in the UNH library to help them move towards modernizing their digital repository. I loved being in the UNH library and in the university setting in general but am very happy to move to DataCite and be able to use the knowledge that I gained at UNH.</p>
<h3 id="what-interested-you-in-working-for-datacite">What interested you in working for DataCite?</h3>
<p>I am a great proponent of open source, open knowledge, open data. We are at a pivotal time with the internet providing unprecedented access to knowledge many people might not normally have, and I think that we are only going to benefit from it. The role that DataCite is playing, connecting researchers with data, making it discoverable and reusable, is extremely valuable and fits right in the goal of opening access to legitimate research data. I hope to be able to contribute to that effort in the areas of maintaining and advancing the supporting technology whether that is the back end, front end, or user experience. Being so new at DataCite, I am still looking at a more specific roles.</p>
<h3 id="what-technologies-are-you-interested-in">What technologies are you interested in?</h3>
<p>I have always been interested in upgrading my knowledge. I have worked with a number of technologies that DataCite uses and am especially happy to add much more depth to my experience in using these technologies to support higher education. Perl, however, has been my favorite programming language for a number of reasons. I have found it to be expressive enough to do everything that I want. Additionally, Perl has always had a thriving, open-source community that is very supportive of its constituents.</p>
<h3 id="given-your-background-working-on-technology-for-an-academic-institution-how-can-datacite-make-it-easier-for-them-to-use-datacite-services">Given your background working on technology for an academic institution, how can DataCite make it easier for them to use DataCite services?</h3>
<p>I would have to say that, in general, the key things to making a service easier to use, for anyone, is great documentation, and a meaningful API.</p>
<p><em><em>This blog post was <a href="https://doi.org/10.5438/d3hs-6s25">originally published</a> on the DataCite</em> B<em>log.</em></em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DataCite Commons at your service]]></title>
        <id>1hy96a0-hxw92q9-vvn709e-72bmm</id>
        <link href="https://blog.front-matter.io/mfenner/datacite-commons-at-your-service"/>
        <updated>2020-10-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[DataCite and the FREYA project partners are proud to announce the official launch of DataCite Commons today. DataCite Commons is the web interface to explore the PID Graph, formed by the publications, datasets, research software,...]]></summary>
        <content type="html"><![CDATA[<p>DataCite and the <a href="https://www.project-freya.eu/en">FREYA project</a> partners are proud to announce the official launch of <a href="https://commons.datacite.org/">DataCite Commons</a> today. DataCite Commons is the web interface to explore the PID Graph, formed by the publications, datasets, research software, and other research outputs generated by researchers working at research institutions and supported by grant funding (<strong><strong>???</strong></strong>). The PID Graph depends on persistent identifiers to uniquely identify all these resources, and metadata that describe these resources and their connections.</p>
<p>We launched a pre-release version of DataCite Commons in August [Fenner (<a href="https://blog.datacite.org/datacite-commons-at-your-service/#ref-https://doi.org/10.5438/f4df-4817">2020a</a>)] and have used the last two months not only for many small improvements and bug fixes, but also to add two important new features: a statistics page that describes the information available in the PID Graph, and personal accounts that allow for ORCID claiming and other functionality going forward.</p>
<figure>
<img src="https://blog.datacite.org/images/uploads/bildschirmfoto-2020-10-28-um-11.42.23.png" title="DataCite Commons data sources summarized on the DataCite Commons statistics page." class="kg-image" />
</figure>
<p>While DataCite Commons includes all PIDs and metadata from DataCite and Research Organization Registry (ROR), it currently includes only a subset of metadata from ORCID, and only a subset of DOIs and metadata from Crossref. Other persistent identifiers for scholarly resources will be added over time. The statistics page shows the current coverage of DataCite Commons and thus the FREYA PID Graph. The statistics page also shows the current numbers of the connections between works, between works and people, and between works and organizations, allowing us to track the growth of the PID Graph over time.</p>
<figure>
<img src="https://blog.datacite.org/images/uploads/bildschirmfoto-2020-10-28-um-12.02.32.png" title="DataCite Commons personal accounts showing the ORCID claiming status." class="kg-image" />
</figure>
<p>User accounts are the other big change in this DataCite Commons release. They make it easier to navigate to your personal DataCite Commons page, listing all your publications, datasets, and software that DataCite Commons knows about. But more importantly, these personal accounts enable ORCID claiming, adding one or more works to your ORCID Record. When you search for works in DataCite Commons after logging in, you can see the works that have already been sent to your ORCID Record by DataCite, or an error message is shown if something went wrong. The next step, actual claiming from DataCite Commons search results, is in the final phase of development and will be open for beta testers in November.</p>
<p>This release of DataCite Commons wraps up the work on the PID Graph in the FREYA project, which will end at the end of November. We went from the initial idea for the PID Graph concept to the implementation of a production service for users, following the process summarized below.</p>
<h3 id="user-story-collection-and-prioritization">User story collection and prioritization</h3>
<p>In 2018 the FREYA project partners started collecting user stories that address important needs of their respective communities. In an August 2018 workshop we discussed these user stories, grouped them together, and prioritized them. The main categories were the aggregation of scholarly outputs, e.g. by research institution, funder, or researcher; the versioning and granularity of data and software, and the grouping of all research outputs and other resources (e.g. data, software, people, funding) for a given publication. All these user stories depend on a PID Graph, with typically two connections needed in the graph, e.g. “show me all citations for datasets funded by a particular grant”.</p>
<h3 id="technical-architecture-and-api-development">Technical architecture and API development</h3>
<p>Based on these requirements we started to investigate if our existing technical architecture supported these user stories, or what changes would be needed. The initial exploration looked at incremental changes to our existing REST APIs, but it became clear that more fundamental changes would be needed, supporting queries of the PID Graph in a number of different ways. In the spring of 2019, we decided to use GraphQL as the underlying technology for the PID Graph, as it supports the kinds of queries common in the PID Graph, is widely adopted in terms of software libraries, documentation, and developer community, and can be easily integrated into existing backend systems such as relational databases and search indexes such as Solr or Elasticsearch. DataCite released a GraphQL API pre-release version in May 2019, and a production version in May 2020 (Fenner, <a href="https://blog.datacite.org/datacite-commons-at-your-service/#ref-https://doi.org/10.5438/yfck-mv39">2020b</a>).</p>
<h3 id="web-frontend-development">Web frontend development</h3>
<p>The GraphQL API we had developed allowed us to address the user stories we identified, and we started to write Jupyter notebooks as a platform that makes it easier to work with the GraphQL API, and in August 2020 the FREYA project released ten Jupyter notebooks addressing some of the user stories we had identified (Fenner &amp; Petryszak, <a href="https://blog.datacite.org/datacite-commons-at-your-service/#ref-https://doi.org/10.5281/zenodo.4004426">2020</a>). But APIs and Juypter notebooks are still a significant hurdle for many users, and in the spring of 2020 we started work on a web frontend for the GraphQL API and thus the PID Graph. In August 2020 we launched a pre-release version of this web frontend and called it DataCite Commons (Fenner, <a href="https://blog.datacite.org/datacite-commons-at-your-service/#ref-https://doi.org/10.5438/f4df-4817">2020a</a>). Today we are officially releasing DataCite Commons (Fenner, Hallett, Garza, &amp; Wimalaratne, <a href="https://blog.datacite.org/datacite-commons-at-your-service/#ref-https://doi.org/10.14454/qgk4-zs88">2020</a>) as the web frontend for the FREYA PID Graph and as a key FREYA contribution to the European Open Science Cloud (EOSC).</p>
<h3 id="monitoring-and-feedback">Monitoring and feedback</h3>
<p>The release of DataCite Commons is an important milestone, but more work is needed to make sure that DataCite Commons addresses the needs of its users, and that the service sees significant use by the community. The focus for the next few months will therefore be on monitoring for traffic, bugs and other issues, and feedback collection regarding additional features. Add your ideas to the <a href="https://datacite.org/roadmap.html">DataCite Public Roadmap</a> or send comments and questions to <a href="mailto:info@datacite.org">mailto:info@datacite.org</a>.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/mkpq-4w71">originally published</a> on the DataCite Blog.</em></p>
<h3 id="references">References</h3>
<p>Fenner, M. (2020a). DataCite commons - exploiting the power of pids and the pid graph. <a href="https://doi.org/10.5438/F4DF-4817">https://doi.org/10.5438/F4DF-4817</a></p>
<p>Fenner, M. (2020b). Powering the pid graph: Announcing the datacite graphql api. <a href="https://doi.org/10.5438/YFCK-MV39">https://doi.org/10.5438/YFCK-MV39</a></p>
<p>Fenner, M., &amp; Petryszak, R. (2020). FREYA webinar - the pid graph in practice - jupyter notebook demonstration. <a href="https://doi.org/10.5281/ZENODO.4004426">https://doi.org/10.5281/ZENODO.4004426</a></p>
<p>Fenner, M., Hallett, R., Garza, K., &amp; Wimalaratne, S. (2020). Frontend for the datacite commons service. DataCite. <a href="https://doi.org/10.14454/QGK4-ZS88">https://doi.org/10.14454/QGK4-ZS88</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DataCite switches to Globus for Authentication]]></title>
        <id>28bwk8c-t4781za-e0p3evm-tazg4</id>
        <link href="https://blog.front-matter.io/mfenner/datacite-switches-to-globus-for-authentication"/>
        <updated>2020-10-14T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Access to some DataCite resources and services requires authentication so that DataCite knows who is making a request. This includes Fabrica, our DOI registration service that requires a member account, but also our integration with ORCID in the Profiles service,...]]></summary>
        <content type="html"><![CDATA[<p>Access to some DataCite resources and services requires authentication so that DataCite knows who is making a request. This includes <a href="https://doi.datacite.org/">Fabrica</a>, our DOI registration service that requires a member account, but also our integration with ORCID in the <a href="https://profiles.datacite.org/">Profiles</a> service, where researchers authenticate with us to allow us to send information about content with DataCite DOIs authored by them to their ORCID record.</p>
<p>Authentication needs to be secure, not allowing access by the wrong people, but also practical, as otherwise poor password behavior with reduced security might result, as highlighted in the 2019 update of the NIST guidelines (Grassi, Garcia, &amp; Fenton (<a href="https://blog.datacite.org/globus-authentication/#ref-https://doi.org/10.6028/nist.sp.800-63-3">2017</a>)) that set the industry standard. With this in mind, we have two main goals for improving authentication with DataCite services: a) phase-out of institutional logins in favor of more secure personal accounts, and b) consolidate authentication into a single service to simply access to all secured resources.</p>
<p>We started work on improving DataCite authentication two months ago, beginning with a major upgrade of the <a href="https://profiles.datacite.org/">Profiles</a> service. We are relaunching this upgraded service today. Most changes are under the hood and not visible to users; they include much-needed maintenance work, but also an improved administration interface for DataCite staff.</p>
<p>One visible change for users is the new sign in via Globus. <a href="https://www.globus.org/">Globus</a>, an initiative at the University of Chicago - and a DataCite member -, is a non-profit service that provides reliable data transfer and sharing, and authentication services for the research community. Globus’ overall mission not only aligns well with DataCite’s mission but its authentication service, Globus Auth, provides the functionality needed by DataCite and DataCite users. The previous version of the Profiles service allowed login via ORCID, Google, and GitHub, but did not support the use of institutional identities. Globus allows users to login with their institutional account, supporting several hundred such federated identities via InCommon and eduGAIN federations and other custom identity providers.</p>
<figure>
<img src="https://blog.datacite.org/images/uploads/bildschirmfoto-2019-10-09-um-19.16.21.png" class="kg-image" />
</figure>
<p>New Profiles sign in screen</p>
<p>DataCite now integrates with Globus via OpenID Connect, and requires that users login either with their ORCID identity or with another identity (e.g., an institutional account) linked to that ORCID identity via Globus. We use the ORCID ID to identify the user independent of email addresses or other information that may change over time, aligned with the recommendation of using the eduPersonORCID property in Federated Identity Management (Demeranville (<a href="https://blog.datacite.org/globus-authentication/#ref-https://doi.org/10.5281/zenodo.1064011">2017</a>)). For authenticating ORCID claiming, a second step is needed, obtaining the required permissions directly from ORCID to write to the user’s ORCID record. This integration of Globus allows DataCite to focus on providing persistent identifier services, while relying on Globus for authentication services.</p>
<p>While connecting authentication via OpenID Connect is straightforward, we made the process even easier for Ruby users by writing the omniauth-globus Ruby gem (Fenner (<a href="https://blog.datacite.org/globus-authentication/#ref-https://doi.org/10.14454/81gp-9y63">2019</a>)) that requires only the minimal configuration of a CLIENT_ID, CLIENT_SECRET, and REDIRECT_URL.</p>
<p>This change is just the first step in our work on improving authentication to DataCite services. As a next step, we will provide personal accounts, roles, and permissions for our DOI registration service Fabrica, which currently uses a separate authentication workflow and institutional accounts. We will continue partnering with Globus to wrap up this work before the end of 2019 but will keep the login option via institutional accounts until the end of 2020. In early 2020, we will start providing authentication tokens (in addition to basic authentication via username/password) for API users, and we will improve our integration with ORCID via a pilot of the <a href="https://members.orcid.org/service-provider-workflow">ORCID token delegation</a> workflow.</p>
<p>If you have any questions about these authentication updates, don’t hesitate to contact us at <a href="mailto:support@datacite.org">mailto:support@datacite.org</a>.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/08jb-pn73">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>Demeranville, T. (2017). Federated identity and identifiers. <a href="https://doi.org/10.5281/ZENODO.1064011">https://doi.org/10.5281/ZENODO.1064011</a></p>
<p>Fenner, M. (2019). Omniauth-globus: Provides basic support for authenticating a client application via the globus service. DataCite. <a href="https://doi.org/10.14454/81GP-9Y63">https://doi.org/10.14454/81GP-9Y63</a></p>
<p>Grassi, P. A., Garcia, M. E., &amp; Fenton, J. L. (2017). <em>Digital identity guidelines: Revision 3</em>. National Institute of Standards; Technology. Retrieved from <a href="https://doi.org/10.6028%2Fnist.sp.800-63-3">https://doi.org/10.6028%2Fnist.sp.800-63-3</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making the most out of available Metadata]]></title>
        <id>xcmpszg-0w9rtts-2ktab4v-fxca</id>
        <link href="https://blog.front-matter.io/mfenner/making-the-most-out-of-available-metadata"/>
        <updated>2020-09-07T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Metadata are essential for finding, accessing, and reusing scholarly content, i.e. to increase the FAIRness [Wilkinson et al. (2016)] of datasets and other scholarly resources. A rich and standardized metadata schema that is widely used is the first step,...]]></summary>
        <content type="html"><![CDATA[<p>Metadata are essential for finding, accessing, and reusing scholarly content, i.e. to increase the FAIRness [Wilkinson et al. (<a href="https://blog.datacite.org/making-the-most/#ref-https://doi.org/10.1038/sdata.2016.18">2016</a>)] of datasets and other scholarly resources. A rich and standardized metadata schema that is widely used is the first step, encouraging users to register these metadata (as many of these are optional and not required) is the second step, while infrastructure providers such as DataCite facilitating metadata registration and making the most of the available metadata is the third step. While we all have put considerable energy into the first two steps, I want to use this blog post to describe what DataCite is doing to improve metadata FAIRness via our services. I will focus on three important optional metadata properties and on two approaches: encouraging metadata registration in a standardized way using the new DOI form in our Fabrica service, and improving discovery via search filters in the next major release of DataCite Search, that we have started development work on.</p>
<h3 id="language">Language</h3>
<p>DataCite metadata can include the primary language of the resource, using either the <a href="https://tools.ietf.org/html/bcp47">BCP 47</a> (e.g. <em>en-US</em>) or <a href="https://en.wikipedia.org/wiki/ISO_639-1">ISO 639-1</a> (e.g. <em>en</em>) controlled vocabularies. In the context of discovery, the ISO 639-1 code is most helpful, as we want to find resources we can understand because we speak the language, and not necessarily care about the nuances of for example U.S. English vs. British English. In Fabrica, the DOI form uses the list of languages in ISO 639-1, and that is also what we will use for filters in search.</p>
<figure>
<img src="https://blog.datacite.org/images/uploads/bildschirmfoto-2020-07-09-um-07.03.28.png" title="DOI form language" class="kg-image" />
</figure>
<figure>
<img src="https://blog.datacite.org/images/uploads/bildschirmfoto-2020-07-09-um-07.03.56.png" title="Search facet language" class="kg-image" />
</figure>
<p>Going forward, it would make sense to consider allowing multiple languages per resource. This will not only allow using both BCP 47 and ISO 639-1, addressing different use cases, but will also allow the proper description of multilingual resources.</p>
<h3 id="rights">Rights</h3>
<p>Rights information about a resource is essential, as it informs the user if and under what conditions the resource can be reused. In order to allow users to filter by a specific license, rights information needs to be normalized. In theory, a URL pointing to a specific license is all that is needed, but we also need a human-readable string, and ideally also an abbreviation for the license (e.g. Creative Commons Attribution 4.0 International and CC-BY-4.0 for https://creativecommons.org/licenses/by/4.0/legalcode), and, more importantly, many licenses have more than one URL, e.g.</p>
<ul>
<li><a href="http://creativecommons.org/licenses/by/4.0">http://creativecommons.org/licenses/by/4.0</a></li>
<li><a href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</a></li>
<li><a href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></li>
<li><a href="https://creativecommons.org/licenses/by/4.0/legalcode">https://creativecommons.org/licenses/by/4.0/legalcode</a></li>
</ul>
<p>We can address this by normalizing the URLs for all licenses, and providing a standard name and abbreviation. Luckily, this work has already been done by the [Software Package Data Exchange](https://spdx.dev/) (SPDX) project of the Linux Foundation. While SPDX focusses on software licenses, it also includes all Creative Commons licenses, which are the most common licenses used in DataCite metadata for data and text. In metadata schema 4.2, released in March 2019, we added the properties <em>rightsIdentifier</em>, <em>rightsIdentifierScheme</em> and <em>schemeURI</em>, and this enables the use of SPDX. In the past few months we have added SPDX support to the DOI form, and we will have search facets based on SPDX.</p>
<figure>
<img src="https://blog.datacite.org/images/uploads/bildschirmfoto-2020-07-09-um-07.35.30.png" title="DOI form rights" class="kg-image" />
</figure>
<figure>
<img src="https://blog.datacite.org/images/uploads/bildschirmfoto-2020-07-09-um-07.36.41.png" title="Search facet license" class="kg-image" />
</figure>
<h3 id="subject-area">Subject Area</h3>
<p>DataCite metadata have a very flexible Subject property, with sub-properties <em>SubjectScheme</em>, <em>SchemeURI</em>, and <em>ValueURI</em>. Unfortunately there is no standard way to describe the subject area covered by the resource. This makes it difficult to find content described by DataCite metadata in for example Mathematics, or to understand to what extend the various disciplines use DataCite DOIs. There are many subject area classification schemes, but the most widely used generic classification scheme is the <a href="https://www.oecd.org/science/inno/38235147.pdf">OECD Fields of Science classification</a> with 6 top-level categories and 42 subcategories. We have implemented the OECD Fields of Science classification in the DOI form, and will do so in search facets.</p>
<figure>
<img src="https://blog.datacite.org/images/uploads/bildschirmfoto-2020-07-09-um-07.51.23.png" title="DOI form subject" class="kg-image" />
</figure>
<figure>
<img src="https://blog.datacite.org/images/uploads/bildschirmfoto-2020-07-09-um-07.52.20.png" title="Search field of science facet" class="kg-image" />
</figure>
<p>While OECD Fields of Science is the most commonly used generic subject classification, the most widely used subject classification we currently find in DataCite metadata is the <a href="https://www.abs.gov.au/Ausstats/abs@.nsf/Latestproducts/6BB427AB9696C225CA2574180004463E?opendocument">Australian and New Zealand Standard Research Classification (ANZSRC) Fields of Research</a>. This classification is much more detailed, supporting different use case. Luckily there is an official ANZSRC mapping to the OECD Fields of Science. This allows us to automatically add the OECD Fields of Science category or subcategory if the ANZSRC Fields of Research is used in DataCite metadata.</p>
<h3 id="going-forward">Going Forward</h3>
<p>We hope that the DOI form makes it easier to register more of the optional but important metadata in a standardized way, and that the new search filters we are launching in a few months will improve discoverability of the content. And that in turn this encourages DataCite members and their repositories to include this information in DOI metadata also when using one of the DataCite APIs for DOI registration. There are sometimes good reason to do things differently, and this also includes metadata for language, rights and subject. The DataCite metadata schema provides the flexibility needed, but we hope that in most cases the standard vocabularies ISO 639-1, SPDX and OECD Fields of Science will be used, improving the finding, accessing, and reusing of scholarly content with DataCite metadata for everyone.</p>
<p>Of course we shouldn’t forget the important work of the DataCite Metadata Working Group, which is busy working on the next DataCite schema version. That is a topic for another blog post, but I can already tell you that DataCite metadata will better support text documents.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/1dgk-1m22">originally published</a> on the DataCite Blog.</em></p>
<h3 id="references">References</h3>
<p>Wilkinson, M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak, A., … Mons, B. (2016). The FAIR guiding principles for scientific data management and stewardship. <em>Scientific Data</em>, <em>3</em>(1). <a href="https://doi.org/10.1038/sdata.2016.18">https://doi.org/10.1038/sdata.2016.18</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DataCite Commons - Exploiting the Power of PIDs and the PID Graph]]></title>
        <id>5n35kpm-emf8hta-pnhb6ep-807mc</id>
        <link href="https://blog.front-matter.io/mfenner/datacite-commons-exploiting-the-power-of-pids-and-the-pid-graph"/>
        <updated>2020-08-27T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Today DataCite is proud to announce the launch of DataCite Commons, available at https://commons.datacite.org. DataCite Commons is a discovery service that enables simple searches while giving users a comprehensive overview of connections between entities in the research landscape....]]></summary>
        <content type="html"><![CDATA[<p>Today DataCite is proud to announce the launch of <a href="https://commons.datacite.org/">DataCite Commons</a>, available at <a href="https://commons.datacite.org/">https://commons.datacite.org</a>. DataCite Commons is a discovery service that enables simple searches while giving users a comprehensive overview of connections between entities in the research landscape. This means that DataCite members registering DOIs with us will have easier access to information about the use of their DOIs and can discover and track connections between their DOIs and other entities. DataCite Commons was developed as part of the EC-funded project Freya and will form the basis of new DataCite services.</p>
<figure>
<img src="https://blog.datacite.org/images/uploads/bildschirmfoto-2020-08-25-um-06.46.42.png" title="Search for works in DataCite Commons" class="kg-image" />
</figure>
<h3 id="content">Content</h3>
<p>DataCite Commons has a lot of content to search for. One of the most important features is the ability to search for all DOIs, no matter whether registered with DataCite, Crossref, or one of the other scholarly DOI registration agencies. Users want to search for content or look up metadata for a particular DOI, and not worry about where to look. DataCite initially focused on registering DOIs for datasets (approaching 8 million DOIs so far), but our members to date have also registered almost 6 million DOIs for text publications. At the same time, Crossref members have given almost 2 million DOIs to datasets in addition to the DOIs for journal articles, book chapters, and other text publications. Other content types can be equally found at both DataCite and Crossref, e.g. dissertations or preprints. And there are 6 more <a href="https://www.doi.org/registration_agencies.html">DOI registration agencies</a> that register DOIs for scholarly content. Including the more than 110 million Crossref DOIs in DataCite Commons is a huge undertaking. We currently have 10 million Crossref DOIs in DataCite Commons with the import of many more DOIs ongoing, together with 20 million DOIs from DataCite.</p>
<h3 id="connections">Connections</h3>
<p>DataCite Commons not only has a lot more content to search for but also exposes the connections between DOIs in the form of citations, versions, and collections. DataCite Commons also shows the connections between content with DOIs and people, research organizations, and funders – what we together call the PID Graph of scholarly resources identified via persistent identifiers (PIDs) and connected in standard ways. We integrate with both the <a href="https://orcid.org/">ORCID</a> and <a href="https://ror.org/">ROR</a> (Research Organization Registry) APIs to enable a search for (10 million) people and (100,000) organizations and to show the associated content. For funding, we take advantage of the inclusion of Crossref Funder IDs in ROR metadata. We combine these connections, showing a funder, research organization, or researcher not only their content but also the citations and views and downloads if available, together with aggregate statistics such as numbers by year or content type.</p>
<figure>
<img src="https://blog.datacite.org/images/uploads/bildschirmfoto-2020-08-25-um-06.34.23.png" title="Works by organization" class="kg-image" />
</figure>
<p>For a single work, e.g. the dataset registered with DOI <a href="https://commons.datacite.org/doi.org/10.5061/dryad.234">https://doi.org/10.5061/dryad.234</a>, we show views, downloads and citations if available:</p>
<figure>
<img src="https://blog.datacite.org/images/uploads/bildschirmfoto-2020-08-25-um-06.33.07.png" title="Views, downloads and citations" class="kg-image" />
</figure>
<h3 id="metadata">Metadata</h3>
<p>By mapping all Crossref metadata to corresponding metadata in DataCite, we can support much more granular search queries compared to just mapping basic metadata. With this release, we are also launching a new set of filters for content search. We added license type, fields of science, primary language, and DOI registration agency to the existing filters publication year and work type. As described in a July blog post (Fenner, <a href="https://blog.datacite.org/power-of-pids/#ref-https://doi.org/10.5438/1dgk-1m22">2020a</a>), we are using existing controlled vocabularies for these filters (license type: <a href="https://spdx.dev/">SPDX</a>, fields of science: <a href="https://www.oecd.org/science/inno/38235147.pdf">OECD</a>, and language: <a href="https://www.iso.org/iso-639-language-codes.html">ISO639-1</a>), and are re-indexing all our metadata (almost completed) to align with these standard vocabularies where possible. We encourage our members to use these standard vocabularies when registering content. This should help to find content that has a license that allows unrestricted re-use, and that is in the research field and language we are interested in. Using these widely used vocabularies should help with interoperability with other services.</p>
<figure>
<img src="https://blog.datacite.org/images/uploads/bildschirmfoto-2020-08-25-um-06.29.39.png" title="Filtering by license" class="kg-image" />
</figure>
<h3 id="technology">Technology</h3>
<p>To make DataCite Commons possible, we built a technology platform that can properly handle the metadata from multiple sources and the rich connections between them. The underlying technology is <a href="https://graphql.org/">GraphQL</a>. Our GraphQL API launched in May 2020 (Fenner, <a href="https://blog.datacite.org/power-of-pids/#ref-https://doi.org/10.5438/yfck-mv39">2020b</a>) and uses the <a href="https://graphql-ruby.org/">graphql-ruby</a> library that also powers the GitHub GraphQL API. The DataCite Commons web frontend is built with <a href="https://reactjs.org/">React</a> (together with <a href="https://www.apollographql.com/docs/react/">Apollo Client</a>), a popular Javascript Framework, to interact with this GraphQL API. Everything we have built is based on open source software and is made available (<a href="https://github.com/datacite/lupo">API</a> and <a href="https://github.com/datacite/akita">web frontend</a>) with a permissive open source license. As always, we welcome contributions to our source code and are more than happy to help others work with GraphQL.</p>
<h3 id="project-freya">Project FREYA</h3>
<p>The work on DataCite Commons is part of the <a href="https://www.project-freya.eu/en">FREYA project</a> that is helping build the European Open Science Cloud (EOSC), funded by the European Commission. DataCite Commons fulfills the specific project goals of delivering one Common DOI Search for DOIs from all DOI registration agencies, and of providing an easy to use interface for the PID Graph powered by GraphQL. FREYA will end in November 2020, and we will use the remaining three months to improve the service based on the input we collected so far, and the feedback we will receive with this release. We will focus on supporting researchers and other end-users, building on the schema.org metadata export (Cousijn, Cruse, &amp; Fenner, <a href="https://blog.datacite.org/power-of-pids/#ref-https://doi.org/10.5438/5aep-2n86">2018</a>), citation formatting, and ORCID claiming (Fenner, <a href="https://blog.datacite.org/power-of-pids/#ref-https://doi.org/10.5438/15x1-bj6r">2015</a>) available in DataCite Search. What we released today is the first public version of the service, we will continue adding many more Crossref DOIs and more connections, and work on improving the performance as the system scales. Beyond FREYA, DataCite Commons will be maintained and further developed by DataCite in coordination with other PID providers and the broader open science community. Watch out for a FREYA webinar on DataCite Commons in September.</p>
<h3 id="next-steps">Next steps</h3>
<p>While DataCite Commons is open to everyone to help with the discovery of scholarly resources and its connections that are part of the PID Graph, it provides particular value to DataCite members. The service makes their metadata and content available to a wide audience, helps them discover and report connections such as citations and affiliation and funding information, and provides an open platform for further integrations with other services going forward. We will closely work with DataCite members to further align the new service with their needs and the needs of the communities they serve. We will be actively seeking input as we continue to build on the DataCite Commons and there will be a dedicated Open Hours session in the coming months. If you have feedback at this point, please reach out to <a href="mailto:support@datacite.org">DataCite Support</a> or post a message in the DataCite channel of the PID Forum.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/f4df-4817">originally published</a> on the DataCite Blog.</em></p>
<h3 id="references">References</h3>
<p>Cousijn, H., Cruse, P., &amp; Fenner, M. (2018). Taking discoverability to the next level: Datasets with datacite dois can now be found through google dataset search. <a href="https://doi.org/10.5438/5AEP-2N86">https://doi.org/10.5438/5AEP-2N86</a></p>
<p>Fenner, M. (2015). Announcing the datacite profiles service. <a href="https://doi.org/10.5438/15X1-BJ6R">https://doi.org/10.5438/15X1-BJ6R</a></p>
<p>Fenner, M. (2020a). Making the most out of available metadata. <a href="https://doi.org/10.5438/1DGK-1M22">https://doi.org/10.5438/1DGK-1M22</a></p>
<p>Fenner, M. (2020b). Powering the pid graph: Announcing the datacite graphql api. <a href="https://doi.org/10.5438/YFCK-MV39">https://doi.org/10.5438/YFCK-MV39</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The DataCite MDC Stack]]></title>
        <id>5y5n8pn-tyf8bar-y2yfp2s-ss69y</id>
        <link href="https://blog.front-matter.io/mfenner/the-datacite-mdc-stack"/>
        <updated>2020-06-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[In May, the Make Data Count team announced that we have received additional funding from the Alfred P. Sloan Foundation for work on the Make Data Count (MDC) initiative. This will enable DataCite to do additional work in two important areas:Implement...]]></summary>
        <content type="html"><![CDATA[<p>In May, the Make Data Count team <a href="https://makedatacount.org/2020/05/05/igniting-change-our-next-steps/">announced</a> that we have received additional funding from the Alfred P. Sloan Foundation for work on the Make Data Count (MDC) initiative. This will enable DataCite to do additional work in two important areas:</p>
<ul>
<li>Implement a bibliometrics dashboard that enables bibliometricians – funded by a <a href="https://www.scholcommlab.ca/2020/05/04/sloan-announcement/">separate Sloan grant</a> – to do quantitative studies around data usage and citation behaviors.</li>
<li>Increase adoption of standardized data usage across repositories by developing a log processing service that offloads much of the hard work from repositories.</li>
</ul>
<p>In this blog post, we want to provide more technical details about the upcoming work on the bibliometrics dashboard; the log processing service will be the topic of a future blog post. The bibliometrics dashboard will be based on several important infrastructure pieces that DataCite has built over the past few years, and that are again briefly described below.</p>
<figure>
<img src="https://blog.datacite.org/images/uploads/noun_dashboard_2172952.png" class="kg-image" />
</figure>
<h3 id="doi-registration-services">DOI registration services</h3>
<p>In the MDC initiative we track data citations in the scholarly literature, focussing on datasets registered with DataCite and publications registered with Crossref.</p>
<h3 id="event-data">Event Data</h3>
<p>We use the joint <a href="https://support.datacite.org/docs/eventdata-guide">Crossref/DataCite Event Data</a> service to exchange information about connections between publications and datasets, contributed via Crossref and DataCite members through the metadata they register. These connections are also made available via a <a href="http://www.scholix.org/">Scholix</a>-compliant REST API. In the previous MDC project, the Event Data service was expanded to include data usage stats and make retrieving information easier for DataCite members.</p>
<h3 id="data-usage-reports">Data Usage Reports</h3>
<p>DataCite members and repositories upload monthly reports about data usage to DataCite using a standard format (<a href="https://www.projectcounter.org/counter-code-practice-research-data-usage-metrics-release-1/">COUNTER Code of Practice for Research Data Usage Metrics</a> and protocol (<a href="https://www.projectcounter.org/code-of-practice-sections/sushi/">SUSHI</a>). COUNTER Code of Practice for Research Data Usage Metrics and the DataCite <a href="https://support.datacite.org/docs/usage-reports-api-guide">usage reports API</a> were developed in the previous MDC project.</p>
<h3 id="graphql-api">GraphQL API</h3>
<p>The DataCite GraphQL API [Fenner (<a href="https://blog.datacite.org/datacite-mdc-stack/#ref-https://doi.org/10.5438/yfck-mv39">2020</a>)] built in the EC-funded <a href="https://www.project-freya.eu/en">FREYA</a> project brings together all of the above information in a single API that supports the complex queries typically needed for retrieving aggregated data citation information.</p>
<h3 id="jupyter-notebooks">Jupyter Notebooks</h3>
<p>We use Jupyter notebooks to analyze and visualize the information made available in the GraphQL API [Fenner (<a href="https://blog.datacite.org/datacite-mdc-stack/#ref-https://doi.org/10.5438/hwaw-xe52">2019</a>)], and have developed documentation, demos, and training material with our partners in the FREYA project.</p>
<h3 id="common-doi-search">Common DOI Search</h3>
<p>Common DOI Search, a service currently under development by DataCite with help from Crossref and others in the FREYA project, and with a first version planned to be released in August, will bring a single search interface for all scholarly DOIs, no matter from which DOI registration agency (DataCite, Crossref, etc.). All DOIs in Common DOI Search are in a single Elasticsearch search cluster using the same DataCite metadata schema.</p>
<h3 id="data-metrics-badge">Data Metrics Badge</h3>
<p>The Data Metrics Badge – developed as part of the <a href="http://www.belmontforum.org/projects/4057/">Parsec</a> project – is an <a href="https://support.datacite.org/docs/displaying-usage-and-citations-in-your-repository">easy to install</a>Javascript widget that displays up-to-date citations, views, and downloads for a single DOI, and links to the DataCite Search page for more detailed information.</p>
<h3 id="researcher-profile">Researcher Profile</h3>
<p>Also as part of the PARSEC project, we have built the <a href="https://support.datacite.org/docs/datacite-researcher-profiles">Researcher Profile</a> that, using the researcher's ORCID ID, brings all academic outputs and their metrics for a given researcher into a single dashboard. This work serves as a blueprint for other aggregations (e.g. by research organization) in the bibliometrics dashboard.</p>
<h3 id="bibliometrics-dashboard">Bibliometrics Dashboard</h3>
<p>All the services described above are required building blocks for the bibliometrics dashboard we will start working on in August. What the dashboard will add is better insights into the data citation data we have collected, primarily helping the bibliometricians in the project, but also available to other users. We will use Jupyter notebooks for exploratory analyses and to address very specific research questions, and data visualizations in the bibliometrics dashboard that address the most common questions, such as the growth of data citations over time.</p>
<p>The bibliometrics dashboard will expand the common DOI search service that we are currently building, beyond FREYA, which ends in November. Common DOI search, and also the bibliometrics dashboard, are built using <a href="https://reactjs.org/">React</a>, not only the most popular Javascript framework right now, but also integrating very nicely with GraphQL APIs. More specifically we are using <a href="https://nextjs.org/">next.js</a> to run react on the server, helping with faster page loading and search engine optimization (SEO).</p>
<p>We have picked the popular <a href="https://vega.github.io/vega/">Vega</a> library for our data visualizations. Vega is not only widely used and very flexible, but also available in versions for Jupyter notebooks (<a href="https://altair-viz.github.io/getting_started/installation.html">Altair</a>) and React (<a href="https://github.com/vega/react-vega">React-Vega</a>).</p>
<h3 id="using-the-bibliometrics-dashboard">Using the Bibliometrics Dashboard</h3>
<p>DataCite members and the repositories they work with contribute to the bibliometrics dashboard in important ways, registering content with a DOI and standard metadata facilitating citation, inclusion of references in the metadata, and submission of data repository usage stats. The bibliometrics dashboard will increase our understanding of data citation and data usage stats through the bibliometrics work, but will also provide aggregations of information of interest to our members – for example data citations and data usage over time, by discipline or by repository – not available before. This information is displayed in the bibliometrics dashboard, and available via Jupyter notebooks and the GraphQL API.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/v9pp-7a27">originally published</a> on the DataCite Blog.</em></p>
<h3 id="references">References</h3>
<p>Fenner, M. (2019). Using jupyter notebooks with graphql and the pid graph. <a href="https://doi.org/10.5438/HWAW-XE52">https://doi.org/10.5438/HWAW-XE52</a></p>
<p>Fenner, M. (2020). Powering the pid graph: Announcing the datacite graphql api. <a href="https://doi.org/10.5438/YFCK-MV39">https://doi.org/10.5438/YFCK-MV39</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Powering the PID Graph: announcing the DataCite GraphQL API]]></title>
        <id>3nyzx03-psc9b1b-mz0kthr-3ewat</id>
        <link href="https://blog.front-matter.io/mfenner/powering-the-pid-graph-announcing-the-datacite-graphql-api"/>
        <updated>2020-05-06T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Today DataCite launches a new API that powers the PID Graph, the graph formed by scholarly resources described by persistent identifiers (PIDs) and the connections between them. The API is powered by GraphQL, a widely adopted Open Source technology that enables queries of this graph,...]]></summary>
        <content type="html"><![CDATA[<p>Today DataCite launches a new API that powers the PID Graph, the graph formed by scholarly resources described by persistent identifiers (PIDs) and the connections between them. The API is powered by GraphQL, a widely adopted Open Source technology that enables queries of this graph, addressing use cases of our community in ways that were not possible before.</p>
<p>We launched a pre-release version of the API in May 2019 (Fenner (<a href="https://blog.datacite.org/powering-the-pid-graph/#ref-https://doi.org/10.5438/qab1-n315">2019b</a>))[], and have used the last 12 months to improve the performance and stability of the service, add functionality based on user feedback, decide on a stable GraphQL schema that describes the resources and links in the Graph, and add many additional resources. The PID Graph now includes all of DataCite's DOIs, nine million Crossref DOIs, all ORCID IDs, and all Research Organization Registry (ROR), Crossref Funder ID, and Registry of Research Data Repositories (re3data) records, for a total of about 35 million resources with PIDs and associated metadata, and about 9 million links between them.</p>
<figure>
<img src="https://blog.datacite.org/images/uploads/pidgraph-2020-05-04.png" title="**PID Graph KPI**: Number of resources and links in the PID Graph available via GraphQL API as of May 4, 2020. Generated using @https://doi.org/10.14454/3bpw-w381" class="kg-image" alt="PID Graph KPI: Number of resources and links in the PID Graph available via GraphQL API as of May 4, 2020. Generated using (Fenner (2019a))." /><figcaption aria-hidden="true"><strong><strong>PID Graph KPI</strong></strong>: Number of resources and links in the PID Graph available via GraphQL API as of May 4, 2020. Generated using (Fenner (<a href="https://blog.datacite.org/powering-the-pid-graph/#ref-https://doi.org/10.14454/3bpw-w381">2019a</a>)).</figcaption>
</figure>
<p>The PID Graph and the <a href="https://graphql.org/">GraphQL</a> API announced today are an important output from the European Commission-funded <a href="https://www.project-freya.eu/en">FREYA Project</a> (grant agreement No 777523), and have been developed in close collaboration with all FREYA partners, including the PID providers Crossref and ORCID. PID Graph is part of the Research Data Alliance (RDA) <a href="https://www.rd-alliance.org/groups/open-science-graphs-fair-data-ig">Open Science Graphs for FAIR Interest Group</a>, where we coordinate with other initiatives building Open Science graphs.</p>
<p>FREYA partners have used the last 12 months to start building client applications that take advantage of the GraphQL API, and you will see two new services built by DataCite using GraphQL launching later this year. While GraphQL is supported by a <a href="https://graphql.org/code/">wide variety of programming languages</a>, with lots of documentation and community support available – including of course the <a href="https://support.datacite.org/docs/datacite-graphql-api-guide">DataCite GraphQL API Guide</a> - it is a relatively new technology for scholarly communication infrastructure. We found that Jupyter notebooks are a powerful way to get started with the PID Graph and the DataCite GraphQL API Fenner (<a href="https://blog.datacite.org/powering-the-pid-graph/#ref-https://doi.org/10.5438/hwaw-xe52">2019c</a>), and you find a number of example notebooks via the PID Graph section in the <a href="https://www.pidforum.org/c/pid-graph/17">PID Forum</a>, which is also a good place to post questions or comments. The visualization in this blog post is of course also generated by a Jupyter notebook using the DataCite GraphQL API Fenner (<a href="https://blog.datacite.org/powering-the-pid-graph/#ref-https://doi.org/10.14454/3bpw-w381">2019a</a>). Expect more Jupyter notebooks coming from FREYA in the coming months, addressing specific user stories that we have identified earlier in the project.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/yfck-mv39">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>Fenner, M. (2019a). FREYA pid graph key performance indicators (kpis). DataCite. <a href="https://doi.org/10.14454/3BPW-W381">https://doi.org/10.14454/3BPW-W381</a></p>
<p>Fenner, M. (2019b). The datacite graphql api is now open for (pre-release) business. <a href="https://doi.org/10.5438/QAB1-N315">https://doi.org/10.5438/QAB1-N315</a></p>
<p>Fenner, M. (2019c). Using jupyter notebooks with graphql and the pid graph. <a href="https://doi.org/10.5438/HWAW-XE52">https://doi.org/10.5438/HWAW-XE52</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[2020 Strategic Priorities for Services and Infrastructure]]></title>
        <id>502dcxa-rna9rbs-cew7d7r-pspb4</id>
        <link href="https://blog.front-matter.io/mfenner/2020-strategic-priorities-for-services-and-infrastructure"/>
        <updated>2020-02-27T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[In a blog post four weeks ago DataCite Executive Director Matt Buys talked about the DataCite strategic priorities for 2020 (Buys, 2020). In this post we want to talk a bit more about the strategic priorities for this year we have regarding services and infrastructure work: a)...]]></summary>
        <content type="html"><![CDATA[<p>In a blog post four weeks ago DataCite Executive Director Matt Buys talked about the DataCite strategic priorities for 2020 (Buys, <a href="https://blog.datacite.org/2020-strategic-priorities-for-services-and-infrastructure/#ref-https://doi.org/10.5438/9j86-bv91">2020</a>). In this post we want to talk a bit more about the strategic priorities for this year we have regarding services and infrastructure work: a) consolidation of our services and infrastructure, and b) stronger emphasis on member-driven product development. They are based on the clear message we heard from our members, e.g. in the member survey that we shared yesterday (Cousijn, <a href="https://blog.datacite.org/2020-strategic-priorities-for-services-and-infrastructure/#ref-https://doi.org/10.5438/0x81-y943">2020</a>).</p>
<h3 id="consolidation-of-our-services-and-infrastructure">Consolidation of our services and infrastructure</h3>
<p>The focus of development work over the past few years has been on upgrading existing DataCite infrastructure and building new services. With the relaunch of the OAI-PMH service in December 2019 (Hallett, <a href="https://blog.datacite.org/2020-strategic-priorities-for-services-and-infrastructure/#ref-https://doi.org/10.5438/ppth-pz62">2019</a>), we have now upgraded all major DataCite infrastructure that existed in 2015. For 2020, we want to spend more time consolidating our existing services and infrastructure, scaling back new service development, of course within the ramifications of what we promised to deliver in ongoing grants such as <a href="https://www.project-freya.eu/en">FREYA</a>. The work planned includes</p>
<ul>
<li>Better monitoring and analytics of our services, e.g. better tracking of API performance</li>
<li>Prioritize stability of our infrastructure, e.g. spending more time fixing bugs</li>
<li>Consolidate the number of services available to our members, e.g. merging the public DataCite Search into the Fabrica service</li>
</ul>
<h3 id="stronger-emphasis-on-member-driven-product-development">Stronger emphasis on member-driven product development</h3>
<p>This strategic priority includes two aspects: improving our process of feedback collection from DataCite members and how that informs our product development, and prioritizing our development efforts on new services or service improvements asked for by our members.</p>
<p>In 2020 we will improve the existing process of collecting input from our members regarding product development. These improvements will be based on how we currently collect member input via support emails, Open Hours, webinars and the DataCite General Assembly, other conversations with members, <a href="https://github.com/datacite/datacite/issues">GitHub issues</a> opened by members and users, and our <a href="https://datacite.org/roadmap.html">public roadmap</a> that allows comments and upvoting.</p>
<p>While continuing to improve the process of collecting feedback from members, there are a number of new services or service improvements that have been consistently asked for by our members, and that we will prioritize working on in 2020. This includes improvements of the DOI registration form in Fabrica to cover all optional metadata properties, a public dump of all DOI metadata available for download, improvements to the query interface for searching for DOIs, and better support for text documents in the metadata schema (e.g. support for journal name, volume, issue, and page numbers).</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/9te8-5h68">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>Buys, M. (2020). Vision 2020. <a href="https://doi.org/10.5438/9J86-BV91">https://doi.org/10.5438/9J86-BV91</a></p>
<p>Cousijn, H. (2020). Setting datacites priorities: The 2019 member survey. <a href="https://doi.org/10.5438/0X81-Y943">https://doi.org/10.5438/0X81-Y943</a></p>
<p>Hallett, R. (2019). OAI-pmh service updates. <a href="https://doi.org/10.5438/PPTH-PZ62">https://doi.org/10.5438/PPTH-PZ62</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Announcing the new Member API]]></title>
        <id>7pqejsg-1hk9hc9-5wj0s1e-66zbx</id>
        <link href="https://blog.front-matter.io/mfenner/announcing-the-new-member-api"/>
        <updated>2020-01-20T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[When we launched the new version of the OAI-PMH service in November (Hallett (2019)), and retired Solr (used by the old OAI-PMH service) in December, we completed the transition to Elasticsearch as our search index, and the REST API as our main API....]]></summary>
        <content type="html"><![CDATA[<p>When we launched the new version of the OAI-PMH service in November (Hallett (<a href="https://blog.datacite.org/announcing-member-api/#ref-https://doi.org/10.5438/ppth-pz62">2019</a>)), and retired Solr (used by the old OAI-PMH service) in December, we completed the transition to Elasticsearch as our search index, and the <a href="https://api.datacite.org/">REST API</a> as our main API. All our services now integrate via Elasticsearch and the REST API, including:</p>
<ol>
<li><a href="https://mds.datacite.org/">MDS API</a> - DOI registration API</li>
<li><a href="https://ez.datacite.org/">EZ API</a> - DOI registration API, compatible with CDL EZID service</li>
<li><a href="https://doi.datacite.org/">Fabrica</a> - DOI registration and account management web interface</li>
<li><a href="https://oai.datacite.org/">OAI-PMH</a> - Metadata harvesting</li>
<li><a href="https://search.datacite.org/">DataCite Search</a> - public DOI search</li>
<li><a href="https://stats.datacite.org/">Stats Portal</a> - statistics about DOI registrations and resolutions</li>
<li><a href="https://data.datacite.org/">Content Negotiation</a> - DOI metadata in other formats</li>
</ol>
<p>This consolidation was a lot of work in 2019, but will greatly simplify the maintenance of our services and the development of new functionality going forward.!</p>
<p>This consolidation of course also has its downsides, and the most important one is supporting both member services and public services via the same API. While DataCite will always make all DOI metadata available in a public API and search interface, we also have to make sure our paying members get the best possible service quality when registering DOIs or otherwise interacting with DataCite services as member.</p>
<p>To address this challenge we have in December split the REST API into two versions: a <strong><strong>Public API</strong></strong> and a <strong><strong>Member API</strong></strong>. These two APIs use exactly the same URLs (starting with https://api.datacite.org), run exactly the same code, and provide exactly the same public data, the only difference being that traffic is directed to a different set of servers if users authenticate as a member. This change is subtle enough that you probably have not noticed the change yet.</p>
<p>The launch of separate <strong><strong>Public</strong></strong> and <strong><strong>Member</strong></strong> APIs allows us to monitor the uptime and response time for DataCite member services separately from our public services, and adjust as needed. We updated the <a href="https://status.datacite.org/">DataCite status page</a> to reflect this change, you can now see separate metrics (both response time and request count) for the Public API and Member API:</p>
<figure>
<img src="https://blog.datacite.org/images/uploads/bildschirmfoto-2020-01-20-um-10.40.10.png" class="kg-image" />
</figure>
<p>There are of course other approaches to separate out member API calls from public API calls, including separate API endpoints, e.g. pub.orcid.org vs. api.orcid.org at ORCID, or separate APIs for members and non-members, e.g. the separate DOI registration API at Crossref. But we feel that our approach for accessing the member vs. public API aligns best with our strategy of offering a consolidated REST API for all services.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/ysq3-p703">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>Hallett, R. (2019). OAI-pmh service updates. <a href="https://doi.org/10.5438/PPTH-PZ62">https://doi.org/10.5438/PPTH-PZ62</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Upcoming Changes to DOI Content Negotiation]]></title>
        <id>7bg9kjn-xba81at-kyz7fk8-4tzga</id>
        <link href="https://blog.front-matter.io/mfenner/upcoming-changes-to-doi-content-negotiation"/>
        <updated>2019-07-26T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[DOI content negotiation is one of the oldest DataCite services, launched in 2012. Content negotiation makes it easy to fetch DataCite metadata in other metadata formats, for example <em>BibTeX</em> or <em>schema.org</em>,...]]></summary>
        <content type="html"><![CDATA[<p>DOI content negotiation is one of the oldest DataCite services, launched in 2012. Content negotiation makes it easy to fetch DataCite metadata in other metadata formats, for example <em>BibTeX</em> or <em>schema.org</em>, or as formatted citation in one of more than 5,000 citation styles. For example:</p>
<pre><code>curl -LH &quot;Accept: application/x-bibtex&quot; https://doi.org/10.5438/0000-0C2G</code></pre>
<p>In 2017 we updated the service, both adding new content types (e.g. schema.org), and improving the support for existing content types (Fenner, <a href="https://blog.datacite.org/changes-to-doi-content-negotiation/#ref-https://doi.org/10.5438/0000-01qj">2017</a>).</p>
<p>In 2018 we launched a new version of the REST API (Dasler, <a href="https://blog.datacite.org/changes-to-doi-content-negotiation/#ref-https://doi.org/10.5438/s8rt-zv48">2018</a>) that complements content negotiation: you can now submit metadata in most of the metadata formats supported by content negotiation to register a DOI.</p>
<p>With this blog post we want to announce an important upcoming change to DataCite DOI content negotiation that addresses a long-standing issue. Content negotiation is used to provide different representations of the same resource in different formats. What content negotiation is not is a protocol to return the content itself described by the DOI instead of the metadata describing the content, e.g. a PDF or CSV file. The initial implementation of DataCite DOI content negotiation didn't make that clear distinction and allows registration of custom content types, including those that typically are associated with content rather than metadata, e.g. <code>application/pdf</code>. Registration of custom content types is still possible with the current version of the service, but we are today announcing that we will retire this functionality on January 1st, 2020.</p>
<p>We know that many organizations make heavy use of custom content types for DataCite DOIs, so we provide both a long transition period and alternative approaches to achieve the same.</p>
<p>On October 1st, 2019 we will retire support for custom content types via the DOI resolver <code>https://doi.org</code>. You can continue to use custom content types by using the DataCite content negotiation service at data.datacite.org directly, e.g.</p>
<pre><code>curl -LH &quot;Accept: application/x-bibtex&quot; https://data.datacite.org/10.5438/0000-0C2G</code></pre>
<p>Starting January 1st, 2020 custom content types will no longer be supported.</p>
<p>There are two alternatives to using custom content types in content negotiation:</p>
<ul>
<li>use the REST API, specifically the <code>/media</code> API endpoint to access exactly the information currently made available via content negotiation</li>
<li>use content negotiation at the landing page for the resource that the DOI resolves to. DataCite content negotiation is forwarding all requests with unknown content types to the URL registered in the handle system.</li>
</ul>
<p>Going forward, the upcoming changes will lead to improvements in two areas:</p>
<ul>
<li>the content negotiation service will become faster, easier to maintain, and more aligned with other DOI registration agencies such as Crossref. The new service is already running at https://data.crosscite.org.</li>
<li>we can improve the workflows for registering URLs to content in the DOI registration service. This will enable direct access to content, and will improve machine access to data.</li>
</ul>
<p>Please read our <a href="https://support.datacite.org/docs/datacite-content-resolver">support documentation for content negotiation</a> for more details, or reach out to <a href="mailto:support@datacite.org">DataCite support</a> if you need help transitioning from custom content types you have registered in the past.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/nz0m-rb06">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>Dasler, R. (2018). Create dois with the rest api. <a href="https://doi.org/10.5438/S8RT-ZV48">https://doi.org/10.5438/S8RT-ZV48</a></p>
<p>Fenner, M. (2017). A content negotiation update. <a href="https://doi.org/10.5438/0000-01QJ">https://doi.org/10.5438/0000-01QJ</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tracking the Growth of the PID Graph]]></title>
        <id>1xfhsf7-8c89qhb-gss9qds-ezav2</id>
        <link href="https://blog.front-matter.io/mfenner/tracking-the-growth-of-the-pid-graph"/>
        <updated>2019-07-01T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The connections between scholarly resources generated by persistent identifiers (PIDs) and associated metadata form a graph: the PID Graph [Fenner &amp; Aryani (2019)]. We developed this PID Graph concept in the EC-funded FREYA project,...]]></summary>
        <content type="html"><![CDATA[<p>The connections between scholarly resources generated by persistent identifiers (PIDs) and associated metadata form a graph: the PID Graph [Fenner &amp; Aryani (<a href="https://blog.datacite.org/tracking-the-growth-of-the-pid-graph/#ref-https://doi.org/10.5438/jwvf-8a66">2019</a>)]. We developed this PID Graph concept in the EC-funded <a href="https://www.project-freya.eu/en">FREYA project</a>, and have identified important use cases and technical requirements. In May, DataCite introduced a GraphQL API to standardize and simplify how users can contribute to and consume the PID Graph [Fenner (<a href="https://blog.datacite.org/tracking-the-growth-of-the-pid-graph/#ref-https://doi.org/10.5438/qab1-n315">2019b</a>)]. Today we are announcing another important milestone: we added the required functionality to the DataCite GraphQL API that allows us to keep track of the growth of the PID Graph in terms of nodes (resources) and edges (connections). As before, we are using a Jupyter notebook to analyze and visualize the data.</p>
<figure>
<img src="https://blog.datacite.org/images/uploads/download-19-.jpeg" title="PID Graph: number of nodes and connections" class="kg-image" />
</figure>
<p><strong><strong>Number of nodes and connections in the PID Graph</strong></strong></p>
<p>The graph visualizes the main resources currently available via the DataCite GraphQL API, and their connections. The required data are fetched via a single GraphQL API call:</p>
<pre><code>{
  publications {
    totalCount
    publicationConnectionCount
    datasetConnectionCount
    softwareConnectionCount
    researcherConnectionCount
    funderConnectionCount
  }
  datasets {
    totalCount
    datasetConnectionCount
    softwareConnectionCount
    researcherConnectionCount
    funderConnectionCount
  }
  softwares {
    totalCount
    softwareConnectionCount
    researcherConnectionCount
    funderConnectionCount
  }
  researchers {
    totalCount
  }
  funders {
    totalCount
  }
}</code></pre>
<p>The numbers reflect what is currently available via the DataCite GraphQL API, not the total number of publications, datasets, etc. with persistent identifiers and linking metadata. This includes all publications, datasets and software with DataCite DOIs, all funders in the <a href="https://support.crossref.org/hc/en-us/articles/214360886-The-Open-Funder-Registry">Crossref Open Funder Registry</a>, publications with Crossref DOIs linked to at least one DataCite DOI, and all researchers with an ORCID identifier linked to at least one DataCite DOI. A lot of work remains to be done to include the other resources with persistent identifiers made available by FREYA partners, including Crossref, ORCID and EMBL-EBI, as well as their connections. With this new API functionality we can now track the growth of the PID Graph with the key performance indicators (KPIs) number of <strong><strong>nodes</strong></strong> and number of <strong><strong>connections</strong></strong>.</p>
<p>A few interesting observations can be made from the visualization: not surprisingly, given that datasets currently make up the largest number of resources in the PID Graph, by far the largest number of connections (9.4 million) is between datasets and other datasets. Looking at the relation type of these connections in the Event Data API, most of them (8.5 million) don't describe versioning or granularity (HasPart/IsPartOf relations), but use the relation type <strong><strong>references</strong></strong> between two <a href="https://www.gbif.org/en/">GBIF</a> DOIs. This reflects the main <a href="https://www.gbif.org/en/document/81771/gbif-overview-powerpoint-slides">use case for DOIs at GBIF</a>, tracking occurrences of species.</p>
<p>The graph shows 1.5 million connections between publications and datasets, representing the data citations made available via the <a href="https://www.eventdata.crossref.org/guide/app-scholix/">Crossref/DataCite Event Data Scholix API endpoint</a>. Twenty-five percent of datasets with DataCite DOIs have been referenced in the scholarly literature, according to this graph. We also see the number software citations using DataCite DOIs for software found in the scholarly literature, and we can use this API call to keep track of them. Similarly, we can track the number of datasets, publications and software linked to researchers (via their ORCID ID), and funding (via the Crossref Funder ID), but keep in mind that connections to researchers and funding via Crossref are still missing.</p>
<p>The Jupyter notebook used to generate the visualization shown here is available via GitHub, where we store all PID Graph notebooks in a <a href="https://github.com/datacite/notebooks">central repository</a>. Starting with this notebook [Fenner (<a href="https://blog.datacite.org/tracking-the-growth-of-the-pid-graph/#ref-https://doi.org/10.14454/3bpw-w381">2019a</a>)] we are also issuing DOIs for the notebooks, which we generate using a <a href="https://codemeta.github.io/">codemeta</a> file hosted in the same folder as the notebook. This should make these notebooks easier to discover, and they also become part of the PID Graph.</p>
<p>Going forward we want to refine the GraphQL API to provide the numbers of <strong><strong>nodes</strong></strong> and <strong><strong>connections</strong></strong> as they change over time, making it easier to track progress. And of course we will be adding more resource types and information from other FREYA partners. Keep an eye on this blog for further updates!</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/bv9z-dc66">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>Fenner, M. (2019a). FREYA pid graph key performance indicators (kpis). DataCite. <a href="https://doi.org/10.14454/3BPW-W381">https://doi.org/10.14454/3BPW-W381</a></p>
<p>Fenner, M. (2019b). The datacite graphql api is now open for (pre-release) business. <a href="https://doi.org/10.5438/QAB1-N315">https://doi.org/10.5438/QAB1-N315</a></p>
<p>Fenner, M., &amp; Aryani, A. (2019). Introducing the pid graph. <a href="https://doi.org/10.5438/JWVF-8A66">https://doi.org/10.5438/JWVF-8A66</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Jupyter Notebooks with GraphQL and the PID Graph]]></title>
        <id>6c444ws-z7g8yj9-ybn7gvx-n3g23</id>
        <link href="https://blog.front-matter.io/mfenner/using-jupyter-notebooks-with-graphql-and-the-pid-graph"/>
        <updated>2019-05-27T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Two weeks ago DataCite announced the pre-release version of a GraphQL API [Fenner (2019)]. GraphQL simplifies complex queries that for example want to retrieve information about the authors, funding and data citations for a dataset with a DataCite DOI....]]></summary>
        <content type="html"><![CDATA[<p>Two weeks ago DataCite announced the pre-release version of a GraphQL API [Fenner (<a href="https://blog.datacite.org/using-jupyter-notebooks-with-graphql-and-the-pid-graph/#ref-https://doi.org/10.5438/qab1-n315">2019</a>)]. GraphQL simplifies complex queries that for example want to retrieve information about the authors, funding and data citations for a dataset with a DataCite DOI. These connections together form the PID Graph [Fenner &amp; Aryani (<a href="https://blog.datacite.org/using-jupyter-notebooks-with-graphql-and-the-pid-graph/#ref-https://doi.org/10.5438/jwvf-8a66">2019</a>)], and DataCite is working with the other partners in the EC-funded <a href="https://www.project-freya.eu/">FREYA project</a> on making it easier to contribute to the PID Graph, and consume information in the PID Graph.</p>
<p><a href="https://jupyter.org/">Jupyter notebooks</a> are a very popular web-based interactive computational environment and are the perfect platform to explore the PID Graph via GraphQL APIs. Since interactions with GraphQL APIs are standardized and GraphQL libraries exist for many programming languages supported by Jupyter notebooks, all the user has to do is come up with interesting queries and process the information returned from the API as JSON, following exactly the format of the query.</p>
<p>An example notebook can best explain this. We have created a GitHub repository for notebooks using the GraphQL API at <a href="https://github.com/datacite/notebooks">https://github.com/datacite/notebooks</a>, and you find <a href="https://github.com/datacite/notebooks/blob/master/pid-graph/r-person-publications/r-person-publications.ipynb">this notebook</a> in there. Open the notebook in your favorite Jupyter client (e.g. <a href="https://nteract.io/">nteract</a>) or look at it directly in GitHub.</p>
<p>The GraphQL query in the notebook is as follows:</p>
<pre><code>{
  researcher(id: &quot;https://orcid.org/0000-0003-1419-2405&quot;) {
    id
    name
    publications(first: 50) {
      totalCount
      nodes {
        id
        relatedIdentifiers {
          relatedIdentifier
        }
      }
    }
  }
}</code></pre>
<p>This query will return the first 50 publications with DataCite DOIs linked to my ORCID ID, together with information about content referenced by these publications. The Jupyter notebook is then processing the JSON API response with two major outputs:</p>
<ul>
<li>two reference lists of all publications, and of all their references, using the APA citation style and the <a href="https://citation.crosscite.org/">DOI citation formatter service</a>.</li>
<li>a network graph of all publications and their references (blue circles), with a node (green circle) representing the author of these publications (me):</li>
</ul>
<figure>
<img src="https://blog.datacite.org/images/uploads/download-3-.jpeg" title="Force-directed graph of publications by a particular researcher and their references" class="kg-image" />
</figure>
<p>Even with only 50 publications the graph already is rather complicated. Many of my publications with DataCite DOIs are for the DataCite Blog, and you see them connected to a blue node in the top part of the graph. In the lower left corner you see a blog post with an unusually high number of references (Fenner, <a href="https://blog.datacite.org/using-jupyter-notebooks-with-graphql-and-the-pid-graph/#ref-https://doi.org/10.5438/ct8b-x1ce">2016</a>). A number of publications appear as pairs linked to each other, reflecting the figshare approach to versioning.</p>
<p>This is only the starting point of what can be done with Jupyter notebooks and GraphQL, but it is clear that the possibilities are almost endless. You can use the above notebook as a starting point, e.g. to generate the graph of publications (with DataCite DOIs) using your ORCID ID. Or you do something very different, or use Python instead of R as programming language. You can of course contribute interesting notebooks to the above GitHub repository using a pull request.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/hwaw-xe52">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>Fenner, M. (2016). Mysteries in reference lists. <a href="https://doi.org/10.5438/CT8B-X1CE">https://doi.org/10.5438/CT8B-X1CE</a></p>
<p>Fenner, M. (2019). The datacite graphql api is now open for (pre-release) business. <a href="https://doi.org/10.5438/QAB1-N315">https://doi.org/10.5438/QAB1-N315</a></p>
<p>Fenner, M., &amp; Aryani, A. (2019). Introducing the pid graph. <a href="https://doi.org/10.5438/JWVF-8A66">https://doi.org/10.5438/JWVF-8A66</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The DataCite GraphQL API is now open for (pre-release) business]]></title>
        <id>1y32yvy-1kz8yvv-3jpr151-nckjs</id>
        <link href="https://blog.front-matter.io/mfenner/the-datacite-graphql-api-is-now-open-for-pre-release-business"/>
        <updated>2019-05-15T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[DataCite DOIs describe resources such as datasets, samples, software and publications with rich metadata. An important part of this metadata is the description of connections between resources that use persistent identifiers (PIDs)...]]></summary>
        <content type="html"><![CDATA[<p>DataCite DOIs describe resources such as datasets, samples, software and publications with rich metadata. An important part of this metadata is the description of connections between resources that use persistent identifiers (PIDs) provided by DataCite and others (Crossref, ORCID, ROR, ISNI, IGSN, etc.). Together these resources and their connections form a graph, the PID Graph (Fenner &amp; Aryani, <a href="https://blog.datacite.org/graphql-api-pre-release/#ref-https://doi.org/10.5438/jwvf-8a66">2019</a>).</p>
<p>Accessing information available in this PID Graph, while preserving the rich connections between resources, is not trivial, and the JSON REST APIs that most PID service providers including DataCite are providing to users, while having a very good track record allowing users to access a single resource or a list of similar resources, might not be the best fit for more complex queries of the PID Graph.</p>
<p>Enter <a href="https://graphql.org/">GraphQL</a>, a query language that uses a graph as the underlying data model and aligns well with the kinds of queries that need to be supported in the PID Graph. GraphQL was started by Facebook in 2012, made available as Open Source software in 2015, and in 2019 has become a mainstream technology with broad support in terms of <a href="https://graphql.org/code/">libraries, tools and services</a>.</p>
<p>Today DataCite is announcing the pre-release version of the DataCite GraphQL API, and we invite you to try it out at <a href="https://api.datacite.org/graphql">https://api.datacite.org/graphql</a>. GraphQL works very differently from the REST APIs that most of us are familiar with: you need a special client application (e.g. <a href="https://electronjs.org/apps/graphiql">this one</a> or <a href="https://www.graphqlbin.com/">this one</a>) to use a GraphQL API, and all API calls are done to the same URL and using the POST method.</p>
<p>A typical starting point for a query in the PID Graph using GraphQL is a resource such as a dataset, researcher, organization, etc. identified by it's PID. For example:</p>
<pre><code>{
  dataset(id: &quot;https://doi.org/10.7910/dvn/nfzli3/cynkam&quot;) {
    titles {
      title
    }
  }
}</code></pre>
<pre><code>{
  publication(id: &quot;https://doi.org/10.5438/jwvf-8a66&quot;) {
    titles {
      title
    }
  }
}</code></pre>
<pre><code>{
  software(id: &quot;https://doi.org/10.5281/zenodo.1013940&quot;) {
    titles {
      title
    }
  }
}</code></pre>
<pre><code>{
  researcher(id: &quot;https://orcid.org/0000-0002-1642-628X&quot;) {
    givenName
    familyName
  }
}</code></pre>
<pre><code>{
  organization(id: &quot;https://ror.org/05rrcem69&quot;) {
    name
  }
}</code></pre>
<p>Some of these PIDs are obviously not provided by DataCite, but the DataCite GraphQL supports them as well via a wrapper layer over, for example, the <a href="https://ror.org/">ROR</a> API.</p>
<p>GraphQL allows you to specify exactly the fields the query should return, including linked resources. For example all publications (with a DataCite DOI) authored by a particular researcher, and including the titles and relatedIdentifiers for those publications:</p>
<pre><code>{
  researcher(id: &quot;https://orcid.org/0000-0002-4695-7874&quot;) {
    id
    name
    givenName
    familyName
    publications {
      totalCount
      edges {
        node {
          id
          titles {
            title
          }
          relatedIdentifiers {
            relationType
            relatedIdentifier
          }
        }
      }
    }
  }
}</code></pre>
<p>In addition to retrieving a specific resource using the PID, you can also do queries. For resources with DOIs, the query syntax is exactly the same as in DataCite Search or DOI Fabrica. For example:</p>
<pre><code>{
  softwares(query: &quot;subjects.subject:python&quot;) {
    totalCount
    
    nodes {
      id
      titles {
        title
      }
      fundingReferences {
        funderIdentifier
        funderName
        awardTitle
        awardNumber
      }
    }
  }
}</code></pre>
<div class="iframe">
<div id="player">

</div>
<section id="an-error-occurred." class="message player-unavailable">
<h1 class="message">An error occurred.</h1>
<div class="submessage">
<a href="https://www.youtube.com/watch?v=efvxGfU_oVM">Sieh dir dieses Video auf www.youtube.com an</a> oder aktiviere JavaScript, falls es in deinem Browser deaktiviert sein sollte.
</div>
</section>
</div>
<p>After this short introduction it should have become clearer how GraphQL works differently from your typical REST API, and that GraphQL is a perfect fit for the kinds of queries one might want to do with the PID Graph. Now that you have had an introduction to the DataCite GraphQL API, you can try your own queries.</p>
<p>The DataCite GraphQL API is a pre-release version, which means it's not yet final. It may contain bugs or might be missing functionality you expect. In particular more complex queries of the PID Graph are not yet supported. Your feedback is valuable for improving the API, so we encourage you to try it out and let us know what you think. Please use the <a href="https://www.pidforum.org/c/pid-graph">PID Graph section in the PID Forum</a> to ask questions or suggest features. We'll keep updating the API, and we aim to get a final release out sometime later this year.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/qab1-n315">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>Fenner, M., &amp; Aryani, A. (2019). Introducing the pid graph. <a href="https://doi.org/10.5438/JWVF-8A66">https://doi.org/10.5438/JWVF-8A66</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exposing DOI metadata provenance]]></title>
        <id>3m9ewcn-n4x8gx8-489n7kk-27nnf</id>
        <link href="https://blog.front-matter.io/mfenner/exposing-doi-metadata-provenance"/>
        <updated>2019-04-10T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[DOI metadata provenance is describing the history of a particular DOI metadata record, i.e. what changes were made when and by whom. This information is now stored and provided via an API for all DOI registrations since March 10,...]]></summary>
        <content type="html"><![CDATA[<p>DOI metadata provenance is describing the history of a particular DOI metadata record, i.e. what changes were made when and by whom. This information is now stored and provided via an API for all DOI registrations since March 10, 2019.</p>
<p>The following provenance information is now available via a new <code>/activities</code> REST API endpoint:</p>
<ul>
<li><strong><strong>prov:wasGeneratedBy</strong></strong>. The unique identifier for the activity making changes to a DOI record.</li>
<li><strong><strong>prov:generatedAtTime</strong></strong>. Timestamp of the activity.</li>
<li><strong><strong>prov:wasDerivedFrom</strong></strong>. The DOI for which the changes are being tracked.</li>
<li><strong><strong>prov:wasAttributedTo</strong></strong>. The client or provider account responsible for the changes.</li>
<li><strong><strong>action</strong></strong>. Can be either create, update or delete.</li>
<li><strong><strong>version</strong></strong>. Version number for the DOI record.</li>
<li><strong><strong>changes</strong></strong>. Changes made to DOI metadata, broken down by attribute, with both old and new value.</li>
</ul>
<p>The main use case is more transparency about changes to DOI metadata, including changes of the URL. You will for example be able to see the provenance for the DOI metadata for this blog post via <em>https://api.datacite.org/dois/10.5438/wy92-xj57/activities</em>. This is mainly useful if something goes wrong, but for example helps explain why a DOI record sometimes has a new <code>updated date</code> even though the member didn't make any changes (the most likely reason is that we stored <a href="https://support.datacite.org/docs/link-checker">link checker</a> results and you would see that in the provenance record).</p>
<p>We have captured more than 3.6 million activities around DOI metadata records since starting this new service in March. Going forward, we will enhance the functionality, e.g. by providing the same information in DOI Fabrica in addition to the API. Tracking the provenance also allows users to revert changes that were made inadvertently, and it allows DataCite to build services that are triggered when a particular change is made to DOI metadata.</p>
<p>This work has been done as part of the <a href="https://www.project-freya.eu/en/about/mission">FREYA project</a>, which aims to expand the technical and social infrastructure for persistent identifiers in Europe. For more information about the new activities API please visit our <a href="https://support.datacite.org/docs/tracking-provenance">support pages for this new service</a>, or <a href="mailto:support@datacite.org">send us an email</a>.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/wy92-xj57">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing the PID Graph]]></title>
        <id>1076b6d-a4v8t18-73352et-hv47d</id>
        <link href="https://blog.front-matter.io/mfenner/introducing-the-pid-graph"/>
        <updated>2019-03-28T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Persistent identifiers (PIDs) are not only important to uniquely identify a publication, dataset, or person, but the metadata for these persistent identifiers can provide unambiguous linking between persistent identifiers of the same type, e.g....]]></summary>
        <content type="html"><![CDATA[<p>Persistent identifiers (PIDs) are not only important to uniquely identify a publication, dataset, or person, but the metadata for these persistent identifiers can provide unambiguous linking between persistent identifiers of the same type, e.g. journal articles citing other journal articles, or of different types, e.g. linking a researcher and the datasets they produced.</p>
<p>Work is needed to connect existing persistent identifiers to each other in standardized ways, e.g. to the outputs associated with a particular researcher, repository, institution or funder, for discovery and impact assessment. Some of the more complex but still important use cases can’t be addressed by simply collecting and aggregating links between two persistent identifiers, including</p>
<ol>
<li>Aggregate the citations for all versions of a dataset or software source code</li>
<li>Aggregate the citations for all datasets hosted in a particular repository, funded by a particular funder, or created by a particular researcher</li>
<li>Aggregate all citations for a <a href="http://www.researchobject.org/">research object</a>: a publication, the data underlying the findings in the paper, and the software, samples, and reagents used to create those datasets.</li>
</ol>
<p>To address these use cases we need a more complex model to describe the resources that are identified by PIDs, and the connections between them: a graph. In graph theory, the resources identified by PIDs correspond to the nodes in this graph, and the connections between PIDs correspond to the edges.</p>
<figure>
<img src="https://blog.datacite.org/images/uploads/pid_graph_image.png" class="kg-image" />
</figure>
<p>Fig 1. A schematic representation of the PID graph with digital objects connected by PIDs, showing three use cases: A: Different versions of software code, B: Datasets hosted by a particular repository, C: All digital objects connected to a research object.</p>
<p>Using a graph makes it easier to describe these more complex use cases and relationships, and this approach has been frequently applied to similar questions in the past. FREYA builds on the expertise and close collaboration with the <a href="http://researchgraph.org/">Research Graph</a> team and adopts the outputs of the <a href="https://www.rd-alliance.org/groups/data-description-registry-interoperability.html">Research Data Alliance DDRI Working group</a> to transform PID connections into an improved graph of research objects. This project takes advantage of the best practices of graph modelling and distributed network analysis techniques. We call this the <strong><strong>PID Graph</strong></strong>.</p>
<h3 id="pid-graph-use-cases">PID Graph Use Cases</h3>
<p>Before starting work on implementing the PID Graph, the FREYA partners collected user stories from their communities relevant to the PID Graph work. We used GitHub issues in a public repository for this activity and then met in person in August 2018 to clarify, group and prioritize these user stories. In total, we identified 48 user stories, described <a href="https://github.com/datacite/freya/issues?utf8=%E2%9C%93&amp;q=is%3Aissue+is%3Aopen+label%3A%22PID+Graph%22++label%3A%22user+story%22+">here</a>. The main outcomes of the August 2018 workshop were:</p>
<ol>
<li>There is a significant number of relevant user stories that can only be addressed by implementing a PID Graph.</li>
<li>While there is a diverse number of stakeholder groups and persistent identifier types in these user stories, a number of common themes and major use cases emerged in the workshop.</li>
<li>We didn’t identify any uses cases that require more than two connections between PIDs, simplifying the required implementation work needed.</li>
</ol>
<p>After identifying and describing the most relevant use cases, summarized above, we started the implementation work for the FREYA PID Graph. Our goal was to implement the PID Graph as standard production service rather than a research activity or pilot service, so scalability and maintainability are of utmost importance. We learned a lot from the extensive experience gained in the <a href="http://researchgraph.org/">Research Graph</a> initiative and decided to build PID Graph using a set of federated RESTful JSON APIs. PID Graph will not be a single service but federated between FREYA PID providers, FREYA disciplinary partners, and organizations outside of FREYA. PID Graph will be provided by <a href="https://restfulapi.net/">RESTful JSON APIs</a> that describe the resources (nodes) and connections (edges) in this graph. All FREYA PID providers use RESTful JSON APIs to provide PID metadata so that this approach aligns with the extensive existing infrastructure.</p>
<h3 id="initial-pid-graph-implementation">Initial PID Graph Implementation</h3>
<p>The first working PID Graph implementation is provided by DataCite, extending the existing Event Data Service (Dasler &amp; Cousijn, <a href="https://blog.datacite.org/introducing-the-pid-graph/#ref-https://doi.org/10.5438/s6d3-k860">2018</a>), a collaboration between Crossref and DataCite. Event Data is a service that provides connections (here called events) between PIDs and other resources, with an initial focus on social media mentions and data citations. The initial PID Graph work done by DataCite since the August 2018 workshop has added these functionalities to <a href="https://datacite.org/eventdata.html">DataCite Event Data</a>:</p>
<h4 id="include-metadata-about-resources">Include metadata about resources</h4>
<p>Include not only metadata about connections but also metadata about the resources identified by PIDs. This dramatically simplifies the API calls needed to construct a PID Graph. We do this by optionally including the metadata for the <strong><strong>subj</strong></strong> and <strong><strong>obj</strong></strong> (the resources linked via the event) in Event Data via an extra query parameter: <a href="https://api.datacite.org/events?include=subj,obj">https://api.datacite.org/events?include=subj,obj</a></p>
<p>Including the metadata for subj and obj also enables queries based on resource metadata, e.g. query by type of content that is connected: <a href="https://api.datacite.org/events?include=subj,obj&amp;citationType=ScholarlyArticle-SoftwareSourceCode">https://api.datacite.org/events?include=subj,obj&amp;citationType=ScholarlyArticle-SoftwareSourceCode</a></p>
<p>This query today returns 1,078 events connecting scholarly articles and software, including 834 from journal articles referencing software via Crossref metadata and 210 from software referencing journal articles via DataCite metadata.</p>
<h4 id="include-implicit-relations-in-metadata-about-resources">Include implicit relations in metadata about resources</h4>
<p>Metadata for resources contain a lot of information about connected PIDs. We can take advantage of this by including the information in DataCite Event Data, allowing queries that in effect connect two PIDs via an intermediary resource and two connections. Specifically, we include these relations and associated PIDs:</p>
<ol>
<li>Version, e.g. dataset A <strong><strong>isVersionOf</strong></strong> dataset B (using DataCite <strong><strong>relatedIdentifier</strong></strong> metadata)</li>
<li>Granularity, e.g. dataset A <strong><strong>isPartOf</strong></strong> dataset B or dataset A <strong><strong>isSupplementTo</strong></strong> article B (using DataCite <strong><strong>relatedIdentifier</strong></strong> metadata)</li>
<li>Funding, e.g. dataset A <strong><strong>isFundedBy</strong></strong> funder B (using DataCite <strong><strong>fundingReferences</strong></strong> metadata)</li>
<li>Author, e.g. dataset A <strong><strong>isAuthoredBy</strong></strong> author B (using DataCite <strong><strong>nameIdentifier</strong></strong> metadata)</li>
</ol>
<p>These connected PIDs can then act as a <strong><strong>proxy</strong></strong> in PID Graph queries, as demonstrated in this example:</p>
<p><a href="https://api.datacite.org/events?include=subj,obj&amp;doi=10.5061/dryad.k5k9074">https://api.datacite.org/events?include=subj,obj&amp;doi=10.5061/dryad.k5k9074</a></p>
<p>The query today returns one data citation of the dataset identified by the DOI, and eight data files that are part of this dataset. If someone decides to cite one of these data files instead of the dataset (following principle 8 Specificity and Verifiability of the Joint Declaration of Data Citation Principles (Data Citation Synthesis Group, <a href="https://blog.datacite.org/introducing-the-pid-graph/#ref-https://doi.org/10.25490/a97f-egyk">2014</a>)), that data citation would also be included in the DataCite Event Data response.</p>
<p>Similarly, the citation of a specific version of a dataset would be included if querying for the parent version of the dataset. Examples for funding and authorship are given in the next paragraph.</p>
<h4 id="include-more-types-of-events">Include more types of events</h4>
<p>The initial focus in Event Data was on social media mentions and data citations. DataCite has added author-resource links and funder-resource links, using ORCID and Crossref Funder ID as PIDs, respectively. DataCite also include dataset usage statistics, as part of the work in the Make Data Count (Lowenberg, Budden, &amp; Cruse, <a href="https://blog.datacite.org/introducing-the-pid-graph/#ref-https://doi.org/10.5438/pre3-2f25">2018</a>) project. This enables the following two use cases:</p>
<ol>
<li>Show all datasets created by a particular researcher and their usage stats <a href="https://api.datacite.org/events?include=subj,obj&amp;orcid=0000-0002-1194-1055">https://api.datacite.org/events?include=subj,obj&amp;orcid=0000-0002-1194-1055</a>. The query today returns four datasets created by a researcher identified via her ORCID ID, plus a combined 21 unique views of these datasets in February and March 2019.</li>
<li>Show all datasets funded by the European Commission that have been cited by a journal article <a href="https://api.datacite.org/events?include=subj,obj&amp;doi=10.13039/501100000780&amp;citation-type=Dataset-ScholarlyArticle">https://api.datacite.org/events?include=subj,obj&amp;doi=10.13039/501100000780&amp;citation-type=Dataset-ScholarlyArticle</a>. The query today returns 69 datasets cited by 37 journal articles.</li>
</ol>
<h3 id="collaborating-on-research-data-graph-initiatives">Collaborating on Research Data Graph Initiatives</h3>
<p>The aim is for any interested parties within and beyond FREYA to implement PID Graph services, meaning that we have to figure out how best to coordinate and enable this federated PID Graph. And of course, there are initiatives outside of FREYA taking similar approaches and addressing similar use cases. These include:</p>
<ol>
<li>The already mentioned Research Graph Foundation</li>
<li><a href="http://www.scholix.org/">Scholix</a>: a framework for Scholarly Link Exchange coordinated by a Research Data Alliance (RDA) Working Group</li>
<li>The OpenAIRE Research Graph (Manghi &amp; Bardi, <a href="https://blog.datacite.org/introducing-the-pid-graph/#ref-https://doi.org/10.5281/zenodo.2600275">2019</a>), an open metadata research graph of interlinked scientific products, with access rights information, linked to funding information and research communities.</li>
<li>Asclepias (Ioannidis &amp; Gonzalez Lopez, <a href="https://blog.datacite.org/introducing-the-pid-graph/#ref-https://doi.org/10.5281/zenodo.2548643">2019</a>), a broker service initially developed to track software citations in astronomy.</li>
</ol>
<p>To coordinate these activities we have organized a Birds of a Feather session at the RDA Plenary in Philadelphia next week (Wednesday at 2:30 PM): <a href="https://rd-alliance.org/bof-Research-Data-Graph-RDA-13th-Plenary-meeting">Research Data Graph</a>.</p>
<p>The initial implementation of the PID Graph in DataCite Event Data contains 5.38 million events as of today and more work is needed to convert existing events to the new format (we expect a total of 25 million events with the current data source), improve documentation, and build visualizations and other frontend services to make it easier to show the PID Graph information we already have. But if you can’t wait and are not afraid working with JSON REST APIs, feel free to explore DataCite Event Data, which is a free service with no registration required, by starting with the <a href="https://support.datacite.org/docs/eventdata-guide">documentation</a>.</p>
<p>And please reach out to us via the <a href="https://www.pidforum.org/">PID Forum</a> if you are interested to learn more about PID Graph, want to see your data in PID Graph, or are working on a related project and want to coordinate. And of course, join us for the RDA Plenary session next week in Philadelphia if you plan to attend the conference.</p>
<p><em>This post has been cross-posted from the <a href="https://www.project-freya.eu/en/blogs/blogs/the-pid-graph">FREYA</a> and <a href="https://doi.org/10.5438/jwvf-8a66">DataCite</a> blogs.</em></p>
<h2 id="references">References</h2>
<p>Dasler, R., &amp; Cousijn, H. (2018). Are your data being used? Event data has the answer! <em>DataCite Blog</em>. <a href="https://doi.org/10.5438/S6D3-K860">https://doi.org/10.5438/S6D3-K860</a></p>
<p>Data Citation Synthesis Group. (2014). Joint declaration of data citation principles. <em>Force11</em>. <a href="https://doi.org/10.25490/a97f-egyk">https://doi.org/10.25490/a97f-egyk</a></p>
<p>Ioannidis, A., &amp; Gonzalez Lopez, J. B. (2019). Asclepias: Flower power for software citation. <a href="https://doi.org/10.5281/ZENODO.2548643">https://doi.org/10.5281/ZENODO.2548643</a></p>
<p>Lowenberg, D., Budden, A., &amp; Cruse, P. (2018). It’s time to make your data count! <a href="https://doi.org/10.5438/PRE3-2F25">https://doi.org/10.5438/PRE3-2F25</a></p>
<p>Manghi, P., &amp; Bardi, A. (2019). The openaire research graph - opportunities and challenges for science. <a href="https://doi.org/10.5281/ZENODO.2600275">https://doi.org/10.5281/ZENODO.2600275</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DataCite's New Search]]></title>
        <id>2g4hk17-18a81g8-r5pssa7-r5a2x</id>
        <link href="https://blog.front-matter.io/mfenner/datacites-new-search"/>
        <updated>2019-01-07T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Today we are announcing our first new functionality of 2019, a much improved search for DataCite DOIs and metadata. While the DataCite Search user interface has not changed, changes under the hood bring many important improvements and are our biggest...]]></summary>
        <content type="html"><![CDATA[<p>Today we are announcing our first new functionality of 2019, a much improved search for DataCite DOIs and metadata. While the <a href="https://search.datacite.org/">DataCite Search</a> user interface has not changed, changes under the hood bring many important improvements and are our biggest changes to search since 2012.</p>
<h2 id="faster-indexing">Faster Indexing</h2>
<p>Newly registered (and tagged findable) DOIs now appear in the DataCite Search index within a few minutes, compared with the previous up to 12 hour lag. The same is true for metadata updates or DOIs removed from the public search index (by changing the DOI state from <strong><strong>findable</strong></strong> to <strong><strong>registered</strong></strong>). Faster indexing is particularly important when related content is published at the same time, e.g. a dataset with a DataCite DOI associated with a journal article with a Crossref DOI.</p>
<h2 id="advanced-doi-search-in-doi-fabrica">Advanced DOI Search in DOI Fabrica</h2>
<p>This faster indexing makes it possible for members and clients to use the Search index also in <a href="https://doi.datacite.org/">DOI Fabrica</a>, enabling the same advanced search functionality available in DataCite Search, but also including DOIs in <strong><strong>draft</strong></strong> or <strong><strong>registered</strong></strong> state. Our Solr search index could not be used in DOI Fabrica, as users would not see newly created or updated DOIs because of the indexing delay. This makes it much easier to manage DOIs and associated metadata, e.g. by filtering for DOIs in draft state or finding DOIs using the retired metadata schemata 2.1 and 2.2. And it is the first time that we provide DOI registration and search in a single user interface; this kind of simplification is one of our themes for 2019 [Dasler (<a href="https://blog.datacite.org/improving-search/#ref-https://doi.org/10.5438/bckb-qy95">2018</a>)].</p>
<figure>
<img src="https://blog.datacite.org/images/uploads/bildschirmfoto-2019-01-05-um-17.30.20.png" class="kg-image" alt="Query for research data management" /><figcaption aria-hidden="true">Query for research data management</figcaption>
</figure>
<h2 id="search-for-everything">Search for Everything</h2>
<p>Our new search index covers all metadata and allows specific searches of every metadata field. For example <strong><strong>geoLocationPlace</strong></strong>:</p>
<figure>
<img src="https://blog.datacite.org/images/uploads/bildschirmfoto-2019-01-05-um-17.40.47.png" class="kg-image" alt="Search for geoLocationPlace Cuba" /><figcaption aria-hidden="true">Search for geoLocationPlace Cuba</figcaption>
</figure>
<p>The supported search syntax is very similar to what was available before, and uses the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html#query-string-syntax">Elasticsearch Query String Syntax</a>. You can for example specify field names, use wildcards, regular expressions, ranges, and boolean operators, e.g. use <code>creators.affiliation:stanford +creators.affiliation:ucsf</code> to find the 174 DOIs with collaborators from both of these two institutions.</p>
<h2 id="it-solves-the-deep-paging-problem">It Solves the Deep Paging Problem</h2>
<p>One important limitation of our previous search index, and a common issue with many search implementations, was the <a href="https://solr.pl/en/2011/07/18/deep-paging-problem/">deep paging problem</a>, making it hard if not impossible to fetch a very large number of results. Our new search index supports cursor-based pagination that overcomes this problem, allowing users to, for example, harvest all DOI metadata from a particular member. This is done in the REST API, specifying a larger number of records per page – e.g. <code>https://api.datacite.org/providers/caltech/dois?page[size]=1000</code> – and the using the URL provided via <code>links.next</code> in the API response for the next query.</p>
<h2 id="implementation">Implementation</h2>
<p>The above changes were made possible by updating our search index service from an old version of Solr (4.0) to a recent version of Elasticsearch (6.3). We switched to Elasticsearch, as it works better with our new JSON-based workflow – see our December blog post about JSON [Fenner (<a href="https://blog.datacite.org/improving-search/#ref-https://doi.org/10.5438/1pca-1y05">2018</a>)] – and we can use a <a href="https://aws.amazon.com/elasticsearch-service/">hosted service</a> tightly integrated with the rest of our infrastructure thereby reducing the support effort needed.</p>
<h2 id="next">Next</h2>
<p>Not all DataCite services have been switched to the new search index, the <a href="https://stats.datacite.org/">Stats Portal</a> and <a href="https://oai.datacite.org/">OAI-PMH</a> service will be migrated within the next three months and continue to use the old Solr search index for now.</p>
<p>In the coming weeks and months we will also provide better documentation, and improve performance and fix any bugs we encounter. We will also work with our members to better understand what kind of queries they are most interested in, and how we can better support these queries in the search interface.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/vyd9-ty64">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>Dasler, R. (2018). DataCite 2018 wrap-up and 2019 preview. <a href="https://doi.org/10.5438/BCKB-QY95">https://doi.org/10.5438/BCKB-QY95</a></p>
<p>Fenner, M. (2018). Introducing datacite json. <a href="https://doi.org/10.5438/1PCA-1Y05">https://doi.org/10.5438/1PCA-1Y05</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing DataCite JSON]]></title>
        <id>2qsydhw-z588y39-pt10hqv-89rng</id>
        <link href="https://blog.front-matter.io/mfenner/introducing-datacite-json"/>
        <updated>2018-12-19T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[All DataCite DOIs have associated metadata, described in the DataCite Metadata Schema Documentation (DataCite Metadata Working Group (2017)), validated and stored as XML in the DataCite Metadata Store (MDS). These metadata are then made available via DataCite APIs and services....]]></summary>
        <content type="html"><![CDATA[<p>All DataCite DOIs have associated metadata, described in the DataCite Metadata Schema Documentation (DataCite Metadata Working Group (<a href="https://blog.datacite.org/introducing-datacite-json/#ref-https://doi.org/10.5438/0014">2017</a>)), validated and stored as XML in the DataCite Metadata Store (MDS). These metadata are then made available via DataCite APIs and services. For these services XML is not always the best format, and we are thus providing the metadata in other formats, most notably JSON. The problem with our approach so far has been that this JSON was not properly defined, creating overhead and ambiguity both for our internal services and for our users. To change this situation, and to make it easier to work with metadata for DataCite DOIs, we today are announcing <strong><strong>DataCite JSON</strong></strong>.</p>
<figure>
<img src="https://blog.datacite.org/images/uploads/bildschirmfoto-2018-12-19-um-15.36.18.png" class="kg-image" alt="Google Trends: Searches for XML API (red) vs. JSON API (blue)" /><figcaption aria-hidden="true">Google Trends: Searches for XML API (red) vs. JSON API (blue)</figcaption>
</figure>
<h3 id="what-is-datacite-json">What is DataCite JSON?</h3>
<p>DataCite JSON represents all metadata elements and attributes available in DataCite XML, and can be converted from and to DataCite XML via several DataCite services (MDS API, EZ API, DOI Fabrica, Content Negotiation) that internally all use the bolognese metadata conversion library (Fenner (<a href="https://blog.datacite.org/introducing-datacite-json/#ref-https://doi.org/10.5438/n138-z3mk">2017</a>)), which also provides a command-line utility. Both our new Elasticsearch Search index and the updated JSON REST API (more on those in another blog post) use DataCite JSON. The bolognese metadata conversion library uses DataCite JSON as the intermediary format, for example when converting BibTeX to schema.org JSON-LD or JATS XML.</p>
<h2 id="is-datacite-json-different-from-datacite-xml">Is DataCite JSON different from DataCite XML?</h2>
<p>There are minor differences between DataCite JSON and DataCite XML, mainly to make working with the metadata easier. This includes an <strong><strong>identifiers</strong></strong> object that combines the <strong><strong>identifier</strong></strong> and <strong><strong>alternateIdentifier</strong></strong> properties, and a <strong><strong>types</strong></strong> object that not only stores <strong><strong>resourceTypeGeneral</strong></strong> and <strong><strong>resourceType</strong></strong> information, but also the type information from RIS, BibTeX, Citeproc and schema.org, to avoid losing type information when converting between these formats. There is also a new <strong><strong>container</strong></strong> property that stores information about the repository or journal where the content is located. We can provide this information in DataCite XML via the relatedidentifier (with relationType <strong><strong>isPartOf</strong></strong>) and description (via descriptionType <strong><strong>SeriesInformation</strong></strong>) elements, but the process is cumbersome. DataCite JSON also includes information not available in DataCite XML, including the url registered for the DOI, and the date the DOI was registered.</p>
<h2 id="do-you-have-examples-using-datacite-json">Do you have examples using DataCite JSON?</h2>
<p>To see DataCite JSON in action, lookup the DOI metadata of your favorite DOI in our JSON REST API, e.g. <a href="https://api.datacite.org/dois/10.5438/0014">https://api.datacite.org/dois/10.5438/0014</a>, or - if you are a DataCite member or client - in <a href="https://doi.datacite.org/">DOI Fabrica</a>. Alternatively install bolognese (via <code>gem install bolognese</code>) and fetch metadata via the command <code>bolognese 10.5438/0014 -t datacite_json</code>. Documentation of DataCite JSON is unfortunately still sparse, in early 2019 we will provide better documentation via our <a href="https://support.datacite.org/">support site</a>, and this will also include updated documentation of the JSON REST API and a JSON Schema to validate the metadata, aligned with our XSD Schema for DataCite XML.</p>
<p>We hope that DataCite JSON makes it easier to work with DataCite metadata, helping to improve metadata quality and re-use. We encourage users to adapt their tools to take advantage of DataCite JSON, and to consider DataCite JSON also when working with metadata not associated with a DataCite DOI, but when a description of scholarly resources with standard metadata and using JSON is needed. Watch out for more information about DataCite JSON in 2019, or reach out to us with questions or feedback via <a href="mailto:support@datacite.org">mailto:support@datacite.org</a>.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/1pca-1y05">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>DataCite Metadata Working Group. (2017). DataCite metadata schema for the publication and citation of research data v4.1. <em>DataCite</em>. <a href="https://doi.org/10.5438/0014">https://doi.org/10.5438/0014</a></p>
<p>Fenner, M. (2017). Bolognese: A ruby library for conversion of doi metadata. DataCite. <a href="https://doi.org/10.5438/n138-z3mk">https://doi.org/10.5438/n138-z3mk</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COUNTER Code of Practice for Research Data Usage Metrics release 1]]></title>
        <id>7fhxpga-c3h8sw8-4tpzf2g-yqccq</id>
        <link href="https://blog.front-matter.io/mfenner/counter-code-of-practice-for-research-data-usage-metrics-release-1"/>
        <updated>2018-09-18T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[There is a need for the consistent and credible reporting of research data usage. Such usage metrics are required as an important component in understanding how publicly available research data are being reused.To address this need,...]]></summary>
        <content type="html"><![CDATA[<p>There is a need for the consistent and credible reporting of research data usage. Such usage metrics are required as an important component in understanding how publicly available research data are being reused.</p>
<p>To address this need, <a href="https://www.projectcounter.org/">COUNTER</a> and members of the <a href="https://makedatacount.org/">Make Data Count</a> team (<a href="https://www.cdlib.org/">California Digital Library</a>, <a href="https://datacite.org/">DataCite</a>, and <a href="https://www.dataone.org/">DataONE</a> collaborated in drafting the <a href="https://www.projectcounter.org/code-of-practice-rd-sections/foreword/">Code of Practice for Research Data Usage Metrics release 1</a>.</p>
<p><a href="https://www.projectcounter.org/code-of-practice-rd-sections/foreword/">The Code of Practice for Research Data Usage Metrics release 1</a> is aligned as much as possible with the <a href="https://www.projectcounter.org/code-of-practice-five-sections/abstract/">COUNTER Code of Practice Release 5</a> which standardizes usage metrics for many scholarly resources, including journals and books. Many definitions, processing rules and reporting recommendations apply to research data in the same way as they apply to the other resources to which the COUNTER Code of Practice applies. Some aspects of the processing and reporting of usage data are unique to research data, and the Code of Practice for Research Data Usage Metrics thus deviates from the Code of Practice Release 5 and specifically address them.</p>
<p>The Code of Practice for Research Data Usage Metrics release 1 provides a framework for comparable data by standardizing the generation and distribution of usage metrics for research data. Data repositories and platform providers can now report usage metrics following common best practices and using a standard report format.</p>
<p>COUNTER welcomes feedback from the data repositories that implement this first release of the Code of Practice. Their experiences will help to refine and improve it and inform a second release.</p>
<p><em>Crossposted from the <a href="https://www.projectcounter.org/counter-code-practice-research-data-usage-metrics-release-1/">COUNTER announcement</a> from September 13, 2018, and from the <a href="https://doi.org/10.5438/nb24-t773">DataCite Blog</a>.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DOI Registrations for Software]]></title>
        <id>1q39f5y-tcy8mav-5bxv8fx-t48mq</id>
        <link href="https://blog.front-matter.io/mfenner/doi-registrations-for-software"/>
        <updated>2018-05-17T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[We know that software is important in research, and some of us in the scholarly communications community, for example, in FORCE11, have been pushing the concept of software citation as a method to allow software developers and maintainers to get academic...]]></summary>
        <content type="html"><![CDATA[<p>We know that software is important in research, and some of us in the scholarly communications community, for example, <a href="https://www.force11.org/group/software-citation-implementation-working-group">in FORCE11</a>, have been pushing the concept of software citation as a method to allow software developers and maintainers to get academic credit for their work: software releases are published and assigned DOIs, and software users then cite these releases when they publish research that uses the software.</p>
<p>DataCite recently examined the DOIs that have been created for software, and found that the number of new DOIs created for software is growing roughly exponentially, now reaching about 2000 software DOIs per month, with spikes of around 4000 per month in some of 2017. The data and results are shown here. The source code for the R script used to generate the data and figures is available (Fenner, Katz, Smith, &amp; Nielsen (<a href="https://blog.datacite.org/doi-registrations-software/#ref-https://doi.org/10.5438/wr0x-e194">2018</a>)).</p>
<p>As of May 16, 2018, <a href="https://search.datacite.org/works?resource-type-id=software">58,301 DOIs</a> have been registered for software. We can break down this number by repository where the software source code is hosted – most DOIs for software have been registered at Zenodo.</p>
<table style="box-sizing: border-box; border-collapse: collapse; border-spacing: 0px; background-color: rgb(255, 255, 255); color: rgb(102, 97, 91); font-family: Raleway, Helvetica, Arial, sans-serif; font-size: 18px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; width: 892.656px;">
<colgroup>
<col style="width: 92%" />
<col style="width: 7%" />
</colgroup>
<tbody style="box-sizing: border-box;">
<tr class="odd odd" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 0px; -webkit-font-smoothing: antialiased; font-family: Raleway, Helvetica, Arial, sans-serif">CERN.ZENODO - ZENODO - Research. Shared.</td>
<td style="text-align: right;" style="box-sizing: border-box; padding: 0px; -webkit-font-smoothing: antialiased; font-family: Raleway, Helvetica, Arial, sans-serif">41346</td>
</tr>
<tr class="even even" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 0px; -webkit-font-smoothing: antialiased; font-family: Raleway, Helvetica, Arial, sans-serif">FIGSHARE.ARS - figshare Academic Research System</td>
<td style="text-align: right;" style="box-sizing: border-box; padding: 0px; -webkit-font-smoothing: antialiased; font-family: Raleway, Helvetica, Arial, sans-serif">4226</td>
</tr>
<tr class="odd odd" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 0px; -webkit-font-smoothing: antialiased; font-family: Raleway, Helvetica, Arial, sans-serif">PURDUE.NCIB - National Cancer Institute, Bioconductor</td>
<td style="text-align: right;" style="box-sizing: border-box; padding: 0px; -webkit-font-smoothing: antialiased; font-family: Raleway, Helvetica, Arial, sans-serif">2769</td>
</tr>
<tr class="even even" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 0px; -webkit-font-smoothing: antialiased; font-family: Raleway, Helvetica, Arial, sans-serif">PURDUE.EZID - Purdue University</td>
<td style="text-align: right;" style="box-sizing: border-box; padding: 0px; -webkit-font-smoothing: antialiased; font-family: Raleway, Helvetica, Arial, sans-serif">2463</td>
</tr>
<tr class="odd odd" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 0px; -webkit-font-smoothing: antialiased; font-family: Raleway, Helvetica, Arial, sans-serif">OSTI.DOE - DOE Generic</td>
<td style="text-align: right;" style="box-sizing: border-box; padding: 0px; -webkit-font-smoothing: antialiased; font-family: Raleway, Helvetica, Arial, sans-serif">736</td>
</tr>
<tr class="even even" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 0px; -webkit-font-smoothing: antialiased; font-family: Raleway, Helvetica, Arial, sans-serif">INIST.INRA - Institut National de Recherche Agronomique</td>
<td style="text-align: right;" style="box-sizing: border-box; padding: 0px; -webkit-font-smoothing: antialiased; font-family: Raleway, Helvetica, Arial, sans-serif">223</td>
</tr>
<tr class="odd odd" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 0px; -webkit-font-smoothing: antialiased; font-family: Raleway, Helvetica, Arial, sans-serif">OCEAN.OCEAN - Code Ocean</td>
<td style="text-align: right;" style="box-sizing: border-box; padding: 0px; -webkit-font-smoothing: antialiased; font-family: Raleway, Helvetica, Arial, sans-serif">206</td>
</tr>
<tr class="even even" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 0px; -webkit-font-smoothing: antialiased; font-family: Raleway, Helvetica, Arial, sans-serif">CRUI.INFNCNAF - Istituto Nazionale di Fisica Nucleare. Centro Nazionale Analisi Fotogrammi</td>
<td style="text-align: right;" style="box-sizing: border-box; padding: 0px; -webkit-font-smoothing: antialiased; font-family: Raleway, Helvetica, Arial, sans-serif">190</td>
</tr>
<tr class="odd odd" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 0px; -webkit-font-smoothing: antialiased; font-family: Raleway, Helvetica, Arial, sans-serif">CDL.UCI - UC Irvine Library</td>
<td style="text-align: right;" style="box-sizing: border-box; padding: 0px; -webkit-font-smoothing: antialiased; font-family: Raleway, Helvetica, Arial, sans-serif">120</td>
</tr>
<tr class="even even" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 0px; -webkit-font-smoothing: antialiased; font-family: Raleway, Helvetica, Arial, sans-serif">ETHZ.DA-RD - ETHZ Data Archive - Research Data</td>
<td style="text-align: right;" style="box-sizing: border-box; padding: 0px; -webkit-font-smoothing: antialiased; font-family: Raleway, Helvetica, Arial, sans-serif">88</td>
</tr>
</tbody>
</table>
<h3 id="changes-over-time">Changes over Time</h3>
<p>How did these numbers change over time, since the he <a href="https://api.datacite.org/works?resource-type-id=software&amp;sort=registered&amp;order=asc&amp;page%5Bsize%5D=1">first DataCite DOI for software</a> was registered September 7th, 2011 by the Leibniz Institute of Plant Genetics and Crop Plant Research (IPK) in Germany (Colmsee, Flemming, Klapperstück, Lange, &amp; Scholz (<a href="https://blog.datacite.org/doi-registrations-software/#ref-https://doi.org/10.5447/ipk/2011/0">2011</a>))?</p>
<p>We can start by looking at the <a href="https://guides.github.com/activities/citable-code/">Zenodo/GitHub integration</a>, where users can archive a GitHub repository in the Zenodo data repository. The integration was launched in February 2014 and we can see a nice correlation with this data, and with a <a href="https://github.com/blog/1840-improving-github-for-science">May 2014 blog post</a> by Arfon Smith on the GitHub blog, describing (and advertising) the integration work.</p>
<figure>
<img src="https://blog.datacite.org/images/2018/05/dois-for-software.png" class="kg-image" alt="Software DOIs registered at DataCite" /><figcaption aria-hidden="true">Software DOIs registered at DataCite</figcaption>
</figure>
<p>In September 2016, the FORCE11 Software Citation Principles (A. M. Smith, Katz, Niemeyer, &amp; FORCE11 Software Citation Working Group (<a href="https://blog.datacite.org/doi-registrations-software/#ref-https://doi.org/10.7717/peerj-cs.86">2016</a>)) were published, the Zenodo/GitHub integration was upgraded ((<strong><strong>???</strong></strong>)/), and in October 2016 the <a href="https://guides.github.com/activities/citable-code/">GitHub Guide to Making your Code Citable</a> was updated. There appears to be a change of in the rate of growth around this time as well.</p>
<h2 id="looking-forward">Looking forward</h2>
<p>We see a nice exponential growth in the number of DOIs for software, and we don't expect this to change in 2018 and beyond. The <a href="https://www.force11.org/group/software-citation-implementation-working-group">FORCE11 Software Citation Implementation Working Group</a> is working on implementation and adoption of the Software Citation Principles, and for a number of use cases, e.g., citation in a journal article, DOIs play an important role. The working group also tries to address the challenges in using DOIs as identifiers for software that still exist, and what is done to resolve them, including pre-registration APIs to smooth the automated push-style deposit; better semantic linkage supported by extensions to the DataCite schema, and group/collective/microcitation DOI use.</p>
<p>We expect initiatives such as <a href="http://citation-file-format.github.io/citation-file-format/">Citation File Format</a> and <a href="https://www.softwareheritage.org/">Software Heritage</a> to have a positive impact on the number of DOIs for software. A paper on persistent identification and citation of software using DOIs by Jones et al (C. M. Jones, Matthews, Gent, Griffin, &amp; Tedds (<a href="https://blog.datacite.org/doi-registrations-software/#ref-https://doi.org/10.2218/ijdc.v11i2.422">2017</a>)) was published in July 2017, based on earlier work from 2015 (Gent, Jones, &amp; Matthews (<a href="https://blog.datacite.org/doi-registrations-software/#ref-http://purl.org/net/epubs/work/24058274">2015</a>)), and the DataCite Metadata 4.1 schema focussing on software citation was released in September 2017 (DataCite Metadata Working Group (<a href="https://blog.datacite.org/doi-registrations-software/#ref-https://doi.org/10.5438/0014">2017</a>), Starr (<a href="https://blog.datacite.org/doi-registrations-software/#ref-https://doi.org/10.5438/nzhx-xx96">2017</a>)).</p>
<p>CodeMeta (Boettiger (<a href="https://blog.datacite.org/doi-registrations-software/#ref-https://doi.org/10.6084/m9.figshare.4490588">2017</a>), M. B. Jones et al. (<a href="https://blog.datacite.org/doi-registrations-software/#ref-https://doi.org/10.5063/schema/codemeta-2.0">2017</a>)) is particularly relevant; this new standard for software metadata simplifies the crosswalk between the wide variety of metadata standards for software, and is increasingly integrated into DOI registration workflows, including the CaltechDATA repository since <a href="https://www.library.caltech.edu/news/enhanced-software-preservation-now-available-caltechdata">March 2018</a>, the DataCite DOI registration service since May 2018 (Fenner (<a href="https://blog.datacite.org/doi-registrations-software/#ref-https://doi.org/10.5438/cxe5-rg55">2018</a>), Dasler (<a href="https://blog.datacite.org/doi-registrations-software/#ref-https://doi.org/10.5438/0yk5-b755">2018</a>)) and is planned for the Zenodo/GitHub integration in autumn 2018. CodeMeta libraries are currently available for R (Codemetar, Boettiger et al. (<a href="https://blog.datacite.org/doi-registrations-software/#ref-https://doi.org/10.5281/zenodo.1241346">2018</a>)), Ruby (Bolognese, Fenner (<a href="https://blog.datacite.org/doi-registrations-software/#ref-https://doi.org/10.5438/n138-z3mk">2017</a>)) and Python (<a href="https://github.com/proycon/codemetapy">CodeMetaPy</a>).</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/1nmy-9902">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>Boettiger, C. (2017, January). Codemeta: A rosetta stone for software metadata. <em>figshare</em>. <a href="https://doi.org/10.6084/m9.figshare.4490588">https://doi.org/10.6084/m9.figshare.4490588</a></p>
<p>Boettiger, C., Salmon, M., Arfon Smith, Ross, N., Leinweber, K., &amp; Krystalli, A. (2018). Ropensci/codemetar: Codemetar: Generate codemeta metadata for r packages. Zenodo. <a href="https://doi.org/10.5281/zenodo.1241346">https://doi.org/10.5281/zenodo.1241346</a></p>
<p>Colmsee, C., Flemming, S., Klapperstück, M., Lange, M., &amp; Scholz, U. (2011). A case study for efficient management of high throughput primary lab data. Leibniz Institute of Plant Genetics; Crop Plant Research (IPK). <a href="https://doi.org/10.5447/ipk/2011/0">https://doi.org/10.5447/ipk/2011/0</a></p>
<p>Dasler, R. (2018). DOI fabrica 1.0 is here! <em>DataCite</em>. <a href="https://doi.org/10.5438/0yk5-b755">https://doi.org/10.5438/0yk5-b755</a></p>
<p>DataCite Metadata Working Group. (2017). DataCite metadata schema for the publication and citation of research data v4.1. <em>DataCite</em>. <a href="https://doi.org/10.5438/0014">https://doi.org/10.5438/0014</a></p>
<p>Fenner, M. (2017). Bolognese: A ruby library for conversion of doi metadata. DataCite. <a href="https://doi.org/10.5438/n138-z3mk">https://doi.org/10.5438/n138-z3mk</a></p>
<p>Fenner, M. (2018). Frontend for the datacite doi fabrica service. DataCite. <a href="https://doi.org/10.5438/CXE5-RG55">https://doi.org/10.5438/CXE5-RG55</a></p>
<p>Fenner, M., Katz, D. S., Smith, A., &amp; Nielsen, L. H. (2018). DOI registrations for software. DataCite. <a href="https://doi.org/10.5438/wr0x-e194">https://doi.org/10.5438/wr0x-e194</a></p>
<p>Gent, I., Jones, C., &amp; Matthews, B. (2015). Guidelines for persistently identifying software using datacite. Retrieved from <a href="http://purl.org/net/epubs/work/24058274">http://purl.org/net/epubs/work/24058274</a></p>
<p>Jones, C. M., Matthews, B., Gent, I., Griffin, T., &amp; Tedds, J. (2017). Persistent identification and citation of software. <em>International Journal of Digital Curation</em>, <em>11</em>(2), 104–114. <a href="https://doi.org/10.2218/ijdc.v11i2.422">https://doi.org/10.2218/ijdc.v11i2.422</a></p>
<p>Jones, M. B., Boettiger, C., Mayes, A. C., Smith, A., Slaughter, P., Niemeyer, K., … Goble, C. (2017). CodeMeta: An exchange schema for software metadata. KNB Data Repository. <a href="https://doi.org/10.5063/schema/codemeta-2.0">https://doi.org/10.5063/schema/codemeta-2.0</a></p>
<p>Smith, A. M., Katz, D. S., Niemeyer, K. E., &amp; FORCE11 Software Citation Working Group. (2016). Software citation principles. <em>PeerJ Computer Science</em>, <em>2</em>, e86. <a href="https://doi.org/10.7717/peerj-cs.86">https://doi.org/10.7717/peerj-cs.86</a></p>
<p>Starr, J. (2017). New datacite metadata updates support software citation. <a href="https://doi.org/10.5438/NZHX-XX96">https://doi.org/10.5438/NZHX-XX96</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Announcing DataCite DOI Fabrica]]></title>
        <id>3xbpw23-kw99j1v-v30n9cb-dfdtg</id>
        <link href="https://blog.front-matter.io/mfenner/announcing-datacite-doi-fabrica"/>
        <updated>2017-10-18T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Today DataCite is launching DOI Fabrica, the next generation of DataCite’s DOI registration service, replacing the Metadata Store (MDS). This is the biggest and most important product release DataCite has done in many years,...]]></summary>
        <content type="html"><![CDATA[<p>Today DataCite is launching <a href="https://doi.datacite.org/">DOI Fabrica</a>, the next generation of DataCite’s DOI registration service, replacing the Metadata Store (<a href="https://mds.datacite.org/">MDS</a>). This is the biggest and most important product release DataCite has done in many years, and the result of nine months of hard work by the entire DataCite team.</p>
<p>DOI registration is the core service that DataCite is providing to its members and the data centers they work with. The requirements for how DOI registration should work have changed significantly since the Metadata Store was originally launched in 2011. We began the DOI Fabrica project in early 2017 by collecting, categorizing, and prioritizing user stories for what is needed from the DataCite DOI registration service. Central to this activity was continuously engaging with our members and the DataCite community to make sure their needs were met. As part of this work, we rethought how we work with user stories and how develop our product roadmap (more details in <a href="https://blog.datacite.org/roadmap/">this</a> blog post), and you can find all user stories directly relevant for DOI registration <a href="https://www.datacite.org/user-stories.html?=&amp;category=create#how-to-provide-feedback">here</a>.</p>
<p>Once it was clear that significant work was needed to improve the existing DOI registration service, and that incremental changes of the Metadata Store wouldn’t be enough, we thought about how to best approach the development of a new service. The following principles guided us:</p>
<ul>
<li>Implement and constantly improve a feedback process that can guide our development. In addition to the user stories and roadmap we also worked closely with the <a href="https://www.datacite.org/steering.html">DataCite Services and Technology Steering Group</a> that was launched in September 2016.</li>
<li>Do gradual, small changes as much as possible to reduce the risk of failures or significant delays – we have all experienced this with scholarly infrastructure projects. DOI Fabrica focusses on changes in the user interface, and we have left the MDS API unchanged for now.</li>
<li>Re-architect the existing DOI registration service into smaller, loosely coupled services to simplify maintenance and improvements. DOI Fabrica consists of one frontend service, one authentication service, and two APIs (the legacy MDS API plus a new API for managing members and data centers).</li>
<li>Make it easy to run the code in development, deploy code changes and receive automatic error messages. This includes continuous integration tools for automatically running tests after commits to the code repository.</li>
<li>Build user interfaces that engage users, and provide clear documentation and mechanisms to provide feedback.</li>
</ul>
<p>What do all of the above have in common? They make the product development process faster, broken down into smaller steps, and more focused on what our members and users really care about. The most important feature of DOI Fabrica is thus the ongoing development it enables going forward, and you can expect important new functionality in the next few months (check our <a href="https://www.datacite.org/roadmap.html">roadmap</a> for details and/or if you want to provide feedback). But of course, this first DOI Fabrica release comes with functionality directly addressing the needs identified in the user stories collected earlier this year. We want to highlight how we addressed five of these user stories:</p>
<h3 id="single-sign-on"><a href="https://github.com/datacite/datacite/issues/132">Single Sign-on</a></h3>
<p><em>As a member or data center, I want a single username/password to access all DataCite services, so that I don't spend extra time managing access.</em></p>
<p>We have consolidated all login options for users into a single service, <a href="https://profiles.datacite.org/">DataCite Profiles</a>, and defined roles and permissions for DOI Fabrica. To not break existing integrations, the MDS API will continue to use the old username/password authentication.</p>
<h3 id="powerful-mds-admin-user-interface-ui"><a href="https://github.com/datacite/datacite/issues/56">Powerful MDS Admin User Interface (UI)</a></h3>
<p><em>As a member, I want a powerful admin UI (prefix transfer from one datacenter to another, editing/hiding the welcome email to data centers, expand the length of the field “domain” etc.) to manage all my data centers so that I reduce admin costs and can track changes.</em></p>
<p>All functionality relevant for DOI registration (including management of data centers and prefixes) is now available via API calls. DOI Fabrica provides a powerful administrator UI that, for example, allows members to automatically fetch new prefixes from the DataCite pool when all their prefixes are in use, instead of sending an email to DataCite support.</p>
<h3 id="link-checker-reports"><a href="https://github.com/datacite/datacite/issues/11">Link checker reports</a></h3>
<p><em>As a national library, I want services that check DOI resolution (broken links), so I can keep my responsibilities of persistence.</em></p>
<figure>
<img src="https://blog.datacite.org/images/2017/10/link-checking.png" class="kg-image" />
</figure>
<p>DOI Fabrica includes a simple link checker that is particularly useful during DOI registration. A more robust link checker is on our <a href="https://www.datacite.org/roadmap.html">roadmap</a> for the coming months.</p>
<h3 id="reporting-tool"><a href="https://github.com/datacite/datacite/issues/52">Reporting tool</a></h3>
<p><em>As a member, I want a reporting tool, so that I can stay informed about my DOIs.</em></p>
<figure>
<img src="https://blog.datacite.org/images/2017/10/stats.png" class="kg-image" />
</figure>
<p>While future DOI Fabrica releases will provide much more detailed reporting, with this release members will see a dashboard that summarizes the numbers of data centers (clients) they work with, and the number of DOIs registered per year.</p>
<h3 id="dois-per-discipline"><a href="https://github.com/datacite/datacite/issues/68">DOIs per discipline</a></h3>
<p><em>As a national library, I want to see how many datasets exist per discipline, so that I can decide about resourcing.</em></p>
<figure>
<img src="https://blog.datacite.org/images/2017/10/re3data.png" class="kg-image" />
</figure>
<p>We are still a long way from understanding how many DOIs for research data exist per discipline, but we took an important first step by enabling the linking of data centers to <a href="https://www.re3data.org/">re3data</a>, allowing us to automatically pull in information about the disciplines covered in any particular repository.</p>
<p>With so many changes in DOI Fabrica we couldn’t include everything we wanted in the first release, and one functionality in particular has to wait for the next release in six weeks: DOI registration. We need to first address technical dependencies before this can be implemented, and rather than delaying the first release of DOI Fabrica, we decided to focus on functionality important for our members, and to add DOI registration via XML upload and web form in the next release. We look forward to your feedback and to suggestions on how we can improve <a href="https://doi.datacite.org/">DOI Fabrica</a>.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/g7q1-7t10">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Content Negotiation Update]]></title>
        <id>468sepg-js9p88z-qrzkxgv-fedb</id>
        <link href="https://blog.front-matter.io/mfenner/a-content-negotiation-update"/>
        <updated>2017-04-28T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[While it is a best practice for DOIs (expressed as URL) to send the user to the landing page for that resource (Starr et al., 2015), sometimes we want something else: <strong><strong>metadata</strong></strong>, e.g. to generate a citation,...]]></summary>
        <content type="html"><![CDATA[<p>While it is a best practice for DOIs (expressed as URL) to send the user to the landing page for that resource (Starr et al., <a href="https://blog.datacite.org/content-negotiation-update/#ref-https://doi.org/10.7717/peerj-cs.1">2015</a>), sometimes we want something else: <strong><strong>metadata</strong></strong>, e.g. to generate a citation, or to go to the <strong><strong>content</strong></strong> itself. The easiest way to do that is to use DOI content negotiation.</p>
<p>In this blog post we will give an introduction to DOI content negotiation, describe some of the issues we identified with our current implementation, and announce a major service update addressing these issues and launching in two weeks.</p>
<p>DOI Content Negotiation uses the HTTP <strong><strong>Accept</strong></strong> header together with the DOI expressed as URL. This is best explained by an example, using a recent post from this blog:</p>
<pre><code>curl -LH &quot;Accept: application/x-bibtex&quot; https://doi.org/10.5438/0000-0C2G</code></pre>
<p>This command returns BibTeX metadata for this DOI:</p>
<pre><code>@misc{https://doi.org/10.5438/0000-0C2G,
  doi = {10.5438/0000-0C2G},
  url = {https://doi.org/10.5438/0000-0C2G},
  author = {Cruse, Patricia},
  publisher = {DataCite},
  title = {The OI Project gets underway planning an Open Organization Identifier Registry},
  year = {2017}
}</code></pre>
<p>Or maybe you want to go directly to the content, in this case the JATS XML for the blog post (Fenner, <a href="https://blog.datacite.org/content-negotiation-update/#ref-https://doi.org/10.5438/0000-00cc">2017c</a>):</p>
<pre><code>curl -LH &quot;Accept: application/xml&quot; https://doi.org/10.5438/0000-0C2G</code></pre>
<p>Sometimes it is easier to use a normal link instead, e.g. in a web browser where sending HTTP headers is a bit involved.</p>
<pre><code>curl -L https://data.datacite.org/application/x-bibtex/10.5438/0000-0C2G</code></pre>
<p>For this to work you have to go directly to the DataCite content negotiation service at <strong><strong>data.datacite.org</strong></strong> instead of using the <strong><strong>doi.org</strong></strong> DOI proxy.</p>
<p>The most popular use of DOI content negotiation is citation formatting in any of the 1000s of citation styles provided by the Citation Style Language (<a href="http://citationstyles.org/">CSL</a>) project. For this we combine DOI content negotiation, which generates a JSON file from the metadata in a format that CSL understands (<a href="https://bitbucket.org/fbennett/citeproc-js/">Citeproc JSON</a>), with our <a href="https://blog.datacite.org/citation-formatting-service-upgrade/">DOI citation formatting service</a>, which generates the formatted citation:</p>
<pre><code>curl -LH &quot;Accept: text/x-bibliography; style=apa&quot; https://doi.org/10.5438/0000-0C2G</code></pre>
<p>This command returns a citation formatted in <a href="http://www.apastyle.org/">APA style</a>:</p>
<pre><code>Cruse, P. (2017). The OI Project gets underway planning an Open Organization Identifier Registry. DataCite. https://doi.org/10.5438/0000-0C2G</code></pre>
<p>There is an easier way to get this information from DataCite if you don't feel comfortable using the command line. First find the DOI you want to cite in DataCite Search, e.g. <a href="https://search.datacite.org/works/10.5438/0000-0C2G">https://search.datacite.org/works/10.5438/0000-0C2G</a> in this case.</p>
<figure>
<img src="https://blog.datacite.org/images/2017/04/search-result.png" class="kg-image" alt="Add to ORCID" /><figcaption aria-hidden="true">Add to ORCID</figcaption>
</figure>
<p>Then click on the <strong><strong>Cite</strong></strong> button and select on of several popular citation styles. We again pick the APA style:</p>
<figure>
<img src="https://blog.datacite.org/images/2017/04/cite-apa.png" class="kg-image" alt="APA style" /><figcaption aria-hidden="true">APA style</figcaption>
</figure>
<p>This is the same formatted citation we saw earlier, as DataCite Search is using the content negotiation and citation formatting services in the background. These services have been around for several years, and we work together with other DOI registration agencies on this. The citation service in DataCite Search for example is based on Crossref <a href="https://github.com/crosscite/doi-metadata-search">open source code</a> originally written for <a href="https://search.crossref.org/">Crossref Metadata Search</a>.</p>
<h2 id="upcoming-changes">Upcoming Changes</h2>
<p>There is still room for improvement of the content negotiation service. As part of work in the EC-funded THOR project (Farquhar et al., <a href="https://blog.datacite.org/content-negotiation-update/#ref-https://doi.org/10.5438/6423">2015</a>) we took a closer look at what you can and can't do with DOI content negotiation. We identified a number of gaps, and will address them by launching an updated content negotiation service in May.</p>
<h3 id="many-content-types-only-supported-with-required-metadata">Many content types only supported with required metadata</h3>
<p>The current DataCite content negotiation only supports the required metadata (identifier, creator, title, publisher, publicationYear, resourceTypeGeneral) for most content types. This means that even for something as straightforward as BibTeX DataCite is not converting the supported optional metadata such as <code>description</code>, <code>subject/keywords</code>, <code>version</code> or <code>license</code>. And for rich metadata such as RDF, a long list of properties is not converted.</p>
<h3 id="limited-person-name-parsing">Limited person name parsing</h3>
<p>Person names are very difficult to parse, in particular when not using the optional properties <code>givenName</code> and <code>familyName</code> introduced in DataCite Schema 4.0 last year (DataCite Metadata Working Group, <a href="https://blog.datacite.org/content-negotiation-update/#ref-https://doi.org/10.5438/0012">2016</a>). But proper parsing is important for citations, as different citation styles use different formatting rules for names. And BibTex expects a particular format that is hard to support without proper person name parsing.</p>
<h3 id="better-support-for-consistent-metadata-across-doi-registration-agencies">Better support for consistent metadata across DOI registration agencies</h3>
<p>Although content negotiation is supported by several DOI registration agencies, there is only limited support for common metadata beyond the basic citation metadata. This makes it harder than it should it to combine metadata records from different DOI registration agencies. The current RDF support is limited, and no DOI registration agency is offering content negotiation that converts metadata from another registration agency.</p>
<h3 id="support-for-additional-content-types">Support for additional content types</h3>
<p>The list of supported content types could be much longer, from emerging metadata standards such as <a href="https://schema.org/">schema.org</a> or <a href="http://www.scholix.org/">scholix</a> to community-specific standards such as <a href="https://www.ddialliance.org/">DDI</a> for social sciences, DATS for the life sciences (Sansone et al., <a href="https://blog.datacite.org/content-negotiation-update/#ref-https://doi.org/10.1101/103143">2017</a>), or <a href="http://codemeta.github.io/">codemeta</a> for software.</p>
<h3 id="unrecognized-content-types-raise-an-error">Unrecognized content types raise an error</h3>
<p>When the DataCite DOI content negotiation encounters an unrecognized content type, it returns a <strong><strong>406 Not Acceptable</strong></strong> error. This means that it is impossible to implement content negotiation downstream at the data center, as all content negotiation requests will be redirected to DataCite. Data centers can register their custom content types in the DataCite MDS using the <code>media</code> API endpoint, but this feature is unfortunately not widely used.</p>
<p>DataCite has rewritten most of the code of the content negotiation service, and has extracted out the metadata conversion into a standalone library (Fenner, <a href="https://blog.datacite.org/content-negotiation-update/#ref-https://doi.org/10.5438/n138-z3mk">2017a</a>) that can also be used locally via the command line. The new service is currently in final testing at <a href="https://data.test.datacite.org/">https://data.test.datacite.org</a>, and we plan to launch this into production ion May 9th. We have worked on all the limitations listed above, and some of the highlights include:</p>
<ul>
<li>extensive support for schema.org/JSON-LD as a common metadata standard independent of a particular DOI registration agency</li>
<li>extensive RDF support as XML or Turtle via schema.org metadata</li>
<li>tested with DOIs from DataCite and Crossref, supports conversion of Crossref metadata to DataCite metadata</li>
<li>BibTeX and RIS files have proper file extension (.bib and .ris) for easier import into other applications.</li>
<li>new content type <code>codemeta</code>, other content types will be added in the coming months based on user feedback</li>
<li>available as open source software via Github (Fenner, <a href="https://blog.datacite.org/content-negotiation-update/#ref-https://doi.org/10.5438/t1jg-hvhn">2017b</a>) and <a href="https://hub.docker.com/r/crosscite/content-negotiation/">Docker Hub</a>.</li>
</ul>
<p>There is one breaking change that users should be aware of: the content type <code>text/html</code> will no longer be supported, as the DOI proxy always forwards requests with this content type to the URL registered in the handle system. Users who want to see a HTML representation of the DOI metadata should go to DataCite Search, i.e. <a href="https://search.datacite.org/works/10.5438/0000-0C2G">https://search.datacite.org/works/10.5438/0000-0C2G</a> instead of https://data.datacite.org/10.5438/0000-0C2G. We will be improving the information shown for a single DOI in DataCite Search in the coming months, and appreciate user feedback.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/0000-01qj">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>DataCite Metadata Working Group. (2016). DataCite metadata schema for the publication and citation of research data v4.0. <em>DataCite</em>. <a href="https://doi.org/10.5438/0012">https://doi.org/10.5438/0012</a></p>
<p>Farquhar, A., Aryani, A., Brown, J., Burton, A., Cruise, P., Dallmeier-Thiessen, S., … Vision, T. (2015). Technical and human infrastructure for open research (thor). DataCite. <a href="https://doi.org/10.5438/6423">https://doi.org/10.5438/6423</a></p>
<p>Fenner, M. (2017a). Bolognese: A ruby library for conversion of doi metadata. DataCite. <a href="https://doi.org/10.5438/n138-z3mk">https://doi.org/10.5438/n138-z3mk</a></p>
<p>Fenner, M. (2017b). Content-negotation: An api for doi content negotiation. DataCite. <a href="https://doi.org/10.5438/t1jg-hvhn">https://doi.org/10.5438/t1jg-hvhn</a></p>
<p>Fenner, M. (2017c). Using schema.org for doi registration. <a href="https://doi.org/10.5438/0000-00CC">https://doi.org/10.5438/0000-00CC</a></p>
<p>Sansone, S.-A., Gonzalez-Beltran, A., Rocca-Serra, P., Alter, G., Grethe, J. S., Xu, H., … Members, b. W. G. (2017). DATS: The data tag suite to enable discoverability of datasets. <em>bioRxiv</em>. <a href="https://doi.org/10.1101/103143">https://doi.org/10.1101/103143</a></p>
<p>Starr, J., Castro, E., Crosas, M., Dumontier, M., Downs, R. R., Duerr, R., … Clark, T. (2015). Achieving human and machine accessibility of cited data in scholarly publications. <em>PeerJ Computer Science</em>, <em>1</em>, e1. <a href="https://doi.org/10.7717/peerj-cs.1">https://doi.org/10.7717/peerj-cs.1</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Schema.org for DOI Registration]]></title>
        <id>7677n74-can9kqb-vtqvrcs-5efg4</id>
        <link href="https://blog.front-matter.io/mfenner/using-schema-org-for-doi-registration"/>
        <updated>2017-01-09T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Three weeks ago we started assigning DOIs to every post on this blog (Fenner, 2016c). The process we implemented uses a new command line utility and integrates well with our the publishing workflow, with (almost)...]]></summary>
        <content type="html"><![CDATA[<p>Three weeks ago we started assigning DOIs to every post on this blog (Fenner, <a href="https://blog.datacite.org/schema-org-register-dois/#ref-https://doi.org/10.5438/4K3M-NYVG">2016c</a>). The process we implemented uses a new <a href="https://github.com/datacite/cirneco">command line utility</a> and integrates well with our the publishing workflow, with (almost) no extra effort compared to how we published blog posts before.</p>
<p>Given that DataCite is a DOI registration agency, we obviously are careful about following best practices for assigning DOIs. DataCite focusses on DOIs for research data, but many of the general principles can also apply to blog posts. And we have learned a few things already.</p>
<h3 id="using-schemaorg-metadata-embedded-in-landing-pages">Using schema.org metadata embedded in landing pages</h3>
<p>Our initial implementation collected the metadata required for DOI registration in a way that is specific to a particular type of blogging software, so-called <a href="https://davidwalsh.name/introduction-static-site-generators">static site generators</a>. While popular, this leaves out a large number of blogs, for example every blog hosted by Wordpress, by far the most popular blogging platform. We have now relaunched our blog to collect metadata differently, generic enough to work for any blog, but also well aligned with best practices for DOIs.</p>
<p>Our practice is that every DOI should resolve to a landing page, and that landing page should provide both human- and machine-readable metadata. Machine-readable metadata can be embedded into web pages in a number of ways. Traditionally this was done using HTML meta tags, more recent approaches to embedding metadata in HTML include <a href="https://html.spec.whatwg.org/multipage/microdata.html">microdata</a>, <a href="http://microformats.org/">microformats</a> and <a href="https://www.w3.org/TR/2015/NOTE-rdfa-primer-20150317/">RDFa</a>. An alternative approach is to embed the metadata using JSON and a <code>&lt;script&gt;</code> tag. The latter approach is easier to implement, as all metadata are in a single place, and the JSON can be embedded dynamically via a script.</p>
<p>As for the vocabulary, the DataCite Metadata Schema has never been widely used for metadata embedded in web pages. Dublin Core Metadata (“Dublin Core Metadata Element Set, Version 1.1,” <a href="https://blog.datacite.org/schema-org-register-dois/#ref-http://dublincore.org/documents/2012/06/14/dces">2012</a>) are often used for metadata in HTML <code>meta</code> tags. <a href="https://schema.org/">Schema.org</a> is an initiative started in 2011 with many of the same goals as Dublin Core, namely to <em>create, maintain, and promote schemas for structured data on the Internet</em>.</p>
<figure>
<img src="https://blog.datacite.org/images/2016/12/schema-org.png" class="kg-image" alt="schema.org" /><figcaption aria-hidden="true">schema.org</figcaption>
</figure>
<p>Schema.org is widely adopted, not the least because the initiative was started by Google, Microsoft, Yahoo, and Yandex to help with indexing web pages for search. Schema.org metadata can be embedded using microdata, RDFa or JSON-LD.</p>
<p>DataCite has recently added support for schema.org in JSON-LD format to <a href="http://citation.crosscite.org/docs.html">DOI content negotiation</a>, for example <code>curl -LH "Accept: application/vnd.schemaorg.ld+json" https://doi.org/10.5438/4K3M-NYVG</code>. Schema.org in JSON-LD is also embedded in search results on <a href="https://search.datacite.org/">DataCite Search</a> using the tag <code>&lt;script type="application/vnd.schemaorg.ld+json"&gt;</code>.</p>
<p>The DataCite blog now uses schema.org in JSON-LD format to embed metadata in machine-readable format, for example for the blog post mentioned earlier:</p>
<pre><code>{
    &quot;@context&quot;: &quot;http://schema.org&quot;,
    &quot;@type&quot;: &quot;BlogPosting&quot;,
    &quot;@id&quot;: &quot;https://doi.org/10.5438/4K3M-NYVG&quot;,
    &quot;name&quot;: &quot;Eating your own Dog Food&quot;,
    &quot;alternateName&quot;: &quot;MS-49-3632-5083&quot;,
    &quot;url&quot;: &quot;https://blog.datacite.org/eating-your-own-dog-food/&quot;,
    &quot;author&quot;: [
        {
            &quot;@type&quot;: &quot;Person&quot;,
            &quot;@id&quot;: &quot;http://orcid.org/0000-0003-1419-2405&quot;,
            &quot;givenName&quot;: &quot;Martin&quot;,
            &quot;familyName&quot;: &quot;Fenner&quot;,
            &quot;name&quot;: &quot;Martin Fenner&quot;
        }
    ],
    &quot;publisher&quot;: {
        &quot;@type&quot;: &quot;Organization&quot;,
        &quot;name&quot;: &quot;DataCite&quot;
    },
    &quot;dateCreated&quot;: &quot;2016-12-20&quot;,
    &quot;datePublished&quot;: &quot;2016-12-20&quot;,
    &quot;dateModified&quot;: &quot;2016-12-20&quot;,
    &quot;keywords&quot;: &quot;datacite, doi, metadata, featured&quot;,
    &quot;version&quot;: &quot;1.0&quot;,
    &quot;description&quot;: &quot;Eating your own dog food is a slang term to describe that an organization should itself use the products and services it provides. For DataCite this means that we should use DOIs with appropriate metadata and strategies for long-term preservation for...&quot;,
    &quot;license&quot;: &quot;https://creativecommons.org/licenses/by/4.0/&quot;,
    &quot;image&quot;: &quot;https://blog.datacite.org/images/2016/12/230785.jpg&quot;,
    &quot;encoding&quot;: {
        &quot;@type&quot;: &quot;MediaObject&quot;,
        &quot;@id&quot;: &quot;https://blog.datacite.org/eating-your-own-dog-food/4K3M-NYVG.xml&quot;,
        &quot;fileFormat&quot;: &quot;application/xml&quot;
    },
    &quot;isPartOf&quot;: {
        &quot;@type&quot;: &quot;Blog&quot;,
        &quot;@id&quot;: &quot;https://doi.org/10.5438/0000-00SS&quot;,
        &quot;name&quot;: &quot;DataCite Blog&quot;
    },
    &quot;citation&quot;: [
        {
            &quot;@type&quot;: &quot;CreativeWork&quot;,
            &quot;@id&quot;: &quot;https://doi.org/10.5438/0012&quot;
        },
        {
            &quot;@type&quot;: &quot;CreativeWork&quot;,
            &quot;@id&quot;: &quot;https://doi.org/10.5438/55E5-T5C0&quot;
        }
    ]
}</code></pre>
<p>If you are familiar with the DataCite Metadata Schema, it is easy to see how schema.org metadata can be converted into DataCite metadata and used with the DataCite Metadata Store, DataCite’s DOI registration and management service. The properties required by DataCite metadata (DOI, author, title, publisher, publicationYear, resourceTypeGeneral) are standard metadata for blog posts with the exception of the DOI. You can see that the JSON-LD <code>@id</code> is the DOI expressed as HTTPS URL (and that the <code>@id</code>for authors is their ORCID ID). And there are some naming differences e.g. <code>name</code> vs. <code>title</code>. The DataCite metadata corresponding to the above schema.org metadata look like this (and <a href="http://data.crosscite.org/application/vnd.datacite.datacite+xml/10.5438/4K3M-NYVG">can be downloaded</a> via DOI content negotiation):</p>
<pre><code>&lt;resource xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
  xmlns=&quot;http://datacite.org/schema/kernel-4&quot;
  xsi:schemaLocation=&quot;http://datacite.org/schema/kernel-4 http://schema.datacite.org/meta/kernel-4/metadata.xsd&quot;&gt;
  &lt;identifier identifierType=&quot;DOI&quot;&gt;10.5438/4K3M-NYVG&lt;/identifier&gt;
  &lt;creators&gt;
    &lt;creator&gt;
      &lt;creatorName&gt;Fenner, Martin&lt;/creatorName&gt;
      &lt;givenName&gt;Martin&lt;/givenName&gt;
      &lt;familyName&gt;Fenner&lt;/familyName&gt;
      &lt;nameIdentifier schemeURI=&quot;http://orcid.org/&quot; nameIdentifierScheme=&quot;ORCID&quot;&gt;0000-0003-1419-2405&lt;/nameIdentifier&gt;
    &lt;/creator&gt;
  &lt;/creators&gt;
  &lt;titles&gt;
    &lt;title&gt;Eating your own Dog Food&lt;/title&gt;
  &lt;/titles&gt;
  &lt;publisher&gt;DataCite&lt;/publisher&gt;
  &lt;publicationYear&gt;2016&lt;/publicationYear&gt;
  &lt;resourceType resourceTypeGeneral=&quot;Text&quot;&gt;BlogPosting&lt;/resourceType&gt;
  &lt;alternateIdentifiers&gt;
    &lt;alternateIdentifier alternateIdentifierType=&quot;Local accession number&quot;&gt;MS-49-3632-5083&lt;/alternateIdentifier&gt;
  &lt;/alternateIdentifiers&gt;
  &lt;subjects&gt;
    &lt;subject&gt;datacite&lt;/subject&gt;
    &lt;subject&gt;doi&lt;/subject&gt;
    &lt;subject&gt;metadata&lt;/subject&gt;
    &lt;/subjects&gt;
  &lt;dates&gt;
    &lt;date dateType=&quot;Created&quot;&gt;2016-12-20&lt;/date&gt;
    &lt;date dateType=&quot;Issued&quot;&gt;2016-12-20&lt;/date&gt;
    &lt;date dateType=&quot;Updated&quot;&gt;2016-12-20&lt;/date&gt;
  &lt;/dates&gt;
  &lt;relatedIdentifiers&gt;
    &lt;relatedIdentifier relatedIdentifierType=&quot;DOI&quot; relationType=&quot;References&quot;&gt;10.5438/0012&lt;/relatedIdentifier&gt;
    &lt;relatedIdentifier relatedIdentifierType=&quot;DOI&quot; relationType=&quot;References&quot;&gt;10.5438/55E5-T5C0&lt;/relatedIdentifier&gt;
    &lt;relatedIdentifier relatedIdentifierType=&quot;DOI&quot; relationType=&quot;IsPartOf&quot;&gt;10.5438/0000-00SS&lt;/relatedIdentifier&gt;
  &lt;/relatedIdentifiers&gt;
  &lt;version&gt;1.0&lt;/version&gt;
  &lt;descriptions&gt;
    &lt;description descriptionType=&quot;Abstract&quot;&gt;
      Eating your own dog food is a slang term to describe that an organization should itself use the products and services it provides. For DataCite this means that we should use DOIs with appropriate metadata and strategies for long-term preservation for...
    &lt;/description&gt;
  &lt;/descriptions&gt;
&lt;/resource&gt;</code></pre>
<p>Schema.org metadata in JSON-LD format can be added to Wordpress blogs using a <a href="https://wordpress.org/plugins/wp-structuring-markup/">plugin</a>, and more generally can be added to any webpage using tools such as <a href="https://moz.com/blog/using-google-tag-manager-to-dynamically-generate-schema-org-json-ld-tags">Google Tag Manager</a>.</p>
<h3 id="doi-minting-workflow">DOI minting workflow</h3>
<p>Publishing a blog post with embedded schema.org metadata, which is then used to mint a DOI and register DOI metadata, changes the DOI minting workflow for this blog. Although the publication workflow of a blog is much simpler than for peer-reviewed content, there are still three distinct phases:</p>
<ul>
<li>post is drafted by author</li>
<li>post is shared for feedback with staff (and possibly others)</li>
<li>post is published</li>
</ul>
<p>A DOI for a DataCite blog post is minted in phase 2, i.e. as soon as it is clear that the post will be published. What we are not doing at this phase is making the metadata public – we set the <code>is_active</code> flag in the DataCite MDS to false. This prevents indexing in DataCite Search, and the post can only be found if you know the DOI. For sensitive content we could redirect the DOI to a generic landing page, but that would be overkill for the typical blog post. Once the post is published, we set the <code>is_active</code> flag to true, enabling indexing, and show the post on the DataCite blog homepage.</p>
<p>With this workflow we have the DOI before publication, which is helpful as a link to collect limited feedback, or for joint blog posts with other organizations, such as our organization identifier blog post in November (Fenner, <a href="https://blog.datacite.org/schema-org-register-dois/#ref-https://doi.org/10.5438/TNHX-54CG">2016a</a>).</p>
<p>On the other hand, we should not register the DOI too early, e.g. for draft posts that are never published. What we should also avoid is using something that looks like a DOI, but is not registered with the Handle system. Geoff Bilder has described the problems with such DOI-like strings as internal identifiers in a June 2016 <a href="http://blog.crossref.org/2016/06/doi-like-strings-and-fake-dois.html">post</a> on the Crossref blog.</p>
<p>The DataCite blog uses "cool" DOIs that are generated from random numbers using the base32 algorithm (Fenner, <a href="https://blog.datacite.org/schema-org-register-dois/#ref-https://doi.org/10.5438/55E5-T5C0">2016b</a>). We have modified this process a little bit: we create an internal identifier (we call them accession number) that contains a random number unique to the DataCite blog for every draft post – this post has accession number <code>MS-12</code>. When we mint the DOI this accession number - ignoring letters and hyphens - is turned into a DOI, and the DOI can be predicted because of the base32 algorithm. This workflow avoids using DOI-like strings as internal identifiers, and eliminates the small risk of using the same random number twice when minting a DOI.</p>
<h3 id="blog-posts-in-jats-xml">Blog posts in JATS XML</h3>
<p>Blog posts are web pages and the landing page for the DOI also contains the fulltext of the post. But there are good reasons to make a blog post also available in downloadable form, most importantly to facilitate reuse, and for archiving. Journal Article Tag Suite (<a href="https://jats.nlm.nih.gov/">JATS</a>) is an XML standard for tagging journal articles, used by the <a href="https://www.ncbi.nlm.nih.gov/pmc/">PubMed Central</a> full-text archive of biomedical literature and by an increasing number of scholarly publishers.</p>
<p>JATS is an appropriate format for the blog posts of this blog, and starting this week all of our posts are also available in JATS XML format. You can see the download URL in the schema.org markup, we will add a more visible link to all posts once some minor tagging issues are resolved. We will also start registering the download URL with the DataCite MDS as <code>media</code>, making the JATS XML available to <a href="http://citation.crosscite.org/docs.html">DOI content negotiation</a>, and thus direct download. This should facilitate reuse by others, e.g. aggregation of content from multiple sources and display of content in different formats. This blog uses the <a href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution</a> license, allowing the copying, redistribution and remixing of the material in any medium or format for any purpose.</p>
<h3 id="the-blog-as-container">The blog as container</h3>
<p>Also this week we assigned a DOI to the DataCite blog itself (Cruse, Rueda, Garza, &amp; Fenner, <a href="https://blog.datacite.org/schema-org-register-dois/#ref-https://doi.org/10.5438/0000-00SS">2015</a>). The blog is added as a <code>isPartOf</code> relation to the schema.org and DataCite metadata of each blog post. This should facilitate discovery of related content via metadata, and allows users to also refer to the blog itself instead of individual posts. The blog is registered as a <code>resourceTypeGeneral</code> of Collection.</p>
<p>The alternative would have been to describe the DataCite blog as the <code>publisher</code> of our blog posts. We are using <strong><strong>DataCite</strong></strong> as <code>publisher</code> instead, as we feel the publisher should be a person or organization.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/0000-00cc">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>Cruse, P., Rueda, L., Garza, K., &amp; Fenner, M. (2015). DataCite blog. DataCite. <a href="https://doi.org/10.5438/0000-00SS">https://doi.org/10.5438/0000-00SS</a></p>
<p>Dublin Core Metadata Element Set, Version 1.1. (2012). Retrieved from <a href="http://dublincore.org/documents/2012/06/14/dces/">http://dublincore.org/documents/2012/06/14/dces/</a></p>
<p>Fenner, M. (2016a). Announcing the organization identifier project: A way forward. <em>DataCite</em>. <a href="https://doi.org/10.5438/TNHX-54CG">https://doi.org/10.5438/TNHX-54CG</a></p>
<p>Fenner, M. (2016b). Cool doi’s. <em>DataCite</em>. <a href="https://doi.org/10.5438/55E5-T5C0">https://doi.org/10.5438/55E5-T5C0</a></p>
<p>Fenner, M. (2016c). Eating your own dog food. <em>DataCite</em>. <a href="https://doi.org/10.5438/4K3M-NYVG">https://doi.org/10.5438/4K3M-NYVG</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mysteries in Reference Lists]]></title>
        <id>651ngj8-g1w8wys-dhpqaeb-d822m</id>
        <link href="https://blog.front-matter.io/mfenner/mysteries-in-reference-lists"/>
        <updated>2016-12-23T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[On Tuesday the journal PLOS ONE celebrated its 10th anniversary (see blog post by PLOS ONE Editor-in-Chief Jörg Heber and blog post by PLOS ONE Managing Editor Iratxe Puebla and PLOS Advocacy Director Catriona MacCallum). PLOS ONE (and PLOS)...]]></summary>
        <content type="html"><![CDATA[<p>On Tuesday the journal PLOS ONE celebrated its 10th anniversary (see <a href="http://blogs.plos.org/plos/2016/12/ten-years-of-advancing-science-as-one/">blog post</a> by PLOS ONE Editor-in-Chief Jörg Heber and <a href="http://blogs.plos.org/everyone/2016/12/20/the-ride-of-your-life-one-to-the-power-of-10/">blog post</a> by PLOS ONE Managing Editor Iratxe Puebla and PLOS Advocacy Director Catriona MacCallum). PLOS ONE (and PLOS) have changed scholarly publishing in many ways, from a DataCite perspective probably most importantly via the data policy <a href="http://blogs.plos.org/everyone/2014/02/24/plos-new-data-policy-public-access-data-2/">updated in February 2014</a> that states that</p>
<blockquote>
PLOS journals require authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception.
</blockquote>
<p>PLOS ONE was not the first journal with a Joint Data Archiving Policy (Whitlock, <a href="https://blog.datacite.org/mysteries-in-reference-lists/#ref-https://doi.org/10.1016/j.tree.2010.11.006">2011</a>), but this policy update moved proper archiving in public repositories of data used in papers into the mainstream, given that PLOS ONE had become the largest journal in the world by number of papers published.</p>
<h2 id="volumes-issues-and-pages">Volumes, issues and pages</h2>
<p>Publishing so many papers, and doing this only in electronic form, means that PLOS ONE doesn't really have journal issues, and that papers are published daily as they become ready – a common pattern with online-only journals. This means that the traditional way of referencing a scholarly article via journal name, volume, issue and page numbers isn't really useful anymore, as a proxy PLOS ONE uses the publication year as volume, month as issue and an <a href="https://jats.nlm.nih.gov/publishing/tag-library/1.1/element/elocation-id.html">electronic location identifier</a> instead of page numbers, for example <em>PLOS ONE</em>, 9(8), e105948. But of course we don't use this information to uniquely identify and locate a PLOS ONE article, we use the DOI instead – <a href="https://doi.org/10.1371/journal.pone.0105948">https://doi.org/10.1371/journal.pone.0105948</a> for this example.</p>
<p>This particular DOI is for an interesting article by Norris et al. (<a href="https://blog.datacite.org/mysteries-in-reference-lists/#ref-https://doi.org/10.1371/journal.pone.0105948">2014</a>) that describes how rocks in Death Valley slide with the help of thin layers of ice and wind, a phenomenon known since the 1940s, but in this paper for the first time systematically analyzed using GPS.</p>
<figure>
<img src="https://blog.datacite.org/images/2016/12/journal.pone.0105948.g004.png" class="kg-image" alt="GPS-instrumented rock with its rock trail, Fig. 4 from Norris et al. (2014), Creative Commons Attribution License" /><figcaption aria-hidden="true">GPS-instrumented rock with its rock trail, Fig. 4 from Norris et al. (<a href="https://blog.datacite.org/mysteries-in-reference-lists/#ref-https://doi.org/10.1371/journal.pone.0105948">2014</a>), <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</a></figcaption>
</figure>
<p>Before joining DataCite in 2015 I worked for PLOS for three years, helping with they Article-Level Metrics initiative (Fenner, <a href="https://blog.datacite.org/mysteries-in-reference-lists/#ref-https://doi.org/10.1371/journal.pbio.1001687">2013</a>). The sliding rocks paper has been a fascinating ALM example, as the paper <a href="http://journals.plos.org/plosone/article/metrics?id=10.1371/journal.pone.0105948">drew a lot of attention</a> beyond traditional citations.</p>
<p>But the sliding rocks paper has of course also been cited, and I counted 12 citations this week. These citations are listed below, together with what is shown in the reference list:</p>
<ol>
<li><strong><strong>Journal of Geophysical Research Planets</strong></strong> (El-Maarry et al., <a href="https://blog.datacite.org/mysteries-in-reference-lists/#ref-https://doi.org/10.1002/2015JE004895">2015</a>). Norris, R., J. Norris, R. Lorenz, J. Ray and B. Jackson, (2014), Sliding rocks on Racetrack Playa, Death Valley National Park: First observation of rocks in motion, PLoS One, 9(8), e105948, doi:10.1371/journal.pone.0105948.</li>
<li><strong><strong>Biophysical Journal</strong></strong> (Rambo &amp; Tainer, <a href="https://blog.datacite.org/mysteries-in-reference-lists/#ref-https://doi.org/10.1016/j.bpj.2015.04.023">2015</a>). Norris, R. D., J. M. Norris, ., B. Jackson. 2014. Sliding rocks on Racetrack Playa, Death Valley National Park: first observation of rocks in motion. PLoS ONE. 9:e105948.</li>
<li><strong><strong>Earth Surface Dynamics Discussions</strong></strong> (R. D. Lorenz et al., <a href="https://blog.datacite.org/mysteries-in-reference-lists/#ref-https://doi.org/10.5194/esurfd-2-1005-2014">2014</a>). Norris, R., Norris, J., Lorenz, R., Ray, J., and Jackson, B.: First observation of rock motion on Racetrack Playa, Death Valley, PLoS ONE, 9, e105948, doi:10.1371/journal.pone.0105948, 2014. <em>Title is different from original publication.</em></li>
<li><strong><strong>Western North American Naturalist</strong></strong> (Baumgardner &amp; Shaffer, <a href="https://blog.datacite.org/mysteries-in-reference-lists/#ref-https://doi.org/10.3398/064.075.0213">2015</a>). Norris, R.D., J.M. Norris, R.D. Lorenz, J. Ray, and B. Jackson. 2014. Sliding rocks on Racetrack Playa, Death Valley National Park: first observation of rocks in motion. PLOS ONE 9:1–11. dx.doi.org/10.1371/journal.pone.0105948. <em>PLOS ONE uses electronic location identifier instead of page numbers.</em></li>
<li><strong><strong>Aeolian Research</strong></strong> (Jones &amp; Hooke, <a href="https://blog.datacite.org/mysteries-in-reference-lists/#ref-https://doi.org/10.1016/j.aeolia.2015.08.001">2015</a>). Norris, R.D., Norris, J.M., Lorenz, R.D., Ray, J., Jackson, B., 2014. Sliding rocks on Racetrack Playa, Death Valley National Park: first observation of rocks in motion. Plos One 9 (8), &lt;www.plosone.org&gt;. <em>Link goes to PLOS ONE homepage.</em></li>
<li><strong><strong>Aeolian Research</strong></strong> (Sanz-Montero, Cabestrero, &amp; Rodríguez-Aranda, <a href="https://blog.datacite.org/mysteries-in-reference-lists/#ref-https://doi.org/10.1016/j.aeolia.2016.01.003">2016</a>). Norris, R.D., Norris, J.M., Lorenz, R.D., Ray, J., Jackson, B., 2014. Sliding rocks on Racetrack playa, death valley national park: first observation of rocks in motion. PLoS ONE 9 (8).</li>
<li><strong><strong>Earth Surfaces Processes and Landforms</strong></strong> (Sanz-Montero, Cabestrero, &amp; Rodríguez-Aranda, <a href="https://blog.datacite.org/mysteries-in-reference-lists/#ref-https://doi.org/10.1002/esp.3677">2015</a>). Norris JM, Lorenz RD, Ray J, Jackson B. 2014. Sliding rocks on Racetrack Playa, Death Valley National Park: first observation of rocks in motion. PLoS One 9: e1059448. <em>Name of first author missing, extra 4 in electronic location identifier</em>.</li>
<li><strong><strong>Journal of Geology &amp; Geophysics</strong></strong> Norris RD, Norris JM, Lorenz RD, Ray J, Jackson B (2014) Sliding Rocks on Racetrack Playa, Death Valley National Park: First Observation of Rocks in Motion. PLoS ONE 9 (8), doi: 10.1371/journal.pone.0105948.</li>
<li><strong><strong>Journal of Archaeological Science</strong></strong> (Grayson &amp; Meltzer, <a href="https://blog.datacite.org/mysteries-in-reference-lists/#ref-https://doi.org/10.1016/j.jas.2015.02.009">2015</a>). Norris, R.D., Norris, J.M., Lorenz, R.D., Ray, J., Jackson, B., 2014. Sliding rocks on Racetrack Playa, Death Valley National Park: first observation of rocks in motion. PLoS One 9 (8), e105948.</li>
<li><strong><strong>Transactions of Tianjin University</strong></strong> (Li, Zhou, &amp; Wang, <a href="https://blog.datacite.org/mysteries-in-reference-lists/#ref-https://doi.org/10.1007/s12209-016-2596-z">2016</a>). Norris R D, Norris J M, Lorenz R D et al. Sliding rocks on Racetrack Playa, Death Valley National Park: First observation of rocks in motion[J]. PLoS One, 2014, 9(8): e105948.</li>
<li><strong><strong>Creative Approaches to Research</strong></strong> (Hannigan, Raphael, White, Bragg, &amp; Cripps Clark, <a href="https://blog.datacite.org/mysteries-in-reference-lists/#ref-http://hdl.handle.net/10536/DRO/DU:30088291">2016</a>). Norris, R. D., Norris, J. M., Lorenz, R. D., Ray, J., &amp; Jackson, B. (2014). Sliding Rocks on Racetrack Playa, Death Valley National Park: First Observation of Rocks in Motion. PLoS One, 9(8), e105948.</li>
<li><strong><strong>ArXiv</strong></strong> (Asorey, Nunez, &amp; Sarmiento-Cano, <a href="https://blog.datacite.org/mysteries-in-reference-lists/#ref-https://arxiv.org/abs/1501.04916">2015</a>). Richard D Norris, James M Norris, Ralph D Lorenz, Jib Ray, and Brian Jackson. Sliding rocks on racetrack playa, death valley national park: First observation of rocks in motion. PloS one, 9(8):e105948, 2014.</li>
</ol>
<p>You can see something interesting in these references: only 4 in 12 include the DOI, only one reference includes a URL, but all 12 include the volume and 8 each include the issue and the electronic location identifier. In other words, the references are formatted for a journal article that can be found as physical copy in a library, sorted and shelved by publication name, volume and issue. Only one in three references use a DOI and/or URL, even though that is the only way to fetch the online-only article.</p>
<p>Journal name, volume, issue and electronic location identifier also don't work too well in uniquely identifying the journal article, as this information is human-readable, but difficult for a machine to extract from the reference list. Many publishers of course link to referenced articles using the DOI behind the scenes even when not displaying the DOI, but that is a brittle implementation. Reference lists are for example also used by authors in manuscript submissions, and without DOIs displayed in the reference list it becomes much harder for the author to provide machine-readable information about the references used.</p>
<h2 id="citation-styles">Citation styles</h2>
<p>Reference lists remain one of the mysteries in scholarly publishing. Not only does this small example demonstrate that they still have not been fully adapted to how journal articles are published, read and cited in 2016, but you can also see that the examples use multiple citation styles. There are <a href="https://github.com/citation-style-language/styles">thousands of them</a>, displaying the same information in so many different ways that generating and consuming references has become a business in itself. Another mystery is the limitation of the number of references in an online-only journal. While it makes sense to set some limit, these numbers sometimes seem arbitrarily low, coming from a time when every extra page printed was costly.</p>
<p>It seems unrealistic in the near future to ever agree on a common citation style. But what we can do is at least use a citation style that is widely used instead of reinventing one for every journal, and use a style that includes the DOI in the reference. PLOS switched from a PLOS-specific style to the Vancouver or NLM style (Patrias, <a href="https://blog.datacite.org/mysteries-in-reference-lists/#ref-https://www.ncbi.nlm.nih.gov/books/NBK7256">2015</a>) in 2015. This style is widely used by biomedical journals, and PLOS ONE articles now include DOIs in reference lists, as you can see for example in this 2015 PLOS ONE paper (Tenopir et al., <a href="https://blog.datacite.org/mysteries-in-reference-lists/#ref-https://doi.org/10.1371/journal.pone.0134826">2015</a>) – the sliding rocks paper was published in 2014.</p>
<p>Vancouver/NLM allows the inclusion of DOIs in references, but isn't really urging users to do so, and it still recommends a traditional style of referencing:</p>
<figure>
<img src="https://blog.datacite.org/images/2016/12/ch1e1.jpg" class="kg-image" alt="The general format for a reference to a journal article, taken from (Patrias, 2015)" /><figcaption aria-hidden="true">The general format for a reference to a journal article, taken from (Patrias, <a href="https://blog.datacite.org/mysteries-in-reference-lists/#ref-https://www.ncbi.nlm.nih.gov/books/NBK7256">2015</a>)</figcaption>
</figure>
<p>This blog uses the <a href="http://www.apastyle.org/">APA style</a>, a style that is not only widely used and documented, but also includes DOIs in references (as you can see in the reference list of this blog post), and is one of the few citation styles with <a href="http://blog.apastyle.org/apastyle/2013/12/how-to-cite-a-data-set-in-apa-style.html">specific support for data citations</a> (adding <em>[Data set]</em> after the title). And in contrast to the Vancouver style this style uses the <a href="http://www.chicagomanualofstyle.org/tools_citationguide.html">Author-Date</a> format for in-text citations, providing more context to the reader of the article.</p>
<p>Using journal name, volume, issue and page numbers in a reference poses a particular challenge for DataCite, as the DataCite metadata schema doesn't support them. The main reason for this is DataCite's focus on providing metadata for datasets. Also, the DataCite metadata schema is based on <a href="http://dublincore.org/documents/dc-citation-guidelines/">Dublin Core</a>, which also doesn't have these properties (<a href="http://dublincore.org/documents/dcmi-terms/#bibliographicCitation">bibliographicCitation</a> can be used instead). More than 1.5 million text documents have been registered with a DataCite DOI, and many of them probably would have had journal name, volume, issue and pages information available. Should we add support for these properties to the DataCite metadata schema, or should we see these properties as no longer essential for citation information in a reference and leave them out of the metadata schema? I would argue that resources that have a digital object identifier don't require volume, issue and pages information to uniquely identify and/or locate the resource.</p>
<p>The other challenge for DataCite is that the current state of reference lists in journal articles makes it harder than needed to include data citations in them. When DOIs are not included in reference lists since the citation style doesn't want them displayed, then manuscripts with data citations submitted by authors need special treatment, which limits adoption because of the extra effort required. Publishers routinely rebuild reference lists from scratch by fetching the DOI and associated metadata based on the citation information provided, and these tools are built around citation metadata typically found in journal articles (including volume, issue and page information) and Crossref DOIs.</p>
<p>The conclusions from the above are simple:</p>
<blockquote>
As a publisher, require a citation style that includes the DOI.
</blockquote>
<h2 id="mystery-solved">Mystery solved?</h2>
<p>The X-Files is an American TV series about FBI special agents Fox Mulder and Dana Scully who investigate unsolved cases involving paranormal phenomena. And in an episode aired in February 2016 (<a href="https://doi.org/10.5240/68D6-DD1B-C9D5-AE84-9A9E-2">The X-Files: Mulder &amp; Scully Meet the Were-Monster</a>, via <a href="http://www.imdb.com/title/tt4549942/quotes">IMDB</a>), Fox Mulder refers to the PLOS ONE Sliding Rocks paper when he says:</p>
<blockquote>
Scully, since we've been away, much of the "unexplained" has been explained. The "Death Valley Racetrack"? Turns out it was just ice formations, moving the rocks around as it melted. Yeah, ice.
</blockquote>
<p>To refer to this episode you can again use <a href="https://doi.org/10.5240/68D6-DD1B-C9D5-AE84-9A9E-2">a DOI</a>, which in this case is not for scholarly content but is an <a href="http://eidr.org/">EIDR</a> – A universal unique identifier for movie and television assets. And now we have come full circle.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/ct8b-x1ce">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>Asorey, H., Nunez, L. A., &amp; Sarmiento-Cano, C. (2015). Exposicion temprana de nativos digitales en ambientes, metodologias y tecnicas de investigacion en la universidad. <em>arXiv [Physics]</em>. Retrieved from <a href="https://arxiv.org/abs/1501.04916">https://arxiv.org/abs/1501.04916</a></p>
<p>Baumgardner, G. D., &amp; Shaffer, B. S. (2015). Sliding Bones: Movement of Skeletal Material Over Smith Creek Playa in Nevada and Its Taphonomic and Paleontologic Implications. <em>Western North American Naturalist</em>, <em>75</em>(2), 236–243. <a href="https://doi.org/10.3398/064.075.0213">https://doi.org/10.3398/064.075.0213</a></p>
<p>El-Maarry, M. R., Watters, W. A., Yoldi, Z., Pommerol, A., Fischer, D., Eggenberger, U., &amp; Thomas, N. (2015). Field investigation of dried lakes in western United States as an analogue to desiccation fractures on Mars. <em>Journal of Geophysical Research: Planets</em>, <em>120</em>(12), 2015JE004895. <a href="https://doi.org/10.1002/2015JE004895">https://doi.org/10.1002/2015JE004895</a></p>
<p>Fenner, M. (2013). What Can Article-Level Metrics Do for You? <em>PLOS Biology</em>, <em>11</em>(10), e1001687. <a href="https://doi.org/10.1371/journal.pbio.1001687">https://doi.org/10.1371/journal.pbio.1001687</a></p>
<p>Grayson, D. K., &amp; Meltzer, D. J. (2015). Revisiting Paleoindian exploitation of extinct North American mammals. <em>Journal of Archaeological Science</em>, <em>56</em>, 177–193. <a href="https://doi.org/10.1016/j.jas.2015.02.009">https://doi.org/10.1016/j.jas.2015.02.009</a></p>
<p>Hannigan, S., Raphael, J.-A., White, P., Bragg, L., &amp; Cripps Clark, J. (2016). Collaborative reflective experience and practice in education explored through self-study and arts-based research. <em>Creative Approaches to Research</em>, <em>9</em>(1), 84–110. Retrieved from <a href="http://hdl.handle.net/10536/DRO/DU:30088291">http://hdl.handle.net/10536/DRO/DU:30088291</a></p>
<p>Jones, R., &amp; Hooke, R. L. (2015). Racetrack Playa: Rocks moved by wind alone. <em>Aeolian Research</em>, <em>19, Part A</em>, 1–3. <a href="https://doi.org/10.1016/j.aeolia.2015.08.001">https://doi.org/10.1016/j.aeolia.2015.08.001</a></p>
<p>Li, M., Zhou, S., &amp; Wang, G. (2016). 3D identification and stability analysis of key surface blocks of rock slope. <em>Transactions of Tianjin University</em>, <em>22</em>(4), 317–323. <a href="https://doi.org/10.1007/s12209-016-2596-z">https://doi.org/10.1007/s12209-016-2596-z</a></p>
<p>Lorenz, R. D., Norris, J. M., Jackson, B. K., Norris, R. D., Chadbourne, J. W., &amp; Ray, J. (2014). Trail formation by ice-shoved “sailing stones” observed at Racetrack Playa, Death Valley National Park. <em>Earth Surface Dynamics Discussions</em>, <em>2</em>(2), 1005–1022. <a href="https://doi.org/10.5194/esurfd-2-1005-2014">https://doi.org/10.5194/esurfd-2-1005-2014</a></p>
<p>Norris, R. D., Norris, J. M., Lorenz, R. D., Ray, J., &amp; Jackson, B. (2014). Sliding Rocks on Racetrack Playa, Death Valley National Park: First Observation of Rocks in Motion. <em>PLOS ONE</em>, <em>9</em>(8), e105948. <a href="https://doi.org/10.1371/journal.pone.0105948">https://doi.org/10.1371/journal.pone.0105948</a></p>
<p>Patrias, K. (2015). <em>Citing Medicine</em> (2nd ed.). National Library of Medicine (US). Retrieved from <a href="https://www.ncbi.nlm.nih.gov/books/NBK7256/">https://www.ncbi.nlm.nih.gov/books/NBK7256/</a></p>
<p>Rambo, R. P., &amp; Tainer, J. A. (2015). Modeling Macromolecular Motions by X-Ray-Scattering-Constrained Molecular Dynamics. <em>Biophysical Journal</em>, <em>108</em>(10), 2421–2423. <a href="https://doi.org/10.1016/j.bpj.2015.04.023">https://doi.org/10.1016/j.bpj.2015.04.023</a></p>
<p>Sanz-Montero, M. E., Cabestrero, Ó., &amp; Rodríguez-Aranda, J. P. (2015). Sedimentary effects of flood-producing windstorms in playa lakes and their role in the movement of large rocks. <em>Earth Surface Processes and Landforms</em>, <em>40</em>(7), 864–875. <a href="https://doi.org/10.1002/esp.3677">https://doi.org/10.1002/esp.3677</a></p>
<p>Sanz-Montero, M. E., Cabestrero, Ó., &amp; Rodríguez-Aranda, J. P. (2016). Comments on Racetrack playa: Rocks moved by wind alone. <em>Aeolian Research</em>, <em>20</em>, 196–197. <a href="https://doi.org/10.1016/j.aeolia.2016.01.003">https://doi.org/10.1016/j.aeolia.2016.01.003</a></p>
<p>Tenopir, C., Dalton, E. D., Allard, S., Frame, M., Pjesivac, I., Birch, B., … Dorsett, K. (2015). Changes in Data Sharing and Data Reuse Practices and Perceptions among Scientists Worldwide. <em>PLOS ONE</em>, <em>10</em>(8), e0134826. <a href="https://doi.org/10.1371/journal.pone.0134826">https://doi.org/10.1371/journal.pone.0134826</a></p>
<p>Whitlock, M. C. (2011). Data archiving in ecology and evolution: Best practices. <em>Trends in Ecology &amp; Evolution</em>, <em>26</em>(2), 61–65. <a href="https://doi.org/10.1016/j.tree.2010.11.006">https://doi.org/10.1016/j.tree.2010.11.006</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Eating your own Dog Food]]></title>
        <id>4t43ty3-z1p97vs-m70wwn8-rb8de</id>
        <link href="https://blog.front-matter.io/mfenner/eating-your-own-dog-food"/>
        <updated>2016-12-20T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Eating your own dog food is a slang term to describe that an organization should itself use the products and services it provides. For DataCite this means that we should use DOIs with appropriate metadata and strategies for long-term preservation for the scholarly outputs we produce....]]></summary>
        <content type="html"><![CDATA[<p><a href="https://newrepublic.com/article/115349/dogfooding-tech-slang-working-out-glitches">Eating your own dog food</a> is a slang term to describe that an organization should itself use the products and services it provides. For DataCite this means that we should use DOIs with appropriate metadata and strategies for long-term preservation for the scholarly outputs we produce. For the most part this is not research data, but rather technical documents such as the DataCite Schema and its documentation (<a href="https://blog.datacite.org/eating-your-own-dog-food/#ref-https://doi.org/10.5438/0012">2016</a>).</p>
<p>These outputs also include the posts on this blog, where we discuss topics relevant for the DataCite community, but also of broader interest to anyone who cares about research data, persistent identifiers, and scholarly infrastructure. And starting today all blog posts on this blog will have a DOI, metadata and use a persistent storage mechanism.</p>
<figure>
<img src="https://blog.datacite.org/images/2016/12/230785.jpg" class="kg-image" alt="Photo by Bill Emrich. CC Zero." /><figcaption aria-hidden="true">Photo by <a href="https://www.pexels.com/photo/black-and-tan-yorkshire-terrier-puppy-230785/">Bill Emrich</a>. <a href="https://creativecommons.org/publicdomain/zero/1.0/">CC Zero</a>.</figcaption>
</figure>
<h3 id="technical-implementation">Technical Implementation</h3>
<p>This blog is powered by the static site generator <a href="https://middlemanapp.com/">Middleman</a>, with blog posts written in <a href="http://commonmark.org/">Markdown</a> and converted to HTML using <a href="http://pandoc.org/">Pandoc</a> and the <a href="https://travis-ci.org/">Travis CI</a> continuous integration service. Static site generator means that there is no database or application server powering the site, making website adminstration simpler, cheaper and safer. In addition to the blog, the <a href="https://www.datacite.org/">DataCite homepage</a> and <a href="https://schema.datacite.org/">Metadata Schema subsite</a> are also generated using Middleman.</p>
<p>The simplicity is particularly important here, as registering the DOIs and metadata can be accomplished using a command line utility written by DataCite staff that doesn't need to know much about the internals of Middleman, and thus can be easily adapted to other static site generators such as <a href="http://jekyllrb.com/">Jekyll</a>, <a href="http://gohugo.io/">Hugo</a> or <a href="https://hexo.io/">Hexo</a>. The command line utility is <a href="https://github.com/datacite/cirneco">Cirneco</a>, generating the metadata XML according to the DataCite Metadata Schema, and registering DOI and metadata with the DataCite MDS. Like all tools mentioned in this post Cirneco is open source software, please reach out to us if you are interested in implementing similar functionality for your blog.</p>
<h3 id="generating-dois">Generating DOIs</h3>
<p>The DOIs for this blog are generated automatically, using a modified base32 encoding algorithm that is provided by Cirneco, as discussed last week (Fenner, <a href="https://blog.datacite.org/eating-your-own-dog-food/#ref-https://doi.org/10.5438/55E5-T5C0">2016</a>). The DOI is generated and minted when a new post is pushed to <a href="https://blog.datacite.org/">https://blog.datacite.org</a>. This avoids two problems: a) DOI-like strings in the wild before publication and b) the randomly generated DOI exists already (we can simply generate a new one). All DOIs are short, without semantic infomation that might change over time, and with a checksum to minimize transcription errors, for example <strong><strong>https://doi.org/10.5438/XCBJ-G7ZY</strong></strong>. Going forward we encourage users to link to the DataCite Blog using the DOI, as these links will continue to work even if we ever move the blog to a different location.</p>
<h3 id="generating-metadata">Generating Metadata</h3>
<p>For the generation of metadata, we need to strike a balance between simple author provided metadata, but rich enough to aid discovery. We are doing this via three mechanisms:</p>
<ul>
<li>metadata provided by the author</li>
<li>default metadata for the blog</li>
<li>metadata automatically extracted from content</li>
</ul>
<p>The metadata provided by the author are the typical metadata for blog posts, provided via <a href="https://gohugo.io/content/front-matter/">YAML front matter</a> at the beginning of each post:</p>
<pre><code>---
layout: post
title: Eating your own Dog Food
author: mfenner
date: 2016-12-19
tags:
- datacite
- doi
- metadata
---</code></pre>
<p>We can reuse all these metadata when generating DataCite metadata, using the tags as <code>subjects</code>.</p>
<p>The default metadata are metadata that always stay the same for the blog, such as <code>publisher</code>, <code>HostingInstitution</code> and <code>rights</code>. We can store them in a site-wide configuration file. We can also assume reasonable defaults that can be overridden in the YAML front matter, e.g. <code>resourceType</code> (we use <a href="https://schema.org/BlogPosting">BlogPosting</a> with <code>resourceTypeGeneral</code> Text) and <code>version</code>. We store more information about authors outside the blog post, including <code>givenName</code>, <code>familyName</code> and <code>nameIdentifier</code> (we now show the ORCID ID of every blog author at the bottom of the post).</p>
<p>Finally, there are metadata that we can automatically extract from the blog post, and we are currently doing this for the <code>description</code> and <code>relatedIdentifier</code>. This blog uses Pandoc and BibTex to generate the references section at the end, and we can fetch this information and convert it into the format needed for <code>relatedIdentifier</code>.</p>
<p>Taken together we can provide all metadata that are <em>required</em> or <em>recommended</em> in the Metadata Schema documentation (<a href="https://blog.datacite.org/eating-your-own-dog-food/#ref-https://doi.org/10.5438/0012">2016</a>), and we can do this without any extra effort for the author. The full XML is avalailable <a href="https://data.crosscite.org/application/vnd.datacite.datacite+xml/10.5438/4K3M-NYVG">here</a>.</p>
<p>Not all blog posts need to be cited formally with metadata in a <em>references</em> list formatted according to a specific citation style. But these metadata greatly help with discovery, a search in DataCite Search for <a href="http://search.datacite.org/works?query=eating+dog+food">eating dog food</a> will for example bring up this blog post as the first hit.</p>
<h3 id="persistent-storage">Persistent storage</h3>
<p>Using DOIs means that readers not only expect rich metadata that help with citation and discovery, but also that DataCite takes extra care to preserve the blog posts, thinking beyond the particular technical implementation or even the contiuing existence of this blog. This is an area where we do need to do more work, starting with a decision about the best archival format for a blog post (HTML, PDF, <a href="https://jats.nlm.nih.gov/">JATS</a>?). For now blog posts are hosted in multiple Git repositories (<a href="https://github.com/datacite/blog">one of them on Github</a>), and in two independent Amazon S3 buckets that each use <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html">versioning</a>. Multiple locations with versioning are a good start, but more work is clearly needed.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/4k3m-nyvg">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>DataCite Metadata Working Group. (2016). DataCite metadata schema for the publication and citation of research data v4.0. <em>DataCite</em>. <a href="https://doi.org/10.5438/0012">https://doi.org/10.5438/0012</a></p>
<p>Fenner, M. (2016). Cool doi’s. <em>DataCite</em>. <a href="https://doi.org/10.5438/55E5-T5C0">https://doi.org/10.5438/55E5-T5C0</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reference Lists and Tables of Content]]></title>
        <id>57eeex2-c0s9mmr-vr1e296-y19sp</id>
        <link href="https://blog.front-matter.io/mfenner/reference-lists-and-tables-of-content"/>
        <updated>2016-12-19T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Geoff Bilder from CrossRef likes to show the following slide at scholarly conferences, and then asks the audience what they see:Most of us probably immediately recognize this document as a scholarly article....]]></summary>
        <content type="html"><![CDATA[<p><a href="https://twitter.com/gbilder">Geoff Bilder</a> from CrossRef likes to show the following slide at scholarly conferences, and then asks the audience what they see:</p>
<figure>
<img src="https://blog.datacite.org/images/2015/08/article-1.png" class="kg-image" />
</figure>
<p>Most of us probably immediately recognize this document as a scholarly article. This immediate recognition includes essential parts of an article such as the title - or the reference list:</p>
<figure>
<img src="https://blog.datacite.org/images/2015/08/article2-1.png" class="kg-image" alt="Paper 2" /><figcaption aria-hidden="true">Paper 2</figcaption>
</figure>
<p>This immediate recognition is a powerful concept, it makes it easy for the reader to navigate a scholarly document, e.g. to quickly jump to the abstract or references.</p>
<p>We don't have the same immediate recognition for datasets. Given that a large number of datasets in DataCite are in CSV (comma separated values) format, the closest we come to a immediately recognized document is probably the spreadsheet:</p>
<figure>
<img src="https://upload.wikimedia.org/wikipedia/commons/2/23/Spreadsheet_animation.gif" class="kg-image" alt="Container.From: Wikimedia Commons, licensed under CC BY-SA 3.0." /><figcaption aria-hidden="true">Container.<em>From: <a href="https://commons.wikimedia.org/wiki/File:Spreadsheet_animation.gif">Wikimedia Commons</a>, licensed under <a href="http://creativecommons.org/licenses/by-sa/3.0">CC BY-SA 3.0</a>.</em></figcaption>
</figure>
<p>A canonical format for datasets goes beyond immediate recognition of the essential parts by the user, it would also greatly facilitate reuse of data. As <a href="https://twitter.com/nickstenning">Nick Stenning</a> from the Open Knowledge Foundation (OKFN) pointed out at CSV.conf last year, the cost of shipping of goods is in large part determined by the cost of loading and unloading, and the container has dramatically changed that equation. He argued that common formats such as the OKFN <a href="https://frictionlessdata.io/specs/data-package/">data package</a> could do the same for data reuse.</p>
<figure>
<img src="https://blog.datacite.org/images/2015/08/break-bulk-sacks.png" class="kg-image" alt="Bulk parcels. From: Wikimedia Commons, licensed under CC BY-SA 3.0." /><figcaption aria-hidden="true">Bulk parcels. <em>From: <a href="https://commons.wikimedia.org/wiki/File:Hafenarbeiter_bei_der_Verladung_von_Sackgut_-_MS_Rothenstein_NDL,_Port_Sudan_1960.png">Wikimedia Commons</a>, licensed under <a href="http://creativecommons.org/licenses/by-sa/3.0">CC BY-SA 3.0</a>.</em></figcaption>
</figure>
<p>Unfortunately there are at least three problems with using spreadsheets as canonical format for datasets:</p>
<ul>
<li>not every dataset can be represented as a CSV file, there are many specialized formats (including of course Excel <code>.xlsx</code>)</li>
<li>we can't include descriptive metadata (not even authors or document title) in a CSV file</li>
<li>many datasets actually include a collecting of files: not only in CSV format, but also other data formats and support files such as a README.</li>
</ul>
<p>The approach taken by the OKFN data package format - and related formats such as the <a href="https://researchobject.github.io/specifications/bundle/">Research Object Bundle</a> - is to put all data files (in CSV or other formats) into a folder, together with a standardized machine-readable file that includes the metadata (e.g. title, authors, publication date and license). This folder can then compressed with <code>zip</code>, again yielding a single file (a very common approach used for example for <code>epub</code> and <code>docx</code>).</p>
<p>The concept described here (a collection of documents in a larger container, and a listing of all included documents) is of course at least as old as the scholarly article: the <strong><strong>book</strong></strong> as a canonical format for collections (of texts), and the <strong><strong>table of contents</strong></strong> to describe what is in the book.</p>
<figure>
<img src="https://upload.wikimedia.org/wikipedia/commons/b/b7/Table_of_Contents_PANARCHIE_published_in_1860.JPG" class="kg-image" alt="Table of contents. From: Wikimedia Commons, licensed under CC BY-SA 3.0." /><figcaption aria-hidden="true">Table of contents. <em>From: <a href="https://commons.wikimedia.org/wiki/File:Table_of_Contents_PANARCHIE_published_in_1860.JPG">Wikimedia Commons</a>, licensed under <a href="http://creativecommons.org/licenses/by-sa/3.0">CC BY-SA 3.0</a>.</em></figcaption>
</figure>
<p>The approach described here would not only help package datasets into a more reusable standard format, but the scholarly article would also greatly benefit from migrating to a container format. We all know that the concept of the scholarly article described at the beginning of this posts is falling apart - an article is simply no longer a single text document. We have not only associated figures and tables, but also associated files that can't be easily included into the article PDF, in particular files that contain the data underlying the findings of the article, but also other supplementary information.</p>
<p>There are currently three common approaches referencing the underlying data in a scholarly article:</p>
<ul>
<li>inclusion in supporting information files without any specific linking</li>
<li>informal citation in the article text, most commonly in the materials and methods section</li>
<li>formal citation with inclusion in the reference list</li>
</ul>
<p>Until not too long ago I was a big proponent of including all data associated with an article in the reference list, mainly to make it easier to find the data. But the reference list isn't the appropriate place for something that is really part of the article - or as colleague <a href="http://bio.unc.edu/people/faculty/vision/">Todd Vision</a> puts it: the data generated for an article are another <strong><strong>output</strong></strong> rather than an <strong><strong>input</strong></strong>. Reference lists summarize all the inputs to an article, whereas outputs belong into a <strong><strong>table of contents</strong></strong>. A table of contents isn't a standard feature of scholarly articles yet, but to me is a logical next step for the journal article format, together with using the underlying concept of a container format described earlier in this post. Extracting references to datasets from a table of contents should be as easy as extracting them from a reference list, in particular if we make sure that this table of contents is openly available.</p>
<p>Journal Article Tag Suite (<a href="http://jats.nlm.nih.gov/">JATS</a>) is the standard machine-readable format for journal articles in the life sciences (and increasingly other sciences). At <a href="http://jats.nlm.nih.gov/jats-con/">JATS-CON</a> in April this year I proposed (starting at minute 210) to extend JATS by providing it also as a container format:</p>
<figure>
<img src="https://videocast.nih.gov/favicon.ico" class="kg-bookmark-icon" alt="JATS-CON 2015 Day 2" /><figcaption aria-hidden="true">JATS-CON 2015 Day 2</figcaption>
</figure>
<p><em>This blog post was <a href="https://doi.org/10.5438/5aeg-weev">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cool DOI's]]></title>
        <id>35wjcz0-96r8sss-fx8d85e-snd9g</id>
        <link href="https://blog.front-matter.io/mfenner/cool-dois"/>
        <updated>2016-12-15T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[In 1998 Tim Berners-Lee coined the term cool URIs (1998), that is URIs that don’t change. We know that URLs referenced in the scholarly literature are often not cool, leading to link rot (Klein et al., 2014) and making it hard or impossible to find the referenced resource.Cool URIs are,...]]></summary>
        <content type="html"><![CDATA[<p>In 1998 Tim Berners-Lee coined the term cool URIs (<a href="https://blog.datacite.org/cool-dois/#ref-https://www.w3.org/Provider/Style/URI">1998</a>), that is URIs that don’t change. We know that URLs referenced in the scholarly literature are often not cool, leading to link rot (Klein et al., <a href="https://blog.datacite.org/cool-dois/#ref-https://doi.org/10.1371/journal.pone.0115253">2014</a>) and making it hard or impossible to find the referenced resource.</p>
<p>Cool URIs are, of course, a fundamental principle behind DOIs, with the two important concepts <a href="https://www.doi.org/doi_handbook/3_Resolution.html"><em>resolution</em></a> (it is very hard to maintain a URL directly pointing at a resource) and <a href="https://www.doi.org/doi_handbook/6_Policies.html"><em>policies</em></a> (that all DOI registration agencies and organizations minting DOIs agree to maintain the redirection). The third essential element for DOIs, their <a href="https://www.doi.org/doi_handbook/4_Data_Model.html"><em>data model</em></a>, is not directly about persistent linking, but about the discoverability of the linked resources via standard metadata in a central index.</p>
<p>All DOIs, expressed as HTTP URI, are therefore cool URIs. So what is a cool DOI? And, furthermore, how to create and use them? To understand what a cool DOI is, we have to explain the three parts that make up a DOI:</p>
<figure>
<img src="https://blog.datacite.org/images/2016/12/doi-parts.png" class="kg-image" />
</figure>
<p>The three parts that make up a DOI</p>
<h3 id="proxy">Proxy</h3>
<p>The proxy is not part of the DOI specification, but almost all scholarly DOIs that users encounter today will be expressed as HTTP URLs. DataCite recommends that all DOIs are displayed as permanent URLs, consistent with the recommendations of other DOI registration agencies, e.g. the <a href="http://www.crossref.org/02publishers/doi_display_guidelines.html">Crossref DOI display guidelines</a>. When the DOI system was originally designed, it was thought that the DOI protocol would become widely used, but that clearly has not happened and displaying DOIs as <strong><strong>doi:10.5281/ZENODO.31780</strong></strong> is therefore not recommended.</p>
<p>The DOI proxy enables the functionality of expressing DOIs as HTTP URIs. Users should also be aware of two these two recommendations:</p>
<ul>
<li>Use <a href="https://www.doi.org/doi_proxy/proxy_policies.html">doi.org</a> instead of dx.doi.org as DNS name</li>
<li>Use the HTTPS protocol instead of HTTP protocol</li>
</ul>
<p>Ed Pentz from Crossref makes the case for HTTPS in a <a href="http://blog.crossref.org/2016/09/new-crossref-doi-display-guidelines.html">September blog post</a>. The web, and therefore also the scholarly web, is moving to HTTPS as the default. It is important that the DOI proxy redirects to HTTPS URLs, and it will take some time until all DataCite data centers use HTTPS for the landing pages their DOIs redirects to.</p>
<p>What many users don’t know is that doi.org is not the only proxy server for DOIs. DOIs use the handle system and any handle server will resolve a DOI, just as doi.org will resolve any handle. This means that <a href="https://hdl.handle.net/10.5281/ZENODO.31780">https://hdl.handle.net/10.5281/ZENODO.31780</a> will resolve to the landing page for that DOI and that https://doi.org/10273/BGRB5054RX05201 is a handle (for a <a href="http://www.igsn.org/">IGSN</a>) and not a DOI.</p>
<h3 id="prefix">Prefix</h3>
<p>The DOI prefix is used as a namespace so that DOIs are globally unique without requiring global coordination for every new identifier. Prefixes in the handle system and therefore for DOIs are numbers without any semantic meaning. One lesson learned with persistent identifiers is that adding meaning to the identifier (e.g. by using a prefix with the name of the data repository) is always dangerous, because – despite best intentions – all names can change over time.</p>
<p>Since the DOI prefix is a namespace to keep DOIs globally unique, there is usually no need for multiple prefixes for one organization managing DOI assignment. The tricky part is that these responsibilities can change, e.g. when an organization manages multiple repositories and one of them is migrated to another organization. It therefore makes sense to assign one prefix per list of resources that always stays together, e.g. one repository. It is possible that one prefix is managed by multiple organizations (as long as they use the same DOI registration agency), but that makes DOI management more complex.</p>
<h3 id="suffix">Suffix</h3>
<p>The suffix for a DOI can be (almost) any string. Which is both a feature and a curse. It is a feature because it gives maximal flexibility, for example when migrating existing identifiers to the DOI system. And it is a curse because it not always works well in the web context, as the list of characters allowed in a URL is limited. A good example of this are SICIs (<a href="https://en.wikipedia.org/wiki/Serial_Item_and_Contribution_Identifier">Serial Item and Contribution Identifier</a>), they were defined in 1996 before the DOI system was implemented, and could then be migrated to DOIs. Unfortunately they can contain many characters that are problematic in a URL or make it difficult to validate the DOI, as in <a href="https://doi.org/10.1002/(sici)1099-1409(199908/10)3:6/7%3C672::aid-jpp192%3E3.0.co;2-8">https://doi.org/10.1002/(sici)1099-1409(199908/10)3:6/7&lt;672::aid-jpp192&gt;3.0.co;2-8</a>. A Crossref <a href="http://blog.crossref.org/2015/08/doi-regular-expressions.html">blog post</a> by Andrew Gilmartin gives a good overview about the characters found in DOIs and suggests the following regular expression to check for valid DOIs:</p>
<pre><code>/^10.\d{4,9}/[-._;()/:A-Z0-9]+$/i</code></pre>
<p>SICIs demonstrate two other pitfalls:</p>
<ul>
<li>they contain semantic information (ISSN, volume, number, etc.) that may change over time, and</li>
<li>they are long, difficult to transcribe, with characters not allowed in URLs, and not very human-readable.</li>
</ul>
<p>Semantic information might also lead users to expect certain functionalities. A common pattern that we see at DataCite is to include information about the version or parent in the suffix, e.g. <a href="https://doi.org/10.6084/M9.FIGSHARE.3501629.V1">https://doi.org/10.6084/M9.FIGSHARE.3501629.V1</a> or <a href="https://doi.org/10.5061/DRYAD.0SN63/7">https://doi.org/10.5061/DRYAD.0SN63/7</a>. While the decision on what to put into the suffix is up to each data center, we should make sure users don't think that these are functionalities of the DOI system (e.g. that adding <strong><strong>.V2</strong></strong> to any DOI name will resolve to version 2 of that resource).</p>
<p>Another issue to keep in mind when assigning suffixes is that DOIs – in contrast to HTTP URIs – are case-insensitive, <a href="https://doi.org/10.5281/ZENODO.31780">https://doi.org/10.5281/ZENODO.31780</a> and <a href="https://doi.org/10.5281/zenodo.31780">https://doi.org/10.5281/zenodo.31780</a> are the same DOI. All DOIs are <a href="https://www.doi.org/doi_handbook/2_Numbering.html#2.4">converted to upper case</a> upon registration and DOI resolution, but DOIs are not consistently displayed in such a way.</p>
<h3 id="generating-cool-dois">Generating cool DOIs</h3>
<p>With all that, what should the ideal DOI look like? Its suffix should be:</p>
<ul>
<li>opaque without semantic information</li>
<li>work well in a web environment, avoiding characters problematic in URLs</li>
<li>short and human-readable</li>
<li>Resistant to transcription errors</li>
<li>easy to generate</li>
</ul>
<p>On Tuesday DataCite released a tool that helps generating such a suffix, an open source command line tool called <a href="https://github.com/datacite/cirneco">cirneco</a> (a lot of our open source software uses Italian dog breed names). Cirneco is a Ruby gem that can be installed via</p>
<pre><code>gem install cirneco</code></pre>
<p>Cirneco uses base32 encoding, as <a href="http://www.crockford.com/wrmg/base32.html">described</a> by Douglas Crockford. The encoding starts with a randomly generated number to guarantee uniqueness of the identifier, and then encodes the number into a string that uses all numbers and uppercase letters. It avoids the letters I, O and L as they can be confused with the letter 1 and 0, using 32 characters (and 5 checksum characters) in total. The last character is a checksum. The resulting string from cirneco always has a length of 8 characters, in groups of 4 separated by a hyphen to help with readability. The advantage of base32 encoding over using only numbers (as for example ORCID is doing) is that the resulting string becomes much more compact, the available 7 characters (plus one for the checksum) can encode 34,359,738,367 strings, compared to 10 million when only using numbers. This number is large enough that the resulting suffix will not only be unique for a given prefix, but also unique for all DOIs (there is a very small chance to get the same random number twice, but this will be rejected when trying to register the DOI).</p>
<p>Another common way to generate random strings would have been universally unique identifiers (<a href="https://en.wikipedia.org/wiki/Universally_unique_identifier">UUID</a>), but they are long and not very human-readable, e.g. <a href="https://doi.org/10.4233/UUID:6D192FE2-DE18-4556-873A-D3CD56AB96A6">https://doi.org/10.4233/UUID:6D192FE2-DE18-4556-873A-D3CD56AB96A6</a>.</p>
<p>An example DOI generated by cirneco would be</p>
<pre><code>cirneco doi generate --prefix 10.5555
10.5555/KVTD-VPWM</code></pre>
<p>The generated DOI is short enough that it should work well in places where space is limited, providing an alternative to the <a href="http://shortdoi.org/">ShortDOI</a> service which shortens existing DOIs, but does this by adding another layer on top of the DOI proxy.</p>
<p>Another cirneco command checks that this is a valid bas32 string using the checksum</p>
<pre><code>cirneco doi check 10.5555/KVTD-VPWM
Checksum for 10.5555/KVTD-VPWM is valid</code></pre>
<p>This can be used to quickly verify a DOI, e.g. in a web form or API. The Ruby base32 encoding library used by cirneco is open source (<a href="https://github.com/datacite/base32">https://github.com/datacite/base32</a>. I added the checksum to the existing library), and implementations of the Crockford base32 encoding pattern are available in many other languages, including <a href="https://github.com/jbittel/base32-crockford">Python</a>, <a href="https://github.com/dflydev/dflydev-base32-crockford">PHP</a>, <a href="https://www.npmjs.com/package/base32-crockford">Javascript</a>, <a href="http://stackoverflow.com/questions/22385467/crockford-base32-encoding-for-large-number-java-implementation">Java</a>, <a href="https://github.com/richardlehane/crock32">Go</a> and <a href="https://www.nuget.org/packages/crockford-base32">.NET</a>.</p>
<p>To answer the question raised at the beginning: a cool DOI is a DOI expressed as HTTPS URI using the doi.org proxy and using a base32-encoded suffix, for example <strong><strong>https://doi.org/10.5555/KVTD-VPWM</strong></strong>. This DOI works well in a web environment, is human readable, easy to parse and detect (e.g. in text mining), and can be generated using an algorithm that is well understood and supported.</p>
<figure>
<img src="https://blog.datacite.org/images/2016/12/cool-dois.svg" class="kg-image" alt="Cool DOIs" /><figcaption aria-hidden="true">Cool DOIs</figcaption>
</figure>
<p><em>This blog post was <a href="https://doi.org/10.5438/55e5-t5c0">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>Berners-Lee, T. (1998). Hypertext Style: Cool URIs don’t change. Retrieved from <a href="https://www.w3.org/Provider/Style/URI">https://www.w3.org/Provider/Style/URI</a></p>
<p>Klein, M., Sompel, H. V. de, Sanderson, R., Shankar, H., Balakireva, L., Zhou, K., &amp; Tobin, R. (2014). Scholarly Context Not Found: One in Five Articles Suffers from Reference Rot. <em>PLOS ONE</em>, <em>9</em>(12), e115253. <a href="https://doi.org/10.1371/journal.pone.0115253">https://doi.org/10.1371/journal.pone.0115253</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A common API for retrieving DataCite Metadata]]></title>
        <id>5tap7hw-xf983h9-8kkvpjs-vrhkk</id>
        <link href="https://blog.front-matter.io/mfenner/a-common-api-for-retrieving-datacite-metadata"/>
        <updated>2016-11-03T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Today we are launching a new version of the DataCite API at http://api.datacite.org. This new version includes numerous bug fixes and now includes related resources (e.g. data centers, members or contributors) according to the JSONAPI spec....]]></summary>
        <content type="html"><![CDATA[<p>Today we are launching a new version of the DataCite API at <a href="http://api.datacite.org/">http://api.datacite.org</a>. This new version includes numerous bug fixes and now includes related resources (e.g. data centers, members or contributors) according to the <a href="http://jsonapi.org/format/#fetching-includes">JSONAPI spec</a>. The changelog can be found <a href="https://github.com/datacite/spinone/blob/master/CHANGELOG.md">here</a>. Current users of the API should watch out for breaking changes in the <code>meta</code> object used for faceting.</p>
<p>We first launched the DataCite API in June as what we hope will become the new standard way to retrieve metadata from DataCite. Most of the content in the API comes from our search index of the MDS and is about DOI metadata, but we are also including information from other services, e.g. our member database and the <a href="https://blog.datacite.org/its-all-about-relations/">Event Data</a> service. We ourselves use the API to power the web frontend for search, to <a href="https://www.datacite.org/members.html">display member information</a> on our homepage, and to provide a search for our <a href="http://blog.datacite.org/">blog</a>.</p>
<p>Other ways to obtain DataCite metadata include <a href="http://oai.datacite.org/">OAI-PMH</a> for harvesting large volumes of metadata, and direct access to the Solr search index. This direct public access to the Solr search index will be discontinued in 2017 for performance and security reasons, so we encourage all users to migrate to the DataCite API as soon as possible. We will be adding missing API functionality in the coming months, most importantly provide all available DOI metadata in the API and thus search web frontend. Please <a href="mailto:tech@datacite.org">contact us</a> if you have suggestions or bug reports regarding the DataCite API.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/6wcf-efw5">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Announcing the Organization Identifier Project: a Way Forward]]></title>
        <id>25fm71y-tb48cf9-pan22f6-ndx0r</id>
        <link href="https://blog.front-matter.io/mfenner/announcing-the-organization-identifier-project-a-way-forward"/>
        <updated>2016-11-01T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The scholarly research community has come to depend on a series of open identifier and metadata infrastructure systems to great success. Content identifiers (through DataCite and Crossref) and contributor identifiers (through ORCID)...]]></summary>
        <content type="html"><![CDATA[<p>The scholarly research community has come to depend on a series of open identifier and metadata infrastructure systems to great success. Content identifiers (through DataCite and Crossref) and contributor identifiers (through ORCID) have become foundational infrastructure for the community. But there is one piece of the infrastructure that is missing -- there currently is no open, stakeholder-governed infrastructure for organization identifiers and associated metadata.</p>
<figure>
<img src="https://blog.datacite.org/images/2016/11/london_institute.jpg" class="kg-image" alt="The London Institution. Source: Wikipedia" /><figcaption aria-hidden="true">The London Institution. Source: <a href="https://de.wikipedia.org/wiki/Datei:London_Institution_at_the_Finsbury_Circus.jpg">Wikipedia</a></figcaption>
</figure>
<p>In early 2016, DataCite began a collaboration with ORCID and Crossref to explore the organization identifiers landscape and on how our organizations could work with the community to solve the organization identifier problem (Demeranville et al., <a href="https://blog.datacite.org/announcing-organization-identifier-project/#ref-https://doi.org/10.6084/M9.FIGSHARE.3479141">2016</a>). Out of those conversations emerged a way forward as expressed in the following documents:</p>
<ol>
<li>Organization Identifier Project: A Way Forward (Cruse, Haak, &amp; Pentz, <a href="https://blog.datacite.org/announcing-organization-identifier-project/#ref-https://doi.org/10.5438/2906">2016</a>)</li>
<li>Organization Identifier Provider Landscape (Bilder, Brown, &amp; Demeranville, <a href="https://blog.datacite.org/announcing-organization-identifier-project/#ref-https://doi.org/10.5438/4716">2016</a>)</li>
<li>Technical Considerations for an Organization Identifier Registry (Fenner, Paglione, Demeranville, &amp; Bilder, <a href="https://blog.datacite.org/announcing-organization-identifier-project/#ref-https://doi.org/10.5438/7885">2016</a>)</li>
</ol>
<p>We invite the community to comment on these papers online via email (<a href="mailto:oi-project@orcid.org">oi-project@orcid.org</a>) or comments in the corresponding Google Docs (<a href="https://docs.google.com/document/d/1PpWRBnlrU_X6TwYzQlB89w4FNXMLqieJv-RW0irNTsg/edit?usp=sharing">1</a>, <a href="https://docs.google.com/document/d/1lcKXWm9PxDvVWBxdlH7BVU7w8esnW0F_dppNiCJ9BW8/edit#">2</a> or <a href="https://docs.google.com/document/d/1Zj5sRRdnjKLjY81AbaeUdal3n6VuQgi1H66vRMaayiA/edit?usp=sharing">3</a>), or in person at <a href="https://crossreflive16.sched.org/">Crossref LIVE16</a> on November 1st and 2nd or at <a href="http://pidapalooza.org/">PIDapalooza</a> on November 9th and 10th. To move The OI Project forward, we will be forming a Community Working Group with the goal of holding an initial meeting before the end of 2016. The Working Group’s main charge is to develop a plan to launch and sustain an open, independent, non-profit organization identifier registry to facilitate the disambiguation of researcher affiliations</p>
<h2 id="datacite%E2%80%99s-focus">DataCite’s Focus</h2>
<p>An important focus of DataCite’s work is to connect resources , which have a DataCite DOI, to other resources - for example <a href="https://blog.datacite.org/dynamic-data-citation-webinar/">new versions of the same dataset</a>, <a href="https://blog.datacite.org/to-better-understand-research-communication-we-need-a-groid-group-object-identifier/">collections of related datasets</a>, or <a href="https://blog.datacite.org/location-of-the-citation/">articles citing the dataset</a>. Equally important is the support for linking these resources to the people and organizations who have contributed to their generation. We are working closely with ORCID to enable <a href="https://blog.datacite.org/announcing-datacite-profiles-service/">linking between DataCite DOIs and ORCID IDs</a>. In July we <a href="https://blog.datacite.org/relaunching-datacite-search/">relaunched our search</a> to better show these relations between DataCite DOIs and other resources. And in September we launched an updated DataCite Metadata Schema (DataCite Metadata Working Group, <a href="https://blog.datacite.org/announcing-organization-identifier-project/#ref-https://doi.org/10.5438/0012">2016</a>) with better support for linking to funding information. Enabling all these relations are persistent identifiers and metadata describing these relations.</p>
<p>DataCite also supports the linking of resources to academic institutions in one of two ways: Using the <strong><strong>HostingInstitution</strong></strong> contributor role, or via the <strong><strong>affiliation</strong></strong> attribute for creators and contributors. One gap we identified when analyzing linking to organizations (Fenner et al., <a href="https://blog.datacite.org/announcing-organization-identifier-project/#ref-https://doi.org/10.5281/ZENODO.30799">2015</a>) last September is the lack of adoption of organizational identifiers. Without broad adoption of identifiers for organizations similar to how DataCite DOIs, Crossref DOIs and ORCID IDs are widely used in the scholarly community, it becomes very difficult to track these relations to institutions in a way similar to how we can track relations to people.</p>
<p>Again, we look forward to the community’s feedback either via email or in person at the <a href="http://pidapalooza.org/">PIDapalooza</a> conference next week.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/tnhx-54cg">originally published</a> on the DataCite Blog. Please see also the corresponding blog posts by <a href="https://orcid.org/blog/2016/10/31/organization-identifier-project-way-forward">ORCID</a> and <a href="http://blog.crossref.org/2016/10/the-oi-project.html">Crossref</a>.</em></p>
<h2 id="references">References</h2>
<p>Bilder, G., Brown, J., &amp; Demeranville, T. (2016). Organisation identifiers: Current provider survey. <em>ORCID</em>. <a href="https://doi.org/10.5438/4716">https://doi.org/10.5438/4716</a></p>
<p>Cruse, P., Haak, L., &amp; Pentz, E. (2016). Organization identifier project: A way forward. <em>ORCID</em>. <a href="https://doi.org/10.5438/2906">https://doi.org/10.5438/2906</a></p>
<p>DataCite Metadata Working Group. (2016). DataCite metadata schema for the publication and citation of research data v4.0. <em>DataCite</em>. <a href="https://doi.org/10.5438/0012">https://doi.org/10.5438/0012</a></p>
<p>Demeranville, T., Brown, J., Fenner, M., Cruse, P., Haak, L., Paglione, L., … Pentz, E. (2016). Organisation identifiers - minimum viable product requirements. <a href="https://doi.org/10.6084/M9.FIGSHARE.3479141">https://doi.org/10.6084/M9.FIGSHARE.3479141</a></p>
<p>Fenner, M., Demeranville, T., Kotarski, R., Vision, T., Rueda, L., Dasler, R., … THOR Consortium. (2015). D2.1: Artefact, contributor, and organisation relationship data schema. <em>Zenodo</em>. <a href="https://doi.org/10.5281/ZENODO.30799">https://doi.org/10.5281/ZENODO.30799</a></p>
<p>Fenner, M., Paglione, L., Demeranville, T., &amp; Bilder, G. (2016). Technical considerations for an organization identifier registry. <a href="https://doi.org/10.5438/7885">https://doi.org/10.5438/7885</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Data Citation Webinar]]></title>
        <id>54b890h-z9g84ht-57jnp9n-smcnn</id>
        <link href="https://blog.front-matter.io/mfenner/dynamic-data-citation-webinar"/>
        <updated>2016-07-18T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[On July 12, 2016, DataCite invited Andreas Rauber to present the recommendations for dynamic data citation of the RDA Data Citation Working Group in a webinar.Dynamic dataAndreas is one of the co-chairs of the RDA working group,...]]></summary>
        <content type="html"><![CDATA[<p>On July 12, 2016, DataCite invited <a href="http://www.ifs.tuwien.ac.at/~andi/">Andreas Rauber</a> to present the recommendations for dynamic data citation of the <a href="https://rd-alliance.org/groups/data-citation-wg.html">RDA Data Citation Working Group</a> in a webinar.</p>
<figure>
<img src="https://blog.datacite.org/images/2016/07/dynamic-data.png" class="kg-image" alt="Dynamic data" /><figcaption aria-hidden="true">Dynamic data</figcaption>
</figure>
<p>Andreas is one of the co-chairs of the RDA working group, and he gave a throughout overview of the recommendations, and the thinking that went into them. The <a href="https://rd-alliance.org/system/files/documents/RDA-DC-Recommendations_151020.pdf">final recommendations</a> are available since last fall, and the current focus of the working group is to help with implementations.</p>
<p>The recommendations have to be implemented in the data center, but DataCite is happy to help coordinate the work, and to provide feedback to Andreas and the rest of the working group where needed. Of particular importance from a DataCite perspective is <strong><strong>recommendation 8</strong></strong>:</p>
<blockquote>
Query PID: Assign a new PID to the query if either the query is new or if the result set returned from an earlier identical query is different due to changes in the data. Otherwise, return the existing PID.
</blockquote>
<p>Assigning a persistent identifier (not only) when a dataset is originally generated, but also when a dataset is about to be cited, is central not only to the working group recommendations for dynamic data citation, but also crucial for other data citation use cases. Data exist at different levels, from raw data possibly generated by a machine, to highly processed data used in a publication. The figure below – presented by Robin Dasler from CERN at the <a href="https://project-thor.eu/2016/06/21/july-7-2016-thor-workshop-identifiers-infrastructure-impact-and-innovation/">THOR Workshop</a> on July 7 in Amsterdam - demostrates this for high-energy physics (HEP):</p>
<figure>
<img src="https://blog.datacite.org/images/2016/07/hep.png" class="kg-image" alt="HEP" /><figcaption aria-hidden="true">HEP</figcaption>
</figure>
<p>DataCite DOIs are intended as citation identifiers. They are persistent identifiers and provide standardized metadata, including links to associated publications, contributors and funders. They thus focus on the data in the top section of the pyramid. While we can also use DataCite DOIs for the other levels of the pyramid, sometimes other identifiers are more appropriate for raw, non-persistent data generated my machines. Dynamic data citation can be seen as a variant of the process that this pyramid describes.</p>
<p>If you could not attend last week or you want to review the session, the recording of the webinar is available:</p>
<div class="iframe">
<a href="https://vimeo.com/174795589"></a>
<div id="crawler_player">
Play
<img src="https://f.vimeocdn.com/p/images/crawler_logo.png" class="logo" alt="Vimeo" />
</div>
</div>
<p>The <a href="https://project-thor.eu/">THOR project</a> will work with interested data centers on dynamic data citation in the coming 12 months, hopefully leading to important feedback and a few more implementations of the RDA working group recommendations. Please contact us if you work for a data center and are interested in participating.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/y4ks-ksbc">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relaunching DataCite Search]]></title>
        <id>1peqbrj-va39dva-tgjzb3c-ng26q</id>
        <link href="https://blog.front-matter.io/mfenner/relaunching-datacite-search"/>
        <updated>2016-07-05T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[This week we relaunched DataCite Search, providing a more user-friendly search interface for DataCite metadata. We also added functionality that was not available before.The new search uses a single entry box for queries, and filters by resource type,...]]></summary>
        <content type="html"><![CDATA[<p>This week we relaunched <a href="https://search.datacite.org/">DataCite Search</a>, providing a more user-friendly search interface for DataCite metadata. We also added functionality that was not available before.</p>
<p>The new search uses a single entry box for queries, and filters by resource type, publication year and data center. A new Cite button will generate a citation in several popular citation styles, and in BibTeX and RIS import formats. Users who sign in using their ORCID credentials can add works to their ORCID record using the DataCite Search and Link service, and will find a menu shortcut to a page with all DataCite DOIs associated with their ORCID ID.</p>
<figure>
<img src="https://blog.datacite.org/images/2016/07/search.png" class="kg-image" alt="Search" /><figcaption aria-hidden="true">Search</figcaption>
</figure>
<p>In addition to information about works, DataCite Search also allows queries for contributors, data centers, and members, and the works associated with them. Information from the <a href="https://www.datacite.org/eventdata.html">DataCite Event Data service</a> is included in the search results where available, and can be specifically looked up via the <strong><strong>Services</strong></strong> tab.</p>
<figure>
<img src="https://blog.datacite.org/images/2016/07/eventdata.png" class="kg-image" alt="Data Citation Example" /><figcaption aria-hidden="true">Data Citation Example</figcaption>
</figure>
<p>In contrast to the previous search user interface the new search is not using the Solr Search API directly, but rather the new DataCite API available at <a href="https://api.datacite.org/">https://api.datacite.org</a>. This API uses the Solr Search API for metadata stored in the DataCite MDS, but also pulls in information from other services, including <a href="https://www.datacite.org/eventdata.html">Event Data</a> and <a href="https://www.datacite.org/profiles.html">Profiles</a> (the latter for information about members). Going forward we plan to add addition information to the DataCite API, e.g. from <a href="http://www.re3data.org/">re3data.org</a>.</p>
<p>The software that is providing the search frontend was originally written by Crossref and is also powering the <a href="http://search.crossref.org/">Crossref Metadata Search</a>. As all DataCite software the code is <a href="https://github.com/crosscite/doi-metadata-search">available</a> as open source software.</p>
<p>The search has been running as Labs Search since last August and many users have provided valuable feedback. The old search user interface is still available at https://search.datacite.org/ui.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/vq2t-vr4k">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Publishing tabular data as blog post]]></title>
        <id>2981cqp-45r821a-dghwsxn-1ycsg</id>
        <link href="https://blog.front-matter.io/mfenner/publishing-tabular-data-as-blog-post"/>
        <updated>2016-05-20T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[CSV in many ways is for data what Markdown is for text documents: a very simple format that is both human- and machine-readable, and that – despite a number of shortcomings - is widely used. Given the popularity of Markdown for writing blog posts,...]]></summary>
        <content type="html"><![CDATA[<p>CSV in many ways is for data what Markdown is for text documents: a very simple format that is both human- and machine-readable, and that – despite a number of shortcomings - is widely used. Given the popularity of Markdown for writing blog posts, using CSV to publish blog posts with tabular data should be an obvious thing to do, and we have just published our first blog post using CSV data. The blog post shows Table 3 from the DataCite Metadata Schema (DataCite Metadata Working Group, <a href="https://blog.datacite.org/publishing-tabular-data-as-blog-post/#ref-https://doi.org/10.5438/0010">2014</a>), describing the mandatory properties.</p>
<figure>
<img src="https://blog.datacite.org/images/2016/05/periodic_table.jpg" class="kg-image" alt="Periodic table of elements. From: Wikipedia" /><figcaption aria-hidden="true">Periodic table of elements. From: <a href="https://en.wikipedia.org/wiki/Periodic_table">Wikipedia</a></figcaption>
</figure>
<p>The DataCite blog uses the <a href="https://jekyllrb.com/">Jekyll</a> static site generator, and all blog posts are written in Markdown format. All posts have their metadata in YAML format at the beginning of the file (separated by <code>---</code> from the main text).</p>
<pre><code>---
layout: post
title: Publishing tabular data as blog post
author: mfenner
tags:
 - csv
 - metadata
 - blog
---</code></pre>
<p>Markdown is a nice format for writing texts, but doesn't work so well for tabular data, as the current Markdown table implementations are difficult to edit and read for humans for all but the simplest tables. CSV is a much better fit for tabular data, and can be written both with a general text editor, or with a spreadsheet program or other specialized tool.</p>
<p>To add the metadata required for every Jekyll blog post we are again adding a YAML header, the resulting file format is <a href="http://csvy.org/">CSVY</a>, about which we have talked before (Fenner, <a href="https://blog.datacite.org/publishing-tabular-data-as-blog-post/#ref-https://blog.datacite.org/thinking-about-csv">2016b</a>). Jekyll can be extended to understand many file formats beyond Markdown. As a <code>CSVY</code> converter doesn't exist yet, we have written this converter and released <strong><strong>jekyll-csvy</strong></strong> as Ruby gem (Fenner, <a href="https://blog.datacite.org/publishing-tabular-data-as-blog-post/#ref-https://github.com/datacite/jekyll-csvy">2016a</a>), so that <code>CSVY</code> support can be easily added to every Jekyll-powered blog.</p>
<p>In HTML tabular data are typically displayed as HTML tables, and this is what we are doing with the <code>CSVY</code> converter. This works well for tables that are not too wide, and the converter supports inline Markdown formatting (bold, italic, links, etc.) in table cells. Block formatting (e.g. lists) is on our list of future improvements, and we will polish the converter based on user feedback. We are of course also interested in embedding CSV tables within Markdown documents, as this is a common use case.</p>
<p>One important feature of using CSVY for blog posts is that the CSV remains available, and can be ingested and processed by tools that can read CSVY, e.g. using the R rio (Becker et al., <a href="https://blog.datacite.org/publishing-tabular-data-as-blog-post/#ref-https://cran.r-project.org/web/packages/rio/index.html">2016</a>) package.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/5aeg-weev">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>Becker, J., Chan, C.-h., Chan, G. C., Leeper, T. J., Gandrud, C., MacDonald, A., &amp; Zahn, I. (2016). Rio: A swiss-army knife for data I/O. CRAN. Retrieved from <a href="https://cran.r-project.org/web/packages/rio/index.html">https://cran.r-project.org/web/packages/rio/index.html</a></p>
<p>DataCite Metadata Working Group. (2014). DataCite metadata schema for the publication and citation of research data v3.1. <em>DataCite</em>. Retrieved from <a href="https://doi.org/10.5438/0010">https://doi.org/10.5438/0010</a></p>
<p>Fenner, M. (2016a). jekyll-csvy: Jekyll converter for CSVY files. GitHub. Retrieved from <a href="https://github.com/datacite/jekyll-csvy">https://github.com/datacite/jekyll-csvy</a></p>
<p>Fenner, M. (2016b). Thinking about CSV. DataCite Blog. Retrieved from <a href="https://blog.datacite.org/thinking-about-csv">https://blog.datacite.org/thinking-about-csv</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data catalog cards: simplifying article/data linking]]></title>
        <id>2my1dn4-2hx850a-xr0zp90-msexr</id>
        <link href="https://blog.front-matter.io/mfenner/data-catalog-cards-simplifying-article-data-linking"/>
        <updated>2016-05-13T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Data citation is core to DataCite's mission and DataCite is involved in several projects that try to facilitate data citation, including THOR, Data Citation Implementation Pilot (DCIP), Research Data Alliance (RDA), and COPDESS....]]></summary>
        <content type="html"><![CDATA[<p>Data citation is core to DataCite's mission and DataCite is involved in several projects that try to facilitate data citation, including <a href="https://project-thor.eu/">THOR</a>, <a href="https://www.force11.org/group/dcip">Data Citation Implementation Pilot (DCIP)</a>, <a href="https://rd-alliance.org/">Research Data Alliance (RDA)</a>, and <a href="http://www.copdess.org/">COPDESS</a>. The biggest roadblock for wider data citation adoption might be insufficient incentives for individual researchers, but another major challenge is that implementing data citation is still too complicated.</p>
<figure>
<img src="https://blog.datacite.org/images/2016/05/Citation_needed_stickers.jpeg" class="kg-image" alt="Citation needed. By User:Tfinc (Own work) CC BY-SA 3.0, via Wikimedia Commons" /><figcaption aria-hidden="true"><a href="https://commons.wikimedia.org/wiki/File%3ACitation_needed_stickers.jpeg">Citation needed</a>. By User:Tfinc (Own work) <a href="http://creativecommons.org/licenses/by-sa/3.0">CC BY-SA 3.0</a>, via Wikimedia Commons</figcaption>
</figure>
<p>When we talk about data citation, we typically mean two related, but different scenarios:</p>
<ol>
<li>an article or other scholarly work cites an already published dataset.</li>
<li>all data and related metadata underlying the findings reported in a submitted manuscript should be deposited in an appropriate public repository (<a href="http://journals.plos.org/plosone/s/data-availability">PLOS data availability statement</a>)</li>
</ol>
<p>The first scenario is not conceptually different from an article citing another article, where the common practice is to put everything that is cited into the reference list.</p>
<p>The second scenario is probably not only more common, but also requires more complex workflows, e.g. coordination of issuing persistent identifiers for article and data and linking them together via metadata. And we as a community are still working on common practices for doing this. Assuming again that incentives are the biggest driver of change, I would argue that researchers, publishers, and funders are all interested in making this work, but that data repositories have the strongest motivation to improve the current situation. If this is true then we should give data repositories a bigger role in the publication of data associated with an article.</p>
<p>While many publishers host supplementary information for articles, they leave the hosting of more complex research data to external data repositories specialized in this task. Properly referencing all associated data in the article is currently the job of the publisher, and I propose that we give more of this responsibility to the data repository. The data repository can create a data catalog card (with associated persistent identifier and metadata) that describes all data associated with an article. The data catalog card is a collection of metadata, and different from a data paper. The data described in the catalog card can be hosted in that repository or elsewhere.</p>
<figure>
<img src="https://blog.datacite.org/images/2016/05/medium_message.jpg" class="kg-image" alt="The medium is the message. By suzanne chapman CC BY-NC-SA 2.0, via Flickr." /><figcaption aria-hidden="true"><a href="https://www.flickr.com/photos/sukisuki/4414318674/">The medium is the message</a>. By suzanne chapman <a href="https://creativecommons.org/licenses/by-nc-sa/2.0/">CC BY-NC-SA 2.0</a>, via Flickr.</figcaption>
</figure>
<p>The publisher then links to this data catalog card via the article metadata and can display the catalog card formatted as a data availability statement. The publisher could (and should) still link to individual data where appropriate, but the proposed solution helps solve several important issues:</p>
<ul>
<li>the data catalog card simplifies manuscript submission for publishers</li>
<li>the data record provides a machine-readable representation of the data availability statement that publishers are increasingly requiring</li>
<li>the publisher doesn't need to provide machine-readable metadata for all data used in an article, but can reference the data catalog card. Accession numbers that are not globally unique can be used in the article if they are properly referenced in the data catalog card. This facilitates the transition from current practices</li>
<li>some articles refer to thousands of datasets (e.g. genomics papers), and this number of links is difficult to describe in the traditional article format (e.g. <a href="http://jats.nlm.nih.gov/">JATS</a>)</li>
</ul>
<p>Several general purpose data repositories already provide most or all of this functionality, I am most familiar with <a href="https://www.datadryad.org/">Dryad</a>, BioStudies (McEntyre, Sarkans, &amp; Brazma, <a href="https://blog.datacite.org/data-catalog-cards-simplifying-article-data-linking/#ref-https://doi.org/10.15252/msb.20156658">2015</a>) and Figshare (Hyndman, <a href="https://blog.datacite.org/data-catalog-cards-simplifying-article-data-linking/#ref-https://figshare.com/blog/Unveiling_figshare_Collections_a_new_way_to_group_content/202">2016</a>). Data catalog cards probably work best for repositories that a flexible in the kinds of data they take, and repositories that already have integrations with publishers. Not every data repository needs to support this functionality. Data catalog cards are also an opportunity for differentiation, e.g. by providing data curation, help with data review, etc.</p>
<p>My thinking about this topic was triggered by a conversation with <a href="https://researchers.mgh.harvard.edu/profile?profile_id=1667831">Tim Clark</a> in the context of the DCIP project. The guest post by Dan S. Katz (Katz, <a href="https://blog.datacite.org/data-catalog-cards-simplifying-article-data-linking/#ref-https://blog.datacite.org/to-better-understand-research-communication-we-need-a-groid-group-object-identifier">2016</a>) and the discussion around it was another important motivation, and a DataCite blog post from last August (Fenner, <a href="https://blog.datacite.org/data-catalog-cards-simplifying-article-data-linking/#ref-https://blog.datacite.org/reference-lists-and-tables-of-content">2015</a>) contains some of the ideas expressed here. Obviously this topic is of great interest to DataCite, as we hope that data catalog cards use DataCite DOIs, and that we can help both with making article/data publishing workflows easier, and with discovering data associated with an article.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/cab5-teg0">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>Fenner, M. (2015). Reference lists and tables of content. DataCite Blog. Retrieved from <a href="https://blog.datacite.org/reference-lists-and-tables-of-content">https://blog.datacite.org/reference-lists-and-tables-of-content</a></p>
<p>Hyndman, A. (2016). Unveiling figshare ’collections’ - a new way to group content. Figshare Blog. Retrieved from <a href="https://figshare.com/blog/Unveiling_figshare_Collections_a_new_way_to_group_content/202">https://figshare.com/blog/Unveiling_figshare_Collections_a_new_way_to_group_content/202</a></p>
<p>Katz, D. S. (2016). To better understand research communication, we need a groid (group object identifier). DataCite Blog. Retrieved from <a href="https://blog.datacite.org/to-better-understand-research-communication-we-need-a-groid-group-object-identifier">https://blog.datacite.org/to-better-understand-research-communication-we-need-a-groid-group-object-identifier</a></p>
<p>McEntyre, J., Sarkans, U., &amp; Brazma, A. (2015). The BioStudies database. <em>Molecular Systems Biology</em>, <em>11</em>(12), 847–847. <a href="https://doi.org/10.15252/msb.20156658">https://doi.org/10.15252/msb.20156658</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thinking about CSV]]></title>
        <id>2k8f9j5-6k79kf9-pz7wc0f-4stn7</id>
        <link href="https://blog.front-matter.io/mfenner/thinking-about-csv"/>
        <updated>2016-05-04T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[This week some of us from DataCite are attending CSVconf in Berlin, and we are a conference sponsor and co-organizer.csv,conf is a non-profit community conference run by some folks who really love data and sharing knowledge....]]></summary>
        <content type="html"><![CDATA[<p>This week some of us from DataCite are attending <a href="http://csvconf.com/">CSVconf</a> in Berlin, and we are a conference sponsor and co-organizer.</p>
<blockquote>
csv,conf is a non-profit community conference run by some folks who really love data and sharing knowledge. If you are as passionate about data and the application it has to society as us then you should join us in Berlin!
</blockquote>
<p>One important reason we are at CSVconf is that providing persistent identifiers and starndard metadata for research data, which in most cases are stored in tabular data formats such as CSV, is central to what DataCite is doing. And while DataCite provides a searchable index of metadata for these datasets, getting the metadata into the index is not as frictionless as one would hope.</p>
<p>The presentations and informal discussions at the conference have been very valuable and entertaining so far, and we still have most of the second day ahead. My personal highlight from the first day: <a href="https://twitter.com/blahah404">Richard Smith-Unna</a> talking about <strong><strong>Easy, massive-scale reuse of scientific outputs</strong></strong>.</p>
<p>One topic that I have been thinking about the past two days is how to add metadata to CSV files while keeping the simplicity of the format. This is important for DataCite, as we want to make the process of registering datasets with metadata painless, and for individual researchers and small research groups the process should be as simple as possible. Two groups have done great work in this area and Jeni Tennison and Dan Fowler gave presentations about their work at CSVconf:</p>
<ul>
<li><strong><strong>Jeni Tennison</strong></strong>: Making CSV part of the web, describing the work of the <a href="https://www.w3.org/2013/csvw/wiki/Main_Page">CSV on the Web</a> W3C working group</li>
<li><strong><strong>Dan Fowler</strong></strong>: Data Packages and Frictionless Data for Research, talking about the work Open Knowledge has done on defining <a href="https://frictionlessdata.io/data-packages/">data packages</a></li>
</ul>
<p>Both groups use a JSON file to describe the metadata of an associated CSV file. While it is a straightforward process, it still feels as if we are leaving the simplicity of the CSV format. And when we generate a JSON file to describe the metadata, we might as well convert the CSV into JSON and put the metadata into the same file.</p>
<p><a href="https://blog.datacite.org/using-yaml-frontmatter-with-csv/">Back in September</a> I wrote about a different approach: adding the metadata directly to the CSV file. The following slides summarize this work:</p>
<div class="iframe">
<div id="player" class="slides">
<div class="sd-player state-initial js-sd-player" data-start-slide="0" data-url="https://speakerdeck.com/mfenner/csvy-csv-reimagined" data-ratio="1.33333333333333">
<div class="sd-player-title">
<div class="sd-player-avatar">
<a href="https://speakerdeck.com/mfenner"><img src="https://secure.gravatar.com/avatar/855ee26b04af97fe0fc421b03a92454e?s=47" class="avatar" alt="855ee26b04af97fe0fc421b03a92454e?s=47" /></a>
</div>
<div class="sd-player-title-name">
<a href="https://speakerdeck.com/mfenner/csvy-csv-reimagined" class="sd-player-title-link">CSVY – CSV reimagined</a>
</div>
<div class="sd-player-title-author">
by <a href="https://speakerdeck.com/mfenner" class="sd-player-title-link">Martin Fenner</a>
</div>
<div class="sd-player-title-mark">
<a href="https://speakerdeck.com/"><img src="https://d1eu30co0ohy4w.cloudfront.net/assets/mark-white-8d908558fe78e8efc8118c6fe9b9b1a9846b182c503bdc6902f97df4ddc9f3af.svg" alt="Speaker Deck" /></a>
</div>
</div>
<div class="sd-player-controls">
<div class="sd-player-previous sd-player-button js-sd-player-previous">
<img src="data:image/svg+xml;base64,PHN2ZyBjbGFzcz0iaWNvbiBpY29uLWNoZXZyb24tbGVmdCBpY29uLXBsYXllciI+PHVzZSB4bGluazpocmVmPSIvaWNvbnMvaWNvbnMuc3ZnI2ljb24tY2hldnJvbi1sZWZ0Ij48L3VzZT48L3N2Zz4=" class="icon icon-chevron-left icon-player" />
</div>
<div class="sd-player-next sd-player-button js-sd-player-next">
<img src="data:image/svg+xml;base64,PHN2ZyBjbGFzcz0iaWNvbiBpY29uLWNoZXZyb24tcmlnaHQgaWNvbi1wbGF5ZXIiPjx1c2UgeGxpbms6aHJlZj0iL2ljb25zL2ljb25zLnN2ZyNpY29uLWNoZXZyb24tcmlnaHQiPjwvdXNlPjwvc3ZnPg==" class="icon icon-chevron-right icon-player" />
</div>
<div class="sd-player-spacer">

</div>
<div class="sd-player-share sd-player-button active js-sd-player-share">
<img src="data:image/svg+xml;base64,PHN2ZyBjbGFzcz0iaWNvbiBpY29uLXNoYXJlIGljb24tcGxheWVyIj48dXNlIHhsaW5rOmhyZWY9Ii9pY29ucy9pY29ucy5zdmcjaWNvbi1zaGFyZSI+PC91c2U+PC9zdmc+" class="icon icon-share icon-player" />
</div>
<div class="sd-player-fullscreen sd-player-button active js-fullscreen-toggle">
<img src="data:image/svg+xml;base64,PHN2ZyBjbGFzcz0iaWNvbiBpY29uLW1heGltaXplIGljb24tcGxheWVyIHNkLXBsYXllci1lbmFibGUtZnVsbHNjcmVlbiI+PHVzZSB4bGluazpocmVmPSIvaWNvbnMvaWNvbnMuc3ZnI2ljb24tbWF4aW1pemUiPjwvdXNlPjwvc3ZnPg==" class="icon icon-maximize icon-player sd-player-enable-fullscreen" /> <img src="data:image/svg+xml;base64,PHN2ZyBjbGFzcz0iaWNvbiBpY29uLW1pbmltaXplIGljb24tcGxheWVyIHNkLXBsYXllci1kaXNhYmxlLWZ1bGxzY3JlZW4iPjx1c2UgeGxpbms6aHJlZj0iL2ljb25zL2ljb25zLnN2ZyNpY29uLW1pbmltaXplIj48L3VzZT48L3N2Zz4=" class="icon icon-minimize icon-player sd-player-disable-fullscreen" />
</div>
</div>
<div class="sd-player-gradient">

</div>
<div class="sd-player-scrubber js-sd-player-scrubber">
<div class="sd-player-scrubber-bar">
<div class="sd-player-scrubber-progress js-sd-player-scrubber-progress">

</div>
</div>
</div>
<div class="sd-player-preview js-sd-player-preview">

</div>
<div class="sd-player-share-menu">
<div class="sd-player-share-menu-overlay js-sd-player-share-close">

</div>
<div class="sd-player-share-menu-container">
<div class="sd-player-share-menu-exit js-sd-player-share-close">
<img src="data:image/svg+xml;base64,PHN2ZyBjbGFzcz0iaWNvbiBpY29uLXggIj48dXNlIHhsaW5rOmhyZWY9Ii9pY29ucy9pY29ucy5zdmcjaWNvbi14Ij48L3VzZT48L3N2Zz4=" class="icon icon-x" />
</div>
<div class="sd-player-share-menu-group">
<div class="sd-player-share-menu-option active js-sd-player-share-selector" data-select="link" data-group="type">
Link
</div>
<div class="sd-player-share-menu-option js-sd-player-share-selector" data-select="embed" data-group="type">
Embed
</div>
<div class="sd-player-share-menu-option js-sd-player-share-selector" data-select="share" data-group="type">
Share
</div>
</div>
<div class="sd-player-share-menu-group">
<div class="sd-player-share-menu-option active js-sd-player-share-selector" data-select="beginning" data-group="start">
Beginning
</div>
<div class="sd-player-share-menu-option js-sd-player-share-selector" data-select="slide" data-group="start">
This slide
</div>
</div>
<div class="sd-player-share-menu-buttons">
<div class="sd-player-share-menu-action active" data-selected-by="link" data-group="type">
<div class="sd-player-btn sd-player-share-menu-action js-sd-player-link-start active" data-selected-by="beginning" data-group="start" data-clipboard-text="https://speakerdeck.com/mfenner/csvy-csv-reimagined">
Copy link URL
</div>
<div class="sd-player-btn sd-player-share-menu-action js-sd-player-link-slide" data-selected-by="slide" data-group="start" data-clipboard-text="">
Copy link URL
</div>
</div>
<div class="sd-player-share-menu-action" data-selected-by="embed" data-group="type">
<div class="sd-player-btn sd-player-share-menu-action active js-sd-player-embed-start" data-selected-by="beginning" data-group="start" data-clipboard-text="&lt;script async class=&quot;speakerdeck-embed&quot; data-id=&quot;0485d6ed325144bcb155f771e6bfd842&quot; data-ratio=&quot;1.33333333333333&quot; src=&quot;//speakerdeck.com/assets/embed.js&quot;&gt;&lt;/script&gt;">
Copy embed code
</div>
<div class="sd-player-btn sd-player-share-menu-action js-sd-player-embed-slide" data-selected-by="slide" data-group="start" data-clipboard-text="">
Copy embed code
</div>
</div>
<div class="sd-player-share-menu-action" data-selected-by="share" data-group="type">
<div class="sd-player-share-menu-action active js-sd-player-share-start" data-selected-by="beginning" data-group="start">
<img src="data:image/svg+xml;base64,PHN2ZyBjbGFzcz0iaWNvbiBpY29uLWZhY2Vib29rICI+PHVzZSB4bGluazpocmVmPSIvaWNvbnMvaWNvbnMuc3ZnI2ljb24tZmFjZWJvb2siPjwvdXNlPjwvc3ZnPg==" class="icon icon-facebook" /> Share
<div class="twitter-share-button">

</div>
</div>
<div class="sd-player-share-menu-action js-sd-player-share-slide" data-selected-by="slide" data-group="start">
<img src="data:image/svg+xml;base64,PHN2ZyBjbGFzcz0iaWNvbiBpY29uLWZhY2Vib29rICI+PHVzZSB4bGluazpocmVmPSIvaWNvbnMvaWNvbnMuc3ZnI2ljb24tZmFjZWJvb2siPjwvdXNlPjwvc3ZnPg==" class="icon icon-facebook" /> Share
<div class="twitter-share-button">

</div>
</div>
</div>
</div>
</div>
</div>
<div class="sd-player-presenter">
<div class="sd-player-presenter-container js-sd-player-presenter">
<div class="sd-player-slide sd-player-presenter-previous js-sd-player-previous-slide">

</div>
<div class="sd-player-slide sd-player-presenter-current js-sd-player-current-slide">

</div>
<div class="sd-player-slide sd-player-presenter-next js-sd-player-next-slide">

</div>
</div>
</div>
<div class="sd-player-slides js-sd-player-slides">
<div class="sd-player-slide js-sd-slide" data-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/slide_0.jpg?6232802" data-preview-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/preview_slide_0.jpg?6232802">

</div>
<div class="sd-player-slide js-sd-slide" data-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/slide_1.jpg?6232803" data-preview-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/preview_slide_1.jpg?6232803">

</div>
<div class="sd-player-slide js-sd-slide" data-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/slide_2.jpg?6232804" data-preview-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/preview_slide_2.jpg?6232804">

</div>
<div class="sd-player-slide js-sd-slide" data-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/slide_3.jpg?6232805" data-preview-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/preview_slide_3.jpg?6232805">

</div>
<div class="sd-player-slide js-sd-slide" data-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/slide_4.jpg?6232806" data-preview-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/preview_slide_4.jpg?6232806">

</div>
<div class="sd-player-slide js-sd-slide" data-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/slide_5.jpg?6232807" data-preview-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/preview_slide_5.jpg?6232807">

</div>
<div class="sd-player-slide js-sd-slide" data-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/slide_6.jpg?6232808" data-preview-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/preview_slide_6.jpg?6232808">

</div>
<div class="sd-player-slide js-sd-slide" data-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/slide_7.jpg?6232809" data-preview-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/preview_slide_7.jpg?6232809">

</div>
<div class="sd-player-slide js-sd-slide" data-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/slide_8.jpg?6232810" data-preview-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/preview_slide_8.jpg?6232810">

</div>
<div class="sd-player-slide js-sd-slide" data-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/slide_9.jpg?6232811" data-preview-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/preview_slide_9.jpg?6232811">

</div>
<div class="sd-player-slide js-sd-slide" data-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/slide_10.jpg?6232812" data-preview-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/preview_slide_10.jpg?6232812">

</div>
<div class="sd-player-slide js-sd-slide" data-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/slide_11.jpg?6232813" data-preview-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/preview_slide_11.jpg?6232813">

</div>
<div class="sd-player-slide js-sd-slide" data-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/slide_12.jpg?6232814" data-preview-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/preview_slide_12.jpg?6232814">

</div>
<div class="sd-player-slide js-sd-slide" data-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/slide_13.jpg?6232815" data-preview-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/preview_slide_13.jpg?6232815">

</div>
<div class="sd-player-slide js-sd-slide" data-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/slide_14.jpg?6232816" data-preview-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/preview_slide_14.jpg?6232816">

</div>
<div class="sd-player-slide js-sd-slide" data-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/slide_15.jpg?6232817" data-preview-url="https://files.speakerdeck.com/presentations/0485d6ed325144bcb155f771e6bfd842/preview_slide_15.jpg?6232817">

</div>
</div>
</div>
<div id="fb-root">

</div>
</div>
</div>
<p>CSVY uses a <a href="https://jekyllrb.com/docs/frontmatter/">YAML header</a> for the metadata. This keeps the CSV file human readable, and is extensible to add even complex metadata. The downside is of course that it breaks the CSV format, but many CSV parsers support comments and can skip lines at the beginning of a file. Implementing CSVY support would thus only be a small step, and should be backwards compatible in many cases. You can for example use Excel to open these files, of course not parsing the metadata in the YAML header.</p>
<p>The beauty of this approach from a DataCite perspective is that we can now build a workflow where sending a single CSVY file to an appropriate API is all that is needed to deposit a CSV file into a data repository, and register a DOI with metadata for it.</p>
<p>There are obviously synergies with <a href="http://commonmark.org/">CommonMark</a>, <em>a strongly defined, highly compatible specification of Markdown</em>. Markdown is a lightweight markup format for text documents, similar to CSV being a lightweight format for data. Tables is one of the things in markdown that are not really lightweight, and CommonMark doesn't (yet) include a syntax for table formatting. We could use CSVY to make tables really simple in markdown. The metadata for the table can be added to the YAML header (something that is commonly used for markdown documents), and the CSV can be added directly to the markdown file. I use <code>,,,</code> to indicate that this is a table.</p>
<pre><code>,,,CSVconf Speakers
id,name,title
rsmithunna,Richard Smith-Unna,&quot;Easy, massive-scale reuse of scientific outputs&quot;
amoser,Aurelia Moser,&quot;This is Not a Map: Building Interactive Maps with CSVs, Creative Themes, and Curious Geometries&quot;
tdoehman,Till Doehmen,There and back again - Automatic detection and conversion of logical table structures
,,,</code></pre>
<p>Alternatively we might want to read in the CSV from an external file, using a tag that could look like this:</p>
<pre><code>,[CSVconf Speakers](/_data/speakers.csvy)</code></pre>
<p>CSVY is compatible with <a href="https://www.w3.org/2013/csvw/wiki/Main_Page">CSV on the Web</a> and <a href="https://frictionlessdata.io/specs/data-package/">data packages</a> described above as it should be easy to convert the CSVY file with YAML header into a CSV file and JSON file with the metadata, and then host the two files on the web using the CSV on the Web recommendations.</p>
<p>CSVY is not meant to cover all use cases for CSV files, but should be useful to many people working with CSV. The critical factor is of course tool support in languages that commonly are used to work with CSV files, e.g. Python, R, and Javascript. I learned today that the <a href="https://cran.r-project.org/web/packages/rio/index.html">rio package for R</a> is supporting CSVY, so that is a great start. For more information about CSVY go to <a href="http://csvy.org/">http://csvy.org</a>.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/4qx3-rp8y">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[We were out in Force]]></title>
        <id>3xm1g4k-kv781ar-mz3e8dk-gxppt</id>
        <link href="https://blog.front-matter.io/mfenner/we-were-out-in-force"/>
        <updated>2016-04-19T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[This week most of the DataCite staff is attending the Force16 conference in Portland, Oregon. Force16 brings together a large group of people who either already work with DataCite in one way or another,...]]></summary>
        <content type="html"><![CDATA[<p>This week most of the DataCite staff is attending the <a href="https://www.force11.org/meetings/force2016">Force16</a> conference in Portland, Oregon. Force16 brings together a large group of people who either already work with DataCite in one way or another, or are doing interesting projects of relevance to DataCite.</p>
<p><a href="https://impactstory.org/">ImpactStory</a> is a non-profit that helps scientists learn where their research is being cited, shared, saved and more. Ten days ago ImpactStory <a href="http://blog.impactstory.org/new-better-freer/">launched a new version</a> that is built all around ORCID IDs and DOIs. Both the data and the software running the service are open, and the new version integrates naturally with the ORCID/DOI integrations that DataCite is working on as part of the <a href="https://project-thor.eu/">THOR project</a>. The ImpactStory co-founders Jason Priem and Heather Piwowar are both attending Force16 and we had a great conversation on how the new ImpactStory could be integrated with what DataCite is doing.</p>
<p><a href="https://github.com/">GitHub</a> is a popular repository for software source code, and GitHub staff member <a href="https://github.com/arfon">Arfon Smith</a> is at Force16 as co-chair of the Force11 <a href="https://www.force11.org/group/software-citation-working-group">Software Citation Working Group</a>. In the past two years we have seen an increasing number of DOIs (currently about 7,200) <a href="https://guides.github.com/activities/citable-code/">minted for archived versions of GitHub software releases</a>, deposited mainly in the <a href="https://zenodo.org/">Zenodo</a> repository. Just as we want to use DOIs to make it easier for software to become part of the scholarly record, we want to link GitHub and ORCID accounts to facilitate the recognition of scientific software engineers in the current scholarly ecosystem.</p>
<p>The latest version of the <a href="https://profiles.datacite.org/">DataCite Profiles</a> service released today has added support for ImpactStory and GitHub. If you have an orcid with publications in it you can see a summary from ImpactStory in DataCite Profiles, and a direct link will send you to your ImpactStory profile page. You can now link your GitHub account via OAuth authentication, linking the ORCID identifier used by DataCite Profiles with the GitHub username.</p>
<figure>
<img src="https://blog.datacite.org/images/2016/04/impactstory.png" class="kg-image" alt="Example of DataCite Profiles Settings Page" /><figcaption aria-hidden="true">Example of DataCite Profiles Settings Page</figcaption>
</figure>
<p>To try out this new functionality, go to the <a href="https://profiles.datacite.org/">DataCite Profiles</a> service, and then after login to your <a href="https://profiles.datacite.org/settings/me">Settings</a>. And sign up with Impactstory and GitHub if you haven't done so already.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/c3by-vyzs">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[It's all about Relations]]></title>
        <id>2p6cm1d-rnd913s-1fx3z85-spdh8</id>
        <link href="https://blog.front-matter.io/mfenner/its-all-about-relations"/>
        <updated>2016-04-14T18:52:00.000Z</updated>
        <summary type="html"><![CDATA[In a guest post two weeks ago Elizabeth Hull explained that only 6% of Dryad datasets associated with a journal article are found in the reference list of that article, data she also presented at the IDCC conference in February (Mayo, Hull, &amp; Vision,...]]></summary>
        <content type="html"><![CDATA[<p>In a <a href="https://blog.datacite.org/location-of-the-citation/">guest post</a> two weeks ago Elizabeth Hull explained that only 6% of Dryad datasets associated with a journal article are found in the reference list of that article, data she also presented at the IDCC conference in February (Mayo, Hull, &amp; Vision, <a href="https://blog.datacite.org/its-all-about-relations/#ref-https://doi.org/10.5281/zenodo.32412">2015</a>). This number has increased from 4% to 8% between 2011-2014, but is still low. One important reason is missing incentives: we don't yet have the same automated citation linking between articles and data that exists between articles thanks to <a href="http://www.crossref.org/">Crossref</a>.</p>
<p>Wouldn't it be nice if a data publisher such as the Oak Ridge National Laboratory is automatically informed about journal articles citing one of their datasets (<strong><strong>???</strong></strong>)?</p>
<figure>
<img src="https://blog.datacite.org/images/2016/04/oak_ridge.png" class="kg-image" alt="Global, Regional, and National Fossil-Fuel CO2 Emissions." /><figcaption aria-hidden="true">Global, Regional, and National Fossil-Fuel CO2 Emissions.</figcaption>
</figure>
<p>The challenge: both DataCite and Crossref collect metadata as part of the respective DOI registration services they provide. These metadata describe the information required for a citation (title, authors, publication date, etc.) (DataCite Metadata Working Group, <a href="https://blog.datacite.org/its-all-about-relations/#ref-https://doi.org/10.5438/0010">2014</a>). And the metadata can contain references to related resources. But what is missing is an automated exchange of the information collected by Crossref and DataCite.</p>
<p>We can't simply store information coming from Crossref in the DataCite Metadata Store (<a href="https://mds.datacite.org/">MDS</a>) for two reasons:</p>
<ol>
<li>Only the organization publishing the DOI can update the metadata, and it is important to keep it this way to to have a single authoritative source.</li>
<li>The DataCite MDS stores information about DataCite DOIs, but can't store metadata (again title, authors, publication date, etc.) for other resources such as Crossref DOIs.</li>
</ol>
<p>DataCite thus needs a service to enhance its DataCite Metadata Store (MDS). Data citations are the most important use case, but his service should be flexible enough to also handle information coming from other providers besides Crossref, for example claims of DataCite DOIs in the ORCID registry or links of DataCite DOIs to code repositories such as Github.</p>
<p>The new service is called <a href="https://eventdata.test.datacite.org/">DataCite Event Data</a>, and the screenshot above shows six data citations coming from Crossref. The software powering the service is called <a href="http://www.lagotto.io/">Lagotto</a>, open source software originally developed in 2009 by the Open Access publisher <a href="http://www.plos.org/">Public Library of Science</a>. While Lagotto provides the basic functionality needed for the Event Data service, significant development effort was required to enable the full functionality described above. This work was done, and will continue, in close collaboration with Crossref, as Crossref wants to address similar use cases. Although the core Crossref infrastructure is built around citation linking of publications, Crossref is working on <a href="http://blog.crossref.org/2016/02/event-data-open-for-your-interpretation.html">registering other online events associated with Crossref DOIs</a>, e.g. a Wikipedia page referencing one or more journal articles.</p>
<p>This Tuesday we released version 5 of the Lagotto software (Fenner et al., <a href="https://blog.datacite.org/its-all-about-relations/#ref-https://doi.org/10.5281/ZENODO.49516">2016</a>) with support for what we need for the Event Data service. The release would not have been possible without developer <a href="https://github.com/afandian">Joe Wass</a> from Crossref. The list of changes is long and can be read about in detail in the <a href="https://github.com/lagotto/lagotto/releases/tag/v.5.0.1">release notes</a>. The highlights include:</p>
<ol>
<li>A <strong><strong>deposits</strong></strong> API allowing anyone with a valid API key to push events into the system using a JSON object which can be (almost) as simple as</li>
</ol>
<pre><code>{ &quot;subj_id&quot;: &quot;https://doi.org/10.1098/rspb.2015.2857&quot;,
  &quot;obj_id&quot;: &quot;https://doi.org/10.5061/DRYAD.7BQ5T&quot;,
  &quot;relation_type_id&quot;: &quot;cites&quot;,
  &quot;sourceid&quot;: &quot;europepmc_fulltext&quot; }</code></pre>
<ol>
<li>A <strong><strong>contributor</strong></strong> model to aggregate resources by contributor, using the ORCID ID as persistent identifier.</li>
<li>Support for <strong><strong>Github</strong></strong>, describing the relations between software release, code repository, and repository owner, for the by now more than 7,000 DataCite DOIs for software linked to a Github release.</li>
</ol>
<p>In the coming months DataCite and Crossref will continue developing the platform to build out their Event Data services, so stay tuned for updates. And if you don’t mind minor bugs and incomplete data (currently about 1.2 million events for about 400,000 DataCite DOIs), take a look at <a href="https://eventdata.test.datacite.org/">DataCite Event Data</a> and send us your feedback.</p>
<figure>
<img src="https://blog.datacite.org/images/2016/04/lagotto.jpg" class="kg-image" alt="A real life lagotto. Credit: Anke Büter and Najko Jahn (Exeter)" /><figcaption aria-hidden="true"><strong><strong>A real life lagotto</strong></strong>. Credit: Anke Büter and Najko Jahn (Exeter)</figcaption>
</figure>
<p><em>This blog post was <a href="https://doi.org/10.5438/pe54-zj5t">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>DataCite Metadata Working Group. (2014). DataCite metadata schema for the publication and citation of research data v3.1. <em>DataCite</em>. Retrieved from <a href="https://doi.org/10.5438/0010">https://doi.org/10.5438/0010</a></p>
<p>Fenner, M., Wass, J., Song, J., Dennis, Z., Whitwell, M., Osowski, J., … Chodacki, J. (2016). Lagotto 5.0.1. Zenodo. Retrieved from <a href="https://doi.org/10.5281/ZENODO.49516">https://doi.org/10.5281/ZENODO.49516</a></p>
<p>Mayo, C., Hull, E. A., &amp; Vision, T. J. (2015). The location of the citation: changing practices in how publications cite original data in the Dryad Digital Repository. <em>Zenodo</em>. <a href="https://doi.org/10.5281/zenodo.32412">https://doi.org/10.5281/zenodo.32412</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Launching the DataCite Status Page]]></title>
        <id>7hhb83m-n4h92wr-w4ysk3t-9r3te</id>
        <link href="https://blog.front-matter.io/mfenner/launching-the-datacite-status-page"/>
        <updated>2016-01-11T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[As a provider of crucial scholarly infrastructure, it is critical that DataCite not only provides a reliable service, but also properly communicates problems. The best way to do this is via a central status page, a best practice used by many organizations from Github and Diqus to Slack....]]></summary>
        <content type="html"><![CDATA[<p>As a provider of crucial scholarly infrastructure, it is critical that DataCite not only provides a reliable service, but also <a href="https://www.crossref.org/blog/problems-with-dx.doi.org-on-january-20th-2015-what-we-know./">properly communicates problems</a>. The best way to do this is via a central status page, a best practice used by many organizations from <a href="https://status.github.com/">Github</a> and <a href="https://status.disqus.com/">Diqus</a> to <a href="https://status.slack.com/">Slack</a>. Because you don't want to run the status page with the rest of your infrastructure (as the page may go down if there is a problem), many organizations use a third-party service.</p>
<p>Today DataCite is launching a status page for all its services at <a href="http://status.datacite.org/">http://status.datacite.org</a>. You can also reach the status page via the navigation menu in the upper right corner of recently launched DataCite services. Below is more information about the main features of the service.</p>
<h2 id="services-status">Services status</h2>
<p>At the top of the status page we provide an overview of the status of all our services.</p>
<figure>
<img src="https://blog.datacite.org/images/2016/01/services-overview.png" class="kg-image" alt="Services Overview" /><figcaption aria-hidden="true">Services Overview</figcaption>
</figure>
<p>We will update the status manually when there is an issue, possible values are</p>
<ul>
<li>Operational</li>
<li>Degraded Performance</li>
<li>Partial Outage</li>
<li>Major Outage</li>
</ul>
<p>The overall status at the top of the page reflects these changes.</p>
<h2 id="live-system-metrics">Live system metrics</h2>
<p>The DataCite status page shows the aggregated uptime for all DataCite services. The data are generated by sending an HTTP/HTTPS request to each service from different locations around the world every 30 sec (using <a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html">Amazon AWS Route 53 Health Checks</a> and aggregation of data in <a href="https://www.librato.com/">Librato</a>).</p>
<figure>
<img src="https://blog.datacite.org/images/2016/01/live-metrics.png" class="kg-image" alt="Live Metrics" /><figcaption aria-hidden="true">Live Metrics</figcaption>
</figure>
<h2 id="incidents">Incidents</h2>
<p>Incidents such as service downtime will be reported with incident status (Investigating|Identified|Monitoring|Resolved) and time. This information will remain available after incidents are resolved on an incident history page.</p>
<figure>
<img src="https://blog.datacite.org/images/2016/01/incidents.png" class="kg-image" alt="Incidents" /><figcaption aria-hidden="true">Incidents</figcaption>
</figure>
<h2 id="scheduled-maintenance">Scheduled maintenance</h2>
<p>The status page will inform users about scheduled maintenance such as server upgrades requiring server downtime.</p>
<h2 id="notifications">Notifications</h2>
<p>Subscribing to updates via email, Twitter or RSS feed is the best way to stay informed about any issues affecting availability of DataCite services. You can do this by clicking on the <strong><strong>Subscribe to Updates</strong></strong> button:</p>
<figure>
<img src="https://blog.datacite.org/images/2016/01/subscribe-to-incidents.png" class="kg-image" alt="Subscribe to incidents via email, Twitter or RSS" /><figcaption aria-hidden="true">Subscribe to incidents via email, Twitter or RSS</figcaption>
</figure>
<p>All DataCite mailing lists will automatically be notified. Please contact us by email if you want to integrate the DataCite service status information via API into your own services.</p>
<h2 id="report-a-problem">Report a problem</h2>
<p>If you encounter a problem with one of the DataCite services despite the status page not showing any issues, please report the problem by sending an email to <a href="mailto:support@datacite.org">DataCite Support</a>. Please always use this email address to make it easier for us to keep track of problem reports.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/yhcj-p5hr">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Announcing the DataCite Blog Relaunch]]></title>
        <id>2x36xkm-mq19n8v-7yb07r1-abcwc</id>
        <link href="https://blog.front-matter.io/mfenner/announcing-the-datacite-blog-relaunch"/>
        <updated>2015-12-28T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The DataCite blog has migrated to a new platform, from a hosted version at Ghost to a self-hosted version using Jekyll. The main reason for this change is that it gives us more control over the formatting of blog posts....]]></summary>
        <content type="html"><![CDATA[<p>The DataCite blog has migrated to a new platform, from a hosted version at <a href="https://ghost.org/">Ghost</a> to a self-hosted version using <a href="https://jekyllrb.com/">Jekyll</a>. The main reason for this change is that it gives us more control over the formatting of blog posts. The migration was easy as both Ghost and Jekyll use <a href="http://support.ghost.org/markdown-guide/">markdown</a> to format blog posts, and the blog post URLs haven't changed.</p>
<p>Other than some layout changes that make the blog look and feel more consistent with other DataCite sites, the other main difference that users will see is that the blog now uses formal citations and reference lists for scholarly content. A September blog post (Fenner, <a href="https://blog.datacite.org/announcing-blog-relaunch/#ref-https://blog.datacite.org/adding-references-to-the-datacite-blog">2015</a>) described the background for this, and a good example where you can see the change is the post summarizing the September persistent identifier workshop in Paris (Cruse, <a href="https://blog.datacite.org/announcing-blog-relaunch/#ref-https://blog.datacite.org/recap">2015</a>).</p>
<p>Below are some tips if you also want to add formal references to your blog:</p>
<ol>
<li>Use markdown format and Jekyll with <a href="http://pandoc.org/">Pandoc</a> to convert markdown to html. You can add Pandoc support to Jekyll by adding <code>gem 'jekyll-pandoc'</code> to the Jekyll Gemfile and the following into <code>_config.yml</code>:</li>
</ol>
<pre><code>pandoc:
  extensions:
    - normalize
    - smart
    - mathjax
    - csl: _styles/apa.csl
    - bibliography: bibliography/references.bib</code></pre>
<p>The DataCite blog uses the <a href="http://www.apastyle.org/">APA citation style</a> and stores the references in BibTex format. Pandoc uses the <a href="http://citationstyles.org/">Citation Style Language</a>, so it is easy to switch to any of the 5000+ available styles.</p>
<ol>
<li>You can use <a href="https://pages.github.com/">Github Pages</a> to host the blog, but if you want to use <code>https</code>, and/or more flexibility with caching and domain names I recommend <a href="https://aws.amazon.com/s3/">Amazon S3</a>. Unless your blog sees a lot of traffic I doubt that the monthly cost is more than $2-5. One other advantage is that you don't have to deal with multiple git branches – which is how Github Pages stores the deployed website – as this can be confusing.</li>
<li>Jekyll is a static site generator, i.e. all blog pages are generated as HTML and no database backend is needed. You can build and deploy to S3 from your local computer, but I highly recommend to use a continuous integration tool for this. DataCite uses <a href="https://travis-ci.com/">Travis CI</a>, which is free to use for open source projects. Travis CI has nice support for deployment to Amazon S3, rebuilding all pages and deploying to Amazon S3 takes about three minutes, and is triggered when we commit new code (e.g. a new blog post) to the <a href="https://github.com/datacite/blog">blog</a> git repository. For testing new features we deploy the <code>test</code> git branch to <a href="https://blog.test.datacite.org/">https://blog.test.datacite.org</a>.</li>
<li>Since the <code>1.15.2</code> release in November Pandoc supports <code>://</code> in citation keys. This makes it easy to generate consistent citation keys using the <code>URL</code> field as the key, in particular when sharing references with others. One nice side effect is that we can use this key (which Pandoc puts into the <code>data-cites</code> attribute of the generated HTML) to generate a link from the in-text citation with a little bit of Javascript, e.g. this one: (Cruse, <a href="https://blog.datacite.org/announcing-blog-relaunch/#ref-https://blog.datacite.org/recap">2015</a>).</li>
</ol>
<p>This blog post is not only the last post on this blog in 2015, but also the 25th post since we launched this blog in August, a nice little milestone at the end of the year. We wish all readers a great start into the new year.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/xcbj-g7zy">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>Cruse, T. (2015). Recap: Persistent identifiers in Paris. DataCite Blog. Retrieved from <a href="https://blog.datacite.org/recap">https://blog.datacite.org/recap</a></p>
<p>Fenner, M. (2015). Adding references to the DataCite blog. DataCite Blog. Retrieved from <a href="https://blog.datacite.org/adding-references-to-the-datacite-blog">https://blog.datacite.org/adding-references-to-the-datacite-blog</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Infrastructure Tips for the Non-Profit Startup]]></title>
        <id>45yrr4x-acd82z8-e1sbv01-97wdg</id>
        <link href="https://blog.front-matter.io/mfenner/infrastructure-tips-for-the-non-profit-startup"/>
        <updated>2015-12-23T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[When I started as DataCite Technical Director four months ago, my first post (Fenner, 2015) on this blog was about what I called <strong><strong>Data-Driven Development</strong></strong>. The post included a lot of ideas on how to approach development and technical infrastructure....]]></summary>
        <content type="html"><![CDATA[<p>When I started as DataCite Technical Director four months ago, my first post (Fenner, <a href="https://blog.datacite.org/tips-for-the-non-profit-startup/#ref-https://blog.datacite.org/data-driven-development">2015</a>) on this blog was about what I called <strong><strong>Data-Driven Development</strong></strong>. The post included a lot of ideas on how to approach development and technical infrastructure. In this post I want to take a second look.</p>
<p>While I think the ideas expressed in the blog post are still true, I also learned that the focus of a Technical Director working for a small non-profit is somewhere else. The main challenge might be to properly run infrastructure and technical development with limited resources, both in terms of staff and money. While DataCite isn't a startup (the organization turned six years old this month), we face many of the same challenges. And as a non-profit, we can't take the approach of the typical startup, which in the early stages might have a small staff, but usually can spend more money than it is taking in.</p>
<h3 id="automate-as-much-as-possible">Automate as much as possible</h3>
<p>The biggest cost is obviously staff, so it is very important to automate the technical infrastructure as much as possible. Luckily many powerful services and best practices have been developed in the last few years, under the umbrella term <a href="http://theagileadmin.com/what-is-devops/">DevOps</a>. The first step is to go with a cloud infrastructure provider rather than hosting your own servers. While the cost seems higher on paper, it is much easier to automate infrastructure using a cloud provider if you have a small technical team. DataCite infrastructure has been hosted by <strong><strong>Amazon Web Services</strong></strong> (AWS) since the beginning, and we currently see have no plans to change that.</p>
<p>A large number of tools integrate with AWS, three services that have become essential for DataCite in the past few months are <a href="https://terraform.io/">Terraform</a>, <a href="https://www.packer.io/">Packer</a> and <a href="https://blog.datacite.org/tips-for-the-non-profit-startup/">Chef</a>:</p>
<ul>
<li><strong><strong>Terraform</strong></strong> treats infrastructure as code and allows us to have our AWS configuration (EC2 instances, Virtual Private Network, Security Groups, etc.) managed with a set of configuration files stored in a private git repo, e.g.</li>
</ul>
<pre><code>resource &quot;aws_route_table&quot; &quot;production&quot; {
    vpc_id = &quot;${aws_vpc.production.id}&quot;
    route {
        cidr_block = &quot;0.0.0.0/0&quot;
        gateway_id = &quot;${aws_internet_gateway.production.id}&quot;
    }

    tags {
        Name = &quot;production&quot;
    }
}

resource &quot;aws_main_route_table_association&quot; &quot;production&quot; {
    vpc_id = &quot;${aws_vpc.production.id}&quot;
    route_table_id = &quot;${aws_route_table.production.id}&quot;
}

resource &quot;aws_route_table_association&quot; &quot;production&quot; {
    subnet_id = &quot;${aws_subnet.production.id}&quot;
    route_table_id = &quot;${aws_route_table.production.id}&quot;
}

resource &quot;aws_security_group&quot; &quot;production&quot; {
  name = &quot;production&quot;
  description = &quot;production&quot;
  vpc_id = &quot;${aws_vpc.production.id}&quot;</code></pre>
<ul>
<li><strong><strong>Packer</strong></strong> automates the creation of machine and container images. We use Packer to automatically build Amazon Machine Images (AMIs) that we then deploy as EC2 instances using terraform</li>
</ul>
<figure>
<img src="https://blog.datacite.org/images/2015/12/Bildschirmfoto-2015-12-23-um-11-29-50.png" class="kg-image" alt="Packer" /><figcaption aria-hidden="true">Packer</figcaption>
</figure>
<ul>
<li><strong><strong>Chef</strong></strong> for automated configuration management. We use Chef to help Packer build AMIs.</li>
</ul>
<pre><code>==&gt; default: [2015-12-22T23:03:53+00:00] INFO: template[/etc/nginx/sites-enabled/dlm.conf] updated file contents /etc/nginx/sites-enabled/dlm.conf
==&gt; default:
==&gt; default: - update content in file /etc/nginx/sites-enabled/dlm.conf from c2a428 to fe08d6
==&gt; default:
==&gt; default: --- /etc/nginx/sites-enabled/dlm.conf  2015-12-22 22:31:52.136854399 +0000
==&gt; default:
==&gt; default: +++ /etc/nginx/sites-enabled/.dlm.conf20151222-13309-1bw1o6p 2015-12-22 23:03:53.161869265 +0000
==&gt; default:
==&gt; default: @@ -1,4 +1,4 @@
==&gt; default:
==&gt; default: -upstream $backend {
==&gt; default:</code></pre>
<p>Terraform, Packer and Chef are open source. We use <a href="https://hashicorp.com/atlas.html">Atlas</a> (commercial, but free for small installations) to combine them into a web-based team workflow. We hope to complete the migration for all DataCite services in the coming months.</p>
<h3 id="think-carefully-about-build-vs-buy">Think carefully about Build vs. Buy</h3>
<p>A common approach in the commercial startup world is to focus on the particular product or service that the organization wants to build, and then outsource almost everything else. This is important when there is only a small number of staff and you want to move fast. While this approach also applies to non-profit organizations, the decision of <strong><strong>build vs. buy</strong></strong> will sometimes be different, because some of these outsourced services would just be too expensive, or create a lock-in that would be a problem later on.</p>
<p>But for the most part I think the risk of trying to build too much yourself is bigger, in particular since many external services have monthly plans, and there are often several alternatives. The biggest consideration is the risk of lock-in, which is of course what all service providers are aiming for.</p>
<h3 id="open-source-where-it-is-important">Open Source where it is important</h3>
<p>The software written by DataCite staff to run the DataCite services is all open source, hosted in a <a href="https://github.com/datacite">public Github repository</a>. This is important for a number of reasons, best explained in the Principles of Open Scholarly Infrastructures (Bilder, Lin, &amp; Neylon, <a href="https://blog.datacite.org/tips-for-the-non-profit-startup/#ref-https://doi.org/10.6084/M9.FIGSHARE.1314859">2015</a>). One nice side effect is that a number of important external services are free for open source projects, for example Github or the <a href="https://travis-ci.org/">Travis CI</a> continuous integration service.</p>
<p>What this doesn't mean is that all software that DataCite uses should be open source. I like the approach that ORCID has taken in the ORCID Principles (“Our principles,” <a href="https://blog.datacite.org/tips-for-the-non-profit-startup/#ref-https://orcid.org/about/what-is-orcid/principles">2011</a>):</p>
<blockquote>
All software developed by ORCID will be publicly released under an Open Source Software license approved by the Open Source Initiative. For the software it adopts, ORCID will prefer Open Source.
</blockquote>
<p>Some of the services mentioned above (e.g. Google Apps, Slack, AWS) are obviously not open source, and that is ok if they don't create a lock-in or serious dependency for running the DataCite infrastructure. An interesting approach is hosted open source software, such as this blog. We are currently paying <a href="https://ghost.org/">ghost.org</a> a small amount of money to host the blog for us, but we can always move the blog somewhere else or start hosting it ourselves. Our use of AWS is more complex, but similar, all the software (databases, web servers, etc.) we are running is open source, and we can move to a different hosting provider if that is ever needed.</p>
<h3 id="get-non-profit-discounts">Get Non-Profit Discounts</h3>
<p>Some organizations provide their infrastructure services for free or a discount to non-profit organizations. DataCite started using <a href="https://apps.google.com/">Google Apps</a> and <a href="https://slack.com/">Slack</a> in August. Both are free for eligible non-profits, for other services such as <a href="https://products.office.com/en-us/nonprofit/office-365-nonprofit">Office 365</a> we get a deep discount.</p>
<p>Slack has become an essential internal communication tool. Not only because the current five staff members are in three different countries, but also because Slack nicely integrates with a large number of services. This greatly helps with keeping everyone on the same page.</p>
<figure>
<img src="https://blog.datacite.org/images/2015/12/Bildschirmfoto-2015-12-23-um-11-26-47.png" class="kg-image" alt="Slack" /><figcaption aria-hidden="true">Slack</figcaption>
</figure>
<h3 id="start-small-but-make-changes-easy">Start small but make changes easy</h3>
<p>One trap you can fall into is to think too big when starting out. You should build or buy what you need now or in the near future. At the same time you should make sure that whatever solution you come up with will scale up when needed.</p>
<p>There are a number of services out there that provide wonderful value, but are not really appropriate for a small non-profit. They solve problems of much larger organizations, e.g. auto-scaling of servers or centralized data analytics.</p>
<h3 id="cooperate-with-other-non-profit-organizations">Cooperate with other non-profit organizations</h3>
<p>One important advantage that non-profits have over commercial startups is that it is easier for them to cooperate with other organizations. In the case of DataCite this primarily means with DataCite members and CrossRef.</p>
<p>Cooperation takes time and effort, and staff time is usually limited when you are a small organization and also have a lot of technical work on your plate. But it is still a worthwhile investment. And I think non-profit organizations in the scholarly communication space could cooperate much more. While there are many cooperations around specific projects and initiatives, I think most of the basics of running the organization, and the technical infrastructure in particular, are not really discussed much. Small non-profits such as DataCite face particular challenges that are different from both commercial organizations and larger non-profits such as academic institutions. I want to spend more time in 2016 working on this, so please contact me if you are interested to help.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/t0ap-d5w7">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>Bilder, G., Lin, J., &amp; Neylon, C. (2015). Principles for open scholarly infrastructures-v1. Retrieved from <a href="https://doi.org/10.6084/M9.FIGSHARE.1314859">https://doi.org/10.6084/M9.FIGSHARE.1314859</a></p>
<p>Fenner, M. (2015). Data-driven development. DataCite Blog. Retrieved from <a href="https://blog.datacite.org/data-driven-development">https://blog.datacite.org/data-driven-development</a></p>
<p>Our principles. (2011). ORCID. Retrieved from <a href="https://orcid.org/about/what-is-orcid/principles">https://orcid.org/about/what-is-orcid/principles</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Announcing the DataCite Profiles Service]]></title>
        <id>2nf8dpx-tz58hm9-y49mghp-zd2fd</id>
        <link href="https://blog.front-matter.io/mfenner/announcing-the-datacite-profiles-service"/>
        <updated>2015-11-09T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[DataCite Labs today is launching the DataCite Profiles service, a central place for users to sign in with DataCite, using their ORCID credentials.The first version of DataCite Profiles focusses on integration with ORCID via the <strong><strong>Search...]]></summary>
        <content type="html"><![CDATA[<p>DataCite Labs today is launching the <a href="https://profiles.datacite.org/">DataCite Profiles</a> service, a central place for users to sign in with DataCite, using their ORCID credentials.</p>
<p>The first version of DataCite Profiles focusses on integration with ORCID via the <strong><strong>Search &amp; Link</strong></strong> and <strong><strong>Auto-Update</strong></strong> services, described in a <a href="https://blog.datacite.org/explaining-the-datacite-orcid-auto-update/">previous blog post</a>. When users first sign-in, or when they go to their Settings page (accessible via the navigation menu in the upper right corner), they are presented with these two choices for adding their works to their ORCID record:</p>
<figure>
<img src="https://blog.datacite.org/images/2015/11/Bildschirmfoto-2015-11-09-um-20-02-45.png" class="kg-image" alt="ORCID Auto-Updated" /><figcaption aria-hidden="true">ORCID Auto-Updated</figcaption>
</figure>
<p><strong><strong>Auto-Update</strong></strong> still needs a bit more work and hasn't launched yet, but by signing up for the Profiles service users give DataCite permission to automatically update their ORCID record with works with DataCite DOIs that include their ORCID identifier.</p>
<p><strong><strong>Search &amp; Link</strong></strong> can be started from the Settings page and automatically searches the DataCite Metadata Store for any of the name variants given in the ORCID record.</p>
<p>Together with the Profiles service we are launching a new common navigation bar with links to the most common services and a place to sign in.</p>
<figure>
<img src="https://blog.datacite.org/images/2015/11/Bildschirmfoto-2015-11-09-um-20-14-22.png" class="kg-image" alt="Search" /><figcaption aria-hidden="true">Search</figcaption>
</figure>
<p>DataCite Profiles, <a href="https://search.test.datacite.org/">DataCite Labs Search</a> and the DataCite Labs Link Store already support this common layout, and other DataCite services will be added over time.</p>
<p>DataCite Profiles uses JSON Web Tokens (<a href="http://jwt.io/">JWT</a>) to provide a single-sign on service for DataCite. JWT are an attractive alternative to setting up an OAuth provider for this use case. We use them to share additional profile information such as the user role across DataCite services.</p>
<p>The Profiles service currently does not provide authenticated access to other DataCite services such as the DataCite <strong><strong>Metadata Store</strong></strong>. And we currently only support sign-in via ORCID, not via other third-party providers such as Google, and not by username/password. As always we appreciate your feedback regarding issues and feature requests. Like all core DataCite services Profiles was written as an open source application, and can be found at <a href="https://github.com/datacite/volpino">https://github.com/datacite/volpino</a>.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/15x1-bj6r">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explaining the DataCite/ORCID Auto-update]]></title>
        <id>15ppsht-j4491pa-95bvxxe-d3mb0</id>
        <link href="https://blog.front-matter.io/mfenner/explaining-the-datacite-orcid-auto-update"/>
        <updated>2015-10-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[This Monday ORCID, CrossRef and DataCite announced (ORCID post, CrossRef post, DataCite post) the new auto-update service that automatically pushes metadata to ORCID when an ORCID identifier is found in newly registered DOI names.This is the first joint announcement by the three organizations,...]]></summary>
        <content type="html"><![CDATA[<p>This Monday ORCID, CrossRef and DataCite announced (<a href="http://blog.orcid.org/blog/2015/10/26/auto-update-has-arrived-orcid-records-move-next-level">ORCID post</a>, <a href="https://www.crossref.org/blog/auto-update-has-arrived-orcid-records-move-to-the-next-level/">CrossRef post</a>, <a href="https://blog.datacite.org/auto-update-has-arrived/">DataCite post</a>) the new auto-update service that automatically pushes metadata to ORCID when an ORCID identifier is found in newly registered DOI names.</p>
<p>This is the first joint announcement by the three organizations, and shows the close collaboration between ORCID, CrossRef and DataCite. A good opportunity to learn more about these joint activities are the <a href="https://orcid.org/content/sf2015">ORCID Outreach Meeting in San Francisco</a> November 3-4, and the <a href="https://www.crossref.org/crossref-live-annual/archive/">CrossRef Annual Meeting in Boston</a> November 17-18.</p>
<p>As promised in the Monday blog post, I want to explain the DataCite implementation of the ORCID auto-update functionality in a separate blog post. To start with the bad news: the DataCite service isn't ready yet, and will launch in November. There are two reasons for this: CrossRef started to work on this functionality much earlier, and – more importantly – DataCite feels that we need to make some major architectural changes in our systems to implement this properly. The good news is that the architectural changes will give us a more solid foundation for additional features we can add over time. Let me explain the most important issues below.</p>
<h3 id="permissions">Permissions</h3>
<p>In order to update an ORCID record, an organization needs permission from the researcher who owns the record. These permissions can be short-lived for a specific update, or long-lived for many years. The latter is obviously the preferred option for the auto-update functionality.</p>
<p>DataCite is already collecting permissions to update ORCID records from our <a href="https://search.datacite.org/">Search and Link service</a> and we want to continue collecting permissions for the Auto-update service in the same place. To do this properly, we will launch a new Profiles service. It will act as a central place to interact with DataCite services. In the first version it will focus on ORCID permissions, but we can build this out over time with additional functionality. The service is currently under testing, we hope to launch <strong><strong>Labs Profiles</strong></strong> next week.</p>
<p>One unresolved issue with the current implementation is that DataCite has to use the same ORCID client credentials for both the Search and Link and Auto-update services. This makes it impossible to distinguish self-claims by a researcher coming in from the Search and Link service, and links coming in from the Auto-update service. CrossRef's has an ORCID membership that allows them to use separate client credentials for Search and Link and Auto-update. We are working with ORCID to solve this.</p>
<h3 id="multiple-sources">Multiple Sources</h3>
<p>Another challenge is the synchronization between the Search and Link and Auto-update services. ORCID records can contain multiple claims for the same DOI, e.g. from Search and Link, Auto-update, and possibly also other sources. Here is an example from my ORCID record with claims from Europe PubMed Central, CrossRef Metadata Search and Scopus:</p>
<figure>
<img src="https://blog.datacite.org/images/2015/10/Bildschirmfoto-2015-10-29-um-11-01-37.png" class="kg-image" alt="ORCID record" /><figcaption aria-hidden="true">ORCID record</figcaption>
</figure>
<p>It is important for DataCite to allow claims from multiple sources, and to show the provenance of every item in a person's ORCID record. The way the Search and Link service currently works (and <a href="http://search.crossref.org/">CrossRef Metadata Search</a> and <a href="http://search.test.datacite.org/">DataCite Labs Search</a> use the same <a href="https://github.com/crosscite/doi-metadata-search">codebase</a> for this) is that researchers can only add a claim to their ORCID records if that DOI is not yet claimed, even if the claim comes from another source. This is a pragmatic decision, as researchers will probably resort to self-claiming only for items that were not automatically added to their profiles. With the launch of Auto-update we need to rethink this approach, possibly allowing users to claim all works that have not yet been claimed via the Search and Link service.</p>
<h3 id="push-api">Push API</h3>
<p><a href="https://support.orcid.org/hc/en-us/articles/360006972953">Notifications</a> are an essential feature of ORCID's Auto-update. They allow CrossRef and DataCite to push information to the ORCID record of a person without prior permission. The ORCID record owner receives a message in his/her inbox and has the opportunity to decide whether or not to accept the claim(s).</p>
<p>The way DataCite's end is implemented results in an information push to ORCID whenever an ORCID is found in our metadata. There is a big overlap between this functionality and what we do in the Search and Link service: authentication with ORCID, background workers for processing, XML generation, error tracking, etc. Therefore we want to move this functionality to a centralised place, separate from the core Search and Link and Auto-update functionalities. While the current focus is on pushing information to ORCID, we could build similar notification services for other organizations in the future, e.g. pushing information to a funder whenever we find a grant identifier in newly issued DOI names. The above requires refactoring of the Search and Link code and it is currently underway.</p>
<h3 id="round-trip">Round-trip</h3>
<p>Auto-update is a milestone in automating the workflow of linking ORCID identifiers and DOI names. What is still missing is the opposite direction: telling publishers and data centres about links to their DOIs found in the ORCID registry, e.g. via self-claiming services such as Search and Link. This is relevant for already published works, and for publishers and data centres that have not yet integrated ORCID into their submission systems.</p>
<p>The ORCID registry focusses on a researcher view, i.e. the works associated with a particular researcher. We also need the inverse here, a service that shows all ORCID identifiers associated with a particular DOI. This information can be very helpful for publishers and data centres, as they can take this information, verify it, and add it to their internal systems and to the metadata they send to CrossRef or DataCite. DataCite is working with ORCID and CrossRef on this. We have aggregated this information for all 205,000 DataCite DOI names that are linked to at least one ORCID identifier. We found a total of about one million links (because most DOI names are linked to multiple ORCID identifiers) and a surprisingly small number of ORCID identifiers (about 2,500) responsibles for these one million links. Or maybe not so surprising if you remember <a href="https://blog.datacite.org/digging-into-data-using-r/">this August blog post</a>.</p>
<p>Please provide feedback regarding auto-update in the comments. We will follow-up with a new blog post in a few weeks when the DataCite Auto-update service has launched.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/3dfw-z4kq">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto-Update Has Arrived!]]></title>
        <id>753vqtk-w458aqt-r4ps75m-ww7k4</id>
        <link href="https://blog.front-matter.io/mfenner/auto-update-has-arrived"/>
        <updated>2015-10-26T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[<em>This post has been cross-posted from the ORCID blog. We will follow up with a blog post later this week explaining the DataCite auto-update implementation.</em>Since ORCID’s inception, our key goal has been to unambiguously identify researchers and...]]></summary>
        <content type="html"><![CDATA[<p><em>This post has been cross-posted from the <a href="http://orcid.org/blog/2015/10/26/auto-update-has-arrived-orcid-records-move-next-level">ORCID blog</a>. We will follow up with a blog post later this week explaining the DataCite auto-update implementation.</em></p>
<p>Since ORCID’s inception, our key goal has been to unambiguously identify researchers and provide tools to automate the connection between researchers and their creative works. We are taking a big step towards achieving this goal today, with the launch of <a href="http://orcid.org/blog/2014/11/21/new-functionality-friday-auto-update-your-orcid-record">Auto-Update</a> functionality in collaboration with <a href="http://www.crossref.org/">Crossref</a> and <a href="https://www.datacite.org/">DataCite</a>.</p>
<p>There’s already been a lot of excitement about Auto-Update: <a href="https://www.crossref.org/blog/crossref-to-auto-update-orcid-records/">Crossref’s recent announcement</a> about the imminent launch generated a flurry of discussion and celebration on social media. Our own <a href="https://twitter.com/ORCID_Org/status/647020600192581633">tweet</a> on the topic was viewed over 10,500 times and retweeted by 60 other accounts.</p>
<p>So why all the fuss? We think Auto-Update will transform the way researchers manage their scholarly record. Until now, researchers have had to manually maintain their record, connecting new activities as they are made public. In ORCID, that meant using <a href="https://support.orcid.org/hc/en-us/articles/360006973653-Add-works-by-direct-import-from-other-systems">Search &amp; Link</a> tools developed by our member organizations to claim works manually. Researchers frequently ask, “Why, if I include my ORCID iD when I submit a manuscript or dataset, isn’t my ORCID record “automagically” updated when the work is published?”</p>
<p>With the launch of Auto-Update, that is just what will happen.</p>
<p><strong><strong>It might seem like magic but there are a few steps to make it work</strong></strong>:</p>
<ul>
<li><strong><strong>Researchers</strong></strong>. You need to do two things: (1) use your ORCID iD when submitting a paper or dataset, and (2) authorize Crossref and DataCite to update your ORCID record. In keeping with our commitment to ensuring that researchers maintain full control of their ORCID record, you may revoke this permission at any time, and may also choose privacy settings for the information posted on your record.</li>
<li><strong><strong>Publishers and data centers</strong></strong>. These organizations also have two things to do: (1) collect ORCID identifiers during the submission workflow, using a process that involves authentication (not a type-in field!), and (2) embed the iD in the published paper and include the iD when submitting information to Crossref or DataCite.</li>
<li><strong><strong>Crossref and DataCite</strong></strong>. Upon receipt of data from a publisher or data center with a valid identifier, Crossref or DataCite can automatically push that information to the researcher’s ORCID record. More information about how to opt out of this service can be found here: <a href="https://support.orcid.org/hc/en-us/articles/360006972953">the ORCID Inbox</a>.</li>
</ul>
<figure>
<img src="https://blog.datacite.org/images/2015/10/graph.png" class="kg-image" alt="Automagically" /><figcaption aria-hidden="true">Automagically</figcaption>
</figure>
<h3 id="why-is-this-so-revolutionary">Why is this so revolutionary?</h3>
<p>A bit of background, first. Crossref and DataCite, both non-profit organizations, are leaders in minting DOIs (Digital Object Identifiers) for research publications and datasets. A <a href="http://www.crossref.org/01company/16fastfacts.html#sthash.o7NGwOnP.dpuf">DOI</a> is a unique alphanumeric string assigned to a digital object – in this case, an electronic journal article, book chapter, or a dataset. Each DOI is associated with a set of basic metadata and a URL pointer to the full text, so that it uniquely identifies the content item and provides a persistent link to its location on the internet.</p>
<p>Crossref, working with over a thousand scholarly publishers, has generated well over 75 million DOIs for journal articles and book chapters. DataCite works with nearly 600 data centers worldwide and has generated over 6.5 million DOIs to date. Between them, Crossref and DataCite have already received almost a half a million works from publishers and data centers that include an ORCID iD validated by the author/contributor. With Auto-Update functionality in place, information about these articles can transit (with the author’s permission) to the author’s ORCID record.</p>
<p>Auto-Update doesn’t stop at a researcher’s ORCID record. Systems that have integrated ORCID APIs and have a researcher’s ORCID record connected to that system -- their faculty profile system, library repository, webpage, funder reporting system -- can receive alerts from ORCID. Information can move easily and unambiguously across systems.</p>
<p>This is the beginning of the end for the endless rekeying of information that plagues researchers -- and anyone involved in research reporting. Surely something to celebrate!</p>
<p><strong><strong>Questions you may have</strong></strong>:</p>
<h4 id="q-what-do-i-need-to-do-to-sign-up-for-auto-update">Q. What do I need to do to sign up for auto-update?</h4>
<p>You need to grant permission to Crossref and DataCite to post information to your ORCID record. You can do this today by using the Search and Link wizard for DataCite available through the ORCID Registry or the <a href="https://search.datacite.org/">DataCite Search</a> page. We also have added a new ORCID Inbox, so that you can receive a message from Crossref or DataCite if they receive a datafile with your iD, and you can grant permission directly. See <a href="https://support.orcid.org/hc/en-us/articles/360006972953">more on the ORCID Inbox</a>.</p>
<h4 id="q-will-crossref-and-datacite-be-able-to-update-my-orcid-record-with-already-published-works-for-which-i-did-not-use-my-orcid-id">Q. Will Crossref and DataCite be able to update my ORCID record with already published works for which I did not use my ORCID iD?</h4>
<p>No. The auto-update process only applies to those works that these organizations receive that include your ORCID iD. For previous works that did not include your ORCID iD, you will need to use the DataCite and Crossref Search and Link wizards to connect information with your iD.</p>
<h4 id="q-what-information-will-be-posted-to-my-record">Q. What information will be posted to my record?</h4>
<p>With your permission, basic information about the article (such as title, list of contributors, journal or publisher) or dataset (such as data center name and date of publication) will be posted, along with a DOI that allows users to navigate to the source paper or dataset landing page.</p>
<h4 id="q-what-if-my-journal-or-data-center-doesn%E2%80%99t-collect-orcid-ids">Q. What if my journal or data center doesn’t collect ORCID iDs?</h4>
<p>Ask them too! This simple step can be accomplished using either the Public or Member ORCID APIs. Information about integrating ORCID iDs in <a href="http://members.orcid.org/publisher-workflow">publishing</a> and <a href="http://members.orcid.org/repository-systems">repository</a> workflows is publicly available.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/ferw-cwhq">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Happy Birthday ORCID]]></title>
        <id>5pcj09z-w499db9-nh6z63e-faaa1</id>
        <link href="https://blog.front-matter.io/mfenner/happy-birthday-orcid"/>
        <updated>2015-10-16T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Three years ago today Open Researcher &amp; Contributor ID (ORCID) launched its service at the Outreach Meeting in Berlin. One of many tweets from the launch day:Congrats to @orcid_org the ORCID registry is live (http://t.co/2lxn0nLa)...]]></summary>
        <content type="html"><![CDATA[<p>Three years ago today Open Researcher &amp; Contributor ID (<a href="http://orcid.org/">ORCID</a>) launched its service at the Outreach Meeting in Berlin. One of many tweets from the launch day:</p>
<blockquote>
<p>Congrats to <a href="https://twitter.com/ORCID_Org?ref_src=twsrc%5Etfw">@orcid_org</a> the ORCID registry is live (<a href="http://t.co/2lxn0nLa">http://t.co/2lxn0nLa</a>) NPG is a proud launch partner (PR): <a href="http://t.co/9eiqe44x">http://t.co/9eiqe44x</a></p>
<p>— Nature Research (@nresearchnews) <a href="https://twitter.com/nresearchnews/status/258256101676564480?ref_src=twsrc%5Etfw">October 16, 2012</a></p>
</blockquote>
<p>Executive Director Laure Haak was written a nice <a href="http://orcid.org/blog/2015/10/15/celebrating-our-third-anniversary">blog post</a> summarizing the achievements in the past few years, going from 0 to 1.7 million registered users, 400 members and a staff of 20. Congratulations!</p>
<p>On October 17, a day after the ORCID launch, DataCite started to work with ORCID in the European Commission-funded ORCID and DataCite Interoperability Network <a href="http://odin-project.eu/">ODIN</a>, a project that has laid the groundwork for the integration of both the two services, and associated datasets. DataCite is continuing work with ORCID in the follow-up project <a href="http://project-thor.eu/">THOR</a> that started in June, and there will soon be an exciting announcement about a new integration of the two services.</p>
<p>This birthday has a special meaning for me, as I was a member of the ORCID Board at the time, and had helped organize the launch event in Berlin, as well as the kickoff meeting of the ODIN project, and a well-attended German-language <a href="https://dini.de/veranstaltungen/workshops/autorenidentifikation/">event on author identification</a> in the same venue two days before the launch, co-organized by <a href="https://dini.de/">DINI</a> and the Helmholtz Association. It is really amazing how far we have come since then.</p>
<p>Thank you ORCID for all your great contributions!</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/c61q-z2k7">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Software Citation Workflows]]></title>
        <id>1gxaeew-mv98qkv-q5yhqda-j8e1c</id>
        <link href="https://blog.front-matter.io/mfenner/software-citation-workflows"/>
        <updated>2015-10-15T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[<em>This blog post provides more detail for a short presentation I will give today at the Software Credit Workshop in London. The aim is to look at the infrastructure pieces needed for software discovery and credit,...]]></summary>
        <content type="html"><![CDATA[<p><em>This blog post provides more detail for a short presentation I will give today at the <a href="http://www.software.ac.uk/software-credit">Software Credit Workshop</a> in London. The aim is to look at the infrastructure pieces needed for software discovery and credit, and at the workflows linking these different parts of the infrastructure.</em></p>
<h3 id="code-repository">Code Repository</h3>
<p>Code repositories are the places where the actual work on software takes place, and for scientific software this often means that it happens in public with the use of an open license. Code repositories are increasingly integrated with additional services from issue trackers to continuous integration testing.</p>
<figure>
<img src="https://blog.datacite.org/images/2015/10/code_repositories.png" class="kg-image" alt="Code repositories" /><figcaption aria-hidden="true">Code repositories</figcaption>
</figure>
<p>One big problem with code repositories is that they are not intended as long-term archives for code. Github and Bitbucket didn't even exist 8 years ago, and Google Code will be <a href="http://google-opensource.blogspot.de/2015/03/farewell-to-google-code.html">shut down in January 2016</a>.</p>
<h3 id="data-repository">Data Repository</h3>
<p>Long-term archiving of software is best done in dedicated data repositories, the two most popular in terms of DataCite DOIs are <a href="https://zenodo.org/">Zenodo</a> (close to <a href="https://search.datacite.org/?query=%2A&amp;resourceType_facet=Software&amp;datacentre_facet=CERN.ZENODO+-+ZENODO+-+Research.+Shared.">5000</a> DOIs for software) and <a href="https://nanohub.org/">NanoHub</a> (about <a href="https://search.datacite.org/?query=%2A&amp;resourceType_facet=Software&amp;datacentre_facet=PURDUE.EZID+-+Purdue+University">2,000</a> DOIs for software). NanoHub uses the open source <a href="https://hubzero.org/">HubZero</a> software that integrates a subversion code repository, whereas Zenodo has built an integration with Github, described in this <a href="https://guides.github.com/activities/citable-code/">guide</a>.</p>
<figure>
<img src="https://blog.datacite.org/images/2015/10/Bildschirmfoto-2015-10-18-um-14-01-39.png" class="kg-image" alt="Making your code citable" /><figcaption aria-hidden="true">Making your code citable</figcaption>
</figure>
<p>Providing a long-term archive for code is needed to properly cite software, similarly to what we expect for research data and scholarly articles. We of course don't have to use DOIs for this, but DOIs make citation easier by requiring basic citation metadata, are supported by reference managers, and we can provide formatted citations via <a href="http://www.crosscite.org/">DOI content negotiation</a>, e.g. in <a href="https://search.datacite.org/?query=10.5281/ZENODO.32193">DataCite Labs Search</a>:</p>
<figure>
<img src="https://blog.datacite.org/images/2015/10/Bildschirmfoto-2015-10-18-um-13-59-34.png" class="kg-image" alt="Citation style" /><figcaption aria-hidden="true">Citation style</figcaption>
</figure>
<p>The Github/Zenodo integration assigns a DOI to a particular <a href="https://github.com/blog/1547-release-your-software">Github release</a> of a software repo. This is perfect for a citation, which should be specific for the software version used in a particular research project. In addition, users of software and software authors want to know who is citing or otherwise re-using all versions of the software. In order for this to work we need to think beyond a specific release and link that release to other releases and to the Github repository itself. The repository has no DOI attached to it in the current workflow, so this has to be done in a service separate from the DataCite Metadata Store.</p>
<h3 id="claim-store">Claim Store</h3>
<p>We can expand the DataCite Data-Level Metrics service or Claim Store <a href="https://blog.datacite.org/announcing-data-level-metrics-in-datacite-labs/">described in an earlier post</a> to properly handle Github repositories, and the first implementation is <a href="https://eventdata.test.datacite.org/">available now</a> in DataCite Labs. Continuing with the earlier example of the Python <strong><strong>librosa</strong></strong> library, the DataCite Claim store tracks links between release, repository and repository owner:</p>
<figure>
<img src="https://blog.datacite.org/images/2015/10/Bildschirmfoto-2015-10-18-um-17-07-48.png" class="kg-image" alt="Claim store" /><figcaption aria-hidden="true">Claim store</figcaption>
</figure>
<p>These links are made available in DataCite Labs Search (<a href="https://search.datacite.org/?query=librosa+python">link</a>), so that users can go directly to either the specific release or the code repository landing page instead of the archived version on Zenodo:</p>
<figure>
<img src="https://blog.datacite.org/images/2015/10/Bildschirmfoto-2015-10-18-um-17-40-51.png" class="kg-image" alt="Zenodo" /><figcaption aria-hidden="true">Zenodo</figcaption>
</figure>
<p>We can use the claim store to not only store those links, but also to track metrics around the software package over time, e.g. the number of Github stars and forks (349 and 68 for a combined 417 in this case):</p>
<figure>
<img src="https://blog.datacite.org/images/2015/10/Bildschirmfoto-2015-10-18-um-17-11-46.png" class="kg-image" alt="GitHub" /><figcaption aria-hidden="true">GitHub</figcaption>
</figure>
<p>We will be building out this service in the coming months with the goal of tracking all software packages with DOIs linked to a Github release. Future iterations may also show the number of Github stars and forks directly in the search results. And because the Claim Store provides an open API, this information can also be integrated in other places, most obviously Zenodo.</p>
<h3 id="language-and-domain-repository">Language and Domain Repository</h3>
<p>Language-specific repositories hold all software packages from a particular language, e.g. <a href="https://pypi.python.org/pypi">pypi</a> for Python or <a href="https://cran.r-project.org/">CRAN</a> for R, with a search interface for discovery and a specific format that allows for automatic installation. Although not all scientific software is submitted to these repositories, they are usually the place that software developers go to first for discovery and installation, using a package manager working with these repositories.</p>
<p>Domain-specific repositories such as the Astrophysics Source Code Library (<a href="http://ascl.net/">ASCL</a>) or <a href="https://www.bioconductor.org/">Bioconductor</a> serve important roles for discovery and community building. Both their strength and limitation is their domain-specific nature. They complement the source code repositories mentioned above. One important function of domain-specific repositories is to act as a filter for scientific software. Other approaches to identify software as scientific include:</p>
<ul>
<li>software that has a DOI (Zenodo, above)</li>
<li>software using specific tags (Depsy, see below)</li>
<li>software cited in the scholarly literature (ScienceToolbox)</li>
</ul>
<h3 id="metrics-data-provider">Metrics Data Provider</h3>
<p>Language-specific repositories provide the detailed information that is needed to more extensively track reuse of a software package. <a href="http://depsy.org/">Depsy</a> is a new software metrics data provider by the Impactstory Team that will launch later this month, and will provide detailed information about reuse, including citations in the scholarly literature. Again using <strong><strong>librosa</strong></strong> as an example, Depsy provides the following information:</p>
<figure>
<img src="https://blog.datacite.org/images/2015/10/Bildschirmfoto-2015-10-18-um-12-14-48.png" class="kg-image" alt="Depsy" /><figcaption aria-hidden="true">Depsy</figcaption>
</figure>
<p>Of particular interest is how Depsy tracks reuse, as the service follows all the dependencies and dependencies of dependencies of a software package, described as transitive credit by Dan Katz (<a href="https://blog.datacite.org/software-citation-workflows/#ref-https://doi.org/10.5334/jors.be">2014</a>). Depsy is currently tracking software packages in <strong><strong>pypi</strong></strong> and <strong><strong>CRAN</strong></strong>, and an open API is available. Once Depsy has launched, it would of course be of great interest to integrate data from the service into the DataCite Claim Store, and Depsy is providing an open API for this.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/1h7n-3cen">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>Katz, D. (2014). Transitive credit as a means to address social and technological concerns stemming from citation and attribution of digital products. <em>Journal of Open Research Software</em>, <em>2</em>(1), e20. Retrieved from <a href="https://doi.org/10.5334/jors.be">https://doi.org/10.5334/jors.be</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contributor Information in DataCite Metadata]]></title>
        <id>226kqgx-x7t9xq8-70b4twd-86pse</id>
        <link href="https://blog.front-matter.io/mfenner/contributor-information-in-datacite-metadata"/>
        <updated>2015-10-12T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The Force11 Joint Declaration of Data Citation Principles (Data Citation Synthesis Group, 2014) highlight the importance of giving scholarly credit to all contributors:Data citations should facilitate giving scholarly credit and normative and legal attribution to all contributors to the data,...]]></summary>
        <content type="html"><![CDATA[<p>The Force11 Joint Declaration of Data Citation Principles (Data Citation Synthesis Group, <a href="https://blog.datacite.org/contributor-information-in-datacite-metadata/#ref-https://doi.org/10.25490/a97f-egyk">2014</a>) highlight the importance of giving scholarly credit to all contributors:</p>
<blockquote>
Data citations should facilitate giving scholarly credit and normative and legal attribution to all contributors to the data, recognizing that a single style or mechanism of attribution may not be applicable to all data.
</blockquote>
<p>The EC-funded <a href="http://project-thor.eu/">THOR project</a> that DataCite is involved in addresses these issues, and I have summarized the findings of one of our first reports in a <a href="https://blog.datacite.org/differences-between-orcid-and-datacite-metadata/">previous blog post</a>. One of problems identified in the report was the use of a single entry field for personal names, as done by DataCite and many other scholarly services. We need separate fields for family and given names, the most important reason is to allow proper formatting of a data citation (different citation styles have different rules about author name formatting). As a small first step I have implemented proper personal name parsing, using the <a href="https://github.com/berkmancenter/namae">Namae</a> tool, in <a href="http://search.test.datacite.org/">DataCite Labs Search</a> and the upcoming <a href="http://eventdata.test.datacite.org/">DataCite Labs claim store</a>. One of the next places we can implement this is in the DOI content negotiation service, where we currently provide personal names as literal strings when using an output format that supports family and given names (<a href="http://data.crosscite.org/application/vnd.citationstyles.csl+json/10.6084/M9.FIGSHARE.791569">http://data.crosscite.org/application/citeproc+json/10.6084/M9.FIGSHARE.791569</a>):</p>
<pre><code>{
  &quot;type&quot;: &quot;dataset&quot;,
  &quot;DOI&quot;: &quot;10.6084/M9.FIGSHARE.791569&quot;,
  &quot;URL&quot;: &quot;http://dx.doi.org/10.6084/M9.FIGSHARE.791569&quot;,
  &quot;title&quot;: &quot;rOpenSci - a collaborative effort to develop R-based tools for facilitating Open Science&quot;,
  &quot;publisher&quot;: &quot;Figshare&quot;,
  &quot;issued&quot;: {
    &quot;raw&quot;: &quot;2013&quot;
  },
  &quot;author&quot;: [{
    &quot;literal&quot;: &quot;Scott Chamberlain&quot;
  }, {
    &quot;literal&quot;: &quot;Edmund Hart&quot;
  }, {
    &quot;literal&quot;: &quot;Karthik Ram&quot;
  }, {
    &quot;literal&quot;: &quot;Carl Boettiger&quot;
  }]
}</code></pre>
<p>To correctly identify contributors we have to use unique identifiers rather than personal names. The Force11 Joint Declaration of Data Citation Principles (Data Citation Synthesis Group, <a href="https://blog.datacite.org/contributor-information-in-datacite-metadata/#ref-https://doi.org/10.25490/a97f-egyk">2014</a>) highlight the importance of unique identifiers for data, and I had suggested in an early draft of the principles to also mention the importance of unique identifiers for contributors.</p>
<p><a href="http://orcid.org/">ORCID</a> identifiers are by far the most widely used identifiers in DataCite metadata – they can be found in the metadata of about <a href="https://search.test.datacite.org/?query=nameIdentifier%3AORCID%5C%3A*">208,000 DOI names</a> (other identifiers such as ISNI are also supported). In addition there are self-claims of DataCite DOI names in the ORCID registry (e.g. generated via the DataCite Search &amp; Link Service that is part of <a href="https://search.test.datacite.org/">Labs Search</a>), the exact number of which we currently don't know. DataCite is working with ORCID on a frictionless exchange of these DataCite/ORCID links in both directions.</p>
<p>But how are these DataCite/ORCID links shared with other services? A good starting point is the DataCite Search API. We can include <code>creator</code> and <code>nameIdentifier</code> in the results, but unfortunately these two fields are not linked together. Until we update the Solr schema for the Search API we therefore have to use the <code>xml</code> field that includes all metadata, and parse out the creator names and associated identifiers. We have recently implemented this in Labs Search, turning names with associated ORCID identifiers into clickable links that return a list of all DataCite DOI names associated with that person (<a href="http://search.test.datacite.org/?query=10.6084%2FM9.FIGSHARE.791569">http://search.test.datacite.org/?query=10.6084%2FM9.FIGSHARE.791569</a>):</p>
<figure>
<img src="https://blog.datacite.org/images/2015/10/Bildschirmfoto-2015-10-12-um-08-30-30.png" class="kg-image" />
</figure>
<p>Link name via ORCID ID</p>
<p>Labs Search also provides a <strong><strong>Cite</strong></strong> button that formats the metadata according to common citation styles such as <strong><strong>APA</strong></strong>, or in common exchange formats such as <strong><strong>BibTeX</strong></strong>. These formats unfortunately don't support ORCID identifiers (nothing has changed since I wrote about this in 2011), so that the DataCite/ORCID links would be lost using these formats.</p>
<p>Citeproc JSON is a modern alternative to BibTeX, RIS and similar exchange formats, and is used as the machine-readable representation to format references in the reference managers Zotero, Mendeley, Papers (and others) using <a href="http://citationstyles.org/">Citation Style Language</a>. Although Citeproc JSON doesn't support ORCID identifiers, it is much easier to extend than for example BibTeX, where adding ORCID identifiers without breaking the format is difficult to impossible. Last week I implemented this modified Citeproc JSON in a new DataCite service I am working on (e.g. using the example from above:</p>
<pre><code>&quot;author&quot;: [{
      &quot;family&quot;: &quot;Chamberlain&quot;,
      &quot;given&quot;: &quot;Scott&quot;,
      &quot;ORCID&quot;: &quot;http://orcid.org/0000-0003-1444-9135&quot;
    }, {
      &quot;family&quot;: &quot;Hart&quot;,
      &quot;given&quot;: &quot;Edmund&quot;
    }, {
      &quot;family&quot;: &quot;Ram&quot;,
      &quot;given&quot;: &quot;Karthik&quot;,
      &quot;ORCID&quot;: &quot;http://orcid.org/0000-0002-0233-1757&quot;
    }, {
      &quot;family&quot;: &quot;Boettiger&quot;,
      &quot;given&quot;: &quot;Carl&quot;,
      &quot;ORCID&quot;: &quot;http://orcid.org/0000-0002-1642-628X&quot;
    }]</code></pre>
<p>DataCite is not the first DOI registration agency to implement this, CrossRef is doing the same for some time in their REST API, e.g. for <a href="http://api.crossref.org/works/10.1111/1365-2745.12293">http://api.crossref.org/works/10.1111/1365-2745.12293</a>:</p>
<pre><code>&quot;author&quot;: [{
  &quot;affiliation&quot;: [{
    &quot;name&quot;: &quot;Department of Biological Sciences; Simon Fraser University; Burnaby BC Canada&quot;
  }],
  &quot;family&quot;: &quot;Chamberlain&quot;,
  &quot;given&quot;: &quot;Scott&quot;,
  &quot;ORCID&quot;: &quot;http://orcid.org/0000-0003-1444-9135&quot;
}, {
  &quot;affiliation&quot;: [{
    &quot;name&quot;: &quot;CONICET; Instituto Argentino de Investigaciones de las Zonas Aridas; Mendoza Argentina&quot;
  }, {
    &quot;name&quot;: &quot;Instituto de Ciencias Básicas; Universidad Nacional de Cuyo; Mendoza Argentina&quot;
  }],
  &quot;family&quot;: &quot;Vázquez&quot;,
  &quot;given&quot;: &quot;Diego P.&quot;
}, {
  &quot;affiliation&quot;: [{
    &quot;name&quot;: &quot;School of Biology; University of Leeds; Leeds UK&quot;
  }, {
    &quot;name&quot;: &quot;Naturalis Biodiversity Center; PoBox 9517 Leiden 2300RA The Netherlands&quot;
  }],
  &quot;family&quot;: &quot;Carvalheiro&quot;,
  &quot;given&quot;: &quot;Luisa&quot;
}, {
  &quot;affiliation&quot;: [{
    &quot;name&quot;: &quot;Department of Biological Sciences; Simon Fraser University; Burnaby BC Canada&quot;
  }],
  &quot;family&quot;: &quot;Elle&quot;,
  &quot;given&quot;: &quot;Elizabeth&quot;
}, {
  &quot;affiliation&quot;: [{
    &quot;name&quot;: &quot;Biology Department; University of Calgary; Calgary AB Canada&quot;
  }],
  &quot;family&quot;: &quot;Vamosi&quot;,
  &quot;given&quot;: &quot;Jana C.&quot;
}]</code></pre>
<p>You see one difference: CrossRef also provides the affiliation, as a list of text fields. DataCite metadata also contain an <code>affiliation</code> field. This is a text string, ideally DataCite should also support unique identifiers for the affiliation, as we already do for <code>HostingInstitution</code> which can have a <code>nameIdentifier</code> and <code>nameIdentifierScheme</code>.</p>
<p>Funding information is similar to affiliation in that it is something not related to the dataset itself, but to one or more contributors. We could therefore encode funding information similar to affiliation, as a <code>funding</code> field for each author. The big advantage would be that DataCite and ORCID would have consisting funding information, rather than DataCite listing funding for works, and ORCID listing funding for people, and no direct connection between the two.</p>
<p>Lastly, we can use Citeproc JSON to describe the contributor role of the author. DataCite distinguishes between <code>creator</code> – <em>the main researchers involved in producing the data, or the authors of the publication, in priority order</em> – and <code>contributor</code> for other contributions, with a controlled vocabulary for <code>contributorType</code>. The THOR report mentioned above (Fenner et al., <a href="https://blog.datacite.org/contributor-information-in-datacite-metadata/#ref-https://doi.org/10.5281/ZENODO.30799">2015</a>) goes into detail in the different contributor role vocabularies used by DataCite and ORCID (there is little overlap), and also describes <a href="https://www.casrai.org/credit.html">Project CRediT</a>, a community initiative to harmonize contributor roles across stakeholders, standardizing on 14 common roles. CRediT is closely link to <a href="https://www.mozillascience.org/contributorship-badges-a-new-project">contributorship badges</a>, a project started by the Mozilla Science Lab, with an example journal article using the CRediT roles and badges <a href="http://gigasciencejournal.com/blog/putting-credit-hands-researchers/">here</a>:</p>
<figure>
<img src="https://blog.datacite.org/images/2015/10/Bildschirmfoto-2015-10-12-um-09-39-34.png" class="kg-image" alt="Contributor badges" /><figcaption aria-hidden="true">Contributor badges</figcaption>
</figure>
<p>Taking all the above together, the JSON to describe all this information could look similar to the following (some of the data are made up):</p>
<pre><code>&quot;author&quot;: [{
      &quot;affiliation&quot;: [{
        &quot;name&quot;: &quot;Department of Biological Sciences; Simon Fraser University; Burnaby BC Canada&quot;,
        &quot;ISNI&quot;: &quot;0000-0004-1936-7494&quot;
      }],
      &quot;funding&quot;: [{
        &quot;funder-name&quot;: &quot;Alfred P. Sloan Foundation&quot;,
        &quot;funder-identifier&quot;: &quot;https://doi.org/10.13039/100000879&quot;,
        &quot;award-number&quot;: &quot;555-1212&quot;,
        &quot;award-uri&quot;: &quot;http://www.sloan.org/awards/555-1212&quot;
      }],
      &quot;family&quot;: &quot;Chamberlain&quot;,
      &quot;given&quot;: &quot;Scott&quot;,
      &quot;ORCID&quot;: &quot;http://orcid.org/0000-0003-1444-9135&quot;,
      &quot;CRediT&quot;: [&quot;conceptualization&quot;, &quot;writing_initial&quot;, &quot;writing_review&quot;]
    }, {
      &quot;family&quot;: &quot;Hart&quot;,
      &quot;given&quot;: &quot;Edmund&quot;
    }, {
      &quot;family&quot;: &quot;Ram&quot;,
      &quot;given&quot;: &quot;Karthik&quot;,
      &quot;ORCID&quot;: &quot;http://orcid.org/0000-0002-0233-1757&quot;
    }, {
      &quot;family&quot;: &quot;Boettiger&quot;,
      &quot;given&quot;: &quot;Carl&quot;,
      &quot;ORCID&quot;: &quot;http://orcid.org/0000-0002-1642-628X&quot;
    }]</code></pre>
<p>The above obviously contains a lot more information than the original Citeproc JSON. And event though <code>affiliation</code>, <code>funding</code> and <code>CRediT</code> are optional fields, this goes beyond the scope of Citeproc JSON, which is used to format references, rather than as a generic bibliographic exchange format. We should therefore call this JSON differently, and I propose <strong><strong>Crosscite JSON</strong></strong>, a common JSON format to describe scholarly works used by the DOI registration agencies CrossRef and DataCite. One particular challenge will be to strike the right balance between important information that we want to share easily, vs. keeping the JSON simple and not move away too much from Citeproc JSON, which after all is already implemented in a lot of tools and workflows. While the above JSON example looks a bit scary at first, it provides the level of detail asked for by institutions and funders, and - in contrast to the Data Citation Principles - <em>uses a single mechanism of attribution applicable to all scholarly works, including data</em>.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/d98m-9125">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>Data Citation Synthesis Group. (2014). Joint declaration of data citation principles. <em>Force11</em>. <a href="https://doi.org/10.25490/a97f-egyk">https://doi.org/10.25490/a97f-egyk</a></p>
<p>Fenner, M., Demeranville, T., Kotarski, R., Vision, T., Rueda, L., Dasler, R., … THOR Consortium. (2015). D2.1: Artefact, contributor, and organisation relationship data schema. <em>Zenodo</em>. <a href="https://doi.org/10.5281/ZENODO.30799">https://doi.org/10.5281/ZENODO.30799</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discussing the Scholarly Container]]></title>
        <id>1962w6m-bz387b8-yq28y8g-3mgx7</id>
        <link href="https://blog.front-matter.io/mfenner/discussing-the-scholarly-container"/>
        <updated>2015-10-02T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[One of my personal highlights in last week's Research Data Alliance (RDA) 6th Plenary Meeting in Paris was the Data Packages Birds of a Feather (BoF), organized by Rufus Pollock from the Open Knowledge Foundation (OKFN)....]]></summary>
        <content type="html"><![CDATA[<p>One of my personal highlights in last week's Research Data Alliance (RDA) <a href="https://rd-alliance.org/plenary-meetings/rda-sixth-plenary-meeting.html">6th Plenary Meeting</a> in Paris was the <a href="https://rd-alliance.org/data-packages-bof-p6-bof-session.html">Data Packages</a> Birds of a Feather (BoF), organized by <a href="http://rufuspollock.org/">Rufus Pollock</a> from the Open Knowledge Foundation (<a href="https://okfn.org/">OKFN</a>). He highlighted the urgent need for packacking data in a standard format to facilitate reuse, and described the extensive work the OKFN has done on <a href="http://data.okfn.org/doc/data-package">data packages</a>. A particular focus is on packacking CSV files, the most widely used format for exchanging data.</p>
<p>I was sold on the importance of CSV and the idea of packacking data in a standard format since attending <a href="http://okfnlabs.org/blog/2014/05/05/csv-conf-2014.html">CSVconf</a> (co-organized by Rufus) in July 2014, and have written about this several times (<a href="http://blog.martinfenner.org/2014/07/18/roads-not-stagecoaches/">Build Roads not Stagecoaches</a>, <a href="https://blog.datacite.org/reference-lists-and-tables-of-content/">Reference Lists and Tables of Content</a>), most recently a few weeks ago (<a href="https://blog.datacite.org/using-yaml-frontmatter-with-csv/">Using YAML frontmatter with CSV</a>). In the RDA session I suggested two important improvements for the OKFN data package format:</p>
<ul>
<li><strong><strong>Single file.</strong></strong> One very important aspect of packaging is to provide everything in a single file, generated by zipping a folder of multiple files. This pattern has become very common and is used for example for electronic books (<code>epub</code>), Microsoft Word documents (<code>docx</code>), or <a href="https://developer.chrome.com/extensions/packaging">Google Chrome extensions</a> (<code>crx</code>).</li>
<li><strong><strong>Identifier</strong></strong>. The OKFN data package spec uses a <code>name</code> attribute to uniquely identify the package. This approach falls short because enforcing the uniqueness of a human readable identifier requires a registry, and this not part of the data package spec. What is needed is a persistent identifier, and DataCite obviously has a lot of experience in this area.</li>
</ul>
<p>Container-based digital infrastructure is a very hot topic thanks to <a href="https://www.docker.com/whatisdocker">Docker</a>, and it has become clear that tools and workflows are at least as important as the spec for the container itself. If we want to move to scholarly infrastructure based on containers, used here interchangeably with packages, we need registries for these containers that not only provide globally unique identifiers, but also a central index for finding these containers - the <a href="https://hub.docker.com/">Docker Hub</a> for scholarly containers. DataCite is providing persistent identifiers and standardized metadata for scholarly content with a focus on research data and is therefore in a perfect position to become such a registry and in the RDA session I therefore said:</p>
<blockquote>
I want DataCite to become a registry for data containers
</blockquote>
<p>While data, in particular in CSV format, are probably the first and most important use case, containers make sense for all scholarly content, and DataCite DOIs are used for text documents, images, software, etc. in addition to datasets. For this reason I prefer the term <strong><strong>scholarly container</strong></strong> over <strong><strong>data container</strong></strong>.</p>
<p>As important as the containers themselves are tools and services that work with them, in particular packing and unpacking. The <code>CSVY</code> format discussed here in a <a href="http://blog.datacite.org/using-yaml-frontmatter-with-csv/">recent blog post</a> could be used by an individual as intermediate step towards a data container. I see tool support as the critical step that decides whether scholarly containers take off as a standard format. Karthik Ram from <a href="https://ropensci.org/">rOpenSci</a> attended the session in Paris (and CSV.conf last year) and expressed great interest in adding support for scholarly containers in their suite of tools.</p>
<h4 id="next-steps">Next Steps</h4>
<ul>
<li>specify the work needed for DataCite to fully support scholarly containers</li>
<li>work with Rufus and OKFN, e.g. on registry support and packaging into a single file</li>
<li>work with the broader community on supporting scholarly containers: data repositories, reference managers, tools to analyze datasets, etc.</li>
<li>propose a pre-conference workshop for the <a href="https://www.force11.org/event/force2016-mark-your-calendars">Force2016 conference</a> in April 2016. This conference started out as Beyond the PDF in 2011, and scholarly containers are a perfect thematic fit.</li>
</ul>
<p><em>This blog post was <a href="https://doi.org/10.5438/d9eq-9dga">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thoughts on the Research Data Alliance 6th Plenary]]></title>
        <id>2tvn67e-r2d92wv-e8x05dj-xbmst</id>
        <link href="https://blog.front-matter.io/mfenner/thoughts-on-the-research-data-alliance-6th-plenary"/>
        <updated>2015-10-01T17:04:00.000Z</updated>
        <summary type="html"><![CDATA[The Research Data Alliance 6th Plenary last week discussed numerous topics very relevant to DataCite. Below is a short subjective list of topics I found interesting. If you attended RDA, feel free to add your thoughts in the comments. And if you didn't attend,...]]></summary>
        <content type="html"><![CDATA[<p>The <a href="https://www.rd-alliance.org/plenaries/rda-sixth-plenary-meeting-paris-france">Research Data Alliance 6th Plenary</a> last week discussed numerous topics very relevant to DataCite. Below is a short subjective list of topics I found interesting. If you attended RDA, feel free to add your thoughts in the comments. And if you didn't attend, you can still provide feedback.</p>
<h3 id="interoperability-between-persistent-identifiers">Interoperability between Persistent Identifiers</h3>
<p>The <a href="https://rd-alliance.org/ig-pid-p6-meeting-session.html">Persistent Identifiers IG</a> discussed the different persistent identifier activities in various RDA working and interest groups. The EC-funded <a href="http://project-thor.eu/">THOR</a> project that started in June was presented by Josh Brown from ORCID and Trisha Cruse from DataCite. I presented the DataCite perspective on persistent identifier interoperability. One important shortcoming of many persistent identifiers including DataCite DOI names is that only one organization – the publisher - can update the metadata. What is also needed is a <strong><strong>claim store</strong></strong> where multiple organizations and not just the publisher can deposit links between different persistent identifiers, e.g. citations, funding information, ORCID identifiers, etc. CrossRef and DataCite are working on such a claim store (more info <a href="https://blog.datacite.org/announcing-data-level-metrics-in-datacite-labs/">here</a> and <a href="https://www.crossref.org/blog/det-poised-for-launch/">here</a>).</p>
<h3 id="data-repository-registries">Data repository registries</h3>
<p>Several sessions discussed repositories that collect information about data repositories. <a href="http://www.re3data.org/">re3data</a> is one of the more important initiatives in this space, and <a href="http://www.re3data.org/2015/05/datacite-to-manage-and-develop-re3data-org/">will become a DataCite service in 2016</a>. One critical step is <a href="http://www.re3data.org/2015/08/introduction-of-the-re3data-org-persistent-identifier/">persistent identifiers for data repositories</a> that are used across data services, including the DataCite Metadata Store.</p>
<h3 id="dynamic-data-citation">Dynamic Data Citation</h3>
<p>Unfortunately I missed the session of the <a href="https://rd-alliance.org/wg-data-citation-p6-meeting-session.html">Data Citation WG</a> - I presented in the parallel <a href="https://rd-alliance.org/wg-rdawds-publishing-data-bibliometrics-p6-meeting-session.html">RDA/WDS Publishing Data Bibliometrics WG</a>. But I had interesting discussions about this topic durig the coffee breaks. I need to better understand the proposed solutions and how they fit into DataCite's existing infrastructure.</p>
<h3 id="data-packages">Data Packages</h3>
<p>The <a href="https://rd-alliance.org/data-packages-bof-p6-bof-session.html">Data packages BoF</a> was organized by Rufus Pollock from the Open Knowledge Foundation (<a href="https://okfn.org/">OKFN</a>), and he kindly invited me to present my perspective. This is an exciting topic and look for more on this topic in a future blog post.</p>
<h3 id="publishing-data-services">Publishing Data Services</h3>
<p>The <a href="https://rd-alliance.org/wg-rdawds-publishing-data-services-p6-meeting-session.html">RDA/WDS Publishing Data Services</a> presented their work on a data - literature <a href="https://dliservice.research-infrastructures.eu/">linking service</a>. DataCite has worked closely with CrossRef on this topic since last year, and it was good to see a lot of overlap in the approaches taken, e.g. using the <code>relationType</code> vocabulary from the DataCite Metadata Schema.</p>
<h3 id="data-citation">Data Citation</h3>
<p>On the last day of RDA I gave a short presentation in the <a href="https://rd-alliance.org/ig-rdawds-publishing-data-p6-joint-session.html">Joint meeting of RDA/WDS Publishing Data WGs and IGs</a>, summarizing some of the challenges implementing data citation:</p>
<p>In the session we had a good discussion on how to move forward with the work started by the working group and its various interest groups.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/7rxd-s8a3">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Persistent Identifiers: Enabling Services for Data Intensive Research]]></title>
        <id>29x4qq5-5as8akb-3x52k3y-khhdy</id>
        <link href="https://blog.front-matter.io/mfenner/persistent-identifiers-enabling-services-for-data-intensive-research"/>
        <updated>2015-09-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Yesterday DataCite and ePIC co-hosted the workshop Persistent Identifiers: Enabling Services for Data Intensive Research. Below is a short summary of the tweets, all using the hashtag #pid_paris.Emerging theme: the number of PID solutions is overwhelming to researchers....]]></summary>
        <content type="html"><![CDATA[<p>Yesterday DataCite and <a href="http://www.pidconsortium.eu/">ePIC</a> co-hosted the workshop <a href="http://www.eventbrite.com/e/persistent-identifiers-enabling-services-for-data-intensive-research-tickets-17500184523">Persistent Identifiers: Enabling Services for Data Intensive Research</a>. Below is a short summary of the tweets, all using the hashtag <a href="https://twitter.com/hashtag/pid_paris?src=hash">#pid_paris</a>.</p>
<div class="iframe">
<div id="app">

</div>
</div>
<div class="iframe">
<div id="app">

</div>
</div>
<div class="iframe">
<div id="app">

</div>
</div>
<div class="iframe">
<div id="app">

</div>
</div>
<div class="iframe">
<div id="app">

</div>
</div>
<div class="iframe">
<div id="app">

</div>
</div>
<div class="iframe">
<div id="app">

</div>
</div>
<div class="iframe">
<div id="app">

</div>
</div>
<div class="iframe">
<div id="app">

</div>
</div>
<div class="iframe">
<div id="app">

</div>
</div>
<blockquote>
Emerging theme: the number of PID solutions is overwhelming to researchers. What we need to provide are reliable suggestions. <a href="https://twitter.com/hashtag/pid_paris?src=hash">#pid_paris</a>(<strong><strong>???</strong></strong>) <a href="https://twitter.com/elisedunham/status/645970130627887105">September 21, 2015</a>
</blockquote>
<div class="iframe">
<div id="app">

</div>
</div>
<div class="iframe">
<div id="app">

</div>
</div>
<div class="iframe">
<div id="app">

</div>
</div>
<div class="iframe">
<div id="app">

</div>
</div>
<div class="iframe">
<div id="app">

</div>
</div>
<div class="iframe">
<div id="app">

</div>
</div>
<p>The last tweet shows the views from the reception. If you have any questions or comments about the event, use the hashtag <a href="https://twitter.com/hashtag/pid_paris?src=hash">#pid_paris</a> on Twitter, or use the comments of this blog.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/jm9f-325f">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differences between ORCID and DataCite Metadata]]></title>
        <id>7k6nf0d-12988ya-r2gjh9y-nq7e6</id>
        <link href="https://blog.front-matter.io/mfenner/differences-between-orcid-and-datacite-metadata"/>
        <updated>2015-09-18T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[One of the first tasks for DataCite in the European Commission-funded THOR project, which started in June, was to contribute to a comparison of the ORCID and DataCite metadata standards. Together with ORCID, CERN, the British Library and Dryad we looked at how contributors,...]]></summary>
        <content type="html"><![CDATA[<p>One of the first tasks for DataCite in the European Commission-funded <a href="http://project-thor.eu/">THOR project</a>, which started in June, was to contribute to a comparison of the ORCID and DataCite metadata standards. Together with ORCID, CERN, the British Library and Dryad we looked at how contributors, organizations and artefacts - and the relations between them - are described in the respective metadata schemata, and how they are implemented in two example data repositories, <a href="http://archaeologydataservice.ac.uk/">Archaeology Data Service</a> and <a href="https://www.datadryad.org/">Dryad Digital Repository</a>.</p>
<p>The focus of our work was on identifying major gaps. Our report was finished and made publicly available last week (Fenner et al., <a href="https://blog.datacite.org/differences-between-orcid-and-datacite-metadata/#ref-https://doi.org/10.5281/ZENODO.30799">2015</a>). The key findings are summarized below:</p>
<ul>
<li>Common Approach to Personal Names</li>
<li>Standardized Contributor Roles</li>
<li>Standardized Relation Types</li>
<li>Metadata for Organisations</li>
<li>Persistent Identifiers for Projects</li>
<li>Harmonization of ORCID and DataCite Metadata</li>
</ul>
<h3 id="common-approach-to-personal-names">Common Approach to Personal Names</h3>
<p>While a single input field for contributor names is common, separate fields for given and family names are required for <a href="http://docs.citationstyles.org/en/stable/specification.html#names">proper formatting of citations</a>. As long as citations to scholarly content rely on properly formatted text rather than persistent identifiers, services holding bibliographic information have to support these separate fields. Further work is needed to help with the transition to separate input fields for given and famliy names, and to handle contributors that are organizations or groups of people.</p>
<h3 id="standardized-contributor-roles">Standardized Contributor Roles</h3>
<p>The currently existing vocabularies for <strong><strong>contributor type</strong></strong> (DataCite) and <strong><strong>contributor role</strong></strong> (ORCID) provide a high-level description, but fall short when trying to describe the author/creator contribution in more detail. <a href="http://docs.casrai.org/CRediT">Project CRediT</a> is a multi-stakeholder initiative that has developed a common vocabulary with 14 different contributor roles, and this vocabulary can be used to provide this detail, e.g. who provided resources such as reagents or samples, who did the statistical analysis, or who contributed to the methodology of a study.</p>
<p>CRediT is complementary to existing contributor role vocabularies such as those by ORCID and DataCite. For contributor roles it is particularly important that the same vocabulary is used across stakeholders, so that the roles described in the data center can be forwarded first to DataCite, then to ORCID, and then also to other places such as institutional repositories.</p>
<h3 id="standardized-relation-types">Standardized Relation Types</h3>
<p>Capturing relations between scholarly works such as datasets in a standardized way is important, as these relations are used for citations and thus the basis for many indicators of scholarly impact. Currently used vocabularies for relation types between scholarly works, e.g. by CrossRef and DataCite, only partly overlap. In addition we see differences in community practices, e.g. some scholars but not others reserve the term citation for links between two scholarly articles. The term data citation is sometimes used for all links from scholarly works to datasets, but other times reserved for formal citations appearing in reference lists.</p>
<h3 id="metadata-for-organisations">Metadata for Organisations</h3>
<p>Both ORCID and DataCite not only provide persistent identifiers for people and data, but they also collect metadata around these persistent identifiers, in particular links to other identifiers. The use of persistent identifiers for organizations lags behind the use of persistent identifiers for research outputs and people, and more work is needed.</p>
<h3 id="persistent-identifiers-for-projects">Persistent Identifiers for Projects</h3>
<p>Research projects are collaborative activities among contributors that may change over time. Projects have a start and end date and are often funded by a grant. The existing persistent identifier (PID) infrastructure does support artefacts, contributors and organisations, but there is no first-class PID support for projects. This creates a major gap that becomes obvious when we try to describe the relationships between funders, contributors and research outputs.</p>
<p>Both the ORCID and DataCite metadata support funding information, but only as direct links to contributors or research outputs, respectively. This not only makes it difficult to exchange funding information between DataCite and ORCID, but also fails to adequately model the sometimes complex relationships, e.g. when multiple funders and grants were involved in supporting a research output. We therefore not only need persistent identifiers for projects, but also infrastructure for collecting and aggregating links to contributors and artefacts.</p>
<h3 id="harmonization-of-orcid-and-datacite-metadata">Harmonization of ORCID and DataCite Metadata</h3>
<p>We identified significant differences between the ORCID and DataCite metadata schema, and these differences hinder the flow of information between the two services. Several different approaches to overcome these differences are conceivable:</p>
<ol>
<li>only use a common subset, relying on linked persistent identifiers to get the full metadata</li>
<li>harmonize the ORCID and DataCite metadata schemata</li>
<li>common API exchange formats for metadata</li>
</ol>
<p>The first approach is the linked open data approach, and was designed specifically for scenarios like this. One limitation is that it requires persistent identifiers for all relevant attributes (e.g. for every creator/contributor in the DataCite metadata). One major objective for THOR is therefore to increase the use of persistent identifiers, both by THOR partners, and by the community at large.</p>
<p>A common metadata schema between ORCID and DataCite is neither feasible nor necessarily needed. In addition, we have to also consider interoperability with other metadata standards (e.g. CASRAI, OpenAIRE, COAR), and with other artifacts, such as those having CrossRef DOIs. What is more realistic is harmonization across a limited set essential metadata.</p>
<p>The third approach to improve interoperability uses a common API format that includes all the metadata that need to be exchanged, but doesn’t require the metadata schema itself to change. This approach was <a href="https://www.crossref.org/blog/crossref-and-datacite-unify-support-for-http-content-negotiation/">taken by DataCite and CrossRef a few years ago</a> to provide metadata for DOIs in a consistent way despite significant differences in the CrossRef and DataCite metadata schema. Using HTTP content negotiation, metadata are provided in a variety of formats.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/bc11-cqw1">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>Fenner, M., Demeranville, T., Kotarski, R., Vision, T., Rueda, L., Dasler, R., … THOR Consortium. (2015). D2.1: Artefact, contributor, and organisation relationship data schema. <em>Zenodo</em>. <a href="https://doi.org/10.5281/ZENODO.30799">https://doi.org/10.5281/ZENODO.30799</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adding References to the DataCite Blog]]></title>
        <id>fscfb1t-dr9q2v8-h63qpxc-m2hy</id>
        <link href="https://blog.front-matter.io/mfenner/adding-references-to-the-datacite-blog"/>
        <updated>2015-09-16T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[We launched this blog six weeks ago on a hosted version of Ghost, the open source blogging platform. Ghost doesn't have all the features of Wordpress or other more mature blogging platforms, but it is a pleasure to use....]]></summary>
        <content type="html"><![CDATA[<p>We launched this blog six weeks ago on a hosted version of <a href="https://ghost.org/">Ghost</a>, the open source blogging platform. Ghost doesn't have all the features of Wordpress or other more mature blogging platforms, but it is a pleasure to use. The other alternative would have been to put the blog up on the Drupal-based main <a href="http://www.datacite.org/">DataCite website</a>, but Drupal is really a content-management system and usually not the best choice for a serious blog.</p>
<p>What all the above systems (Ghost, Wordpress, Drupal) have in common is that they need a web server and database backend. This is fine for the standard blog, but it becomes a problem once you start thinking about customizing your blog. In the case of the DataCite blog I want to be able to provide the following:</p>
<ul>
<li>the addition of proper citations and references</li>
<li>a PDF (and possibly ePub) version for downloading and archiving</li>
<li>a register blog posts with a DOI</li>
</ul>
<p>Since DataCite is in the business of providing DOI names for scholarly content, the above is a pretty obvious wish list. Some people might argue about the content of this blog needing a DOI, but DOIs have been used for similar content for many years, whether it is for frontmatter content (editorials, opinion pieces, etc.) in journals, or in services such as <a href="https://www.nature.com/news">Nature News</a>.</p>
<p>Out of the box the standard blogging platforms mentioned above don't support references or DOI registration, so a bit of extra work is needed. The easiest way to do this is to switch to a simpler blogging platform. Luckily there are a lot of choices among these so-called <strong><strong>static site generators</strong></strong>, which don't need a database and simply generate HTML files. Adding the features from the above wish list then becomes a straightforward process and that is what I have started doing.</p>
<p>As of this week, we support references, as you can see in this blog post(<strong><strong>???</strong></strong>) from two weeks ago.</p>
<figure>
<img src="https://blog.datacite.org/images/2015/09/Bildschirmfoto-2015-09-15-um-20-19-48.png" class="kg-image" alt="References" /><figcaption aria-hidden="true">References</figcaption>
</figure>
<p>The picture is from the PDF version of the post, where the integration with the blog is still ongoing.</p>
<p>The <em>DataCite Labs Blog</em> looks very similar to the <em>DataCite Blog</em>, but under the hood is using the <a href="https://github.com/jekyll/jekyll">Jekyll</a> static site generator, and the <a href="http://pandoc.org/README.html">Pandoc</a> document conversion software. This is a popular combination used by several scholarly bloggers, e.g. <a href="http://www.carlboettiger.info/2015/01/07/automated-knitr-in-jekyll.html">Carl Boettiger</a>. Jekyll is written in Ruby, and there are at least two Ruby gems that allow automatic deposition in Zenodo (<a href="https://github.com/sprotocols/zenodo">zenodo gem</a>), or the DataCite Metadata Store (<a href="https://github.com/datacite/datacite_doi_ify">datacite_doi_ify gem</a>) to automate archiving and DOI registration.</p>
<p>Of course DOI registration doesn't all of the sudden make blog content more ​​<em>scholarly</em>​, but it can make it easier to find. For example, blog posts can be found by searching CrossRef or DataCite's metadata, and links can be discovered between blog posts and scholarly articles (or datasets) by using the DataCite metrics pilot, which we <a href="https://blog.datacite.org/announcing-data-level-metrics-in-datacite-labs">announced last week</a>. To facilitate this we need to deposit the references with the metadata we send to DataCite, e.g. for the three scholarly articles, one software repository, one dataset and one data paper you see in the picture above.</p>
<p>A little more work is needed before the <em>Labs Blog</em> can become the official <em>DataCite Blog</em>, and as always we appreciate your feedback. The blog itself is stored in a <a href="https://github.com/datacite/blog">public Github repository</a>, so feel free to reuse any of the code. We use the <a href="https://travis-ci.org/">Travis</a> continuous integration tool to automatically generate the HTML pages for the blog, and then push the newly generated HTML content to <a href="https://aws.amazon.com/s3/">Amazon S3</a> for hosting.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/2wfx-2hz1">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Announcing Data-Level Metrics in DataCite Labs]]></title>
        <id>7008hzz-ra98e5r-rp6kjcv-5vp29</id>
        <link href="https://blog.front-matter.io/mfenner/announcing-data-level-metrics-in-datacite-labs"/>
        <updated>2015-09-09T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Last week Jennifer Lin shared information on the <strong><strong>Making Data Count</strong></strong> (MDC) project on this blog. MDC is a project funded by the U.S. National Science Foundation (NSF) to design and develop metrics that track and measure...]]></summary>
        <content type="html"><![CDATA[<p>Last week Jennifer Lin shared information on the <strong><strong>Making Data Count</strong></strong> (MDC) project <a href="https://blog.datacite.org/when-counting-is-hard/">on this blog</a>. MDC is a project funded by the U.S. National Science Foundation (NSF) to design and develop metrics that track and measure data use – <strong><strong>data-level metrics</strong></strong> (DLM).</p>
<p>Funding for the 12 month project ends October 1st, with a no-cost extension until March 1st. MDC is a research project and has delivered some interesting questions and important results. One open question is whether and how to turn MDC research into a service that is not limited to the grant-funding period and possibly includes other datasets beyond those from the <a href="https://www.dataone.org/current-member-nodes">DataONE repository network</a>. These important decisions require analysis and feedback from the broader community.</p>
<p>In order to better understand and analyze these questions DataCite has taken over hosting of the DLM service from PLOS and will provide this service until at least March 1st, when the MDC funding formally ends. The DLM service is now hosted by DataCite Labs and can be found at <a href="https://dlm.datacite.org/">https://dlm.datacite.org</a>. The following chart (directly from <a href="https://dlm.datacite.org/sources">https://dlm.datacite.org/sources</a>) gives an overview about the data we have collected so far:</p>
<figure>
<img src="https://blog.datacite.org/images/2015/09/Bildschirmfoto-2015-09-09-um-17-02-56.png" class="kg-image" alt="Works with events" /><figcaption aria-hidden="true">Works with events</figcaption>
</figure>
<p>DLM is primarily an API for metrics, with documentation found at <a href="https://dlm.datacite.org/docs/sources">https://dlm.datacite.org/docs/sources</a>, and live API documentation at <a href="https://dlm.datacite.org/api">https://dlm.datacite.org/api</a>. API clients in <a href="https://github.com/ropensci/alm">R</a>, <a href="https://github.com/lagotto/pyalm">Python</a> and <a href="https://github.com/lagotto/lagotto-rb">Ruby</a> exist, and there is <a href="https://github.com/lagotto/almviz">example code</a> in Javascript. A discussion forum helps with questions regarding the API or clients.</p>
<p>Most users will not be using the API directly, but rather want to see the metrics data displayed together with the datasets they are interested in. About half of the about 140,000 datasets in the DLM service use DataCite DOIs and, as a first step, we have integrated DataCite DOIs into <a href="https://search.test.datacite.org/">Labs Search</a>. If your search results include datasets from one of the DataONE data centers that use DOIs (including Long Term Ecological Research Network, National Center for Ecological Analysis and Synthesis and Dryad Digital Repository) and we have found links for them (e.g. for <a href="https://search.datacite.org/works?query=10.5061/dryad.f1cb2">10.5061/dryad.f1cb2</a>), we will display them:</p>
<figure>
<img src="https://blog.datacite.org/images/2015/09/Bildschirmfoto-2015-09-09-um-17-18-36.png" class="kg-image" />
</figure>
<p>Data from: rise of the machines</p>
<p>Let's dig into the Dryad example a bit more -- every Dryad data package is associated with a journal article (or other textual output) and the metadata deposited in DataCite links to that particular article. Dryad has provided <code>Is referenced by</code> for this relationship:</p>
<figure>
<img src="https://blog.datacite.org/images/2015/09/Bildschirmfoto-2015-09-09-um-17-21-42.png" class="kg-image" />
</figure>
<p>Show "isReferencedBy" relationship</p>
<p>When you click on the <code>Is Cited By</code> link you will find 6 citations, all of which are different from the article in the <code>Is referenced by</code> link.</p>
<p>We included the names of the data sources (e.g. Europe PMC, PLOS, etc.) to distinguish DataCite metadata from external data pulled in from DLM. We are not currently deduplicating links if they are found in different sources - in this case two citations where found both via PLOS fulltext search and via Europe PMC API:</p>
<figure>
<img src="https://blog.datacite.org/images/2015/09/Bildschirmfoto-2015-09-09-um-17-21-21.png" class="kg-image" />
</figure>
<p>PLOS fulltext search and Europe PMC</p>
<p>Although citations in the scholarly literature are the most interesting links the DLM service can discover, DLM also searches other data sources such as the bookmarking service <a href="http://citeulike.org/">CiteULike</a>, the <a href="http://orcid.org/">ORCID</a> registry of personal author identifiers, and Wikipedia (in this case for <a href="http://search.datacite.org/?query=10.5061/DRYAD.868SM%5D">10.5061/DRYAD.868SM</a>:</p>
<figure>
<img src="https://blog.datacite.org/images/2015/09/Bildschirmfoto-2015-09-09-um-17-33-21.png" class="kg-image" />
</figure>
<p>Data from: ontogeny, morphology and taxonomy</p>
<p>We hope you find this information useful when you use <a href="http://search.test.datacite.org/">Labs Search</a>, but there still remains a lot of work to do, including:</p>
<ul>
<li>display numerical data, e.g. download counts or Mendeley readers,</li>
<li>integrate the DLM data into the search index, so that we can use the data for filtering and sorting, e.g. <em>show me all datasets from data center x that have been cited at least 3 times</em>),</li>
<li>harmonize the use of <code>relationType</code>, e.g. be consistent with <code>Is referenced By</code> vs. <code>Is cited By</code>,</li>
<li>display more information rather than only links (DLM stores at least title, authors and publication date for all links),</li>
<li>potentially show citations for the corresponding journal article if the data were published together with the article (as in the Dryad case above) - DLM is collecting this information, and</li>
<li>learn from the community about the utility of a DLM service.</li>
</ul>
<p>As always with DataCite Labs projects, your feedback is greatly appreciated.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/jzg5-vcqv">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using YAML Frontmatter with CSV]]></title>
        <id>a4m5s16-gf9ex9b-d0f5e09-8p2y</id>
        <link href="https://blog.front-matter.io/mfenner/using-yaml-frontmatter-with-csv"/>
        <updated>2015-09-03T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[CSV (comma-separated values) is a popular file format for data. It is popular because it is very simple: CSV is text-based and any application that can open text files can read or write CSV. This makes it a good fit for digital preservation....]]></summary>
        <content type="html"><![CDATA[<p><a href="https://en.wikipedia.org/wiki/Comma-separated_values">CSV</a> (comma-separated values) is a popular file format for data. It is popular because it is very simple: CSV is text-based and any application that can open text files can read or write CSV. This makes it a good fit for <a href="http://www.digitalpreservation.gov/formats/fdd/fdd000323.shtml">digital preservation</a>. We don't know how many of the datasets in DataCite use CSV because the <code>format</code> metadata attribute is not used much (<a href="https://search.datacite.org/works?query=format%3Acsv">this query</a> gives you some examples), but we know that the number is big.</p>
<p>The CSV format has two important shortcomings: a) it is not clearly defined, and b) because of its simplicity it is almost impossible to add metadata describing the data in the CSV file. The closest thing we have to a CSV standard definition is <a href="https://tools.ietf.org/html/rfc4180">RFC 4180</a>, but that RFC clearly states that <em>It does not specify an Internet standard of any kind</em>.</p>
<p>If for some reason you think that the above sounds very similar to the situation with <em>markdown</em>, a simple format for text documents that <a href="http://spec.commonmark.org/0.22/">until recently</a> was not clearly defined and that provides no easy way to add metadata (such as author, title or date), then you are smarter than me, since I didn't see the connection until <a href="https://github.com/jrovegno">Javier Rovegno</a> <a href="http://blog.martinfenner.org/2013/06/29/metadata-in-scholarly-markdown/#comment-2220461075">commented on my personal blog last week</a>. He <a href="http://jrovegno.github.io/csvy/">proposes</a> to use the <em>YAML frontmatter</em> spec for CSV files, and I think it is a brilliant idea.</p>
<p><a href="http://jekyllrb.com/docs/frontmatter/">YAML frontmatter</a> is a popular way to add metadata to markdown files. <a href="http://yaml.org/">YAML</a> is a data serialization format that is very human-readable (in contrast to XML and to a lesser degree JSON). Frontmatter simply means to have the YAML section at the beginning of the document, e.g. the following, taken from the <a href="http://pandoc.org/README.html">Pandoc documentation</a>:</p>
<pre><code>---
title:  &#39;This is the title: it contains a colon&#39;
author:
- name: Author One
  affiliation: University of Somewhere
- name: Author Two
  affiliation: University of Nowhere
tags: [nothing, nothingness]
abstract: |
  This is the abstract.

  It consists of two paragraphs.
---</code></pre>
<p>By adding a similar section to CSV files we can add arbitrary metadata, including longer text such as a file description. In other words, all the metadata required to submit a CSV to a data repository and obtain a DOI. This makes data submission even simpler than using a zip bundle <a href="https://blog.datacite.org/reference-lists-and-tables-of-content/">as discussed in an earlier post</a>, or using the <a href="https://doi.org/10.12688/f1000research.3-6.v2">DataUp tool</a>, a Microsoft Excel add-in that is unfortunately no longer available.</p>
<p>Javier has picked <code>.csvy</code> as file extension for this modified file format (I like <code>.ycsv</code> a little bit better). I don't think we need to define what metadata can go into the YAML frontmatter, because there a number of different use cases. The only exception would be a standardized way to describe the columns in the CSV file, e.g.:</p>
<pre><code>---
columns:
  - title: Purchase Date
    type: date
  - title: Item
    type: string
  - title: Amount (€)
    type: float
---</code></pre>
<p>The best alternative to using CSV in combination with YAML is JSON, but that format is probably less popular than CSV for the typical data scientist (I like JSON for nested data, which would be very painful in CSV). Most people generate a separate file for CSV metadata now, risking that the data and metadata are separated.</p>
<p>Many tools for reading CSV files - including Microsoft Excel and the <code>read.csv</code> function in R - can ignore an arbitrary number of lines at the beginning of a CSV file, making the proposed format at least to some extend backwards-compatible. But I hope to soon see tools reading and writing YAML frontmatter in CSV files, taking full advantage of the format. In my own work I produce a lot of CSV files using R, and adding metadata to them will greatly enhance their usability. Even better if we start to see YAML frontmatter support for CSV in multiple languages, including Python, Javascript, Ruby and Julia.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/5hzj-5kds">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Digging into Metadata using R]]></title>
        <id>1eb9e8p-r93977t-pg54nvr-e163d</id>
        <link href="https://blog.front-matter.io/mfenner/digging-into-metadata-using-r"/>
        <updated>2015-08-20T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[In the first post of this new blog a few weeks ago I talked about Data-Driven Development, and that service monitoring is an important aspect of this. The main service DataCite is providing is registration of digital object identifiers (DOIs)...]]></summary>
        <content type="html"><![CDATA[<p>In the first post of this new blog a few weeks ago I talked about <a href="https://blog.datacite.org/data-driven-development/">Data-Driven Development</a>, and that service monitoring is an important aspect of this. The main service DataCite is providing is registration of digital object identifiers (DOIs) for scholarly content, in particular research data.</p>
<p>Monitoring this service should include the following:</p>
<ol>
<li>number of DOIs registered</li>
<li>metadata associated with these DOIs</li>
<li>are the DOIs working as expected, e.g. are they resolving to the appropriate landing page</li>
<li>are these DOIs actually used, based on number of downloads, citations, etc. of the resources they are describing</li>
</ol>
<p>We can use the DataCite Search API to address #1 and #2. The <a href="http://stats.datacite.org/">DataCite Statistics Portal</a> uses the API and is an excellent starting point for #1, showing the number of DOIs registered broken down by allocator and data center.</p>
<p>To get more detailed information about #1, and to look into #2, we can use the statistical programming language <a href="https://www.r-project.org/">R</a> and the <a href="https://github.com/ropensci/rdatacite">rdatacite</a> package by Scott Chamberlain from the <a href="https://ropensci.org/">rOpenSci</a> to talk to the DataCite API. I have started to work on this and have created the public repository <a href="https://github.com/datacite/metadata-reports">metadata-reports</a> on Github for this purpose. The first two reports are</p>
<ul>
<li><a href="https://github.com/datacite/metadata-reports/blob/master/overview/index.md">overview</a>: number of registered DOI names</li>
<li><a href="https://github.com/datacite/metadata-reports/blob/master/orcid/index.md">orcid</a>: DOI names with ORCIDs in the metadata</li>
</ul>
<p>In the overview report I look at the number of DOI names registered over time, with some examples where these numbers are broken down by data center and resource type. Below are two examples for data packages from <strong><strong>Dryad</strong></strong> and software from <strong><strong>Zenodo</strong></strong>:</p>
<figure>
<img src="https://blog.datacite.org/images/2015/08/unnamed-chunk-10-1.png" class="kg-image" alt="Dryad data packages by month" /><figcaption aria-hidden="true">Dryad data packages by month</figcaption>
</figure>
<figure>
<img src="https://blog.datacite.org/images/2015/08/unnamed-chunk-12-1.png" class="kg-image" alt="Zenodo resources of type software created by month" /><figcaption aria-hidden="true">Zenodo resources of type software created by month</figcaption>
</figure>
<p>In the orcid report I look at the number of DOI names that have at least one Open Researcher and Contributor ID (ORCID) in the metadata.</p>
<figure>
<img src="https://blog.datacite.org/images/2015/08/unnamed-chunk-3-1.png" class="kg-image" alt="DataCite DOI names with ORCID IDs by month" /><figcaption aria-hidden="true">DataCite DOI names with ORCID IDs by month</figcaption>
</figure>
<p>The report goes in more detail explaining the two peaks, basically two small group of researchers producing a large number of data sets (at Pangaea and Imperial College, respectively), and including their ORCID identifiers will all of them.</p>
<p>Removing these two groups of researchers shows a more organic pattern, with about 500 DOIs with associated ORCIDs created every month.</p>
<figure>
<img src="https://blog.datacite.org/images/2015/08/unnamed-chunk-11-1.png" class="kg-image" alt="DOI names with ORCID IDs filtered" /><figcaption aria-hidden="true">DOI names with ORCID IDs filtered</figcaption>
</figure>
<p>R is a nice reporting tool for these kinds of data, and the <a href="https://github.com/ropensci/rdatacite">rdatacite</a>, <a href="http://rmarkdown.rstudio.com/">rmarkdown</a> and <a href="http://yihui.name/knitr/">knitr</a> packages make the analysis and visualization a straightforward process. Feel free to adapt the code in the Github repository to your specific questions, or let me know what other reports you would like to see.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/1hv8-2gc2">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Pilot to Service]]></title>
        <id>1ht5dmd-mrg848r-e0ka5ef-5hxf8</id>
        <link href="https://blog.front-matter.io/mfenner/from-pilot-to-service"/>
        <updated>2015-08-17T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Today I am pleased to announce the launch of a new service, DataCite Labs Search – the service is available immediately at https://search.datacite.org/. This is one of THOR’s first services and is based on work in the earlier EC-funded ODIN Project.The...]]></summary>
        <content type="html"><![CDATA[<p>Today I am pleased to announce the launch of a new service, DataCite Labs Search – the service is available immediately at <a href="https://search.datacite.org/">https://search.datacite.org/</a>. This is one of <a href="http://project-thor.eu/">THOR</a>’s first services and is based on work in the earlier EC-funded <a href="http://odin-project.eu/">ODIN Project</a>.</p>
<p>The ODIN project <a href="http://odin-project.eu/2013/05/13/new-orcid-integrated-data-citation-tool/">launched the DataCite/ORCID claiming tool</a> in June 2013. The DataCite/ORCID claiming tool allows users to add works from the DataCite Metadata Store (MDS) to their ORCID profile. This was a successful pilot, enabling researchers to add their datasets to the ORCID service infrastructure.</p>
<p>THOR, the follow-up project to ODIN, started in June 2015. One of the goals of THOR is to build sustainable persistent identifier services based upon the piloting work done in ODIN.</p>
<p>The new DataCite Labs Search includes all functionality of the DataCite/ORCID claiming tool, but we have made some additional changes:</p>
<ul>
<li>re-deployed the service in the DataCite data center,</li>
<li>merged the code repository back with the original CrossRef repo, and</li>
<li>started work on making the service the default DataCite search.</li>
</ul>
<p><a href="http://blog.datacite.org/overcoming-development-pain/">Last week</a> I spoke a bit about the differences between how to deploy a pilot project and a production service.</p>
<h3 id="crosscite">Crosscite</h3>
<p>The DataCite/ORCID claim tool was started in 2013 as a fork of the <a href="https://github.com/CrossRef/cr-search">open source code</a> for the <a href="http://search.crossref.org/">CrossRef Metadata Search</a>. There are some subtle, but important differences between the CrossRef and DataCite Search API, and some other changes were introduced as well. Going forward we wanted to bring the two code bases together again, both for faster development and to make it easier for users: there shouldn't for example be much of a difference for a user between claiming a work to your ORCID profile from the CrossRef Metadata Search and the DataCite Labs Search. Since this month development happens in a common code repository in the <strong><strong>Crosscite</strong></strong> Github organization, and the project has been renamed to <a href="https://github.com/crosscite/doi-metadata-search">doi-metadata-search</a>. You can follow along development via Github Issues or <a href="https://waffle.io/crosscite/doi-metadata-search">this waffle.io board</a>:</p>
<figure>
<img src="https://blog.datacite.org/images/2015/08/Bildschirmfoto-2015-08-17-um-15-27-14.png" class="kg-image" alt="Waffle.io" /><figcaption aria-hidden="true">Waffle.io</figcaption>
</figure>
<h3 id="search">Search</h3>
<p>Allowing users to add works with DataCite DOIs to their ORCID profile should not be done in a separate service, but is ideally part of the standard search interface to DataCite that users use anyway. We therefore have to make <a href="https://search.datacite.org/">DataCite Labs Search</a> at least as good as the current DataCite Search. Both services use the DataCite Search API as their backend, but the user interface they provide is different. Some of the differences in Labs Search include:</p>
<ul>
<li>a single search box</li>
<li>all information is shown in a list view, no linking to pages on data.datacite.org</li>
<li>citation formatter</li>
<li>filter results by Creative Commons license</li>
<li>an updated user interface</li>
</ul>
<figure>
<img src="https://blog.datacite.org/images/2015/08/Bildschirmfoto-2015-08-17-um-14-05-16.png" class="kg-image" alt="DataCite Search" /><figcaption aria-hidden="true">DataCite Search</figcaption>
</figure>
<p>The single input field for searching should be appropriate for most queries, but Labs Search also supports <a href="https://cwiki.apache.org/confluence/display/solr/The+DisMax+Query+Parser">DisMax</a> query syntax for more complex searches, and automatically detects DOI and ORCID names. Try some more complex queries, e.g. all presentations by a particular person.</p>
<figure>
<img src="https://blog.datacite.org/images/2015/08/Bildschirmfoto-2015-08-17-um-14-13-09.png" class="kg-image" alt="Citation" /><figcaption aria-hidden="true">Citation</figcaption>
</figure>
<p>Providing the metadata as formatted citation is an important feature that facilitates data citation. With BibTeX and RIS two standard import formats for reference managers are also supported. This feature is basically unchanged since 2013, and relies on the <a href="http://crosscite.org/">DOI content negotiation</a> API provided by several DOI registration agencies. In addition, COinS is supported, allowing the import of multiple items at once, e.g. into a reference manager.</p>
<figure>
<img src="https://blog.datacite.org/images/2015/08/Bildschirmfoto-2015-08-17-um-14-22-48.png" class="kg-image" alt="COinS" /><figcaption aria-hidden="true">COinS</figcaption>
</figure>
<p>Allowing users to search for content based on what they are allowed to do with it is another important feature of Labs Search. We are using the <strong><strong>rightsURI</strong></strong> metadata field and the standard Creative Commons licenses. You can for example limit your search to content that uses the standard Wikipedia license <a href="https://en.wikipedia.org/wiki/Wikipedia:CC_BY-SA_Compliance">CC-BY-SA</a> (see above), or you might exclude content that doesn't allow commercial reuse and/or derivative works. The <strong><strong>rightsURI</strong></strong> field allows for multiple licenses, but unfortunately less than 25% of DataCite DOI metadata include a Creative Commons license.</p>
<h3 id="labs">Labs</h3>
<p>We are launching this updated tool as a DataCite Labs service instead of replacing the production service at search.datacite.org. There are two reasons: a) there are still some rough edges and possibly bugs, and b) we need to collect broader feedback from users before this can replace the current DataCite search. We plan to do this switch in the next 1-3 months, depending on user feedback. Please comment on this blog post, open a <a href="https://github.com/crosscite/doi-metadata-search/issues">Github issue</a>, tweet about it mentioning <strong><strong>(<strong><strong>???</strong></strong>)</strong></strong>, or send an email to <strong><strong>support@datacite.org</strong></strong>.</p>
<p>Expect to see more new/updated DataCite services first appearing as DataCite Labs in the future.</p>
<h3 id="thanks">Thanks</h3>
<p>This is a collaborative effort and I want to thank the DataCite and THOR teams, and in particular Karl Ward, Gudmundur Thorisson, Sebastian Peters, Rob Peters, Laura Rueda, Tom Demeranville, Geoff Bilder, Laure Haak and Laura Paglione.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/s8gf-0ck9">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overcoming Development Pain]]></title>
        <id>4csdknf-1wh8d6v-t0w03tb-mkbnj</id>
        <link href="https://blog.front-matter.io/mfenner/overcoming-development-pain"/>
        <updated>2015-08-15T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Today DataCite received an email from a user alerting us that there are some small inconsistencies with our recommended data citation format:Creator (PublicationYear): Title. Publisher. Identifierat https://www.datacite.org/services/cite-your-data.htmlCreator; (PublicationYear): Title; Publisher....]]></summary>
        <content type="html"><![CDATA[<p>Today DataCite received an email from a user alerting us that there are some small inconsistencies with our recommended data citation format:</p>
<pre><code>Creator (PublicationYear): Title. Publisher. Identifier</code></pre>
<p>at <a href="https://www.datacite.org/services/cite-your-data.html">https://www.datacite.org/services/cite-your-data.html</a></p>
<pre><code>Creator; (PublicationYear): Title; Publisher. Identifier</code></pre>
<p>at <a href="https://search.datacite.org/works/10.5061/DRYAD.8C1P6">search.datacite.org</a></p>
<p>Removing two semicolons at <strong><strong>data.datacite.org</strong></strong> looks like an easy fix, but this is a bit more complicated since the data citation at <strong><strong>data.datacite.org</strong></strong> is automatically generated using an existing citation style, which looks just slightly different (<a href="http://citationstyles.org/">http://citationstyles.org/</a> doesn't have a <strong><strong>DataCite</strong></strong> style). Support for data citation style is a topic for another blog post, but here I want to talk about what it takes to make changes to DataCite's website or services to fix a bug or add a feature.</p>
<p>In most cases even small changes like this one require us to deploy new code. Fixing the code in place is usually a bad idea because it makes it hard to track changes over time. When we deploy new code we have to make sure that we are really only making that small change and not changing something else in the process, e.g. because a support library is automatically updated to a newer version. So we should run tests to make sure everything is ok and we have to deploy the new code to our test system first. Before we know it this seemingly small change becomes a bigger undertaking.</p>
<p>The end result is of course - and this is not something limited to DataCite - that we deploy code that fixes bugs, adds new features or applies security updates far less frequently than we would like.</p>
<p>This is frustrating to everyone involved, and not surprisingly, many people have tried to speed up the software deployment process with concepts such as <a href="http://12factor.net/dev-prod-parity">dev-prod-parity</a> (keep development, staging, and production as similar as possible) and <a href="http://www.thoughtworks.com/continuous-integration">continuous integration</a>, and tools such as <a href="https://www.vagrantup.com/">Vagrant</a> and <a href="https://www.docker.com/">Docker</a>.</p>
<p>For the project I am <a href="https://github.com/crosscite/doi-metadata-search">currently working on</a> I have started to add better test coverage and the continuous integration build is currently passing:</p>
<figure>
<img src="https://travis-ci.org/crosscite/doi-metadata-search.svg?branch=datacite" class="kg-image" />
</figure>
<p>It is a Ruby project and I can use <a href="http://capistranorb.com/">capistrano</a> to deploy a new version to the production server in about a minute.</p>
<h3 id="the-data-center">The data center</h3>
<p>The above is unfortunately a very developer-centric view. Deployment is more than pushing updated code to a server. We also need to worry about installing/updating all required software on the server and making configuration changes where needed. And we need to worry about how the server is configured in our data center in terms of access to the internet, security settings, etc. Although there is often an overlap in the tools we can use, deployment really has three aspects:</p>
<ol>
<li>code deployment</li>
<li>server configuration/bootstrapping</li>
<li>infrastructure configuration</li>
</ol>
<p>Since starting at DataCite last week, I have spent a good amount of my time working on #2 and #3. The basic assumption is that <a href="http://www.thoughtworks.com/insights/blog/infrastructure-code-reason-smile">infrastructure is code</a>.</p>
<p>Server configurations don't change that often, so it is more important to make sure the server is configured exactly as expected rather than saving a few minutes of time. A good approach is therefore to automate the building of a virtual machine or container of the server. DataCite is using Amazon AWS for hosting, and I am using <a href="https://www.packer.io/">Packer</a> and <a href="https://www.chef.io/">Chef</a> to automatically build an Amazon Machine Image (AMI) for the server I am working on currently. If you want to follow along (and have an AWS account and the open source Packer and Chef installed), <a href="https://github.com/crosscite/doi-metadata-search">git clone the repo</a> and issue this command:</p>
<pre><code>packer build template.json</code></pre>
<p>To add the AMI we just build to the data center; I use <a href="https://www.terraform.io/">terraform</a>. The beginning of the server configuration (in a private repo because security groups, etc. are also included) is:</p>
<pre><code>provider &quot;aws&quot; {
    access_key = &quot;${var.access_key}&quot;
    secret_key = &quot;${var.secret_key}&quot;
    region = &quot;${var.region}&quot;
}

resource &quot;atlas_artifact&quot; &quot;labs-search&quot; {
    name = &quot;datacite/doi-metadata-search&quot;
    type = &quot;amazon.ami&quot;
    build = &quot;latest&quot;
}</code></pre>
<p>A <code>terraform apply</code> will build the infrastructure described by terraform. As the last step we need to run <strong><strong>capistrano</strong></strong> to deploy the latest code (not included in the AMI because the app is currently under heavy development).</p>
<p>To link the above workflows together we use <a href="https://atlas.hashicorp.com/">Atlas</a>, a commercial tool, but free for the number of servers that we need to manage at DataCite. One of the nice features of Atlas is that we can trigger terraform runs by changing the terraform configuration files stored in a Github repository, so really infrastructure as code:</p>
<figure>
<img src="https://blog.datacite.org/images/2015/08/Bildschirmfoto-2015-08-11-um-20-14-30.png" class="kg-image" alt="Infrastructure as code" /><figcaption aria-hidden="true">Infrastructure as code</figcaption>
</figure>
<p>Finally, we want to notify the team when applying these changes to the DataCite infrastructure, and we use email and Slack for this:</p>
<figure>
<img src="https://blog.datacite.org/images/2015/08/Bildschirmfoto-2015-08-11-um-20-20-48.png" class="kg-image" alt="Email and Slack" /><figcaption aria-hidden="true">Email and Slack</figcaption>
</figure>
<p>I am working on integrating <strong><strong>Docker</strong></strong> into this workflow, as Docker containers are much more flexible than Amazon Machine Images.</p>
<p>This post is a pretty long-winded way of saying we need to fix a typo at <strong><strong>data.datacite.org</strong></strong>, but this is of course only one of several issues on our list of <a href="https://github.com/datacite/content-resolver/issues">bugs to fix</a>.</p>
<p><em>This blog post was <a href="https://doi.org/10.5438/xc6c-ef54">originally published</a> on the DataCite Blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-Driven Development]]></title>
        <id>498e6sf-ya189n9-xg7a8s7-pzzm0</id>
        <link href="https://blog.front-matter.io/mfenner/data-driven-development"/>
        <updated>2015-08-03T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[This week I start as the new DataCite Technical Director. While I get up to speed with existing DataCite services and infrastructure, and we start to launch new services (e.g. this blog), this is also a good time to communicate the overall approach I am taking....]]></summary>
        <content type="html"><![CDATA[<p>This week I start as the new DataCite Technical Director. While I get up to speed with existing DataCite services and infrastructure, and we start to launch new services (e.g. this blog), this is also a good time to communicate the overall approach I am taking. I like to call it <strong><strong>Data-Driven Development</strong></strong>, or <strong><strong>DDD</strong></strong> as we all love acronyms.</p>
<figure>
<img src="https://blog.datacite.org/images/2015/08/Fel_048248-RE-2.jpg" class="kg-image" alt="Blick auf Hannover (Unknown, 1931)" /><figcaption aria-hidden="true">Blick auf Hannover (Unknown, <a href="https://blog.datacite.org/data-driven-development/#ref-https://doi.org/10.3932/ETHZ-A-000159123">1931</a>)</figcaption>
</figure>
<h2 id="definition">Definition</h2>
<p>Data-Driven Development and related terms are in use in several contexts, in particular <a href="http://reports.weforum.org/data-driven-development/">economics</a>, and <a href="https://en.wikipedia.org/wiki/Data-driven_programming">programming</a>. The term sounds similar to <a href="https://en.wikipedia.org/wiki/Test-driven_development">test-driven development</a> and <a href="https://en.wikipedia.org/wiki/Behavior-driven_development">behavior-driven development</a>, two related software development processes. <a href="https://en.wikipedia.org/wiki/Business_intelligence">Business intelligence</a> and <a href="https://en.wikipedia.org/wiki/Data_science">data science</a> are of course closely related. My definition is as follows:</p>
<blockquote>
We develop and maintain our services based on data.
</blockquote>
<p>This shouldn't come as a surprise as DataCite's mission is <strong><strong>Helping you to find, access and reuse data</strong></strong>. And my last job at the Open Access publisher <a href="http://plos.org/">PLOS</a> was all about <a href="https://doi.org/10.1371/journal.pbio.1001687">collecting and presenting data</a> about the reuse of scholarly articles (citations, downloads, social media mentions, etc.). But here I mean <strong><strong>data</strong></strong> in a much broader sense.</p>
<h2 id="product-development">Product Development</h2>
<p>While the overall strategic direction is determined by the Board together with the DataCite working groups and members, we can collect data that help with decisions in product development, for example</p>
<ul>
<li>service monitoring (see below): how are our services used over time, are there any components that are particularly popular, etc.</li>
<li><a href="https://www.uservoice.com/">user feedback</a>: ideas, feedback, <a href="https://www.optimizely.com/ab-testing/">A/B testing</a></li>
<li><a href="https://github.com/blog/1866-the-new-github-issues">bug reports</a></li>
<li><a href="http://www.discourse.org/">discussion boards</a> and <a href="https://slack.com/is">direct group messages</a>: related to the last two points, but more allowing a more open discussion</li>
<li>community events</li>
</ul>
<p>Compared with the next two sections, tools for data-driven product development are less commonplace (unless I missed them, in which case please provide feedback).</p>
<h2 id="software-development">Software Development</h2>
<p>The data generated during software development are increasingly made available through automated tools. We can</p>
<ul>
<li>get detailed information out of the <a href="https://github.com/datacite">version control system</a></li>
<li>check for passing and failing tests in <a href="https://travis-ci.org/">continuous integration servers</a></li>
<li>check <a href="https://codeclimate.com/">test coverage and overall code quality</a></li>
<li><a href="https://houndci.com/">check for consistent coding style</a></li>
</ul>
<h2 id="service-monitoring">Service Monitoring</h2>
<p>Any web-based service can and should be monitored for</p>
<ul>
<li><a href="https://bugsnag.com/">crashes and other serious errors</a></li>
<li><a href="http://newrelic.com/">server load</a>, <a href="https://www.pingdom.com/">server outages</a> and <a href="https://www.nagios.org/">internal server problems</a></li>
<li><a href="http://www.google.com/analytics/">server traffic</a>, including traffic to particular pages, percentage of mobile and non-English users, etc.</li>
<li>specific monitoring for the services you are offering, e.g. in the case of DataCite <a href="http://stats.datacite.org/">number of DOIs registered</a> (broken down by data center), number of DOIs with specific metadata (e.g. ORCID identifiers for creators and funding information), and number of DOI resolutions (tricky because there is no easy way to filter out bots)</li>
<li>user-generated feedback (see section product development)</li>
</ul>
<h2 id="communication">Communication</h2>
<p>We don't want to stop at collecting all these data, we also need a strategy for providing them to the DataCite Board, DataCite working groups, DataCite members and data centers, DataCite staff, and everyone else who cares about these data. The default should be open, exceptions are mostly data that would raise privacy or security concerns, e.g. IP addresses in usage stats. Most of the services mentioned in this post are open for everyone to look at.</p>
<h2 id="synthesis">Synthesis</h2>
<p>Good data-driven development should not only collect lots of data and make them available, but we also need to aggregate the information in meaningful ways. Service monitoring is a good example where staff needs to understand exactly what is going on, but the typical DataCite user only cares about whether all services are running as expected. A <a href="https://status.github.com/">status dashboard</a> would be a good solution here.</p>
<p>The data we are generating also need to be put into the broader context. We need</p>
<ul>
<li>the DataCite Board to use them for strategic planning</li>
<li>to provide these data to the DataCite working groups to feed into their work (e.g. stats on what metadata are submitted by data centers for the <a href="https://www.datacite.org/tags/metadata-working-group">Metadata Working Group</a></li>
<li>the DataCite staff to integrate them in their work (e.g. the Communications Director utilizing the website usage stats)</li>
<li>these data to adapt the software development roadmap and service infrastructure</li>
</ul>
<h2 id="implementation">Implementation</h2>
<p>Of course I am aware that this is an ambitious agenda, in particular since DataCite is a small non-profit that has limited staff and financial resources. But I don't think that data-drive development should be left to for-profit organizations and/or to organizations of a certain size. There are several things DataCite can do:</p>
<ul>
<li>implement DDD practices over time, starting with one service and one aspect</li>
<li>use service providers wherever it makes sense (<a href="http://thenewstack.io/new-stack-mitchell-hashimoto-containers-no-containers-one-question-2015/">there is a future where you yourself are running less servers</a>). This means anything that is not core to the DataCite mission and where the service provider is better and/or cheaper than what you could do internally. This evaluation can of course change over time</li>
<li>collaborate with other scholarly non-profits on infrastructure, including DataCite members and data centers, and other persistent identifier providers such as CrossRef and ORCID</li>
</ul>
<p><em>This blog post was <a href="https://doi.org/10.5438/dhsm-8219">originally published</a> on the DataCite Blog.</em></p>
<h2 id="references">References</h2>
<p>Unknown. (1931). Hannover, blick auf hannover. ETH-Bibliothek Zürich, Bildarchiv. <a href="https://doi.org/10.3932/ETHZ-A-000159123">https://doi.org/10.3932/ETHZ-A-000159123</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thank you PLOS]]></title>
        <id>5ee13mc-vp69p8b-29prg83-06v9f</id>
        <link href="https://blog.front-matter.io/mfenner/thank-you-plos"/>
        <updated>2015-07-29T10:52:00.000Z</updated>
        <summary type="html"><![CDATA[Starting next week I will work as the DataCite Technical Director, and I am excited about this new opportunity. But this is material for another post, here I want to reflect on the last three years working as Technical Lead for the PLOS Article-Level...]]></summary>
        <content type="html"><![CDATA[<p><a href="https://www.datacite.org/news/martin-fenner-and-laura-rueda-join-datacite-team.html">Starting next week</a> I will work as the DataCite Technical Director, and I am excited about this new opportunity. But this is material for another post, here I want to reflect on the last three years working as Technical Lead for the <a href="http://lagotto.io/plos/">PLOS Article-Level Metrics</a> project.</p>
<p>It feels much longer than three years, but until May 2012 I worked as medical oncologist at Hannover Medical School, treating patient with cancer, attending interdisciplinary tumor boards and helping with clinical trials. It was a very brave move by PLOS to hire me at this point, especially since I <a href="https://sensiblescience.io/mfenner/why-should-we-work-where-we-live">worked remotely</a> from Germany rather than in the San Francisco office. I will be forever thankful to PLOS for giving me this opportunity.</p>
<p>Two factors probably played a role in this decision: I have been blogging about how the internet is changing scholarly communication since 2007, and since September 2010 I had <a href="http://blogs.plos.org">my blog on the PLOS Blogs Network</a>. I had also visited the PLOS offices in San Francisco, and had met several PLOS people at conferences, including Pete Binfield, Rich Cave, Mark Patterson, Brian Mossop, Jennifer Lin and Liz Allen. I had <a href="https://sensiblescience.io/mfenner/plos-one-interview-with-peter-binfield">interviewed</a> Pete Binfield about PLOS ONE and the PLOS Article-Level Metrics project in August 2009, shortly after the project was launched.</p>
<p>The other factor was the hackathon at the 2011 Science Online London conference. We were a really small group of people (I remember Jason Hoyt, Victor Henning, Kristi Holmes and Cameron Neylon, Mendeley was hosting the event), but I had the idea to hack the open source PLOS Article-Level Metrics application. This hack turned into <a href="https://sensiblescience.io/mfenner/announcing-sciencecard/">ScienceCard</a>, a version of the PLOS Article-Level Metrics application focussing on people rather than articles, and the application was a finalist for the <a href="http://blog.mendeley.com/highlighting-research/the-top-101-apps-in-the-mendeley-plos-binary-battle/">Mendeley/PLOS API Binary Battle</a>. ScienceCard doesn’t exist anymore, but the concept of organizing metrics around a person lives on in ImpactStory, facilitated by the launch of ORCID in October 2012. More importantly - without me knowing it - ScienceCard demonstrated that I could work with and extend the PLOS Article-Level Metrics code, and I think I was the first person outside of PLOS doing this. Which must have helped when PLOS was looking for a technical lead for the project a few months later.</p>
<p>In other words, blogging and hacking code can lead to great job opportunities.</p>
<p>While at PLOS I not only learned a ton of things about article-level metrics and all its challenges and opportunities, but also many basic skills needed in software development. Which is important, as my formal training is in clinical medicine and molecular biology, and doing software development in your free time (which I had done since the 1990s) only gets you so far. Some of the unexpected things I learned:</p>
<ul>
<li><strong><strong>Visualizations</strong></strong>: while it was clear that I was expected to generate visualizations for the PLOS Article-Level Metrics data, I didn’t expect this to go so deep, first with R and later with <a href="http://d3js.org/">d3.js</a>. Najko Jahn introduced me to using R to analyze the PLOS data, and I later worked closely with Scott Chamberlain from the <a href="https://ropensci.org/">rOpenSci</a> project to help improve their <a href="https://ropensci.org/tutorials/alm_tutorial.html">alm package</a>. The Javascript work with d3.js started with AlmViz at the 2012 ALM hackathon and later was done in close collaboration with Juan Alperin from the <a href="https://pkp.sfu.ca/">Public Knowledge Project</a>.</li>
<li><strong><strong>DevOps</strong></strong>: the intersection of software development and system administration. I became a big fan and have spent endless hours learning how to automate the configuration and deployment of servers and other infrastructure.</li>
<li><strong>O<strong>pen source community building</strong></strong>: again something I was expected to do around the PLOS article-level metrics open source application, but I never expected this to be so challenging and time-consuming, but also rewarding.</li>
</ul>
<p>I thank everyone at PLOS who I had the pleasure to work with over the years, in particular Kristen Ratan, Cameron Neylon, Donna Okubo, Mei Yan Leung, Liz Allen, Catriona MacCallum, Matt Hodgkinson, Theo Bloom, Damian Pattinson, Ginny Barbour, Emma Ganley, Roli Roberts, Eric Martens, Susan Au, Matt Willman, Edgar Munoz, Rachel Drysdale, CJ Rayhill, Lisa Siegel, Jennifer Song, Polina Grinbaum, John Bertrand, Mike Baehr, Clark Hartsock, Adam Hyde, and Holly Allen. A very special thanks goes to Jennifer Lin, Rich Cave and John Chodacki who worked with me on a daily basis.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Component DOIs Revisited]]></title>
        <id>9gdggps-6z9qzsd-q2xwgy5-d8hq</id>
        <link href="https://blog.front-matter.io/mfenner/component-dois-revisited"/>
        <updated>2015-07-09T10:19:00.000Z</updated>
        <summary type="html"><![CDATA[Four years ago I wrote a blog post about component DOIs. It is time to revisit the topic, in particular since our approach to citing data associated with a publication has changed since 2011.Component DOIs are explained in the CrossRef Help System:DOIs...]]></summary>
        <content type="html"><![CDATA[<p>Four years ago I wrote a <a href="https://front-matter.io/mfenner/direct-links-to-figures-and-tables-using-component-dois/">blog post</a> about component DOIs. It is time to revisit the topic, in particular since our approach to citing data associated with a publication has changed since 2011.</p>
<p>Component DOIs are explained in the <a href="http://help.crossref.org/components">CrossRef Help System</a>:</p>
<blockquote>
DOIs may be assigned to items that are part of a journal article, book chapter, or any other content item. A component would typically be a figure, table, or image which is part of or referred to by the parent item. Assigning a DOI to a component allows direct linking to the component item.
</blockquote>
<p>Component DOIs are DOIs, i.e. persistent identifiers that link directly to the resource in question, e.g. a figure in a publication. The component DOI for a figure in a PLOS paper used in the 2011 post still <a href="https://doi.org/10.1371/journal.pone.0006022.g002">works as expected</a>, despite changes to the URL of the journal landing page.</p>
<p>The problem with component DOIs is the problem with DOIs in general: there is basic functionality common to all DOIs, and there are additional services specific to subgroups of DOIs. This confuses users - in particular since there is no easy way to immediately see what kind of DOI they have in front of them - and in the case of component DOIs there is one important feature missing.</p>
<p>DOis are assigned by registration agencies (CrossRef and DataCite are the most relevant ones for scholarly content), and these RAs have built different services around DOIs, e.g. different ways to describe and search the metadata (title, authors, etc.) associated with a DOI. Component DOIs are again different, the most important difference is that in the CrossRef implementation they they are not discoverable by <a href="https://doi.org/10.3789/isqv22n3.2010.06">querying the CrossRef system</a>. Component DOIs are also always associated with a parent DOI (for the article, book, etc.). Although this is the expected behavior, we shouldn’t expect component DOIs to always look like an extension of the parent DOI, as in <code>10.1371/journal.pone.0006022.g002</code> used in the example above.</p>
<p>In essence, a component DOI is a <strong><strong>DOI light</strong></strong>. We can use them for persistent linking, but we can’t use them for discovery via the CrossRef Metadata Search (and by extension other indexing services). A common use case for component DOIs is supplementary information in a journal article. Content in supplementary information files is already much harder to find than content in the body of an article, using component DOIs instead of regular DOIs makes the content again harder to find.</p>
<p>All of this might not have been much of an issue when I wrote the 2011 post, but making the data underlying a publication publicly available and discoverable is increasingly becoming something that funders, publishers and institutions expect. Most of these data are not deposited in dedicated data repositories, but in supplementary information files (for PLOS articles published since March 2014 this is true for more than 50% of papers). Using regular DOIs for supplementary information files with proper metadata and proper inclusion in indexing services will make it easier to find, access and reuse these data.</p>
<p>Unfortunately that still leaves us with the problem that the supplementary information files then will have CrossRef DOIs, whereas data repositories typically use DataCite DOIs, so that we need to search for these datasets in two different places. But that is material for another post.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why should we work where we live?]]></title>
        <id>36t06b0-8y69tes-arbxnc9-zzak3</id>
        <link href="https://blog.front-matter.io/mfenner/why-should-we-work-where-we-live"/>
        <updated>2015-06-28T10:23:00.000Z</updated>
        <summary type="html"><![CDATA[Photo by Danyu Wang / UnsplashAt the SciFoo Camp this weekend Erin McKiernan and I moderated an unconference session on the topic <strong>Why should we work where we live?</strong> This was a spontaneous idea after we had talked about this topic on Friday (Erin lives in Mexico with a job in Canada,...]]></summary>
        <content type="html"><![CDATA[<figure>
<img src="https://images.unsplash.com/photo-1502786129293-79981df4e689?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDI0fHxsYW5kc2NhcGV8ZW58MHx8fHwxNjE4MTUzMjA2&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" class="kg-image" srcset="https://images.unsplash.com/photo-1502786129293-79981df4e689?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDI0fHxsYW5kc2NhcGV8ZW58MHx8fHwxNjE4MTUzMjA2&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=600 600w, https://images.unsplash.com/photo-1502786129293-79981df4e689?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDI0fHxsYW5kc2NhcGV8ZW58MHx8fHwxNjE4MTUzMjA2&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=1000 1000w, https://images.unsplash.com/photo-1502786129293-79981df4e689?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDI0fHxsYW5kc2NhcGV8ZW58MHx8fHwxNjE4MTUzMjA2&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=1600 1600w, https://images.unsplash.com/photo-1502786129293-79981df4e689?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDI0fHxsYW5kc2NhcGV8ZW58MHx8fHwxNjE4MTUzMjA2&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2400 2400w" width="3968" height="2232" alt="Photo by Danyu Wang / Unsplash" /><figcaption aria-hidden="true">Photo by <a href="https://unsplash.com/@dandandan0101?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">Danyu Wang</a> / <a href="https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">Unsplash</a></figcaption>
</figure>
<p>At the <a href="http://www.digital-science.com/events/scifoo-camp-2015/">SciFoo Camp</a> this weekend <a href="https://emckiernan.wordpress.com/">Erin McKiernan</a> and I moderated an unconference session on the topic <strong>Why should we work where we live?</strong> This was a spontaneous idea after we had talked about this topic on Friday (Erin lives in Mexico with a job in Canada, I live in Germany and work for an organization in San Francisco).</p>
<p>We quickly realized that this situation is far from uncommon in the space we work in (science and science communication). Most commonly the reason is compromises we have to make when both partners have to find an adequate job. It can be a big challenge for a couple to find senior jobs in academia in the same city or region, especially outside of academic clusters such as Boston, New York or London.</p>
<p>The other big reason for work remote is that some research can only happen in special places, for example in high-energy physics, astronomy or the geosciences. And of course there are other flavors of the same situation, e.g. when a principal investigator moves to a new institution and PhD students or postdocs can’t or don’t want to move with him/her. And most academics have to do at least some remote work, since they will spend a good amount of time traveling to conferences or collaboration partners.</p>
<p>The discussion in the session centered on the social and technical challenges of working remotely. We didn’t have time to go into the legal aspects (e.g. taxes when you work in a different country), or the challenges organizing your personal life, particular difficult when you have children.</p>
<p>We shared our experience with online collaboration tools, and video conferencing with Skype, Google Hangouts or similar was central to this. Videoconferencing can be a challenge with slow internet connectivity, a situation that luckily is constantly improving.</p>
<p>Private group chat tool such as <a href="https://en.wikipedia.org/wiki/HipChat">HipChat</a> or <a href="https://slack.com/">Slack</a> are becoming increasingly popular outside the Tech sector and are a great alternative to email. They not only provide a platform for quick messages between two people, but also serve as a backchannel for informal “water cooler” discussions in an organization.</p>
<p>Another essential category is tools that track your work so that your remote colleagues not only can collaborate with you, but also see the work you are doing. As a supervisor you quickly see the work that was done the past week, a much more reasonable approach than looking at physical presence at work (where people might be doing all kinds of other things and personal productivity varies). Tracking your work is easy if you are a software developer like me and can look at code committed to version control, tickets closed, etc. For research this is more challenging, in particular if the workflow is not digital yet and for example all experiments are documented in a paper notebook. It seems that one requirement for remote work in science is digitalization of your work, but that is a direction we are heading anyway and which has other advantages (e.g. improving reproducibility). If there are no specialized tools for documenting your work available, then a note-taking tool such as <a href="https://www.onenote.com/">OneNote</a> or <a href="https://evernote.com/">Evernote</a> can be helpful. The digitization and automation of work is obviously limited in wet labs that require direct interactions with samples and instruments.</p>
<p>The social aspects of remote work might be the bigger challenge. There is still a big reluctance in supervisors and administrators to this, assuming that people will only be productive if someone is watching them. This assumption is very short sighted, as what drives PhDs and postdocs to work hard is not supervision, but the intrinsic motivation to accomplish something, in particular in light of the very competitive situation for permanent jobs in academia. The book <a href="http://37signals.com/remote/">Remote</a> by Jason Fried talks about this in great detail in the context of software development, but the same principles apply to work in science. What supervisors and administrators loose in direct oversight they can in attracting talent they would otherwise not get. Remote work only works if supported by the host institution, for example by adapting internal workflows and communications to make remote work the default rather than an exception.</p>
<p>Remote work is usually more successful and satisfying if combined with physical presence at the workplace. Reasons for this are not only the part of the work that can’t be done remotely, but more importantly the social aspect. How extensive this physical presence is depends on the circumstances. Some level of remote work has become part of almost everyone’s job in science, as it includes working at home in the evenings or on weekends, or work while traveling.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Persistent Identifiers and URLs]]></title>
        <id>7rkrmsf-af494jb-tw0p4cd-txgz6</id>
        <link href="https://blog.front-matter.io/mfenner/persistent-identifiers-and-urls"/>
        <updated>2015-06-03T10:43:00.000Z</updated>
        <summary type="html"><![CDATA[Just like the rest of the internet, much of our scholarly infrastructure is built around the Hypertext Transfer Protocol (HTTP), increasingly HTTPS for security, and soon HTTP/2 for better performance. In this infrastructure Universal Resource Locators (URLs)...]]></summary>
        <content type="html"><![CDATA[<p>Just like the rest of the internet, much of our scholarly infrastructure is built around the Hypertext Transfer Protocol (HTTP), increasingly HTTPS for security, and soon <a href="https://http2.github.io/">HTTP/2</a> for better performance. In this infrastructure Universal Resource Locators (URLs) are essential to locate resources (sic) such as scholarly articles, datasets, researchers, organizations, or grants. Read <a href="http://site.thomsonreuters.com/site/data-identifiers/">this</a> recent Thomson Reuters report for a good recent perspective on this topic. While this works for the most part, there are some issues with URLs - not specific to scholarly content, but particularly import here:</p>
<ol>
<li>multiple URLs can point to the same resource</li>
<li>URLs can be long and look ugly</li>
<li>URLs can change or break, making it hard or impossible to locate the resource</li>
<li>we are used to central indexes (or databases) describing these resources, allowing us to do sophisticated queries not possible in a generic web search, e.g. find all publications by author John Doe, published since 2012.</li>
</ol>
<p>No. 1 is a problem relevant to all URLs, e.g. web searches or liking/commenting a particular web page. Originally suggested by Google, <a href="https://support.google.com/webmasters/answer/139066?hl=en">Canonical URLs</a> are essential for services such as Facebook or <a href="https://hypothes.is/blog/cross-format-annotation/">Hypothes.is</a>. They have been formalized in <a href="http://tools.ietf.org/html/rfc6596">rfc6596</a> and are commonly used.</p>
<p>No. 2 can be a problem, in particular if we are not careful in designing appropriate URLs for landing pages (see next paragraph), but rather use something long and unreadable that also includes query parameters, etc. If we have no control over how the URL looks like, we can use URL shortener services such as <a href="https://bitly.com/">bit.ly</a>, which of course have become a common sight on the web. <a href="http://shortdoi.org/">ShortDOIs</a> are an URL shortener for DOIs, but they don’t seem to have gained much traction.</p>
<p>No. 3 is a particularly important issue, commonly referred to as <strong><strong>link rot</strong></strong> and described extensively for the scholarly literature, e.g. by <a href="https://doi.org/10.1371/journal.pone.0115253">Klein</a>. There are several technical solutions to this problem, a common approach is to use a landing page for the resource that will never change (and follows the recommendations by Tim Berners-Lee for <a href="http://www.w3.org/Provider/Style/URI.html">Cool URIs</a>, and then use redirection to point to the current location of the resource. This is easily for changes of the URL path using web server <a href="http://httpd.apache.org/docs/2.4/rewrite/remapping.html">redirect rules</a>. It gets more complicated if the server name also changes, in particular if it is the server holding the landing page. Thinking this through you realize that the only way this can be done on a larger scale is via one or more centralized services that not only provide the technical infrastructure for a central redirection (or resolver) service, but also come with a social contract of rules that everyone submitting URLs to the service has to follow - a major difference to URL shorteners, which don’t solve the link rot problem.</p>
<p>The above is of course a description of the DOI service provided by CrossRef, DataCite, and others, as well as similar persistent identifier services. Unfortunately some persistent identifier services don’t do the above: they create and use persistent identifiers, but there is no central resolver service that maps these identifiers back to URLs. This breaks the integration with the bigger scholarly infrastructure based on URLs. One common example are nucleotide sequences such as U65091, there is no single corresponding URL because the sequence can be found in all three main nucleotide databases: <a href="http://www.ncbi.nlm.nih.gov/nuccore/U65091">http://www.ncbi.nlm.nih.gov/nuccore/U65091</a>. It would help to have a central resolver, e.g. http://nucleotide.org/U65091 that then redirects to one of the three databases based on geographical location or user preference.</p>
<p>There are also problems with DOIs. They use the <a href="http://www.handle.net/">Handle</a> system to resolve the identifier to a location, and this system was built in the 1990s as infrastructure <a href="http://www.handle.net/faq.html">independent of</a> URLs or DNS (Domain Name Service), at a time when it wasn’t clear yet that URLs and associated standards would become ubiquitous. I don’t have numbers, but practically all DOIs are of course now resolved to URLs using the <a href="http://www.doi.org/factsheets/DOIProxy.html">DOI proxy server</a> at http://doi.org (preferred) or http://dx.doi.org. One main consequence of this is that DOIs are frequently not written as URLs - e.g. doi:10.5555/24242424x instead of <a href="https://doi.org/10.5555/24242424x">https://doi.org/10.5555/24242424x</a> - again breaking the integration with the bigger scholarly infrastructure. The CrossRef <a href="http://www.crossref.org/02publishers/doi_display_guidelines.html">DOI display guidelines</a> clearly state that DOIs should be written as URLs in <em>the online environment</em>, which basically is whenever DOIs are used, as PDFs and even Word documents know how to handle URLs. Unfortunately this guideline is still frequently ignored. The above is of course also true for other persistent identifiers using the Handle system, e.g. <a href="http://www.pidconsortium.eu/">ePIC</a>.</p>
<p>The other problem with the DOI system is that it doesn’t address issue No. 4, i.e. provide a central metadata index for the resources that use the system. This job is left to the DOI registration agencies such as CrossRef and DataCite, who have implemented a central metadata store (e.g. <a href="https://search.crossref.org/">CrossRef</a> or <a href="https://search.datacite.org/">DataCite</a>) in different ways (e.g. using different metadata schemata), or not at all. This means that we have to look in several places to find all DOIs associated with author John Doe, published since 2012. Obviously we are used to looking up information in multiple places, but not being able to look up the metadata for a DOI without some extra work (finding out the registration agency for the DOI and then going to the respective metadata store) is a problem. One way around these problems is to use the <a href="https://citation.crosscite.org/docs.html">DOI Content Negotiation Service</a>.</p>
<p>Another problem with the DOI system is more a social than a technical issue. Neither CrossRef nor DataCite seem to enforce that DOIs should always resolve to URLs when using a computer program. DOI resolution for humans works fine, but computers, e.g. command line tools such as cURL, can run into issues such as requiring cookies, javascript or user input, or permission problems getting to the journal landing page (see <a href="https://sensiblescience.io/mfenner/challenges-in-automated-doi-resolution">this earlier blog post</a> for some numbers). People seem to forget that a DOI that is not actionable is not really useful, and that scholarly infrastructure is not only used by people, but of course also by automated tools.</p>
<p>The persistent identifiers used in our scholarly infrastructure would benefit from a clearer focus on the problems they should solve, starting with No. 1-4 above. One problem is that we probably focus too much on the persistence problem, implied also by the term <strong><strong>persistent identifier</strong></strong> or <strong><strong>PID</strong></strong>. What we have neglected is the resolvable problem, i.e. making as easy as possible to get from the persistent identifier to the resource and/or its metadata. Based on the Den Haag Manifesto and suggested by Todd Vision, we therefore proposed the term <strong><strong>trusted identifier</strong></strong> with the following characteristics in the <a href="https://doi.org/10.6084/m9.figshare.824314">conceptual model of interoperability</a> for the <a href="http://odin-project.eu/">ODIN Project</a>:</p>
<ul>
<li>are unique on a global scale, allowing large numbers of unique identifiers</li>
<li>resolve as HTTP URI’s with support for content negotiation, and these HTTP URI’s should be persistent.</li>
<li>come with metadata that describe their most relevant properties, including a minimum set of common metadata elements. A search of metadata elements across all trusted identifiers of that service should be possible.</li>
<li>are interoperable with other identifiers through metadata elements that describe their relationship.</li>
<li>are issued and managed by an organization that focuses on that goal as its primary mission, has a sustainable business model and a critical mass of member organizations that have agreed to common procedures and policies, has a governing body, and is committed to using open technologies.</li>
</ul>
<p>While not directly relevant for resolving persistent identifiers as URLs, the last point is really important for any persistent identifier infrastructure, <a href="https://doi.org/10.6084/m9.figshare.1314859">described in detail recently</a>.</p>
<p>If I would design a persistent identifier service today (as if we would need yet another persistent identifier service), I would build the system around an URL shortening service that I control. The URLs could look very similar to what we have with DOIs now, e.g. <a href="http://doi.org/10.5555/12345678">https://doi.org/10.5555/12345678</a>, but it would be clear that persistent identifiers are URLs, not something separate. Plus we could take advantage of all the lessons learned - and possibly even reuse open source code - with URL shorteners, which are much more widely used than scholarly persistent identifiers.</p>
<p><em>Update 6/4/15: added link to Thomson Reuters <a href="http://site.thomsonreuters.com/site/data-identifiers/">report</a> on identifiers and open data.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human-readable and machine-readable Persistent Identifiers]]></title>
        <id>795ray6-h299sqb-j0fkdan-wdysp</id>
        <link href="https://blog.front-matter.io/mfenner/human-readable-and-machine-readable-persistent-identifiers"/>
        <updated>2015-05-27T10:45:00.000Z</updated>
        <summary type="html"><![CDATA[Yesterday Julie McMurry and co-authors published a preprint <strong><strong>10 Simple rules for design, provision, and reuse of persistent identifiers for life science data</strong></strong>. This is an important paper trying to address a fundamental...]]></summary>
        <content type="html"><![CDATA[<p>Yesterday Julie McMurry and co-authors <a href="https://doi.org/10.5281/zenodo.18003">published a preprint</a> <strong><strong>10 Simple rules for design, provision, and reuse of persistent identifiers for life science data</strong></strong>. This is an important paper trying to address a fundamental problem: how can we make persistent identifiers both human-readable and machine-readable?</p>
<p>Don’t be fooled by the title (used frequently by <a href="https://collections.plos.org/collection/ten-simple-rules/">PLOS Computational Biology</a>) - the paper doesn’t describe simple rules that help the average life sciences researcher. Rather, the paper deals with rather complex issues, and has 36 authors.</p>
<p>There is general agreement that we need persistent identifiers for scholarly communication, and that also includes life sciences datasets, the focus of the paper. What is less clear is how to express these persistent identifiers. An identifier such as <strong><strong>AB020317</strong></strong> - for the mouse p53 gene - is ambiguous. It is not clear without additional information that this is an identifier for the GenBank nucleotide database, rather than <a href="https://www.flickr.com/photos/alexcycu/8936663973/">something completely different</a>. One common approach to make this identifier unambiguous is to use URIs (Uniform Resource Identifiers), e.g. <a href="http://www.ncbi.nlm.nih.gov/nuccore/AB020317">http://www.ncbi.nlm.nih.gov/nuccore/AB020317</a> in this case.</p>
<p>The paper doesn’t like this approach, and even states that “URIs are still among the most commonly used and most problematic identifiers in the bio-data ecosystem”. The text also states that “their length makes them unwieldy for humans working with the data or for referencing in publications or other text”, but doesn’t go into any detail why URIs are “problematic identifiers”, or why length is an issue in an online environment.</p>
<p>This is an important weakness of the paper, because the authors propose an alternative: CURIEs or <strong><strong>compact URIs</strong></strong>. CURIEs were <a href="http://www.w3.org/TR/curie/">proposed</a> by the W3C a few years ago, as a way to make URIs <a href="https://www.crossref.org/blog/curies-a-cure-for-uris/">more human-readable</a>. The idea is simple, we use a namespace in addition to the local identifier, separated by a colon, e.g. <strong><strong><a href="http://www.ebi.ac.uk/ena/data/view/AB020317">Genbank:AB020317</a></strong></strong>.</p>
<p>This approach has of course been common practice in the life sciences before CURIEs or even the WWW existed, and is still the most common approach how identifiers for life sciences data are referenced in the scholarly literature. Unfortunately there are important problems with CURIEs, most of them mentioned in the paper:</p>
<ul>
<li>Persistent identifiers need to be resolvable, without additional information we don’t know what to do with <strong><strong><a href="http://www.ebi.ac.uk/ena/data/view/AB020317">Genbank:AB020317</a></strong></strong>. Most life sciences researchers understand this CURIE, but that might not necessarily be true for less commonly used namespaces</li>
<li>Namespaces are not necessarily unique, the paper uses <strong><strong>GEO</strong></strong> (which could mean Gene Expression Omnibus or GeoNames Ontology) as an example</li>
<li>Rule 3 in the paper goes into great detail what characters and patterns should be avoided in local identifiers that are part of a CURIE. It is not clear whether these recommendations will always be followed or how to check them</li>
<li>CURIEs should follow a pattern (regular expression) so that they can be extracted from a text. We <a href="https://doi.org/10.1371/journal.pone.0063184">know</a> that extracting identifiers from journal articles is possible, but difficult.</li>
</ul>
<p>URIs don’t have the problems listed above: they resolve, are unique, and there is good understanding (and available tools) of how a valid URI should look like and how to extract URIs from text documents. That is why URIs are good representations of persistent identifiers.</p>
<p>Another problem I have with CURIEs: the idea doesn’t seem to have caught on from the initial work more than five years ago (background reading <a href="http://manu.sporny.org/2011/case-for-curies/">here</a>). I’m not even sure what percentage of persistent identifier experts know about CURIEs.</p>
<p>My recommendation for life sciences data: express persistent identifiers as URIs. Now that can go into 10 simple rules for the average life sciences researcher.</p>
<p><em>P.S. This blog uses a tool <a href="http://sensiblescience.io/mfenner/auto-generating-links-to-data-and-resources/">I wrote two years ago</a> that automatically turns CURIEs in the text into links.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing the Scholarly Markdown Bundle]]></title>
        <id>657vhtb-8828j1b-w4g9ymw-pchmt</id>
        <link href="https://blog.front-matter.io/mfenner/introducing-the-scholarly-markdown-bundle"/>
        <updated>2015-04-23T11:48:00.000Z</updated>
        <summary type="html"><![CDATA[Using Markdown to author scholarly documents is an attractive alternative to the standard authoring tools Microsoft Word and LaTeX. The feeling shared by many is that Scholarly Markdown is 80% there, and that more effort is needed for the remaining 20%...]]></summary>
        <content type="html"><![CDATA[<p>Using Markdown to author scholarly documents is an attractive alternative to the standard authoring tools Microsoft Word and LaTeX. The feeling shared by many is that <a href="https://sensiblescience.io/mfenner/what-is-scholarly-markdown/">Scholarly Markdown</a> is 80% there, and that more effort is needed for the remaining 20% - moving markdown from a niche into the mainstream. What is mainly needed is building tools that connect the existing tools and ideas, resulting in one or more services attractive to a critical number of users. But maybe we also need to rethink the essential parts of Scholarly Markdown. In this post I propose that we expand the concept and define the <em>Scholarly Markdown Bundle</em>.</p>
<p>It is becoming increasingly clear that scholarly work can’t be adequately described in a single text document, most commonly the journal article. Not only are there associated metadata, assets such as figures and supplementary information, but also the research data and software needed to produce the work described in the publication. The obvious next step is to think of scholarly work as a collection of objects, most clearly described by Carol Goble and others as <a href="https://researchobject.github.io/specifications/bundle/">Research Object Bundle</a>.</p>
<p>There will probably never be a single authoring tool and format that pleases everyone. Markdown has particular inherent strengths and weaknesses, complex math or tables will probably always be easier with other formats. The strength of markdown is the simplicity of the format. Some things are hard or impossible to do, but many other things are much simpler. Creating a useful markdown editor is much easier than a word processor reading/writing <code>docx</code> format. Markdown is also a perfect format to <a href="https://sensiblescience.io/mfenner/using-microsoft-word-with-git/">work with</a> version control systems such as git.</p>
<p>This low barrier of entry makes markdown perfect to be integrated into many workflows. And we can go one step further than ePub and Research Object Bundle, which use the related Universal Container Format (<a href="https://wikidocs.adobe.com/wiki/display/PDFNAV/Universal+Container+Format">UCF</a>) and ePub Open Container Format (<a href="http://www.idpf.org/epub/301/spec/epub-ocf.html">OCF</a>), respectively. Instead of using zip to compress a folder into a single file we can use git version control instead: git provides the commands <code>git bundle</code> and <code>git archive</code> to compress a project under version control with or without version history. I feel this format is both more powerful So I propose the <em>Scholarly Markdown Bundle</em>:</p>
<ul>
<li>a git repository with one or more markdown files, either as a folder, or compressed into a single file using <code>git bundle</code></li>
<li>a particular flavor or markdown called Scholarly Markdown, and discussed here and elsewhere before</li>
<li>a <code>citeproc.json</code> file in the root of the project that contains all metadata relevant to the container, including references</li>
</ul>
<p>The <code>citeproc.json</code> file is similar to the minimal metadata schema <a href="https://github.com/mbjones/codemeta">codemeta</a> proposed by Matt Jones and others, but is in the format used by Pandoc today. This is <a href="https://sensiblescience.io/mfenner/citeproc-yaml-for-bibliographies/">important</a> because it adds citation parsing support out of the box. The last two points rely on the <a href="http://pandoc.org/">Pandoc</a> document conversion tool, so Scholarly Markdown bundles are really <strong><strong>markdown</strong></strong> + <strong><strong>Pandoc</strong></strong> + <strong><strong>Citeproc/CSL</strong></strong> + <strong><strong>git</strong></strong>. The format is flexible enough to not only describe scholarly articles, but also other kinds of scholarly works, including scientific software managed with git version control. And it integrates nicely with a number of existing workflows, e.g. an R project using RStudio for both code and text (in Rmarkdown). This format should also work for blogs like this one, but I would have to separate the blog posts from the Jekyll site generator code, a direction I suggested in the <a href="https://sensiblescience.io/mfenner/blogging-beyond-jekyll/">last</a> post.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blogging Beyond Jekyll]]></title>
        <id>2bvme2r-2vf80tr-h8jwa7m-t682n</id>
        <link href="https://blog.front-matter.io/mfenner/blogging-beyond-jekyll"/>
        <updated>2015-03-23T11:50:00.000Z</updated>
        <summary type="html"><![CDATA[This blog has been on four different platforms since starting in 2007: a custom blogging engine and then Movable Type on Nature Network 2007-2010, Wordpress on the PLOS Blogs Network 2010-2013, and the static blogging engine Jekyll hosted on Github Pages since 2013....]]></summary>
        <content type="html"><![CDATA[<p>This blog has been on four different platforms since starting in 2007: a custom blogging engine and then <a href="https://movabletype.org/">Movable Type</a> on <a href="http://network.nature.com/">Nature Network</a> 2007-2010, Wordpress on the <a href="http://blogs.plos.org">PLOS Blogs Network</a> 2010-2013, and the static blogging engine <a href="https://jekyllrb.com/">Jekyll</a> hosted on Github Pages since 2013. It might be time for yet another blogging platform change.</p>
<p>The main reason to switch from Wordpress to Jekyll was the concept of a static site generator: write posts in <a href="http://commonmark.org/">markdown format</a>, store them in a Github repository, and then have Jekyll automatically generate the HTML pages hosted on <a href="https://pages.github.com/">Github Pages</a>. The main attraction was the blog posts in markdown format stored in git version control without the need of a database. Jekyll is the glue to make all this work, and I was able to customize Jekyll to my needs, e.g. by using <a href="https://pandoc.org/">Pandoc</a> for the markdown to html conversion.</p>
<p>While this workflow still makes sense for this blog, there are a number of shortcomings:</p>
<ul>
<li>Jekyll needs to rebuild the entire site every time I publish a new post. While this isn’t much of a problem for the size of this blog, it doesn’t scale well for larger sites. And the process is more complex if you use custom jekyll plugins like this blog, as you can’t use the automatic Jekyll pipeline provided by Github (hint: use a Travis continuous integration server <a href="https://sensiblescience.io/mfenner/continuous-publishing/">to build the site</a>)</li>
<li>the web is moving to increasingly sophisticated javascript frontends, using frameworks such as <a href="https://angularjs.org/">Angular.js</a>, <a href="http://emberjs.com/">Ember.js</a>, or frontend libraries for scholarly documents such as <a href="http://elifesciences.org/elife-news/lens">Lens</a>. While they can be used together with Jekyll, that is not a typical use case.</li>
<li>the tight integration between the code to generate the website and the content (Wordpress and other blogging engines have the same approach) is not always the best solution, e.g. when you want to want to generate the pages for something that is not a blog (e.g. a <a href="http://book.openingscience.org/">book</a>).</li>
</ul>
<p>What could we do instead?</p>
<blockquote>
Build a Javascript frontend where the content is served via an API built around markdown documents, stored in git version control.
</blockquote>
<h3 id="api">API</h3>
<p>The blog posts are still written in markdown, stored (and version-controlled in a Github repository), but we would now access the content via API. The easiest solution is to use the <a href="https://developer.github.com/v3/repos/contents/">Github Contents API</a> and either do the markdown to html conversion in javascript yourself, or let the Github API do the conversion to HTML for you. Alternatively we could build our own API, e.g. because we want to control the markdown to html conversion, or need additional functionality such as fulltext search. And of course the two approaches can be combined, e.g. via a Github webhook that triggers the markdown to html conversion every time a document is added or updated, and stores the converted documents in the same repo.</p>
<h3 id="frontend">Frontend</h3>
<p>The frontend should be written as a one-page javascript application, not requiring a server backend. In contrast to the Jekyll workflow the frontend code doesn’t need to be updated every time we post a blog post. Since this is a very common scenario, there are probably several solutions out there already. Please mention them in the comments if you have suggestions. One candidate is <a href="https://github.com/elifesciences/lens/">Lens</a> mentioned above - a beautiful frontend for scholarly documents. Lens displays documents in the <a href="http://jats.nlm.nih.gov/publishing/tag-library/1.0/index.html">JATS</a> XML format, so your API would have to provide that format.</p>
<h3 id="conclusions">Conclusions</h3>
<p>The separation into API and frontend is of course old news. But for blogs this seems to still be a fairly new concept, in particular when combined with a backend using documents stored in git version control rather than in a database. Wordpress added a <a href="https://wordpress.org/plugins/json-rest-api/">REST API Plugin</a> in 2014, and the Ghost blogging framework (which uses a database backend) also seems to <a href="https://trello.com/b/EceUgtCL/ghost-roadmap">go into that general direction</a>. Please ping me if you like the idea and want to contribute, or have implemented something like this already.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Metadata in Microsoft Word documents]]></title>
        <id>2kk2m6n-zac8ees-ewsma9x-qkb3x</id>
        <link href="https://blog.front-matter.io/mfenner/metadata-in-microsoft-word-documents"/>
        <updated>2015-03-20T11:52:00.000Z</updated>
        <summary type="html"><![CDATA[Metadata such as author, title, journal or persistent identifier are essential for scholarly documents, and some of us are spending a significant part of our time adding or fixing metadata. Unfortunately we sometimes don’t pay enough attention to the flow of metadata,...]]></summary>
        <content type="html"><![CDATA[<p>Metadata such as author, title, journal or persistent identifier are essential for scholarly documents, and some of us are spending a significant part of our time adding or fixing metadata. Unfortunately we sometimes don’t pay enough attention to the flow of metadata, i.e. we ignore already existing metadata, or reinvent the wheel in how we describe or store them.</p>
<p>Storing metadata in text-based formats is usually straightforward. This blog post is written in markdown with a <a href="http://yaml.org/">YAML header</a> - think of YAML as the more human-readable version of JSON - at the beginning of the document:</p>
<pre><code>---
title: Metadata in Microsoft Word documents
---</code></pre>
<p>This is then translated into this HTML when the blog post is published:</p>
<pre><code>&lt;meta property=&quot;dc:title&quot; content=&quot;Metadata in Microsoft Word documents&quot; /&gt;</code></pre>
<p>XML is of course a very natural format for metadata, here for example <a href="http://jats.nlm.nih.gov/publishing/tag-library/1.0/index.html">JATS</a> used for scholarly articles:</p>
<pre><code>&lt;article-title&gt;Metadata in Microsoft Word documents&lt;/article-title&gt;</code></pre>
<p>Many scholarly documents start out as Microsoft Word documents. And while the <code>docx</code> format introduced by Microsoft in Microsoft Office 2007 <a href="http://officeopenxml.com/">is XML-based</a>, few users are aware of this fact. And probably even fewer users (including myself) ever go to the <code>Properties…</code> settings of a <code>docx</code> document and add a <code>title</code>, <code>keywords</code> or other metadata (the <code>author</code> is usually set automatically).</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/9th/IC164149.gif" class="kg-image" width="419" height="488" alt="Microsoft Word 2007 Properties. Image from Microsoft Developer Network" /><figcaption aria-hidden="true">Microsoft Word 2007 Properties. Image from <a href="https://msdn.microsoft.com/en-us/library/bb308936(v=office.12).aspx">Microsoft Developer Network</a></figcaption>
</figure>
<p>This is very unfortunate, as these metadata are very often required, e.g. in a journal article submission, and then need to be collected again, usually either by asking the author to fill out a web form, and/or by extracting the metadata (e.g. title) from the document.</p>
<p>The best place for metadata is with the document (not <em>in</em> the document), and if the file format (<code>docx</code> in this case) supports it, we should take advantage of this. The main benefit: metadata stay with the text when the document is sent to co-authors via email, or put on a file server, or into Dropbox.</p>
<p>In the case of <code>docx</code>, the metadata support is actually pretty good, using the standard <a href="http://dublincore.org/">Dublin Core</a>, and storing the metadata in a separate file called <code>core.xml</code>. You can see this file if you unzip your <code>docx</code> file (e.g. after giving it a <code>zip</code> extension). The <code>core.xml</code> file for this blog post (after converting the markdown file to <code>docx</code> using <a href="https://pandoc.org">Pandoc</a>) looks like this:</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;cp:coreProperties xmlns:cp=&quot;http://schemas.openxmlformats.org/package/2006/metadata/core-properties&quot; xmlns:dc=&quot;http://purl.org/dc/elements/1.1/&quot; xmlns:dcterms=&quot;http://purl.org/dc/terms/&quot; xmlns:dcmitype=&quot;http://purl.org/dc/dcmitype/&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;&gt;&lt;dc:title&gt;Metadata in Microsoft Word documents&lt;/dc:title&gt;&lt;dc:creator&gt;&lt;/dc:creator&gt;&lt;/cp:coreProperties&gt;</code></pre>
<p>Because <code>docx</code> is XML, we can read/write this file not only in Microsoft Word, e.g. using macros, but also outside of Microsoft Word, e.g. in workflows that converts <code>docx</code> documents into other formats, or tools that check <code>docx</code> files for required metadata (e.g. by using <a href="https://sensiblescience.io/mfenner/introducing-rakali/">rakali</a> that I wrote last year). So please encourage authors to use the Microsoft Word <code>Properties…</code> settings, and update existing tools to take advantage of the Dublin Core metadata stored in every <code>docx</code> file.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[First analysis of software metrics]]></title>
        <id>7pd18wx-4h49gdv-h02fmst-x06r5</id>
        <link href="https://blog.front-matter.io/mfenner/first-analysis-of-software-metrics"/>
        <updated>2015-02-28T11:55:00.000Z</updated>
        <summary type="html"><![CDATA[Last week I wrote about software.lagotto.io, an instance of the lagotto open source software collecting metrics for the about 1,400 software repositories included in Sciencetoolbox. In this post I want to report the first results analyzing the data.Number of software repositories (out of 1,404)...]]></summary>
        <content type="html"><![CDATA[<p>Last week <a href="https://sensiblescience.io/mfenner/metrics-for-scientific-software/">I wrote about</a> software.lagotto.io, an instance of the <a href="https://github.com/articlemetrics/lagotto">lagotto</a> open source software collecting metrics for the about 1,400 software repositories included in Sciencetoolbox. In this post I want to report the first results analyzing the data.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/9th/software.lagotto.io_2.png" class="kg-image" width="1444" height="722" alt="Number of software repositories (out of 1,404) with at least one event. Data from software.lagotto.io" /><figcaption aria-hidden="true">Number of software repositories (out of 1,404) with at least one event. Data from <a href="http://software.lagotto.io">software.lagotto.io</a></figcaption>
</figure>
<p>If you want to follow along, please go to <a href="https://github.com/mfenner/software-analysis">https://github.com/mfenner/software-analysis</a>, this repository holds all the data, as well as the R code used for analysis. A special thanks goes to <a href="http://scottchamberlain.info/">Scott Chamberlain</a> who greatly helped me by tweaking the <a href="https://github.com/ropensci/alm">alm</a> R package to support URLs instead of DOIs as identifiers.</p>
<p>The first step in the analysis is to get an overview of the external sources citing or discussing the software package:</p>
<p>This is basically the same figure as in the <a href="https://sensiblescience.io/mfenner/metrics-for-scientific-software/">previous post</a>, but with two differences: I have added a <a href="http://www.nature.com/opensearch/">Nature.com OpenSearch</a> data source, and I have found an additional 64 repositories cited in scholarly articles via an Europe PMC full-text Search that also includes the reference lists (thanks to <a href="http://www.ebi.ac.uk/about/people/johanna-mcentyre">Jo McEntyre</a>).</p>
<p>I am not sure why we are not picking up any Wikipedia citations, and have to take a closer look. The ORCID source also needs tweaking, and there are some issues with the <a href="http://wordpress.com/">Wordpress.com</a> data that I have to look into as well. Citations in the scholarly literature are obviously the most interesting data, and we have three Github repos with more than 25 citations, including <a href="https://github.com/najoshi/sickle">https://github.com/najoshi/sickle</a> with 54 citations. As most repositories in our sample are cited only once if at all, a correlation with Github stars and forks is not useful. Sickle is popular on Github (52 stars and 32 forks), but it is not clear that this activity is correlated to citations (e.g. because there are more citations than stars).</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/9th/github_likes_readers-1.png" class="kg-image" width="672" height="480" alt="Correlation between Github stargazers and forks, log-log scale. Data from software.lagotto.io" /><figcaption aria-hidden="true">Correlation between Github stargazers and forks, log-log scale. Data from <a href="http://software.lagotto.io">software.lagotto.io</a></figcaption>
</figure>
<p>The vast majority of software repos in this analysis are hosted by Github, so we have the numbers of stars and forks for those. It is interesting, although probably not very surprising, that the number of Github stargazers and forks is highly correlated:</p>
<p>We can find Facebook activity (likes, comments or shares) for one third of the repositories. There is a reasonably good correlation between Facebook activity and number of Github forks:</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/9th/facebook_github_readers-1.png" class="kg-image" width="672" height="480" alt="Correlation between combined Facebook activity and Github forks, log-log scale. Data from software.lagotto.io" /><figcaption aria-hidden="true">Correlation between combined Facebook activity and Github forks, log-log scale. Data from <a href="http://software.lagotto.io">software.lagotto.io</a></figcaption>
</figure>
<p>One interesting analysis would be to look at the repositories that have been forked much more often relative to their Facebook activity, e.g. <a href="https://github.com/cloudera/impala">Impala</a> with 1,207 Github stars and 458 forks, but only 5 Facebook shares. One limitation of the analysis is that we are not tracking Facebook (or other social media) activity for all forks of a repo.</p>
<p>We found Reddit discussions mentioning one of the repositories in 7% of cases. Once we have a larger sample size it would be interesting to correlate this activity with Github stars and forks, similar to what we did for Facebook. By far the most popular repository from our sample on Reddit is <a href="https://github.com/Bitcoin/Bitcoin">Bitcoin</a>, followed by <a href="https://github.com/jquery/jquery">JQuery</a>. Twitter activity is notoriously difficult to collect since Twitter doesn’t keep tweets very long, hence probably the low numbers compared to Facebook and Reddit.</p>
<p>Feel free to play with the data and scripts provided at <a href="https://github.com/mfenner/software-analysis">https://github.com/mfenner/software-analysis</a>, my next step is probably to include a much larger number of software repositories.</p>
<p>It has not escaped our notice that the kind of analysis described above could be applied to any software repository, not just scientific software.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why there is no iTunes for science papers]]></title>
        <id>5rk3bdg-c1g8neb-12ggg8a-vmawq</id>
        <link href="https://blog.front-matter.io/mfenner/why-there-is-no-itunes-for-science-papers"/>
        <updated>2015-02-23T11:58:00.000Z</updated>
        <summary type="html"><![CDATA[The iTunes Store was opened by Apple in 2003 to sell digital music and other digital assets. Since 2009 music purchased in the iTunes store is free of Digital Rights Management (DRM). Apple became the largest music vendor worldwide in 2010,...]]></summary>
        <content type="html"><![CDATA[<p>The iTunes Store was opened by Apple in 2003 to sell digital music and other digital assets. Since 2009 music purchased in the iTunes store is free of Digital Rights Management (DRM). Apple became the largest music vendor worldwide in 2010, and by 2013 had sold 25 billion songs.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/9th/pay_per_view_nature.png" class="kg-image" width="1150" height="546" alt="For an article in Nature" /><figcaption aria-hidden="true">For an article in Nature</figcaption>
</figure>
<p>Scholarly articles are distributed almost exclusively in digital form. While there is an increasing number of journal articles freely available via green or gold open access, the majority of them still can only be read if the reader works at an institution with a subscription to the journal. Many journals also allow the reader to buy a single article of interest, for prices between $10 and more than $30:</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/9th/pay_per_view_lancet.png" class="kg-image" width="606" height="554" alt="For an article in Lancet" /><figcaption aria-hidden="true">For an article in Lancet</figcaption>
</figure>
<p>There is also a document delivery service provided by libraries, but that option varies considerably by country and in Germany for example means a scanned article as printout rather than the original PDF because of a change in German copyright law a few years ago. There are also the services <a href="https://www.deepdyve.com/">DeepDyve</a> and <a href="https://www.readcube.com/">ReadCube</a>, but again you don’t get the PDF (or only for prices similar to those quoted above), but rather limited access for reading and printing.</p>
<p>In summary, affordable access to scholarly content by subscription publishers is in a dire state: you either have to work at an academic institution subscribing to the desired journal, get only a crippled version of the article (online viewing only), or pay up to $30 for a single article, which clearly doesn’t scale beyond very occasional use.</p>
<p>With this background it is obvious that several people have discussed the iTunes Store-like model to sell scholarly articles:</p>
<ul>
<li><a href="http://crosstech.crossref.org/2009/09/prc_report_and_ipub_revisited.html">PRC Report and “iPub” revisited</a></li>
<li><a href="http://www.popsci.com/science/article/2009-10/deepdyve-launches-itunes-science-papers">DeepDyve launches iTunes Store-like service for science papers</a></li>
<li><a href="http://scienceblogs.com/digitalbio/2012/01/10/could-an-itunes-like-model-wor/">Could an iTunes-like model work with scientific publishing?</a></li>
<li><a href="http://www.bostonglobe.com/business/2012/10/07/start-readcube-program-uses-itunes-payment-model-for-access-scientific-articles/1UopCX1qfEE3uO2UEzuM7L/story.html">A plan to open up science journals</a></li>
<li><a href="http://www.newyorker.com/tech/elements/when-the-rebel-alliance-sells-out">When the Rebel Alliance Sells Out</a></li>
</ul>
<p>The best already existing platforms to build such as service are reference managers, as most of them have learned now to manage PDF files, and have an online component. ReadCube is offering a pay-per-view option already, Papers, Mendeley, Endnote or others could get into this business.</p>
<p>One of the big advantages of payments for single articles is transparency, as institutions and users only pay for what they actually use. Price transparency is one of the big problems with the <em>big deal</em> contracts that academic institutions have with publishers - read <a href="https://doi.org/10.1073/pnas.1403006111">this article</a> for more info.</p>
<p>But rather than becoming the predominant way to pay for digital music, services such as DeepDyve and ReadCube are only playing a marginal role. Why is that so?</p>
<ul>
<li>whereas digital music is paid for by the consumer, there is usually a middleman in the form of the library for scholarly articles, which makes the payment process more complex.</li>
<li>subscription publishers have focused all their efforts on selling big deals with increasing numbers of journals to libraries. Prices of $30 per article are clearly intended to discourage payment for single articles (which could jeopardize journal bundles) rather than offering an earnest payment option.</li>
<li>Apple was in a strong negotiation position with record labels when starting the iTunes store (the extremely popular iPod, record labels scared of file-sharing platforms such as Napster). No organization is in a similar position with scientific publishers, and services such as ReadCube or Mendeley are handicapped because they are associated with a particular publisher</li>
</ul>
<p>Unless several large publishers and/or a smart third-party with enough muscle start an initiative in this space, e.g. by bringing the pay-per-view prices to a reasonable level (e.g. $4.99), we will never see an iTunes Store-like service for scholarly articles, and this currently looks like the most likely outcome. We may have reached the point where it is too late, as most publishers seem to already work towards another payment model: gold open access where the authors pay the article costs.</p>
<p><em>Update 3/2/15: added link to 2009 CrossTech blog post.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Metrics for scientific software]]></title>
        <id>1hr027a-ynz9f48-632b6yx-fm0ts</id>
        <link href="https://blog.front-matter.io/mfenner/metrics-for-scientific-software"/>
        <updated>2015-02-19T12:00:00.000Z</updated>
        <summary type="html"><![CDATA[One of the challenges of collecting metrics for scholarly outputs is persistent identifiers. For journal articles the Digital Object Identifier (DOI) has become the de-facto standard, other popular identifiers are the pmid from PubMed,...]]></summary>
        <content type="html"><![CDATA[<p>One of the challenges of collecting metrics for scholarly outputs is persistent identifiers. For journal articles the Digital Object Identifier (DOI) has become the de-facto standard, other popular identifiers are the pmid from PubMed, the identifiers used by Scopus and Web of Science, and the arxiv ID for ArXiV preprints.</p>
<p>For other research outputs the picture is less clear. DOIs are also used for datasets, but so are many other identifiers, in particular in the life sciences.</p>
<p>To collect metrics for research outputs, the requirements are slightly different. We need identifiers understood by the services collecting the metrics, not by the data repository or other service that is holding the research output (the only exception is usage stats, which are generated locally). For many services, in particular social media such as Facebook, Twitter or Reddit, the primary identifier for a resource is a URL. This means that we should have one or more URLs for every research output where we want to track the metrics - typically the publisher or data repository landing page. Since URLs can be messy, Google, Facebook and others have come up with the concept of a <a href="http://googlewebmastercentral.blogspot.de/2009/02/specify-your-canonical.html">canonical URL</a>, and some care should go into constructing proper canonical URLs (see <a href="https://sensiblescience.io/mfenner/challenges-in-automated-doi-resolution">this blog post</a> for examples of what can go wrong).</p>
<p>The Den Haag Manifesto is the result of a <strong><strong>Knowledge Exchange</strong></strong> workshop held in June 2011 and tries to bring Persistent Identifiers and Linked Open Data together. The first principle is very much in line with what I said above:</p>
<blockquote>
Make sure PIDs can be referred to as HTTP URI’s, including support for content negotiation.
</blockquote>
<p>Or, to put this differently: URLs are good enough to start collecting metrics for scholarly outputs. Scientific software is a good example where persistent identifiers are not commonly used (despite efforts such as <a href="https://guides.github.com/activities/citable-code/">this one</a>), but we can still collect many meaningful metrics using the repository URL (and the open source software <a href="https://github.com/articlemetrics/lagotto">lagotto</a>):</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/software.lagotto.io.png" class="kg-image" width="1468" height="686" alt="Number of software repositories (out of 1,404) with at least one event. Data from software.lagotto.io" /><figcaption aria-hidden="true">Number of software repositories (out of 1,404) with at least one event. Data from software.lagotto.io</figcaption>
</figure>
<p>The last three rows are citations in the scholarly literature found via fulltext search of BioMed Central, Europe PMC and PLOS. URLs (in contrast to persistent identifiers represented as strings and/or numbers) are easy to find, the main limitation is not so much using a URL rather than a DOI, but that scientific software typically is mentioned in the text without appearing in the reference list. This makes it hard to impossible to find articles mentioning the software that are not open access, which unfortunately is still the majority of them.</p>
<p>We are of course also tracking the discussion of the software in social media, and are collecting the number of stars and forks in Github and Bitbucket. Overall there is quite a lot of activity, here are some examples:</p>
<ul>
<li><a href="https://github.com/najoshi/sickle">Windowed Adaptive Trimming for fastq files using quality</a></li>
<li><a href="https://github.com/lh3/wgsim">Reads simulator</a></li>
<li><a href="http://software.lagotto.io/works/url/https://github.com/lh3/seqtk">Toolkit for processing sequences in FASTA/Q formats</a></li>
</ul>
<p>All three software repos have been cited in the scholarly literature at least ten times. What is missing is infrastructure that tracks the citations of scientific software, so that we can give proper scientific credit to the authors of the software, and can discover other research projects using the same tools. software.lagotto.io uses a list of software repos collected by Jure Triglav for ScienceToolbox, and a scientific software index is indeed one of the important missing pieces.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manifests and Reference Lists]]></title>
        <id>61khfcc-vjg8zyb-a6p5ram-qn630</id>
        <link href="https://blog.front-matter.io/mfenner/manifests-and-reference-lists"/>
        <updated>2015-02-05T12:03:00.000Z</updated>
        <summary type="html"><![CDATA[Photo by Matthew Waring / UnsplashLast month at the Force15 conference in Oxford Ian Mulvany and I ran a workshop on data citation support in reference managers. The report of that workshop isn’t done yet,...]]></summary>
        <content type="html"><![CDATA[<figure>
<img src="https://images.unsplash.com/photo-1567966181174-55151ffbd185?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDZ8fG94Zm9yZHxlbnwwfHx8fDE2MTgxNTM0MzA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" class="kg-image" srcset="https://images.unsplash.com/photo-1567966181174-55151ffbd185?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDZ8fG94Zm9yZHxlbnwwfHx8fDE2MTgxNTM0MzA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=600 600w, https://images.unsplash.com/photo-1567966181174-55151ffbd185?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDZ8fG94Zm9yZHxlbnwwfHx8fDE2MTgxNTM0MzA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=1000 1000w, https://images.unsplash.com/photo-1567966181174-55151ffbd185?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDZ8fG94Zm9yZHxlbnwwfHx8fDE2MTgxNTM0MzA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=1600 1600w, https://images.unsplash.com/photo-1567966181174-55151ffbd185?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDZ8fG94Zm9yZHxlbnwwfHx8fDE2MTgxNTM0MzA&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2400 2400w" width="6000" height="4000" alt="Photo by Matthew Waring / Unsplash" /><figcaption aria-hidden="true">Photo by <a href="https://unsplash.com/@matthewwaring?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">Matthew Waring</a> / <a href="https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">Unsplash</a></figcaption>
</figure>
<p>Last month at the <a href="https://www.force11.org/meetings/force2015/pre-conference-meeting-list">Force15 conference</a> in Oxford <a href="https://twitter.com/IanMulvany">Ian Mulvany</a> and I ran a workshop on <a href="https://sensiblescience.io/mfenner/data-citation-support-in-reference-managers/">data citation support in reference managers</a>. The report of that workshop isn’t done yet, but I can say that it was a success - we now have a pretty good idea what the problems are and what needs to be done to fix them. The short summary of the workshop is in <a href="https://speakerdeck.com/mfenner/workshop-summary-reference-managers-and-data-citation">this</a> slide deck of the presentation that summarized the workshop for the other Force15 attendees.</p>
<p>The whole idea of the workshop was to treat data citation as similar as possible to the citation of journal articles, i.e. to allow authors to use the same tools (reference managers) and conventions (citation styles). Putting a data citation into a reference list makes it easier to find that data citation because reference lists contain more metadata, are more structured, and more accessible than data citations in the form of identifiers or links within the body text of the article.</p>
<p>But I have to admit that there is one problem with reference lists: although there is always some self-citation, reference lists usually contain references to articles (and other resources) created by other people and before the article was published. It feels a little bit odd to put a dataset created by the same group of people and published at the same time into the reference list. And although we could use a separate reference list or highlight the data associated with the article in some other way, what we really want is something slightly different, a manifest file.</p>
<p>The journal article has been a (mainly) textual document for many centuries not because this is the essence of science communication, but rather because there was no practical way to include all the other information (raw data, tools used for experiments, etc.). Very few of these limitations remain with the digital journal article that we have since the 1990s, but we have for the most part failed to change the format other than going from paper to PDF. One of many examples: figures in publications typically still are has limited as they were decades ago with no way to see the data underlying the figure, options for selecting what data points are shown, or animation for time-based information.</p>
<p>So what we really care about is the sum of artifacts and resources that together make what Carol Goble and others call <a href="https://doi.org/10.1038/npre.2010.4626.1">research object</a>, the journal article is an important part, but clearly doesn’t include everything that is needed to understand and reproduce the work. Reference lists can help with linking to some of the resources not included in the article text, but they typically don’t link to supplementary information or other places where the underlying data are made available, or to the figures of the article. Although some publishers provide navigation tools for readers to get to this information, what we really need is a machine-readable list of all the resources used in an article.</p>
<p>As it happens, this is exactly what the ePub format for electronic books is doing, as every ePub must include a manifest file that lists all the files that are part of the publication, defined in the <a href="http://www.idpf.org/epub/20/spec/OPF_2.0.1_draft.htm">Open Packaging Format (OPF)</a>. I need to do more research to figure out how to do this with <a href="http://jats.nlm.nih.gov/archiving/tag-library/1.0/index.html">JATS</a>, the standard for scholarly articles, and how to generate something similar to the manifest file when using different formats, e.g. html or markdown. This has to be linked to some of the information we are collecting already, e.g. described in <a href="https://doi.org/10.3998/3336451.0014.106">JATS</a>, or the <code>relatedIdentifier</code> in the <a href="https://doi.org/10.5438/0010">DataCite metadata</a>.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Citation Support in Reference Managers]]></title>
        <id>5kd8w70-s4r8ma9-20hewhv-3xxss</id>
        <link href="https://blog.front-matter.io/mfenner/data-citation-support-in-reference-managers"/>
        <updated>2015-01-05T14:55:00.000Z</updated>
        <summary type="html"><![CDATA[This is the title of an upcoming workshop next Sunday organized by Ian Mulvany and myself. The workshop is a pre-conference event of the Force15 conference in Oxford. This blog post summarizes some of the issues and work that needs to be done.Data...]]></summary>
        <content type="html"><![CDATA[<p>This is the title of an upcoming workshop next Sunday organized by Ian Mulvany and myself. The workshop is a <a href="https://www.force11.org/meetings/force2015/pre-conference-meeting-list">pre-conference event</a> of the <a href="https://www.force11.org/meetings/force2015">Force15</a> conference in Oxford. This blog post summarizes some of the issues and work that needs to be done.</p>
<p>Data Citation is one of the big themes of the Force15 conference, and a lot of progress has been made, including the Joint Declaration of Data Citation Principles (Data Citation Synthesis Group 2014) that start with the following paragraph on <strong><strong>Importance</strong></strong>:</p>
<blockquote>
Data should be considered legitimate, citable products of research. Data citations should be accorded the same importance in the scholarly record as citations of other research objects, such as publications.
</blockquote>
<p>Convincing researchers, funders, university administrators and others that data citation is important is crucial. But for researchers to actually adopt data citation to the same degree as citations of the scholarly literature, more needs to be done:</p>
<ul>
<li>incentives (both carrots and sticks) by funders, institutions, and scholarly societies</li>
<li>training in data management</li>
<li>data repositories and other tools and services for the public sharing of data</li>
<li>tools and services that help citing those datasets</li>
</ul>
<p>The focus of the workshop is on the last bullet point, and I would argue that more work still needs to be done here compared to the first three bullet points.</p>
<h2 id="reference-managers">Reference Managers</h2>
<p>Researchers use reference managers to handle the citations in the manuscripts they write. This is both a common practice that everybody understands, and there are a plethora of tools - both free and paid - available. Most reference managers were originally built to handle citations of journal articles and maybe books or book chapters, and many of them also help with managing the associated PDF files. In the last 15 years we have seen an dramatic increase of non-article citations in reference lists, mainly to web resources (Klein et al., 2014):</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/journal.pone.0115253.g002.png" class="kg-image" width="2785" height="2595" alt="From Fig. 2: STM articles and URI references per publication year - Elsevier corpus (Klein et al. 2014)" /><figcaption aria-hidden="true">From Fig. 2: STM articles and URI references per publication year - Elsevier corpus (Klein et al. 2014)</figcaption>
</figure>
<p>References managers have started to adapt to these changes in citation patterns. Similarly they have become better in handling non-textual resources such as slide decks, datasets, or movies. Nobody should type in references by hand in 2015, as reference managers have come up with several ways of importing metadata about citations:</p>
<ul>
<li>import references stored in a file using a format such as BibTex or RIS</li>
<li>import references by talking to an external API</li>
<li>import references via a bookmarklet that grabs information from the current webpage in the browser</li>
</ul>
<p>Endnote and Papers typically use the second approach whereas Mendeley, Zotero (and others) work almost exclusively via bookmarklets (and there are of course combinations of both). Bookmarklets in general work better for web resources and other content that is not indexed in a central service such as Web of Science or Scopus. This is also true for research data, as there are currently few central research data indexing services - the Thomson Reuters <a href="http://wokinfo.com/products_tools/multidisciplinary/dci/">Data Citation Index</a> and <a href="https://www.datacite.org/">DataCite</a> are two examples in this category. But there are also thousands of data repositories, many of them listed in re3data (Pampel et al., 2013).</p>
<p>The reference manager <a href="https://www.zotero.org/">Zotero</a> has built a large open source ecosystem around bookmarklets (what they call <a href="https://github.com/zotero/translators">web translators</a>), making it straightforward to add support for a new resource, as I have done for <a href="https://github.com/zotero/translators/blob/master/NCBI%20Nucleotide.js">GenBank nucleotide sequence datasets</a> in November after learning the basics in a <a href="http://sensiblescience.io/mfenner/webinar-on-writing-zotero-translators/">webinar</a> given by Sebastian Karcher, a frequent contributor to Zotero web translators.</p>
<p>There is no technical reason that reference managers can’t support a broad range of objects to cite, including datasets. And integration of data citation into the reference manager workflow is not only the easiest and most natural way for the author of a paper, but also makes it easier to discover these citations - reference lists are simply much better for that than links in the text, in particular if the content is behind subscription walls. There is a long tradition in the life sciences to put identifiers for genetic sequences used in a publication right into the text (usually into the methods section). Links in the body text are worse than references in reference lists, <a href="http://sensiblescience.io/mfenner/auto-generating-links-to-data-and-resources/">identifiers without a link</a> are even worse, as they are very hard to find in an automated way (Kafkas, Kim, &amp; McEntyre, 2013).</p>
<p>Please come to our workshop on Sunday afternoon if you are in Oxford and are interested in this topic. <a href="https://www.eventbrite.com/e/data-citation-support-in-reference-managers-tickets-15136593960">Registration</a> is free, and the workshop will include both presentations about the current state of data citation support in the reference managers Endnote, Papers, Mendeley and Zotero, and work in smaller groups on practical implementations.</p>
<h2 id="references">References</h2>
<p>Kafkas, Ş., Kim, J.-H., &amp; McEntyre, J. R. (2013). Database Citation in Full Text Biomedical Articles. <em>PLoS ONE</em>. https://doi.org/<a href="https://doi.org/10.1371/journal.pone.0063184">10.1371/journal.pone.0063184</a></p>
<p>Klein, M., Van de Sompel, H., Sanderson, R., Shankar, H., Balakireva, L., Zhou, K., &amp; Tobin, R. (2014). Scholarly context not found: one in five articles suffers from reference rot. <em>PLoS ONE</em>, <em>9</em>(12), e115253. https://doi.org/<a href="https://doi.org/10.1371/journal.pone.0115253">10.1371/journal.pone.0115253</a></p>
<p>Pampel, H., Vierkant, P., Scholze, F., Bertelmann, R., Kindling, M., Klump, J., … Dierolf, U. (2013). Making Research Data Repositories Visible: The re3data.org Registry. <em>PLoS ONE</em>, <em>8</em>(11), e78080. https://doi.org/<a href="https://doi.org/10.1371/journal.pone.0078080">10.1371/journal.pone.0078080</a></p>
<p>Data Citation Synthesis Group. (2014). <em>Joint Declaration of Data Citation Principles</em>. Force11. <a href="https://doi.org/10.25490/A97F-EGYK">https://doi.org/10.25490/A97F-EGYK</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Webinar on Writing Zotero Translators]]></title>
        <id>570t0g8-rx6903v-4acvy97-arsmk</id>
        <link href="https://blog.front-matter.io/mfenner/webinar-on-writing-zotero-translators"/>
        <updated>2014-10-17T14:59:00.000Z</updated>
        <summary type="html"><![CDATA[In a blog post two weeks ago I argued for the need for reference managers to properly support data citation, if we want data citation to become a standard activity. I am happy to announce two events working towards that goal.November 3rd: Webinar on writing Zotero web translatorsSebastian Karcher,...]]></summary>
        <content type="html"><![CDATA[<p>In a <a href="http://sensiblescience.io/mfenner/please-keep-it-simple-citations-links-and-references/">blog post two weeks ago</a> I argued for the need for reference managers to properly support data citation, if we want data citation to become a standard activity. I am happy to announce two events working towards that goal.</p>
<h2 id="november-3rd-webinar-on-writing-zotero-web-translators">November 3rd: Webinar on writing Zotero web translators</h2>
<p><a href="https://www.zotero.org/blog/community-spotlight-sebastian-karcher/">Sebastian Karcher</a>, one of the most prolific authors of Zotero web translators (and citation styles), has kindly offered to hold an introductory webinar on writing Zotero web translators. These web translators allow Zotero to import metadata about a scholarly work from a variety of places, and new web translators for repositories that hold research data (or software) would go a long way towards making data citation easier for authors. <a href="https://www.zotero.org/support/dev/translators">Web translators</a> are written in Javascript and only basic Javascript knowledge is required. The free webinar takes place on November 3rd on 5 PM UK time (12 PM EST) and the registration form is <a href="http://www.eventbrite.com/e/writing-zotero-translators-webinar-tickets-13768797845">here</a>.</p>
<h2 id="january-11-force11-pre-conference-workshop-on-data-citation-support-in-reference-managers">January 11: Force11 Pre-Conference workshop on Data Citation Support in Reference Managers</h2>
<p><a href="https://www.force11.org/meetings/force2015/pre-conference-meeting-list">This workshop</a>, co-organized with Ian Mulvany, will extend the Zotero web translator work to other reference managers, including Papers and Mendeley. This will be a hackathon with the goal to get some things working in these reference managers, but it should also be interesting for others, as we will discuss what is missing to make data citation work in reference managers.</p>
<p>My personal goal is to learn to write a Zotero web translator in the webinar, and then write a working web translator for the three biological databases ENA, PDB and Uniprot before the January workshop. And hopefully these activities generate enough interest that other people write web translators for their favorite research data database or software repository, and that the proprietary reference managers Papers and Mendeley (and hopefully others) also add support for these data sources.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Let's do an unconference]]></title>
        <id>3ga89e2-k39eqvj-0db7m5r-9dhh</id>
        <link href="https://blog.front-matter.io/mfenner/lets-do-an-unconference"/>
        <updated>2014-10-14T15:02:00.000Z</updated>
        <summary type="html"><![CDATA[Photo by Anthony DELANOIX / UnsplashThis year’s SpotOn London conference takes place November 14-15 and the registration has opened this Monday. I am helping organize this conference since 2009, and I again look forward to the sessions,...]]></summary>
        <content type="html"><![CDATA[<figure>
<img src="https://images.unsplash.com/photo-1448906654166-444d494666b3?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEzfHxsb25kb258ZW58MHx8fHwxNjE4MTUzNDk3&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" class="kg-image" srcset="https://images.unsplash.com/photo-1448906654166-444d494666b3?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEzfHxsb25kb258ZW58MHx8fHwxNjE4MTUzNDk3&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=600 600w, https://images.unsplash.com/photo-1448906654166-444d494666b3?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEzfHxsb25kb258ZW58MHx8fHwxNjE4MTUzNDk3&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=1000 1000w, https://images.unsplash.com/photo-1448906654166-444d494666b3?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEzfHxsb25kb258ZW58MHx8fHwxNjE4MTUzNDk3&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=1600 1600w, https://images.unsplash.com/photo-1448906654166-444d494666b3?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDEzfHxsb25kb258ZW58MHx8fHwxNjE4MTUzNDk3&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2400 2400w" width="5587" height="3725" alt="Photo by Anthony DELANOIX / Unsplash" /><figcaption aria-hidden="true">Photo by <a href="https://unsplash.com/@anthonydelanoix?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">Anthony DELANOIX</a> / <a href="https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit">Unsplash</a></figcaption>
</figure>
<p>This year’s <a href="http://blogs.nature.com/ofschemesandmemes/2014/10/09/how-to-get-a-ticket-for-this-years-spoton-london">SpotOn London conference</a> takes place November 14-15 and the registration has opened this Monday. I am helping organize this conference since 2009, and I again look forward to the sessions, and - more importantly - the discussions with people in and between sessions this year.</p>
<p>The name (ScienceBlogging London, ScienceOnline London, SpotOn London), the location (Royal Institution, British Library, Wellcome Conference Center), the people organizing (too many to mention, but Nature Publishing Group always at the core), and the fringe events (lots of cool things from <a href="http://blog.mendeley.com/academic-life/science-blogging-2008-part-i/">science tours</a> to <a href="http://www.nature.com/spoton/event/spoton-london-2012-fringe-event-the-story-collider-2/">Story Collider</a>) and the format have always changed slightly over the years, and this year again is a bit different. The biggest change is obviously that <a href="https://twitter.com/louwoodley">Lou Woodley</a> is no longer an organizer (as she announced at last year’s conference), but this is also the first SpotOn conference with a theme:</p>
<blockquote>
The challenges of balancing the public and the private in the digital age
</blockquote>
<p>This is obviously a very broad topic, but nicely encompasses many important issues that we are dealing with in scholarly communication today. The draft program is posted <a href="http://blogs.nature.com/ofschemesandmemes/2014/10/13/spoton-london-2014-draft-programme">here</a>, and I’m helping organize the sessions on <strong><strong>sharing sensitive data</strong></strong> and <strong><strong>open peer review</strong></strong>. More details will follow for all these sessions.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/9th/2817131778_336979a571_z.jpg" class="kg-image" width="640" height="480" alt="Flickr photo by Duncan Hull" /><figcaption aria-hidden="true"><a href="https://www.flickr.com/photos/dullhunk/2817131778/">Flickr photo by Duncan Hull</a></figcaption>
</figure>
<p>The second day of the conference will be in unconference (or barcamp) format and the program drafted by the delegates in the morning. This format is popular in the science communications community (I first heard about the project that became my current job at <a href="https://editor.front-matter.io/lets-do-an-unconference/i-was-at-scibarcamp-palo-alto">SciBarCamp in 2009</a>), and SpotOn London has used this format in the first conference in 2008 (and again in 2009):</p>
<p>For people not familiar with this format the idea of a conference (day) without predetermined topics or speakers sounds scary. As it turns out, the problem is usually not the lack of ideas or people wanting to talk, but rather how to coordinate this in a way that everyone who wants to get involved can do so, and it doesn’t become a discussion among those with the loudest voices (and biggest egos). My experience with SpotOn London and other conferences I enjoyed is that the best sessions are usually those that allow for a good discussion, and not those with the most polished PowerPoint slides. Some suggestions for when you attend an unconference for the first time:</p>
<ul>
<li>go to sessions with topics you know little about, but want to learn more</li>
<li>when suggesting a session, do this together with others</li>
<li>suggest topics that are focussed and unusual, not the obvious ones we always talk about</li>
<li>don’t even think about doing a PowerPoint presentation</li>
<li>when moderating a session, be a good moderator, not a good speaker</li>
</ul>
<h2 id="further-reading">Further reading</h2>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Science_Foo_Camp">Wikipedia: SciFoo</a></li>
<li><a href="http://blogs.nature.com/nascent/2007/08/barcamb_cambridge.html">Ian Mulvany: BarCamp Cambridge 2007</a></li>
<li><a href="http://science.easternblot.net/?p=613">Eva Amsen: SciBarCamp Toronto 2008</a></li>
<li><a href="https://sensiblescience.io/mfenner/action_points">Me: BibCamp Hannover 2010</a></li>
</ul>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Please keep it simple: citations, links and references]]></title>
        <id>43rtncc-9rc9hr9-ff4ky6b-929y1</id>
        <link href="https://blog.front-matter.io/mfenner/please-keep-it-simple-citations-links-and-references"/>
        <updated>2014-10-01T15:06:00.000Z</updated>
        <summary type="html"><![CDATA[In my last post I wrote about the importance of keeping things simple in scholarly publishing, today I want to go into more detail with one example: citations in scholarly documents.LEGO scientists discuss how they can cite their dataCitations are an essential part of scholarly documents,...]]></summary>
        <content type="html"><![CDATA[<p>In my <a href="https://sensiblescience.io/mfenner/please-keep-it-simple/">last post</a> I wrote about the importance of keeping things simple in scholarly publishing, today I want to go into more detail with one example: citations in scholarly documents.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/11th/lego_discussion.jpg" class="kg-image" width="700" height="600" alt="LEGO scientists discuss how they can cite their data" /><figcaption aria-hidden="true">LEGO scientists discuss how they can cite their data</figcaption>
</figure>
<p>Citations are an essential part of scholarly documents, and they are summarized in the references section at the end of the article or book chapter. The problem is that not everything that is cited in a scholarly document ends up in the references list. Examples of this include:</p>
<ul>
<li>web links, e.g. to reagents or other resources</li>
<li>identifiers for biological databases such as GenBank that are typically included in the text as identifiers or as links</li>
<li>footnotes with links to external resources</li>
</ul>
<p>In other words: we are not consistent in how we cite other content. And this is a problem because we are making it more difficult than necessary for authors, publishers and everyone else to handle these various citation flavors and, more importantly, we are loosing citations along the way. This is a particular problem for data citation, as the seminal 2013 paper by <a href="https://doi.org/10.1371/journal.pone.0063184">Kafkas et al</a>. has shown for citations to the three biological databases ENA (European Nucleotide Archive), PDB and Uniprot:</p>
<ul>
<li>there is a large numbers of accession numbers in the Open Access subset of PubMed Central (e.g. 160,112 ENA accession numbers for papers published up until June 2012)</li>
<li>text mining using the <a href="http://www.ebi.ac.uk/webservices/whatizit/">Whatizit</a> tool can retrieve most of these identifiers</li>
<li>there is only partial overlap between database identifiers annotated by publishers and database identifiers found by text mining</li>
<li>the overlap is even smaller between papers citing database identifiers, and papers cited in biological databases such as ENA</li>
<li>the study was limited to Open Access journals, as only for them the full-text articles could be text mined</li>
</ul>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/18th/ena_overlap.png" class="kg-image" width="415" height="194" alt="Comparison between article-to-database and database to citations (Kafkas et al., 2013)." /><figcaption aria-hidden="true">Comparison between article-to-database and database to citations (Kafkas et al., 2013).</figcaption>
</figure>
<p>In other words, even though including identifiers for biological databases has been an accepted community standard that every author and publisher is following for a long time, the proper citation of these identifiers is still often broken. The picture doesn’t seem to be any better for DOIs for datasets: while they are fairly common by now, their use in scholarly articles differs widely from appearance in the references list to links in the materials and methods section to no mention at all.</p>
<p>There are various ways how this can be fixed (e.g. requiring authors to use biological database identifiers in a consistent way, better text mining tools, opening up subscription content to text mining), but the best solution is the simplest one: every citation in a paper should go into the references list. As an example I have added the ENA mRNA U65091 (Shioda, Fenner, &amp; Isselbacher, 1997) - something I worked on a long time ago - to the references list of this post.</p>
<h2 id="technology">Technology</h2>
<p>For this to work, it is essential that reference managers - the software authors use to generate the references list - properly support citations to data, including biological databases. It appears that all major reference managers support datasets as reference type and there is good community agreement what a data citation should look like (<a href="https://www.force11.org/datacitation">Joint Declaration of Data Citation Principles</a>). What is missing is support for easily importing the required metadata for these datasets, and reference managers use two approaches for this:</p>
<ul>
<li>query external databases via API and pull in the required metadata (e.g. Papers, Endnote)</li>
<li>browse to the webpage describing the database entry and import the metadata via bookmarklet/web importer (e.g. Zotero, Mendeley)</li>
</ul>
<p>Both approaches require custom code for every database. Whereas many reference managers use Citation Style Language (<a href="http://citationstyles.org/">CSL</a>) as a standard way to format references, no such standard exists for web importers. Which means that every reference manager has to implement this separately, and most of them are not open source software so that the community could help.</p>
<p>PLOS Labs is holding a <a href="http://www.ploslabs.org/citation-hackathon/">Citation Hackathon</a> on October 18 in their San Francisco office. While I can’t attend in person, I want to contribute to this hackathon in three ways:</p>
<ul>
<li>do an evaluation of how the reference managers Papers, Mendeley and Zotero (the three reference managers I use) support citations to the biological databases ENA, PDB and Uniprot and what is missing</li>
<li>look at existing aggregators of this information (e.g. <a href="http://identifiers.org/">Identifiers.org</a>) to figure out whether the import process can be simplified</li>
<li>start work on Zotero <a href="https://www.zotero.org/support/dev/translators/coding#web_translators">web translators</a> for these three databases. Zotero is open source software and the web translators are written in Javascript</li>
</ul>
<p>Please contact me if you are interested in helping with this, e.g. with a joint virtual hackathon on the 18th (or in person in London or Cambridge on October 15 if that works better).</p>
<p>Together with <a href="https://twitter.com/IanMulvany">Ian Mulvany</a> from eLife and others from Papers and Mendeley we have also submitted a proposal for a pre-conference workshop/hackathon for the <a href="https://www.force11.org/meetings/force2015">Force2015 Conference</a> in January to work on this for a broader set of databases, which should for example also include software repositories. One question is how we properly handle the citation of large numbers of datasets (1000s to millions), we could for example allow a range of identifiers in a citation. We also need tools to convert identifiers and links in existing documents to proper references, something that we <a href="https://sensiblescience.io/mfenner/citations-in-markdown-part-3/">have also discussed on this blog</a>, and we need to discuss how our bibliographic file formats (e.g. bibtex) support these citation types. I <a href="https://sensiblescience.io/mfenner/citeproc-yaml-for-bibliographies/">said before</a> that I am a big fan of Citeproc YAML (or JSON, the bibliographic format used by CSL) as bibliographic exchange format, and I know that the PLOS Labs hackathon will also touch on this.</p>
<h2 id="community">Community</h2>
<p>While adding reference manager support for a wider range of citations is the first step, the bigger challenge is community support. I don’t think that it is a big mental jump for an author to use the reference manager to cite a biological database rather than typing in the identifier directly in the text (the hard work is registering the identifier in the first place), but this needs support by the community, and in particular journal editors. The important message is that citations should be done in a consistent way and authors don’t have to think about doing this differently for datasets or other relevant resources, or different publishers implementing this differently. I think the paper by Kafkas et al. (2013) clearly shows that our current recommendations for adding identifiers to biological databases is broken, and that we need to do something if we take data citation seriously.</p>
<p>There are several concerns about adding every citation to the references list. One of them is that we shouldn’t mix citations of scholarly articles with citations of other things, e.g. research data. I would argue that not only are we seeing an increasing number of <a href="https://doi.org/10.1016/j.ipm.2011.10.002">citations to other resources in reference lists</a>, but that we can of course group citations by citation type, in addition to the sorting by appearance in the text or last name of first author that is common now.</p>
<p>Another concern is that citations of datasets are something else that citations to scholarly articles, because the former are typically citations of content created by the same group of people at the time the journal article was also created. I would argue that again we can highlight this by how we display the references, and that I hope that this changes once data citation becomes more widespread.</p>
<p>What should or should not be cited in a scholarly document is of course a big discussion topic. What I am arguing is that everything that is cited should go into the references list, but that doesn’t change at all what should be cited. Personal communications are an example of something that should probably not be cited and therefore should also not go into the references list.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Please keep it simple]]></title>
        <id>7gx62ee-0ek92cv-j7rgged-m1nmc</id>
        <link href="https://blog.front-matter.io/mfenner/please-keep-it-simple"/>
        <updated>2014-09-16T15:08:00.000Z</updated>
        <summary type="html"><![CDATA[Doing scientific research is becoming increasingly complex, both in terms of the tools and technologies used, and in the collaboration across disciplines and locations that is increasingly commonplace. While the way we write up and publish research is of course also very different from 25 years ago,...]]></summary>
        <content type="html"><![CDATA[<p>Doing scientific research is becoming increasingly complex, both in terms of the tools and technologies used, and in the collaboration across disciplines and locations that is increasingly commonplace. While the way we write up and publish research is of course also very different from 25 years ago, I would argue that our tools and services haven’t quite evolved at the same pace.</p>
<p>Of course there are important trends that enable what the Royal Institution <a href="https://royalsociety.org/policy/projects/science-public-enterprise/Report/">calls</a> <em>Science as an Open Enterprise</em>, most importantly Open Access, which has broken down many barriers for open collaboration. But very few organizations - commercial or non-profit - see it as their primary mission to make it easier for researchers to collaborate and produce great science, in the sense that everything else is secondary and this focus is really obvious to everyone.</p>
<p>The following are just some examples that make you laugh hard or cry out loud:</p>
<ul>
<li>Finding relevant scholarly content. Why is still so hard?</li>
<li>Reading a paper. The majority of scholalry content is still not Open Access. It is embarassing how difficult it can be to get the fulltext paper from a subscription journal - too slow, too expensive, and sometimes even crippled in functionality.</li>
<li>Creating figures for publication. This process is still so painful that it hurts. And publishers often create artificial limitations in file type (TIFF or Postscript) and file size (10 MB??).</li>
<li>Licenses for scholarly content. We don’t need choice, but a few licenses that everyone understands and that don’t hinder sharing and collaboration</li>
<li>Secure login. I can use my Facebook or Google login almost everywhere, but as a scholar I have a different username and password at my institution, funder, the various publishers I submit too, and the scholarly services I frequently use?</li>
<li>Citation styles. Why do we still have at least 3,000 styles?</li>
</ul>
<p>Citation styles is a perfect example of a problem that should have been solved as soon as we made the switch to digital publishing. I can travel through half of Europe without showing my passport, and using the same currency, but I need to reformat citations every time I submit to a different journal? And I have to use the same tool for this as my coauthors, as the different reference managers don’t work with each other?</p>
<p>Too often there are other intentions at work in parallel. While notable, they sometimes stand in conflict with the goal of making a researcher’s life easier. A perfect example is the manuscript submission process. In parallel to the tools getting better and easier to use, the demands on the author seem to be increasing at an even greater rate, both in the data and metadata he or she should provide, and in the work submitting authors are asked to do that traditionally have been done by publishers. Another good example are peer review and evaluation. The proportion of time spent doing research vs. time spent doing administrative work seems to decreasing and not increasing.</p>
<p>I wish more people and organizations would stand up and state that keeping it simple is their primary goal.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CommonMark and the Future of Scholarly Markdown]]></title>
        <id>1tsgzx6-pwm8j3a-k9tnpnw-3gg26</id>
        <link href="https://blog.front-matter.io/mfenner/commonmark-and-the-future-of-scholarly-markdown"/>
        <updated>2014-09-07T15:10:00.000Z</updated>
        <summary type="html"><![CDATA[One of the important outcomes of the Markdown for Science workshop that took place in June 2013 was a decision on a name - <em>Scholarly Markdown</em> - and a brief definition:Markdown that supports the requirements of scientific textsMarkdown as format...]]></summary>
        <content type="html"><![CDATA[<p>One of the important outcomes of the <a href="https://github.com/scholmd/scholmd/wiki">Markdown for Science</a> workshop that took place in June 2013 was a decision on a name - <em>Scholarly Markdown</em> - and a brief <a href="https://github.com/scholmd/scholmd/wiki/What-is-Markdown">definition</a>:</p>
<ol>
<li>Markdown that supports the requirements of scientific texts</li>
<li>Markdown as format that glues open scientific text resources together</li>
<li>A reference implementation with documentation and tests</li>
<li>A community</li>
</ol>
<p>In my eyes this is still a great definition. And this week something important happened that is very relevant for Scholarly Markdown. A small group of people deeply involved in Markdown announced <a href="http://commonmark.org/">Standard Markdown</a>:</p>
<blockquote>
We propose a standard, unambiguous syntax specification for Markdown, along with a suite of comprehensive tests to validate Markdown implementations against this specification. We believe this is necessary, even essential, for the future of Markdown.
</blockquote>
<p>Markdown is in widespread use, but a lack of standard syntax and set of comprehensive tests has hindered the adoption for more complex use cases, the development of cross-platform tools, and the use of markdown as a document interchange format. I am therefore 100% behind this initiative. In particular since this is not just an initiative by large commercial organizations heavily using Markdown such as Stack Exchange, Github or Reddit, but that the entire spec and both reference implementations have been written by <a href="http://johnmacfarlane.net/">John MacFarlane</a>, the author of Pandoc, the universal document converter. Not only does Pandoc already support many of the features required by Scholarly Markdown (e.g. math and citations), but John is the Chair of the Department of Philosophy at UC Berkeley.</p>
<p>Markdown was developed in 2004 by John Gruber, and he <a href="http://daringfireball.net/projects/markdown/license">holds the rights</a> to the name Markdown. He didn’t want this initiative to use the name <strong><strong>Standard Markdown</strong></strong>, so the implementation was <a href="http://blog.codinghorror.com/standard-markdown-is-now-common-markdown/">renamed</a> to <a href="http://commonmark.org/">CommonMark</a>.</p>
<p>The consequences of all this for Scholarly Markdown?</p>
<ul>
<li>CommonMark focusses on the basic features of the language, but once the specification is agreed upon and implemented by a critical mass of tools, it is clear that there needs to be a standardized way to handle extensions of the language. This is both about features used by lots of people such as tables, but also functionality relevant only for scholarly content.</li>
<li>This brings us one gigantic step closer to a reference implementation and set of tests for Scholarly Markdown, as hopefully Scholarly Markdown can build upon the work by John and the CommonMark team.</li>
<li>The name Scholarly Markdown might not be a good idea going forward. We should either change the name to align with CommonMark, or we should come up with a totally different name, something that the screenwriters have done with <a href="http://fountain.io/">their</a> implementation of Markdown.</li>
</ul>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Microsoft Word with git]]></title>
        <id>6wp2f0b-rh198hs-assyeb7-04wbc</id>
        <link href="https://blog.front-matter.io/mfenner/using-microsoft-word-with-git"/>
        <updated>2014-08-25T15:12:00.000Z</updated>
        <summary type="html"><![CDATA[One of the major challenges of writing a journal article is to keep track of versions - both the different versions you create as the document progresses, and to merge in the changes made by your collaborators. For most academics Microsoft Word is the default writing tool,...]]></summary>
        <content type="html"><![CDATA[<p>One of the major challenges of writing a journal article is to keep track of versions - both the different versions you create as the document progresses, and to merge in the changes made by your collaborators. For most academics Microsoft Word is the default writing tool, and it is both very good and very bad in this. Very good because the <em>track changes</em> feature makes it easy to see what has changed since the last version and who made the changes. Very bad because this feature is built around keeping everything in a single Word document, so that only one person can work on on a manuscript at a time. This usually means sending manuscripts around by email, and being very careful about not confusing different versions of the document, which requires <a href="http://www.phdcomics.com/comics/archive.php?comicid=1531">creativity</a>.</p>
<p>Approaches to overcome these challenges are to a) integrate the Word documents into collaboration tools such as Sharepoint and Office 365, or document sharing services such as Dropbox and Google Docs (if you use it just for that), or b) use a different authoring tool altogether. If neither of these approaches works for you, you have a third option: use the version control system <strong><strong>git</strong></strong>.</p>
<p><a href="http://www.mulvany.net/presentations/WikimaniaOpenScholarshipTalk.slides.html#/3">Git</a> is software that helps with <a href="https://git-scm.com/book/en/Getting-Started-About-Version-Control">tracking changes to files</a> so that you can recall specific versions later. Git is typically used to track changes of software source code (and was originally developed by Linus Torvalds for Linux kernel development in 2005), but in fact git can be used for any file where we need to keep track of versions over time. Git is open source software that runs locally on your computer, so please go ahead and start tracking changes to your manuscripts (or other complex documents) with git. Any time you want to store a version, do a <code>git commit</code> with a little description and an optional tag.</p>
<p>This approach is not ideal, as git was written with source code in text format in mind and for example doesn’t understand what has changed between two revisions of a Word document. Some people will tell you to never store binary files in a version control system, but don’t listen to them. Instead give git a tool to convert Word documents into plain text, and git will then happily tell you what has changed between revisions. Several tools can do this, but since earlier this month Pandoc can read Word documents in <code>docx</code> format. Do the following to have Pandoc convert Word documents into markdown, and to compare the revisions by word and not by line (which makes more sense):</p>
<pre><code># .gitattributes file in root folder of your git project
*.docx diff=pandoc</code></pre>
<pre><code># .gitconfig file in your home folder
[diff &quot;pandoc&quot;]
  textconv=pandoc --to=markdown
  prompt = false
[alias]
  wdiff = diff --word-diff=color --unified=1</code></pre>
<p>You can then use <code>git wdiff important_file.docx</code> to see the changes (with deletions in red and insertions in green), or <code>git log -p --word-diff=color important_file.docx</code> to see all changes over time.</p>
<p>While you can now track revisions of a Word document and see the changes, you also want to be able to merge different versions of a Word document together so that you and your collaborators can work on the manuscript in parallel. Git can’t merge binary files together, so you need to first convert the Word document into a format that git understands. Just as in the previous example we can use Pandoc for that, with markdown as the textual format. This would also work with HTML or LaTeX, but the simplicity of markdown makes it better suited for version control which doesn’t know about the markup of these formats.</p>
<p>One of the reasons that git became so popular with software developers is that it is a <strong><strong>distributed version control system</strong></strong> instead of a centralized system such as Subversion. This means that you can track all revisions locally on your computer, but can still synchronize your revisions with another user. <strong><strong>Github</strong></strong> is a popular service that facilitates this synchronization and adds some nice features on top. One way to collaborate with your co-authors is therefore to set up a Github repository (public or private) for your manuscript, and store the master version of the manuscript in markdown format. Instead of working on the master version directly, you would use Pandoc to convert back and forth between this master version in markdown format and your Word document, and would continue to use Word as authoring tool. <a href="https://sensiblescience.io/mfenner/introducing-rakali/">Rakali</a> is a Pandoc tool that I released last week that can help automate this document conversion. Github has a a number of features to facilitate collaboration that can be used here, e.g. Github issues for discussion and task management.</p>
<p>There are still a few rough edges in the workflow described above (e.g. only partial support of Word track changes), but it is an interesting approach to collaborate using Microsoft Word and git. And this workflow can of course be enhanced to also include authors that write in LaTeX or one of the other formats that Pandoc supports. One nice side effect of using markdown is that Github will automatically render a webpage for the document (which it will not do for HTML without extra effort).</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing Rakali]]></title>
        <id>3d4ea7m-rer832b-082hq98-dngyh</id>
        <link href="https://blog.front-matter.io/mfenner/introducing-rakali"/>
        <updated>2014-08-18T15:16:00.000Z</updated>
        <summary type="html"><![CDATA[In July and August I attended the Open Knowledge Festival and Wikimania. At both events I had many interesting discussions around open source tools for open access scholarly publishing, and I was part of a panel on that topic at Wikimania last Sunday....]]></summary>
        <content type="html"><![CDATA[<p>In July and August I attended the <a href="http://2014.okfestival.org/">Open Knowledge Festival</a> and <a href="http://wikimania2014.wikimedia.org/wiki/Programme">Wikimania</a>. At both events I had many interesting discussions around open source tools for open access scholarly publishing, and I was part of a <a href="http://wikimania2014.wikimedia.org/wiki/Submissions/The_Full_OA_Stack_-_Open_Access_and_Open_Source">panel</a> on that topic at Wikimania last Sunday. Some of my thoughts were summarized in a blog post a few weeks ago (<a href="https://sensiblescience.io/mfenner/roads-not-stagecoaches/">Build Roads not Stagecoaches</a>). Today I am happy to announce the first public release of a tool that hopefully contributes to making publishing of open content a bit easier.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/rakali.jpg" class="kg-image" width="1000" height="750" alt="LEGO Researchers are excited that they don’t have to use Microsoft Word for manuscript writing anymore." /><figcaption aria-hidden="true">LEGO Researchers are excited that they don’t have to use Microsoft Word for manuscript writing anymore.</figcaption>
</figure>
<p><a href="https://github.com/rakali/rakali.rb">Rakali</a> is a Ruby gem that acts as a wrapper for the <a href="https://pandoc.org">Pandoc</a> universal document converter. Pandoc is a wonderful tool to convert documents between file formats and supports many file formats and features important for scholarly publishing. Pandoc 1.13 was <a href="http://johnmacfarlane.net/pandoc/releases.html">released</a> last Friday, and one of the most exciting new features is a reader for Microsoft Word (<code>docx</code>) documents. Pandoc has supported the conversion to <code>docx</code> for a while, but now you can use the most popular file format for writing scholarly documents and turn your <code>docx</code> files into HTML, PDF, LateX, markdown, or a number of other formats, making it much easier to collaborate, and to use <code>docx</code> with Pandoc in scholarly publishing workflows. A good example would be arXiv, which <a href="http://arxiv.org/help/submit#text">doesn’t support</a> <code>docx</code> for text submissions. Instead of turning it into PDF the manuscript can now be converted to LaTeX - the preferred file format at arXiv - before submission.</p>
<p>I built <strong><strong>Rakali</strong></strong> to make it easier to use Pandoc to convert large numbers of documents in an automated way:</p>
<ul>
<li>bulk conversion of all files in a folder with a specific extension, e.g. <code>md</code>.</li>
<li>input via a configuration file in yaml format instead of via the command line</li>
<li>validation of documents via <a href="http://json-schema.org/">JSON Schema</a>, using the <a href="https://github.com/hoxworth/json-schema">json-schema</a> Ruby gem.</li>
<li>Logging via <code>stdout</code> and <code>stderr</code>.</li>
</ul>
<p>One interesting way to use Rakali and Pandoc is as part of a <a href="https://sensiblescience.io/mfenner/continuous-publishing/">continuous publishing</a> workflow that involves git and Github, automatically converting all files in a folder when something is pushed to the repository using a continuous integration tool, and exiting the continuous integration run when one of the files doesn’t validate. Look into the Rakali <a href="https://github.com/rakali/rakali.rb">repo</a> for an example.</p>
<p>The most interesting aspect of Rakali is probably validation via JSON Schema. File conversion with Pandoc is a two-step process, the intermediate format is an internal representation of the document in something called the <a href="https://sensiblescience.io/mfenner/the-grammar-of-scholarly-communication/">abstract syntax tree</a> or AST. Pandoc makes the AST accessible in JSON format, making it straightforward to manipulate a document before the conversion into the target format with something called <a href="http://johnmacfarlane.net/pandoc/scripting.html">JSON filters</a>.</p>
<p>Validation of XML documents using <a href="https://en.wikipedia.org/wiki/Document_type_definition">DTDs</a>, <a href="http://relaxng.org/">RELAX NG</a> and other standards has of course been around for a long time, but validation of JSON documents is still relatively new. Since many Pandoc document conversion workflows don’t involve any XML I thought it would make more sense to validate against the AST, and we can use JSON Schema for that. I have started a <a href="https://github.com/rakali/pandoc-schemata">Github repository</a> with schemata for the Pandoc AST, and hope to evolve them over time using Rakali as a tool. An example log output (from the Rakali test suite, stopping file conversion because title and layout metadata are missing) looks like this:</p>
<pre><code>Validation Error: The property &#39;#/0/unMeta&#39; did not contain a required property of &#39;title&#39; in schema 9b6d454d-e609-537b-b761-9599b6c01072# for file empty.md
Validation Error: The property &#39;#/0/unMeta&#39; did not contain a required property of &#39;layout&#39; in schema 9b6d454d-e609-537b-b761-9599b6c01072# for file empty.md
Fatal: Conversion of file empty.md failed.</code></pre>
<p>As I had argued before, the challenge for building open source tools for science is to <a href="https://sensiblescience.io/mfenner/dont-reinvent-the-wheel/">not duplicate the work of others</a>, and to integrate well with existing tools by focussing on one aspect and doing that aspect well. It also helps to think about infrastructure (<a href="https://sensiblescience.io/mfenner/roads-not-stagecoaches/">the roads</a>) instead of only focussing on the user-facing aspects. There are obviously many document conversion tools out there, but Pandoc is certainly one of the oldest and most established ones for scholarly content. Rakali therefore builds on top of Pandoc and tries to play well with other existing tools and services, e.g. by using the UNIX <code>stdout</code> and <code>stderr</code> for reporting, and by using a file-based approach that works well with version control systems such as git. And since Rakali is a Ruby gem it can not only be used as a standalone command line tool, but can also be easily integrated into other Ruby applications.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visualizing Scholarly Content]]></title>
        <id>6bb7e5f-saq8e5b-gz8x18k-b0vx1</id>
        <link href="https://blog.front-matter.io/mfenner/visualizing-scholarly-content"/>
        <updated>2014-08-09T15:19:00.000Z</updated>
        <summary type="html"><![CDATA[One topic I will cover this Sunday in a presentation on Open Scholarship Tools at <em>Wikimania 2014</em> together with Ian Mulvany is visualization.Data visualization is all about <em>telling stories with data</em>, something that is of course not only important for scholarly content,...]]></summary>
        <content type="html"><![CDATA[<p>One topic I will cover this Sunday in a presentation on <a href="http://wikimania2014.wikimedia.org/wiki/Submissions/Open_Scholarship_Tools_-_a_whirlwind_tour.">Open Scholarship Tools</a> at <em>Wikimania 2014</em> together with <a href="https://twitter.com/ianmulvany">Ian Mulvany</a> is visualization.</p>
<p>Data visualization is all about <em>telling stories with data</em>, something that is of course not only important for scholarly content, but for example increasingly common in journalism. This is a big and complex topic, but I hope the following will get you started.</p>
<h3 id="learn-the-basics">Learn the Basics</h3>
<p>Work on visualization of scientific data should start with a good understanding of the best practices and pitfalls of data visualization in general, as well as the specific aspects of visualizing scientific data. The following resources have helped me get started - please suggest more in the comments:</p>
<ul>
<li><a href="http://book.flowingdata.com/">Visualize this</a>. A book from Nathan Yau published in 2011. Very helpful in understanding the different ways data can be visualized (e.g. when to use a treemap or what is a <a href="https://en.wikipedia.org/wiki/Choropleth_map">chloropleth map</a>), and an introduction to some tools using practical examples. Nathan’s <a href="http://flowingdata.com/">FlowingData</a> blog is also a great resource.</li>
<li><a href="https://github.com/mbostock/d3/wiki/Gallery">D3 Gallery</a>. Lots of examples generated using Mike Bostock’s d3.js visualization library. A great inspiration for data visualization on the web, even if you use a different visualization tool.</li>
<li><a href="http://docs.ggplot2.org/current/index.html">ggplot2</a>. Not only a very popular visualization library for the R language by Hadley Wickham, but also an implementation of Leland Wilkison’s Grammar of Graphics. The <a href="http://www.springer.com/statistics/computational+statistics/book/978-0-387-98140-6">ggplot2 book</a> describes this powerful concept (p. 14):</li>
</ul>
<blockquote>
In brief, the grammar tells us that a statistical graphic is a mapping from data to aesthetic attributes (colour, shape, size) of geometric objects (points, lines, bars). The plot may also contain statistical transformations of the data and is drawn on a specific coordinate system. Faceting can be used to generate the same plot for different subsets of the dataset. It is the combination of these independent components that make up a graphic.
</blockquote>
<h3 id="learn-to-use-at-least-one-visualization-tool">Learn to use at least one visualization tool</h3>
<p>There are many great tools available, pick one and learn it well. Some options include:</p>
<ul>
<li><strong><strong>Excel</strong></strong>. Probably the most popular tool for data visualization. Commercial, with open source alternatives such as Libre Office.</li>
<li><strong><strong>R</strong></strong>. Software for statistical computing and analysis. Open source. <a href="http://www.rstudio.com/">RStudio</a> is a powerful user interface for R and a good way to get started.</li>
<li><a href="http://d3js.org/"><strong>d3.js</strong></a>. A visualization library for Javascript. Open source.</li>
<li><a href="http://www.graphpad.com/scientific-software/prism/"><strong>Prism</strong></a>. A popular visualization tool among scientists. Commercial.</li>
<li><a href="https://datawrapper.de/"><strong>Datawrapper</strong></a>. An open source tool and hosted service for data visualization.</li>
</ul>
<p>I do most visualizations in either R or d3.js. Both are open source tools with a large community and a rich set of libraries, examples and documentation, and both take a systematic approach to data visualization (see grammar of graphics above).</p>
<h3 id="learn-data-analysis">Learn data analysis</h3>
<p>Unless your interest is more in information design - see <a href="http://www.informationisbeautiful.net/">Information is beautiful</a> for some great examples - data visualization is tightly coupled with data analysis. You need to know at least the basics of data analysis to do proper data visualizations, e.g. how to handle wrongly formatted data (e.g. text in a number column), missing values and outliers. The most time-consuming step in my experience is data transformation, i.e. bringing data into the format that you want for the analysis and visualization.</p>
<p>R, Python and the relatively new <a href="http://julialang.org/">Julia</a> are popular languages for data analysis available as open source. There are many packages for these languages that help with common data analysis problems. One additional advantage of using a proper language over a set of tools cobbled together is that it is easy to automatically recreate a visualization with a new set of data - convenient when you need to analyze and visualize an ongoing experiment that repeatedly produces new data.</p>
<h3 id="use-a-vector-file-format">Use a vector file format</h3>
<p>Too many scientific data are still visualized using bitmap graphic formats such as <code>tiff</code>, <code>jpg</code> and <code>png</code>. These formats are not appropriate for charts and only make sense for images. They don’t scale to the screen resolution, and it is <a href="http://blog.f1000research.com/2014/02/20/the-importance-of-providing-data-and-not-just-images-of-data/">very hard to impossible</a> to reuse or even modify them. Use vector graphic formats such as <code>svg</code> or <code>pdf</code> instead. <code>svg</code> is my preferred format because in contrast to <code>pdf</code> it can be embedded into a larger HTML document, and R and d3.js (my preferred visualization tools) can generate this format. <a href="http://www.inkscape.org/">Inkscape</a> is an open source SVG editor, and the commercial <strong><strong>Adobe Illustrator</strong></strong> can be used to manually polish graphics in <code>svg</code> or <code>pdf</code> format, e.g. for journal publication.</p>
<h3 id="get-inspired-by-great-visualizations">Get inspired by great visualizations</h3>
<p>At the end of the day data visualization is all about telling a story with data. Unfortunately the current state of affairs for scientific visualizations is very different. In my opinion most graphs and figures used in publications don’t provide the data underlying the visualization (<a href="https://datawrapper.de/">Datawrapper</a> is a great example how this can be done), focus too much on detail rather than the overall message, don’t take advantage of the different chart types available, and are sometimes even misleading. And I’m not even talking about the fact that figures in scholarly papers are <a href="https://doi.org/10.12688/f1000research.4263.1">almost never</a> interactive. It rarely happens that I read a paper and get excited by looking at a figure - if I do it is usually because the underlying data are so compelling that even the simplest visualization will convey the right message.</p>
<p>We should become more creative with visualizing data in scholarly documents, and one important step towards that goal is publishers accepting more reasonable file formats in manuscript submissions - instead of just <code>tiff</code> and <code>eps</code> (<a href="http://www.plosone.org/static/figureGuidelines#figures">PLOS</a>), or <code>tiff</code>, <code>eps</code> and <code>pdf</code> (<a href="http://www.sciencemag.org/site/feature/contribinfo/prep/prep_revfigs.xhtml#format">Science</a>), and often with a 10 MB file site limit.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is a DOI?]]></title>
        <id>1cn8kzx-g3986gb-xmmk3at-n5gb3</id>
        <link href="https://blog.front-matter.io/mfenner/what-is-doi"/>
        <updated>2014-08-06T15:24:00.000Z</updated>
        <summary type="html"><![CDATA[This Sunday Ian Mulvany and I will do a presentation on Open Scholarship Tools at <em>Wikimania 2014</em> in London. From the abstract:This presentation will give a broad overview of tools and standards that are helping with Open Scholarship today.One...]]></summary>
        <content type="html"><![CDATA[<p>This Sunday <a href="https://twitter.com/ianmulvany">Ian Mulvany</a> and I will do a presentation on <a href="http://wikimania2014.wikimedia.org/wiki/Submissions/Open_Scholarship_Tools_-_a_whirlwind_tour.">Open Scholarship Tools</a> at <em>Wikimania 2014</em> in London. From the abstract:</p>
<blockquote>
This presentation will give a broad overview of tools and standards that are helping with Open Scholarship today.
</blockquote>
<p>One of the four broad topics we have picked are <em>digital object identifiers (DOI)s</em>. We want to introduce them to people new to them, and we want to show some tricks and cool things to people who already now them. Along the way we will also try to debunk some myths about DOIs.</p>
<h3 id="what-a-doi-looks-like">What a DOI looks like</h3>
<p>DOIs - or better DOI names - start with a prefix in the format <code>10.x</code> where x is 4-5 digits. The suffix is determined by the organization registering the DOI, and there is no consistent pattern across organizations. The DOI name is typically expressed as a URL (see below). An example DOI would look like: <a href="http://dx.doi.org/10.5555/12345678">http://dx.doi.org/10.5555/12345678</a>. Something in the format <strong><strong>10/hvx</strong></strong> or <a href="http://doi.org/hvx">http://doi.org/hvx</a> is a <a href="http://shortdoi.org/">shortDOI</a>, and <strong><strong>1721.1/26698</strong></strong> or <a href="http://hdl.handle.net/1721.1/26698">http://hdl.handle.net/1721.1/26698</a> is a handle. BTW, all DOIs names are also handles, so <a href="http://hdl.handle.net/10/hvx">http://hdl.handle.net/10/hvx</a> for the shortDOI example above will resolve correctly.</p>
<h3 id="dois-are-persistent-identifiers">DOIs are persistent identifiers</h3>
<p>Links to resources can change, particularly over long periods of time. Persistent identifiers are needed so that readers can still find the content we reference in a scholarly work (or anything else where persistent linking is important) 10 or 50 years later. There are many kinds of persistent identifiers, one of the key concepts - and a major difference to URLs - is to separate the identifier for the resource from its location. Persistent identifiers require technical infrastructure to resolve identifiers (DOIs use the <a href="http://www.handle.net/">Handle System</a>) and to allow long-term archiving of resources. DOI registration agencies such as DataCite or CrossRef are required to provide that persistence. Other persistent identifier schemes besides DOIs include <a href="http://en.wikipedia.org/wiki/PURL">persistent uniform resource locators (PURLs)</a> and <a href="http://en.wikipedia.org/wiki/Archival_Resource_Key">Archival Resource Keys (ARKs)</a>.</p>
<h3 id="dois-have-attached-metadata">DOIs have attached metadata</h3>
<p>All DOIs have metadata attached to them. The metadata are supplied by the resource provider, e.g. publisher, and exposed in services run by registration agencies, for example metadata search and content negotiation (see below). There is a minimal set of required metadata for every DOI, but beyond that, different registration agencies will use different metadata schemata, and most metadata are optional. Metadata are important to build centralized discovery services, making it easier to describe a resource, e.g. journal article citing another article. Some of the more recent additions to metadata schemata include persistent identifiers for people (<a href="http://orcid.org/">ORCID</a>) and funding agencies (<a href="http://www.crossref.org/fundref/">FundRef</a>), and license information. The following API call will retrieve all publications registered with CrossRef that use a <a href="http://creativecommons.org/licenses/by/3.0/deed.en_US">Creative Commons Attribution license</a> (and where this information has been provided by the publisher):</p>
<pre><code>http://api.crossref.org/funders/10.13039/100000001/works?filter=license.url:http://creativecommons.org/licenses/by/3.0/deed.en_US</code></pre>
<h3 id="dois-support-link-tracking">DOIs support link tracking</h3>
<p>Links to other resources are an important part of the metadata, and describing all citations between a large number scholarly documents is a task that can only really be accomplished by a central resource. To solve this very problem DOIs were invented and the CrossRef organization started around 15 years ago.</p>
<h3 id="not-every-doi-is-the-same">Not every DOI is the same</h3>
<p>The DOI system <a href="http://www.doi.org/doi_handbook/1_Introduction.html">originated from an initiative by scholarly publishers</a> (first announced at the Frankfurt Book Fair in 1997), with citation linking of journal articles its first application. This citation linking system is managed by <a href="http://www.crossref.org/">CrossRef</a>, a non-profit member organization of scholarly publishers, and <a href="http://search.crossref.org/help/status">more than half</a> of the about <a href="http://www.doi.org/faq.html">100 million DOIs</a> that have been assigned to date are managed by them.</p>
<p>But many DOIs are assigned by one of the other 8 <a href="http://www.doi.org/RA_Coverage.html">registration agencies</a>. You probably know <a href="http://www.datacite.org/">DataCite</a>, but did you know that the <a href="http://publications.europa.eu/index_en.htm">Publications Office of the European Union (OP)</a> and the <a href="http://www.eidr.org/">Entertainment Identifier Registry (EIDR)</a> also assign DOIs? The distinction is important, because some of the functionality is a service of the registration agency - metadata search for example is offered by CrossRef (<a href="http://search.crossref.org/">http://search.crossref.org</a>) and DataCite (<a href="http://search.datacite.org/">http://search.datacite.org</a>), but you can’t search for a DataCite DOI in the CrossRef metadata search. There is an API to find out the registration agency behind a DOI so that you know what services to expect:</p>
<pre><code>http://api.crossref.org/works/10.6084/m9.figshare.821213/agency

{
  &quot;status&quot;: &quot;ok&quot;,
  &quot;message-type&quot;: &quot;work-agency&quot;,
  &quot;message-version&quot;: &quot;1.0.0&quot;,
  &quot;message&quot;: {
    &quot;DOI&quot;: &quot;10.6084/m9.figshare.821213&quot;,
    &quot;agency&quot;: {
      &quot;id&quot;: &quot;datacite&quot;,
      &quot;label&quot;: &quot;DataCite&quot;
    }
  }
}</code></pre>
<h3 id="dois-are-urls">DOIs are URLs</h3>
<p><a href="http://www.doi.org/faq.html">DOI names may be expressed as URLs (URIs) through a HTTP proxy server</a> - e.g. <a href="http://dx.doi.org/10.5555/12345679">http://dx.doi.org/10.5555/12345679</a>, and this is how DOIs are typically resolved. For this reason the <a href="http://www.crossref.org/02publishers/doi_display_guidelines.htm">CrossRef DOI Display Guidelines</a> recommend that <em>CrossRef DOIs should always be displayed as permanent URLs in the online environment</em>. Because DOIs can be expressed as URLs, they also have their features:</p>
<h4 id="special-characters">Special characters</h4>
<p>Because DOIs can be expressed as URLs, DOIs <a href="http://www.crossref.org/02publishers/15doi_guidelines.html">should only include characters allowed in URLs</a>, something that wasn’t always true in the past and can cause problems, e.g. when using SICIs (<a href="https://en.wikipedia.org/wiki/Serial_Item_and_Contribution_Identifier">Serial Item and Contribution Identifier</a>), an extension of the ISSN for journals:</p>
<pre><code>10.4567/0361-9230(1997)42:&lt;OaEoSR&gt;2.0.TX;2-B</code></pre>
<h4 id="content-negotiation">Content negotiation</h4>
<p>The DOI resolver at <em>doi.org</em> (or <em>dx.doi.org</em>) normally resolves to the resource location, e.g. a landing page at a publisher website. Requests that are not for content type <code>text/html</code> are redirected to the registration agency metadata service (currently for CrossRef, DataCite and mEDRA DOIs). Using <a href="http://www.crosscite.org/cn/">content negotiation</a>, we can ask the metadata service to send us the metadata in a format we specify (e.g. Citeproc JSON, bibtex or even a formatted citation in one of thousands of citation styles) instead of getting redirected to the resource. This is a great way to collect bibliographic information, e.g. to format citations for a manuscript. In theory we could also use content negotiation to get a particular representation of a resource, e.g. <code>application/pdf</code> for a PDF of a paper or <code>text/csv</code> for a dataset in CSV format. This is not widely support and I don’t know the details of the implementation in the DOI resolver, but you can try this (content negotation is easier with the command line than with a browser):</p>
<pre><code>curl -LH &quot;Accept: application/pdf&quot; http://dx.doi.org/10.7717/peerj.500 &gt;peerj.500.pdf</code></pre>
<p>This will save the PDF of the 500th PeerJ paper published last week.</p>
<h4 id="fragment-identifiers">Fragment identifiers</h4>
<p>As discussed in <a href="http://sensiblescience.io/mfenner/fragment-identifiers-and-dois/">my last blog post</a>, we can use fragment identifiers to subsections of a document with DOIs, e.g. <a href="http://dx.doi.org/10.1371/journal.pone.0103437#s2">http://dx.doi.org/10.1371/journal.pone.0103437#s2</a> or <a href="http://doi.org/10.5446/12780#t=00:20,00:27">http://doi.org/10.5446/12780#t=00:20,00:27</a>, just as we can with every other URL. This is a nice way to directly link to a specific document section, e.g. when discussing a paper on Twitter. Fragment identifiers are implemented by the client (typically web browser) and depend on the document type, but for DOIs that resolve to full-text HTML documents they can add granularity to the DOI without much effort.</p>
<h4 id="queries">Queries</h4>
<p>URLs obviously support queries, but that is a feature I haven’t yet seen with DOIs. Queries would allow interesting features, partly overlapping with what is possible with fragment identifiers and content negotiation, e.g. <code>http://dx.doi.org/10.7717/peerj.500?format=pdf</code>. II hope to find out more until Sunday.</p>
<h3 id="outlook">Outlook</h3>
<p>My biggest wish? Make DOIs more machine-readable. They are primarily intended for human users, enabling them to find the content associated with a DOI. But they sometimes don’t work as well as they could with automated tools, one example are the <a href="http://sensiblescience.io/mfenner/broken-dois/">challenges automatically resolving a DOI</a> that I described in a blog post last year. Thinking about DOIs as URLs - and using them this way - is the right direction.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fragment Identifiers and DOIs]]></title>
        <id>1krynvj-yas9z7s-6t2ph0n-8tm3p</id>
        <link href="https://blog.front-matter.io/mfenner/fragment-identifiers-and-dois"/>
        <updated>2014-08-02T15:26:00.000Z</updated>
        <summary type="html"><![CDATA[Before all our content turned digital, we already used <strong><strong>page numbers</strong></strong> to describe a specific section of a book or longer document, with older manuscripts using the folio before that....]]></summary>
        <content type="html"><![CDATA[<p>Before all our content turned digital, we already used <strong><strong>page numbers</strong></strong> to describe a specific section of a book or longer document, with older manuscripts using the <a href="https://en.wikipedia.org/wiki/Folio">folio</a> before that. Page numbers have transitioned to electronic books with readers such as the Kindle <a href="http://pogue.blogs.nytimes.com/2011/02/08/page-numbers-for-kindle-books-an-imperfect-solution/?_php=true&amp;_type=blogs&amp;_r=0">supporting them eventually</a>.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/Folio_(number).jpg" class="kg-image" width="746" height="543" alt="Image by Al Silonov from Wikimedia Commons. This file is licensed under the Creative Commons Attribution-Share Alike 3.0 Unported license." /><figcaption aria-hidden="true">Image by Al Silonov from <a href="http://commons.wikimedia.org/wiki/File:Folio_(number).jpg">Wikimedia Commons</a>. This file is licensed under the <a href="http://creativecommons.org/licenses/by-sa/3.0/deed.en">Creative Commons Attribution-Share Alike 3.0 Unported</a> license.</figcaption>
</figure>
<p>For content on the web we can use the <code>#</code> fragment identifier, e.g. <a href="https://en.wikipedia.org/wiki/Fragment_identifier#Proposals">https://en.wikipedia.org/wiki/Fragment_identifier#Proposals</a> to navigate to a specific section of a web page. How the linking to this fragment is handled, depends on the <strong><strong>MIME</strong></strong> type of the document, and will for example be done differently for a text page than a video - YouTube understands minutes and seconds into a video as fragment identifier, e.g. <a href="https://www.youtube.com/watch?v=0UNRZEsLxKc#t=54m52s">https://www.youtube.com/watch?v=0UNRZEsLxKc#t=54m52s</a>. Fragment identifiers are not only helpful to link to a subsection of a document, but of course also for navigation within a document.</p>
<p>All this is of course very relevant to scholarly content, which is usually much more structured, with most journal articles following the <a href="https://en.wikipedia.org/wiki/IMRAD">IMRAD</a> - introduction, methods, results, and discussion - format, usually with additional sections such as abstract, references, etc. One approach to link to figures and tables within a scholarly articles is using <a href="https://blog.martinfenner.org/posts/direct-links-to-figures-and-tables-using-component-dois/">component DOIs</a>, e.g. specific DOIs for parts of a larger document. The publisher <strong><strong>PLOS</strong></strong> has been using them for a long time, and the <a href="https://editor.front-matter.io/2014/07/24/dont-reinvent-the-wheel/">number of component DOIs is rising</a>, but most scholarly journal articles don’t use component DOIs. And whereas component DOIs are a great concept for content such as figures (allowing us to describe the MIME type and other relevant metadata), they are probably not the best tool to link to a section or paragraph of a scholarly document.</p>
<p>As it turns out, we already have a tool for that, as the DOI proxy server gracefully forwards fragment identifiers (how did I miss this?). We can therefore use a DOI with a fragment identifier to</p>
<ul>
<li>Results section: <a href="https://doi.org/10.1371/journal.pone.0103437#s2">http://doi.org/10.1371/journal.pone.0103437#s2</a></li>
<li>Specific reference: <a href="https://doi.org/10.12688/f1000research.4263.1#ref-7">http://doi.org/10.12688/f1000research.4263.1#ref-7</a></li>
<li>Decision letter: <a href="https://doi.org/10.7554/eLife.00471#decision-letter">http://doi.org/10.7554/eLife.00471#decision-letter</a></li>
</ul>
<p>Obviously this only works if the DOI is resolved to the full-text of a resource, and not a landing page. And how the fragment identifiers are named and implemented is up to the publisher, and the DOI resolver has no information about them. These specific links are particularly nice for discussions of a paper, whether it is on Twitter or in a discussion forum. It appears that at least the Twitter link shortener keeps the fragment identifier, the link to the eLife decision letter is shortened to <a href="http://t.co/URWaYmGHnY">http://t.co/URWaYmGHnY</a>. This kind of linking works particularly well if the publisher is using a fine-grained system of fragment identifiers, the publisher PeerJ for example allows links to a specific paragraph - e.g. <a href="http://doi.org/10.7717/peerj.500#p-15">http://doi.org/10.7717/peerj.500#p-15</a> - and allows users to <a href="http://blog.peerj.com/post/62886292466/peerj-questions-a-new-way-to-never-publish-forget">ask a question</a> right next to that section.</p>
<p>The examples above all use MIME type <code>text/html</code>, as this is what the example DOIs resolve to by default. I don’t if and how publishers have implemented fragment identifiers for other formats such as PDF or ePub, and what happens if you combine fragment identifiers with <a href="http://www.crosscite.org/cn/">content negotiation</a>. The shortDOI service works with fragment identifiers as well: <a href="http://doi.org/pxd#decision-letter">http://doi.org/pxd#decision-letter</a>. Another interesting question would be how fragment identifiers are handled for datasets. Typically separate DOIs are assigned for multiple related datasets, but there could also be a place for fragment identifiers as well, e.g. to specify a subset via a date range. The solution depends again on the content type, and the popular <code>text/csv</code> is unfortunately not well suited for this, whereas JSON – using <a href="http://tools.ietf.org/html/rfc6901">JSON Pointer</a> – would work well.</p>
<p><em>Update 8/2/14: <a href="https://twitter.com/ldodds">Leigh Dodds</a> points out that handling the fragment identifier is up to the client and the fragment identifier is not sent to the server. Acrobat reader for example supports the <code>#page=</code> fragment identifier. He also mentions that there is a <a href="http://tools.ietf.org/html/rfc7111">RFC7111</a> for fragment identifiers for the text/csv media type - browsers in the future might support something like <code>http://example.com/data.csv#row=5-7</code>.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Ring to Rule them All]]></title>
        <id>2q6pe3s-4mx94ns-azr3wf5-0xwrb</id>
        <link href="https://blog.front-matter.io/mfenner/one-ring-to-rule-them-all"/>
        <updated>2014-07-30T15:29:00.000Z</updated>
        <summary type="html"><![CDATA[One Ring to rule them all, One Ring to find them, One Ring to bring them all and in the darkness bind them.Yesterday 60 years ago the first volume of the <em>Lord of the Rings</em> trilogy by <em>J.R.R. Tolkien</em> was published....]]></summary>
        <content type="html"><![CDATA[<blockquote>
One Ring to rule them all, One Ring to find them, One Ring to bring them all and in the darkness bind them.
</blockquote>
<p>Yesterday 60 years ago the first volume of the <em>Lord of the Rings</em> trilogy by <em>J.R.R. Tolkien</em> was published. The quote above obviously doesn’t quiet apply to scholarly publishing, but one recurring theme that I have often heard in the last few years is that of a need for a canonical digital document format for scholarly content that rules all other formats.</p>
<figure>
<img src="http://blog.martinfenner.org/images/rings.png" class="kg-image" alt="Document formats in scholarly Publishing" /><figcaption aria-hidden="true">Document formats in scholarly Publishing</figcaption>
</figure>
<p>A few years ago almost everyone you would have said that <code>xml</code> is that format, with the NLM Archiving and Interchange Tag Suite - which has evolved into <a href="http://jats.nlm.nih.gov/publishing/">JATS</a> - probably the most commonly used Document Type Definition (DTD). <code>xml</code> does many things really well, but also has important shortcomings, most importantly that it is probably not a good format for authors (and don’t tell me that <code>docx</code> and <code>odt</code> are XML-based). We therefore don’t really expect authors to submit manuscripts in JATS <code>xml</code>, but rather convert documents into this format after a manuscript has been accepted for publication. This conversion step is often time-consuming and labor-intensive.</p>
<p>More recently <code>html</code> has become the most interesting candidate for a canonical scholarly document format. The big advantage over <code>xml</code> is that <code>html</code> - or at least <code>html5</code> which is most popular today - is an attractive format for online authoring tools (that is why <code>html</code> is listed both as input and output format) The downside of this flexibility is that it is much harder to embed structure and metadata into <code>html5</code> compared to <code>xml</code>. There are initiatives such as <a href="http://schema.org/">schema.org</a> and <a href="https://github.com/oreillymedia/HTMLBook">HTMLBook</a> that hope to change that, but we aren’t quite there yet.</p>
<p>Or maybe we should learn from Tolkien and give up on the idea of a canonical document format and rather spend our energy on building tools that make it easier to transition from one format to another. <a href="https://pandoc.org">Pandoc</a> is such as tool, but can’t do all the required conversions, e.g. it can’t yet use <code>docx</code> as input. The downside here is that every file conversion runs the risk of loosing important information. But the increase in flexibility hopefully outweights these shortcomings.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Don't Reinvent the Wheel]]></title>
        <id>7s7b2sh-pbf83p8-bv792w4-rxk7y</id>
        <link href="https://blog.front-matter.io/mfenner/dont-reinvent-the-wheel"/>
        <updated>2014-07-24T15:33:00.000Z</updated>
        <summary type="html"><![CDATA[In a post last week I talked about roads and stagecoaches, and how work on scholarly infrastructure can often be more important than building customer-facing apps. One important aspect of that infrastructure work is to not duplicate efforts.Image by...]]></summary>
        <content type="html"><![CDATA[<p>In a <a href="https://sensiblescience.io/mfenner/build-roads-not-stagecoaches/">post last week</a> I talked about roads and stagecoaches, and how work on scholarly infrastructure can often be more important than building customer-facing apps. One important aspect of that infrastructure work is to not duplicate efforts.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/5673321593_e6a7faa36d_z.jpg" class="kg-image" width="478" height="640" alt="Image by Cocoabiscuit on Flickr" /><figcaption aria-hidden="true">Image by Cocoabiscuit <a href="https://www.flickr.com/photos/jfgallery/5673321593/">on Flickr</a></figcaption>
</figure>
<p>A good example is information (or metadata) about scholarly publications. I am the technical lead for the open source <a href="http://articlemetrics.github.io/">article-level metrics (ALM) software</a>. This software can be used in different ways, but most people use it for tracking the metrics of scholarly articles, with articles that have DOIs issued by CrossRef. The ALM software needs three pieces of information for every article: <strong><strong>DOI</strong></strong>, <strong><strong>publication date</strong></strong>, and <strong><strong>title</strong></strong>. This information can be entered via a web interface, but that is of course not very practical for adding dozens or hundreds of articles at a time. The ALM software has therefore long supported the import of multiple articles via a text file and the command line.</p>
<p>This approach is working fine for the ALM software <a href="http://articlemetrics.github.io/plos/">running at PLOS since 2009</a>, but is for example a problem if the ALM software runs as a service for multiple publishers. A more flexible approach is to provide an API to upload articles, and I’ve <a href="http://articlemetrics.github.io/docs/api/">added an API</a> for creating, updating and deleting articles in January 2014.</p>
<p>While the API is an improvement, it still requires the integration into a number of possibly very different publisher workflows, and you have to deal with setting up the permissions, e.g. so that publisher A can’t delete an article from publisher B.</p>
<p>The next ALM release (3.3) will therefore add a third approach to importing articles: using the <a href="http://api.crossref.org/">CrossRef API</a> to look up article information. Article-level metrics is about tracking already published works, so we really only care about articles that have DOIs registered with CrossRef and are therefore published. ALM is now talking to a single API, and this makes it much easier to do this for a number of publishers without writing custom code. Since ALM is an open source application already used by several publishers that aspect is important. And because we are importing, we have don’t have to worry about permissions. The only requirement is that CrossRef has the correct article information, and has this information as soon as possible after publication.</p>
<p>At this point I have a confession to make: I regularly use other CrossRef APIs, but wasn’t aware of <strong><strong>api.crossref.org</strong></strong> until fairly recently. That is sort of understandable since the reference platform was deployed only September last year. The documentation to get you started is on <a href="https://github.com/CrossRef/rest-api-doc/blob/master/rest_api.md">Github</a> and the version history shows frequent API updates (now at v22). The API will return all kinds of information, e.g.</p>
<ul>
<li>how many articles has publisher x published in 2012</li>
<li>percentage of DOIs of publisher Y that include at least one ORCID identifier</li>
<li>list all books with a Creative Commons CC-BY license that were published this year</li>
</ul>
<p>Funder (via FundRef) information is also included, but is still incomplete. Another interesting result is the number of <a href="https://sensiblescience.io/mfenner/direct-links-to-figures-and-tables-using-component-dois/">component DOIs</a> (DOIs for figures, tables or other parts of a document) per year:</p>
<div class="iframe">
<div id="__svelte-dw" class="dw-chart chart vis-height-fit theme-datawrapper-data vis-column-chart">
<div id="header" class="dw-chart-header">
<h1 class="block headline-block" id="component-dois-per-year"><span class="block-inner">Component DOIs per year</span></h1>
<p><span class="block-inner">Number of component DOIs (DOIs for figures, tables or other parts of a document) per year.</span></p>
</div>
<div id="chart" class="dw-chart-body" aria-hidden="false">

</div>
<div id="footer" class="dw-chart-footer">
<div class="footer-left">
<span class="footer-block source-block"> <span class="block-inner"><span class="source-caption">Source:</span> <span class="source">Crossref</span></span> </span><span class="separator separator-before-get-the-data"></span> <span class="footer-block get-the-data-block"> <span class="block-inner"><a href="data" class="dw-data-link">Get the data</a></span> </span>
</div>
<div class="footer-center">

</div>
<div class="footer-right">

</div>
</div>
<div class="dw-after-body">

</div>
</div>
</div>
<p>For my specific use case I wanted an API call that returns all articles published by PLOS (or any other publisher) in the last day which I can then run regularly. To get all DOIs from a specific publisher, use their CrossRef member ID - DOI prefixes don’t work, as publishers can own more than one DOI prefix. To make this task a little easier I built a CrossRef member search interface into the ALM application:</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/crossref_api.png" class="kg-image" width="1170" height="774" />
</figure>
<p>We can filter API responses by publication date, but it is a better idea to use the update date, as it is possible that the metadata have changed, e.g. a correction of the title. We also want to increase the number of results per page (using the <code>rows</code> parameter). The final API call for all DOIs updated by PLOS since the beginning of the week would be</p>
<pre><code>http://api.crossref.org/members/340/works?filter=from-update-date:2014-07-21,until-update-date:2014-07-24&amp;rows=1000</code></pre>
<p>The next step is of course to parse the JSON of the API response, and you will notice that CrossRef is using <a href="http://gsl-nagoya-u.net/http/pub/citeproc-doc.html">Citeproc JSON</a>. This is a standard JSON format for bibliographic information used internally by several reference managers for citation styles, but increasingly also by APIs and other places where you encounter bibliographic information.</p>
<p>Citeproc JSON is helpful for one particular problem with CrossRef metadata: the exact publication date for an article is not always known, and CrossRef (and similarly DataCite) only requires the publication year. Citeproc JSON can nicely handle partial dates, e.g. year-month:</p>
<pre><code>issued: {
  date-parts: [
    [
      2014,
      7
    ]
  ]
},</code></pre>
<p>I think that a similar approach will work for many other systems that require bibliographic information about scholarly content with CrossRef DOIs. If are not already using <strong><strong>api.crossref.org</strong></strong>, consider integrating with it, I find the API fast, well documented, easy to use - and CrossRef is very responsive to feedback. As you can always wish for more, I would like to see the following: fix the problem were some journal articles are missing the publication date (a required field, even if only the year), and consider adding the canonical URL to the article metadata (which ALM currently has to look up itself, and which is needed to track social media coverage of an article).</p>
<p><em>Update July 24, 2014: added chart with number of component DOIs per year</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Build Roads not Stagecoaches]]></title>
        <id>6hkr37k-n08tes5-4g32v7w-1xk1</id>
        <link href="https://blog.front-matter.io/mfenner/build-roads-not-stagecoaches"/>
        <updated>2014-07-18T15:36:00.000Z</updated>
        <summary type="html"><![CDATA[I attended the Open Knowledge Festival this week and I had a blast. For three days (I also attended the fringe event csv,conf on Tuesday) I listed to wonderful presentations and was involved in great discussions - both within sessions,...]]></summary>
        <content type="html"><![CDATA[<p>I attended the <a href="http://2014.okfestival.org/">Open Knowledge Festival</a> this week and I had a blast. For three days (I also attended the fringe event <a href="http://csvconf.com/">csv,conf</a> on Tuesday) I listed to wonderful presentations and was involved in great discussions - both within sessions, but more importantly all the informal discussions between and after sessions.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/9th/okfest-2014-logo.png" class="kg-image" width="900" height="344" />
</figure>
<p>Of all the things that were discussed I want to pick one theme that resonated in particular with me. It surfaced in many places, but was articulated particularly well by <a href="https://twitter.com/erichysen">Eric Hysen</a> - who heads the <a href="http://www.google.com/elections/ed/us">Google Politics &amp; Elections Group</a> - in his keynote yesterday (starting at 54:52, but please also watch the keynote by Neelie Kroes, Vice-President of the European Commission):</p>
<div class="iframe">
<div id="player">

</div>
<section id="an-error-occurred." class="message player-unavailable">
<h1 class="message">An error occurred.</h1>
<div class="submessage">
<a href="https://www.youtube.com/watch?v=0UNRZEsLxKc">Sieh dir dieses Video auf www.youtube.com an</a> oder aktiviere JavaScript, falls es in deinem Browser deaktiviert sein sollte.
</div>
</section>
</div>
<p>In his keynote he described how travel from Cambridge to London in the 18th and early 19th century improved mainly as a result of better roads, made possible by changes in how these roads were financed. Translated to today, he urged the audience to think more about the infrastructure and less about the end products:</p>
<blockquote>
Ecosystems, not apps – Eric Hysen
</blockquote>
<p>On Tuesday at <a href="http://csvconf.com/#nickstenning">csv,conf</a>, <a href="https://twitter.com/nickstenning">Nick Stenning</a> - Technical Director of the Open Knowledge Foundation - talked about <a href="http://dataprotocols.org/data-packages/">data packages</a>, an evolving standard to describe data that are passed around betwen different systems. He used the metaphor of containers, and how they have dramatically changed the transportation of goods in the last 50 years. He <a href="https://github.com/nickstenning/put-it-in-a-box">argued</a> that the cost of shipping was in large part determined by the cost of loading and unloading, and the container has dramatically changed that equation. We are in a very similar situation with datasets, where most of the time is spent translating between different formats, joining things together that use different names for the same thing, etc.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/9th/break-bulk-sacks.png" class="kg-image" width="1543" height="1087" alt="Wikimedia Commons image used in Nick Stelling&#39;s presentation" /><figcaption aria-hidden="true"><a href="https://commons.wikimedia.org/wiki/File:Hafenarbeiter_bei_der_Verladung_von_Sackgut_-_MS_Rothenstein_NDL,_Port_Sudan_1960.png">Wikimedia Commons</a> image used in <a href="https://github.com/nickstenning/put-it-in-a-box">Nick Stelling's presentation</a></figcaption>
</figure>
<p>What the two presentations have in common is not only that they link the building of an open digital infrastructure to important transforming events in the history of transportation, but also the emphasis on the building blocks rather than the finished product. When I thought more about this I realized that these building blocks are exactly the projects I get most excited about, i.e. projects that develop standards or provide APIs or libraries. Some examples would be</p>
<ul>
<li><a href="http://orcid.org/">ORCID</a>: unique identifiers for scholarly authors</li>
<li><a href="http://citationstyles.org/">Citation Style Language</a>: a language to describe the formatting of citations and bibliographies</li>
<li><a href="https://pandoc.org">Pandoc</a>: a universal document converter</li>
<li><a href="http://ropensci.org/">rOpenSci</a>: packages for the statistical programming language R to access data repositories</li>
<li><a href="http://www.niso.org/topics/tl/altmetrics_initiative/">NISO Alternative Assessment Metrics</a>: standards and best practices for novel scholarly metrics</li>
<li><a href="http://www.re3data.org/">re3data</a>: a registry of research data repositories</li>
<li><a href="http://creativecommons.org/">Creative Commons</a>: copyright licenses for creative works</li>
<li><a href="https://github.com/articlemetrics/alm">ALM</a>: software to collect comprehensive information about the discussion of scholarly articles on the web</li>
</ul>
<p>This list doesn’t include all the generic software needed to build open science tools, with <strong>git</strong> being a perfect example. The last project is obviously the project I have been working on the past two years for PLOS, but I have tried to support the other projects mentioned in various ways from small code contributions to promotion via this blog and presentations, or direct work in these projects. But strangely enough, I haven’t really realized this until now.</p>
<p>Not surprisingly infrastructure, servers, libraries and other building blocks are exactly the areas where open source software has been most successful so far, and this is of course a core part of the UNIX philosophy of building parts that work well together rather than big monolithic programs that do everything.</p>
<h2 id="next">Next</h2>
<p>We need more <strong><strong>Open Science Infrastructure</strong></strong> and it is the stuff that I really care about. I think we need to better support those projects that build these essential building blocks via advice, cooperation, promotion, and financial support. I am willing to help with that effort, and I have started to think how I can best contribute.</p>
<p>On the other hand there are many great open science projects that don’t fall in this category, maybe even the majority of them. I wish them good luck, but I would advice them to think more about infrastructure, and whether there is a small area where they can focus on. It still amazes me how successful projects such as <strong><strong>Citation Style Language</strong></strong> and <strong><strong>Pandoc</strong></strong> have been with no or almost no funding and a very small core group of people doing the majority of the work. One critical ingredient is the total focus on a very specific problem that is both important and can be solved with specific actions. Too many open science projects want to solve too many problems at once, try to solve the exact same problems that many other parallel projects work on, don’t cooperate enough with those parallel projects, and require a critical mass of users to work.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Literate Blogging]]></title>
        <id>qqqt2h8-jy8qcb5-859y9qc-2pbb</id>
        <link href="https://blog.front-matter.io/mfenner/literate-blogging"/>
        <updated>2014-04-14T15:39:00.000Z</updated>
        <summary type="html"><![CDATA[Literate programming is a methodology that combines a programming language with a documentation language, thereby making programs more robust, more portable, more easily maintained, and arguably more fun to write than programs that are written only in a high-level language....]]></summary>
        <content type="html"><![CDATA[<blockquote>
Literate programming is a methodology that combines a programming language with a documentation language, thereby making programs more robust, more portable, more easily maintained, and arguably more fun to write than programs that are written only in a high-level language. The main idea is to treat a program as a piece of literature, addressed to human beings rather than to a computer. The program is also viewed as a hypertext document, rather like the World Wide Web.
</blockquote>
<p>Literate Programming by <a href="http://www-cs-faculty.stanford.edu/~uno/">Donald Knuth</a> (1983) is a seminal book that introduces the concept of literate programming. Using technology available in 2014 we can make a small but important change to the last sentence:</p>
<blockquote>
The program is also viewed as a hypertext document on the World Wide Web.
</blockquote>
<p>This blog post is an example for such a document. The page is written in <strong><strong>markdown</strong></strong> (markdown file available <a href="https://github.com/mfenner/mfenner.github.io/blob/source/_posts/2014-04-04-literate-blogging.Rmd">here</a>), and all embedded code was executed when this page was generated, i.e. when the markdown was converted to HTML and the blog post was published. To demonstrate this I have embedded code in three different languages below - the output is the second code block.</p>
<p>In R you have</p>
<pre><code>cat(&#39;Hello, R world!\n&#39;)</code></pre>
<pre><code>Hello, R world!</code></pre>
<p>Or Python</p>
<pre><code>print &quot;Hello, Python world!&quot;</code></pre>
<pre><code>Hello, Python world!</code></pre>
<p>Or Ruby</p>
<pre><code>puts &#39;Hello, Ruby world!&#39;</code></pre>
<pre><code>Hello, Ruby world!</code></pre>
<p>You can also embed code within text blocks (inline), so that <code>3.48 * 723</code> becomes <strong><strong>2516.04</strong></strong>. Another important option is to generate figures using the embedded code, e.g. the following figure taken from a recent publication.</p>
<pre><code># code for figure 1: density plots for citation counts for PLOS Biology
# articles published in 2010

# load May 20, 2013 ALM report
alm &lt;- read.csv(&quot;data/alm_report_plos_biology_2013-05-20.csv&quot;, stringsAsFactors = FALSE)

# only look at research articles
alm &lt;- subset(alm, alm$article_type == &quot;Research Article&quot;)

# only look at papers published in 2010
alm$publication_date &lt;- as.Date(alm$publication_date)
alm &lt;- subset(alm, alm$publication_date &gt; &quot;2010-01-01&quot; &amp; alm$publication_date &lt;=
    &quot;2010-12-31&quot;)

# labels
colnames &lt;- dimnames(alm)[[2]]
plos.color &lt;- &quot;#1ebd21&quot;
plos.source &lt;- &quot;scopus&quot;

plos.xlab &lt;- &quot;Scopus Citations&quot;
plos.ylab &lt;- &quot;Probability&quot;

quantile &lt;- quantile(alm[, plos.source], c(0.1, 0.5, 0.9), na.rm = TRUE)

# plot the chart
opar &lt;- par(mai = c(0.5, 0.75, 0.5, 0.5), omi = c(0.25, 0.1, 0.25, 0.1), mgp = c(3,
    0.5, 0.5), fg = &quot;black&quot;, cex.main = 2, cex.lab = 1.5, col = plos.color,
    col.main = plos.color, col.lab = plos.color, xaxs = &quot;i&quot;, yaxs = &quot;i&quot;)

d &lt;- density(alm[, plos.source], from = 0, to = 100)
d$x &lt;- append(d$x, 0)
d$y &lt;- append(d$y, 0)
plot(d, type = &quot;n&quot;, main = NA, xlab = NA, ylab = NA, xlim = c(0, 100), frame.plot = FALSE)
polygon(d, col = plos.color, border = NA)
mtext(plos.xlab, side = 1, col = plos.color, cex = 1.25, outer = TRUE, adj = 1,
    at = 1)
mtext(plos.ylab, side = 2, col = plos.color, cex = 1.25, outer = TRUE, adj = 0,
    at = 1, las = 1)

par(opar)</code></pre>
<figure>
<img src="http://blog.martinfenner.org/figures/density_plot_example-1.svg" class="kg-image" alt="Figure 1. Citation counts for PLOS Biology articles published in 2010. Scopus citation counts plotted as a probability distribution for all 197 PLOS Biology research articles published in 2010. Data collected May 20, 2013. Median 19 citations; 10% of papers have at least 50 citations. From Fenner (2013)." /><figcaption aria-hidden="true"><strong>Figure 1. Citation counts for PLOS Biology articles published in 2010.</strong> Scopus citation counts plotted as a probability distribution for all 197 <em>PLOS Biology</em> research articles published in 2010. Data collected May 20, 2013. Median 19 citations; 10% of papers have at least 50 citations. From Fenner <span class="citation" data-cites="fenner2013" style="box-sizing: border-box;">(2013)</span>.</figcaption>
</figure>
<p>All this functionality is provided by <a href="http://yihui.name/knitr/">knitr</a>, a package for the R statistical programming language. knitr has been around for a while, but integration into the <a href="http://jekyllrb.com/">Jekyll</a> blogging platform is still fragile. Earlier this week at the <a href="https://github.com/ropensci/hackathon">rOpenSci hackathon</a> (more on this later) a group of us worked hard to improve this integration. We are still not completely done, but the source code is available <a href="https://github.com/ropensci/docs">here</a>. Most importantly, all the conversion happens on the server, and we are only using freely available tools. I have now enabled this functionality for this blog, so expect more code embedded examples in the future.</p>
<h2 id="references">References</h2>
<p>Fenner, M. (2013). What can article-level metrics do for you? <em>PLoS Biol</em>, <em>11</em>(10), e1001687. <a href="http://doi.org/10.1371/journal.pbio.1001687">doi:10.1371/journal.pbio.1001687</a></p>
<p>Knuth, D. E., Stanford University, &amp; Computer Science Department. (1983). <em>Literate programming</em>. Stanford, CA: Dept. of Computer Science, Stanford University.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous Publishing]]></title>
        <id>45a366p-hf98rgt-nnqtvxc-jyxm1</id>
        <link href="https://blog.front-matter.io/mfenner/continuous-publishing"/>
        <updated>2014-03-10T15:44:00.000Z</updated>
        <summary type="html"><![CDATA[Earlier this week Björn Brembs wrote in a blog post (What Is The Difference Between Text, Data And Code?):To sum it up: our intellectual output today manifests itself in code, data and text.The post is about the importance of publication of data and...]]></summary>
        <content type="html"><![CDATA[<p>Earlier this week Björn Brembs wrote in a blog post (<a href="http://bjoern.brembs.net/2014/03/what-is-the-difference-between-text-data-and-code/">What Is The Difference Between Text, Data And Code?</a>):</p>
<blockquote>
To sum it up: our intellectual output today manifests itself in code, data and text.
</blockquote>
<p>The post is about the importance of publication of data and software where currently <em>the rewards are stacked disproportionately in favor of text publications</em>. The intended audience is probably mainly other scientists (Björn is a neurobiologist) who are reluctant to publish data and/or code, but there is another interesting aspect to this.</p>
<p>Just as scientific publication increasingly means more than just text and includes data and software, we are also increasingly seeing tools and methodologies common in software development applied to scientific publishing. This in particular includes the ideas behind Open Source software (which shares many commonalities with Open Access and Open Science), but also tools like the git version control system (<a href="http://marciovm.com/i-want-a-github-of-science/">We Need a Github of Science</a>) or the markdown markdown language (<a href="https://sensiblescience.io/mfenner/a-call-for-scholarly-markdown/">A Call for Scholarly Markdown</a>).</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/13th/Agile-vs-iterative-flow.jpg" class="kg-image" width="672" height="335" alt="Delivery, image from Wikimedia Commons" /><figcaption aria-hidden="true">Delivery, image from <a href="http://commons.wikimedia.org/wiki/File:Agile-vs-iterative-flow.jpg">Wikimedia Commons</a></figcaption>
</figure>
<p>Continuous Delivery is another concept increasingly popular in software development that has many implications on how research can be performed and reported. Martin Fowler describes it as:</p>
<blockquote>
Continuous Delivery is a software development discipline where you build software in such a way that the software can be released to production at any time.
</blockquote>
<p>The concept of frequent small releases is of course familiar to everyone practicing <a href="http://usefulchem.wikispaces.com/">Open Notebook Science</a>, writing science blogs, presenting preliminary data at conferences or publishing <a href="http://arxiv.org/">preprints</a>, and is even relevant to <a href="http://www.crossref.org/crossmark/">CrossMark</a>, a service that tracks corrections, enhancements and other changes of scholarly documents.</p>
<p>When you read the definition given by Martin Fowler carefully, you see that Continuous Delivery is about more than the frequency of software updates – it is in fact about improving the process of releasing software. The scientific publication is the corresponding event in science, and I think that nobody would argue with me that the experience publishing a paper is too complex, time-consuming and often frustrating. The focus here is not on the time it takes to do peer review, or the multiple revisions needed before a manuscript is accepted. I am talking about the pain submitting a manuscript, the back and forth regarding file formats, citation styles and other technical requirements, the reformatting of manuscripts, and also the time it takes from accepting a manuscript to finally publishing it online.</p>
<p>I would argue that the main reason publishing is so painful for everyone involved is that it is still very much a manual process. Just as software development is creative work, but still can benefit tremendously from tools such as automated tests and build tools, we can apply the same principles to scientific publishing. This means that everything that can be automated should be automated so that we can focus on those areas that need human judgement. The mistake that I think is commonly made is that automation for many publishers means automation for the publisher, with even more work for the author who submits a manuscript. A good example is that authors are increasingly asked to submit publication-ready manuscripts even though typesetting and desktop publishing is not their area of expertise and the manuscript text will be very different after one or more rounds of revision. The pain of processing manuscripts into something that can be published was summarized perfectly by typesetter and friend Kaveh Bazargan at the <a href="http://www.youtube.com/watch?feature=player_embedded&amp;v=CGkcsvofjdg">SpotOn London 2012 Conference</a> (via <a href="http://rossmounce.co.uk/2012/11/19/yet-another-solo12-recap-part2/">Ross Mounce’s blog</a>):</p>
<blockquote>
It’s madness really. I’m here to say I shouldn’t be in business.
</blockquote>
<p>The promise of Continuous Delivery for publishing is to develop tools and best practices that make the process of publication faster, with better quality, and less frustrating. Continuous Integration (<a href="http://martinfowler.com/articles/continuousIntegration.html">again Martin Fowler</a>) is an important part of Continuous Delivery and means frequently merging all developer working copies of a software project into a central repository, combined with running automated unit tests and software builds using an integration server.</p>
<p>We can apply Continuous Integration to scholarly documents - instead of automated tests and software builds we can automate the transformation of documents into <a href="http://sensiblescience.io/mfenner/from-markdown-to-jats-xml-in-one-step/">JATS XML</a> and other output formats, and we can automate the process of checking for required metadata, correct file formats for images, etc. And we can use the same software tools for this, many of which are freely available to Open Source projects.</p>
<p>As an example of how this can be done <a href="https://github.com/mfenner/jekyll-travis">I have integrated</a> the <a href="https://travis-ci.org/">Travis CI</a> Continuous Integration server with the book project <a href="http://book.openingscience.org/">Opening Science</a>. The recently published book is a dynamic book that hopefully is updated frequently in the coming months. Every time an editor approves a correction to the text - <a href="https://github.com/openingscience/book">hosted in markdown format on Github</a> - the Travis CI server is automatically triggered to build a new HTML version of the book and to push the new version to the book website. The Travis server is running the <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a> document converter to not only convert the changed document from markdown to HTML, but Pandoc will also insert and format references, and the <a href="http://jekyllrb.com/">Jekyll</a> site generator will build a nice website around the markdown files. Over time this build process can be extended to do other things as well, from <a href="https://sensiblescience.io/mfenner/auto-generating-links-to-data-and-resources/">auto-generating links to data and resources</a> to transforming the document into <a href="https://sensiblescience.io/mfenner/from-markdown-to-jats-xml-in-one-step/">other file formats</a> besides HTML.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are static Websites the Future or the Past?]]></title>
        <id>5evv1ac-b7a8ccv-eagfy70-d0fty</id>
        <link href="https://blog.front-matter.io/mfenner/are-static-websites-the-future-or-the-past"/>
        <updated>2014-03-05T15:47:00.000Z</updated>
        <summary type="html"><![CDATA[Last week I had a little discussion on Twitter about a great blog post by Zach Holman: Only 90s Web Developers Remember This. The post is not only fun to read, but also reminded me that it is now almost 20 years (1995)...]]></summary>
        <content type="html"><![CDATA[<p>Last week I had a little discussion on Twitter about a great blog post by Zach Holman: <a href="http://zachholman.com/posts/only-90s-developers/">Only 90s Web Developers Remember This</a>. The post is not only fun to read, but also reminded me that it is now almost 20 years (1995) that I built my first website - of course using some of the techniques (the one pixel gif!, the <code>&amp;nbsp;</code> tag!) described in the post.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/scriptwebtitle.gif" class="kg-image" width="250" height="50" alt="ScriptWeb logo 1995" /><figcaption aria-hidden="true">ScriptWeb logo 1995</figcaption>
</figure>
<p>We started ScriptWeb back in 1995 as a central resource for scripting on the Mac (AppleScript and Frontier). It was a nice collaborative effort and I was resposible for a directory of scripting additions (or osaxen), joining forces with MacScripter.net a few years later:</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/osaxen.png" class="kg-image" width="587" height="639" alt="Scripting Additions at Macscripter.net 2000" /><figcaption aria-hidden="true">Scripting Additions at Macscripter.net 2000</figcaption>
</figure>
<p>Since then I have built many other websites for fun and work, adapting to how technology changed over the years:</p>
<ul>
<li>1995: website running on a Mac Quadra 610 using the WebSTAR HTTP server and server side includes (<a href="https://en.wikipedia.org/wiki/Server_Side_Includes">SSI</a>)</li>
<li>1995: static site generation with outline navigation using AppleScript. FTP to transfer files</li>
<li>1995: Visual HTML editors (Adobe PageMill 1.0)</li>
<li>1999: database server and application layer (too long ago to remember the technology)</li>
<li>2001: Open source database and application code with MySQL and PHP. CVS for version control</li>
<li>2001: web frameworks with PHP and MySQL: PostNuke and Xaraya</li>
<li>2005: more complex web application frameworks: Ruby on Rails. Subversion version control</li>
<li>2008: git for version control</li>
<li>2011: more complex frontend Javascript</li>
<li>2013: static site generator Jekyll</li>
</ul>
<p>Since last June this blog is running on Github pages and the site is generated with <a href="http://jekyllrb.com/">Jekyll</a>. Jekyll works really well to build static websites such as this blog, but I am increasingly using it for more complex projects, e.g. for the online version of a <a href="http://book.openingscience.org/">book on Open Science</a>.</p>
<p>What I find interesting in this timeline is that with Jekyll there is a shift in focus. Rather than building even more complex web pages that are generated dynamically by the server, we are going back to a two-stage process where the HTML pages are built first and then served as HTML, CSS and Javascript without any database or server application layer. Doesn’t sound too different from what we did in the 1990s. This approach obviously works well for content-heavy sites like this blog or book chapters, not so much for dynamically generated content that changes every few minutes, or where the page is put together from many different page fragments.</p>
<p>What I don’t know, and I am really interested to find out, is how well this scales to larger sites, specifically publisher websites that host thousands of scholarly journal articles - again content that is very text-heavy and doesn’t change that much. The potential benefits of replacing the paradigm of a database layer that holds all content with a paradigm that stores all content in files managed by git version control are clear: serving the content on the web becomes less complex, cheaper and faster. The tradeoff is of course that generating the static content becomes more complex and time-consuming, and it can become a challenge to mix the static content with dynamic content generated by servers as well as the user’s browser. For a now infamous example using this technology, look no further than <a href="http://www.huffingtonpost.com/john-pavley/obamacare-website-problems_b_4057618.html">Heathcare.gov</a>. I don’t know enough details to understand what went wrong, and it might have more to do with the scale of the project and the tight timeline to launch. For scholarly journal articles this might be a reasonable approach, as even when there is no longer a printed version of the journal, articles are still published on a specific date, and changing the content is a very formal process.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/3781208877_936e1a162c_z.jpg" class="kg-image" width="640" height="427" alt="Netscape Navigator 1. Flickr photo by bump" /><figcaption aria-hidden="true">Netscape Navigator 1. <a href="http://www.flickr.com/photos/bump/3781208877/">Flickr photo</a> by bump</figcaption>
</figure>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Six Misunderstandings about Scholarly Markdown]]></title>
        <id>42e6znr-mhy9h69-0zx5hyf-x16q8</id>
        <link href="https://blog.front-matter.io/mfenner/six-misunderstandings-about-scholarly-markdown"/>
        <updated>2014-03-03T15:50:00.000Z</updated>
        <summary type="html"><![CDATA[In this post I want to talk about some of the misunderstandings I frequently encounter when discussing markdown as a format for authoring scholarly documents.Scholars will always use Microsoft WordMicrosoft Word is of course what almost all authors use...]]></summary>
        <content type="html"><![CDATA[<p>In this post I want to talk about some of the misunderstandings I frequently encounter when discussing <a href="https://sensiblescience.io/mfenner/what-is-scholarly-markdown/">markdown as a format for authoring scholarly documents</a>.</p>
<h2 id="scholars-will-always-use-microsoft-word">Scholars will always use Microsoft Word</h2>
<p>Microsoft Word is of course what almost all authors use in the life sciences and many other disciplines. One big reason for this is the file formats accepted my manuscript submission systems. By limiting the options to Microsoft Word (and maybe LaTeX), you make it impossible for authors to use other tools, even if they wanted to. Publishers should accept manuscripts in any reasonable file format, as I have <a href="https://sensiblescience.io/mfenner/the-grammar-of-scholarly-communication/">argued before</a>.</p>
<h2 id="my-markup-language-is-better-than-markdown">My markup language is better than markdown</h2>
<p>There are of course numerous alternatives to markdown, including <a href="http://txstyle.org/">Textile</a>, <a href="http://www.methods.co.nz/asciidoc/">AsciiDoc</a>, <a href="http://www.mediawiki.org/wiki/Help:Formatting">MediaWiki Markup</a> and <a href="http://docutils.sourceforge.net/docs/ref/rst/introduction.html">reStructuredText</a>. There will always be features that are better implemented in one of these languages, but I don’t think there is room for more than one major initiative for a scholarly markup language. And markdown has the right mix of features and broad support by tools and the community.</p>
<p>Related to this there is the argument against markdown that the format <a href="http://blog.codinghorror.com/the-future-of-markdown/">is a mess</a> and that there are too many versions (or flavors) of it. While that is certainly a big problem with markdown, I would argue that with <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a> we have a nice standard and reference implementation for Scholarly Markdown. Pandoc is constantly evolving, and the addition of support for arbitrary YAML metadata was the biggest new feature in 2013 for me.</p>
<h2 id="scholarly-markdown-is-too-complex-and-we-might-as-well-use-latex">Scholarly Markdown is too complex and we might as well use LaTeX</h2>
<blockquote>
LaTeX is a high-quality typesetting system; it includes features designed for the production of technical and scientific documentation.<br />
The LaTeX Team, 2014
</blockquote>
<p>Although LaTeX has solved many of the problems Scholarly Markdown tries to tackle a long time ago, it is still something else. LaTeX at its core is a typesetting system, which is not something Scholarly Markdown cares about for two reasons: a) the focus is on authoring documents, which are then submitted to other systems at publishers and elsewhere that are specialized in producing the final document, and b) the focus is on HTML and the web as this is where we want most of the interactions with scholarly documents to take place. This means that</p>
<ul>
<li>Markdown is a great input format to convert into other formats, including XML (see for example my <a href="https://github.com/mfenner/pandoc-jats">pandoc-jats</a>).</li>
<li>LaTeX will always be the best choice for some content, e.g. documents rich in mathematical formulas</li>
<li>If the ultimate goal was to produce high-quality PDF documents, Scholarly Markdown would be a bad choice. It is the right format for HTML and the related ePub.</li>
</ul>
<p>We have to be very careful that we keep the right balance of simplicity and features in Scholarly Markdown. This means that sometimes we should just include the LaTeX code, e.g. for math.</p>
<h2 id="scientists-need-a-wysiwyg-editor-and-then-the-file-format-doesn-t-matter">Scientists need a WYSIWYG Editor, and then the file format doesn’t matter</h2>
<p><a href="http://en.wikipedia.org/wiki/WYSIWYG">WYSIWYG</a> - What You See Is What You Get - is a user interface metaphor that is both a blessing and a curse. We desperately need better writing tools, and this of course also means user interfaces that help with that task. But the focus on creating a new authoring environment that focusses too much on WYSIWYG creates several problems:</p>
<ul>
<li>WYSIWYG is not always a good metaphor for scholarly documents. Typographic features such as fonts, line spacing, etc. are not something that belong into an authoring environment - this is done during the publishing step, as is the formatting of references according to a specific citation style.</li>
<li>WYSIWYG is for human interactions, but content in scholarly documents is increasingly created by computers. Two good examples are statistics and figures created in <a href="http://yihui.name/knitr/">R/knitr</a> or <a href="http://ipython.org/notebook.html">iPython Notebook</a>. Scholarly Markdown works perfectly with these workflows.</li>
<li>WYSIWYG authoring environments run the high risk of vendor lock-in. This is understandable if you run a startup and want to promote your tool, but is not in the best interest of the scholarly community.</li>
</ul>
<p>Version control via git is central to Scholarly Markdown, and this can also be challenging for a WYSIWYG environment. But there are many good examples of how to make this work.</p>
<h2 id="scientists-should-submit-their-manuscripts-in-jats-xml-the-standard-format-for-scholarly-documents">Scientists should submit their manuscripts in JATS XML, the standard format for scholarly documents</h2>
<p>At the end of the day most scholarly publications in the life sciences are converted into JATS XML. Unfortunately central aspects of the format (e.g. the required document structure or required attributes) are difficult to enforce in an authoring environment. Even if you build a tool that can nicely handle this, I’m not so sure we want to burden an author with this, especially since the manuscript will usually undergo a lot of changes before it is accepted and then published.</p>
<h2 id="the-future-is-html">The future is HTML</h2>
<p>Although the future for consuming scholarly documents is clearly HTML (and ePub), and there are great HTML editors, I’m not so sure that HTML will become the default for authoring environments. This is the reason why markdown and related markup languages were invented, and even with modern WYSIWYG editors working directly with HTML is not always the best choice. HTML has two problems: a) it is not as human-readable as markdown and therefore requires an additional layer for authoring, and b) it is not as structured as XML, which makes it difficult to create some of the rigid document structure required for scholarly documents. O’Reilly is trying to get more structure into HTML for print and digital books with <a href="https://github.com/oreillymedia/htmlbook">HTMLBook</a>, but with too much structure you might run into similar problems for authoring as discussed above for JATS XML. And of course you can include HTML in markdown documents.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Markdown to JATS XML in one Step]]></title>
        <id>5w3r1bm-sxk8h59-vn7gmhd-cwxn6</id>
        <link href="https://blog.front-matter.io/mfenner/from-markdown-to-jats-xml-in-one-step"/>
        <updated>2013-12-12T15:54:00.000Z</updated>
        <summary type="html"><![CDATA[The Journal Article Tag Suite (JATS) is a NISO standard that defines a set of XML elements and attributes for tagging journal articles. JATS is not only used for fulltext content at PubMed Central (and JATS has evolved from the NLM Archiving and...]]></summary>
        <content type="html"><![CDATA[<p>The Journal Article Tag Suite (<a href="http://jats.nlm.nih.gov/">JATS</a>) is a NISO standard that defines a set of XML elements and attributes for tagging journal articles. JATS is not only used for fulltext content at PubMed Central (and JATS has evolved from the NLM Archiving and Interchange Tag Suite originally developed for PubMed Central), but is also increasinly used by publishers.</p>
<p>For many publishers the <em>version of record</em> of an article is stored in XML, and other formats (currently HTML, PDF and increasingly ePub) are generated from this XML. Unfortunately the process of converting author-submitted manuscripts into JATS-compliant XML is time-consuming and costly, and this is a problem in particular for small publishers.</p>
<p>In a recent blog post (<a href="http://sensiblescience.io/mfenner/the-grammar-of-scholarly-communication/">The Grammar of Scholarly Communication</a>) I argued that publishers should accept manuscripts in any reasonable file format, including Microsoft Word, Open Office, LaTeX, Markdown, HTML and PDF. Readers of this blog know that I am a big fan of <a href="http://blog.martinfenner.org/tags.html#markdown-ref">markdown</a> for scholarly documents, but I am of course well aware that at the end of the day these documents have to be converted into JATS.</p>
<p>As a small step towards that goal I have today released the first public version of <a href="https://github.com/mfenner/pandoc-jats">pandoc-jats</a>, a <a href="http://johnmacfarlane.net/pandoc/README.html#custom-writers">custom writer for Pandoc</a> that converts markdown documents into JATS XML with a single command, e.g.</p>
<pre><code>pandoc -f example.md --filter pandoc-citeproc --bibliography=example.bib --csl=apa.csl -t JATS.lua -o example.xml</code></pre>
<p>Please see the <a href="https://github.com/mfenner/pandoc-jats">pandoc-jats</a> Github repository for more detailed information, but using this custom writer is as simple as downloading a single <code>JATS.lua</code>file. The big challenge is of course to make this custom writer work with as many documents as possible, and that will be my job the next few weeks. Two example JATS documents are below (both markdown versions of scholarly articles and posted on this blog as HTML):</p>
<ul>
<li>Nine simple ways to make it easier to (re)use your data (<a href="https://sensiblescience.io/mfenner/nine-simple-ways-to-make-it-easier-to-reuse-your-data/">HTML</a>, <a href="http://blog.martinfenner.org/files/10.7287.peerj.preprints.7v2.xml">JATS</a>)</li>
<li>What Can Article Level Metrics Do for You? (<a href="https://sensiblescience.io/mfenner/what-can-article-level-metrics-do-for-you/">HTML</a>, <a href="http://blog.martinfenner.org/files/10.1371.journal.pbio.1001687.xml">JATS</a>)</li>
</ul>
<p>Both JATS files were validated against the JATS DTD and XSD and showed no errors with the NLM XML StyleChecker - using the excellent <a href="https://github.com/PeerJ/jats-conversion">jats-conversion</a> conversion and validation tools written by Alf Eaton. Markdown is actually a nice file format to convert to XML - in contrast to HTML authors can’t for example put closing tags at the wrong places. And a Pandoc custom writer written in the Lua scripting language is an interesting alternative to XSLT transformations, the more common way to create JATS XML. The custom writer has not been tested with other Pandoc input formats besides markdown, of particular interest are of course HTML and LaTeX - Microsoft Word .docx is unfortunately only a Pandoc output format.</p>
<p>This is the first public release and there is of course a lot of room for improvement. Many elements and attributes are not yet supported - although <a href="http://orcid.org/blog/2013/03/22/orcid-how-more-specifying-orcid-ids-document-metadata">ORCID author identifiers</a> are of course included. Please help me improve this tool using the Github <a href="https://github.com/mfenner/pandoc-jats/issues">Issue Tracker</a>.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Example article with embedded code and data]]></title>
        <id>db6mtmv-p98z2sn-2vqr3rq-3523</id>
        <link href="https://blog.front-matter.io/mfenner/example-article-with-embedded-code-and-data"/>
        <updated>2013-12-11T16:04:00.000Z</updated>
        <summary type="html"><![CDATA[In October I published an essay on Article-Level Metrics (ALM) in PLOS Biology (Fenner, 2013). The essay is a good introduction into Article-Level Metrics, and I am proud that it is part of the Tenth Anniversary PLOS Biology Collection. Like all PLOS content,...]]></summary>
        <content type="html"><![CDATA[<p>In October I published an essay on Article-Level Metrics (ALM) in PLOS Biology (Fenner, 2013). The essay is a good introduction into Article-Level Metrics, and I am proud that it is part of the <a href="http://dx.doi.org/10.1371/issue.pcol.v06.i03">Tenth Anniversary PLOS Biology Collection</a>. Like all PLOS content, the article was published with a <a href="http://blogs.plos.org/tech/creative-commons-for-science-interview-with-puneet-kishor/">Creative Commons attribution license</a>, allowing me to republish the article on this blog. I have now done so and the article is available <a href="http://blog.martinfenner.org/2013/12/11/what-can-article-level-metrics-do-for-you/">here</a>.</p>
<p>Of course I didn’t want to simply republish the article, but I wanted to publish an improved version. The article has five figures, four of them show visualizations of ALM data that were generated using R (the fifth figure is a table reproduced from another article). The PLOS article includes the ALM dataset and the R scripts used to generate the figures as <a href="http://dx.doi.org/10.1371/journal.pbio.1001687.s001">supplementary information</a>. What I have done now is to recreate the article as a single markdown file (available <a href="https://github.com/mfenner/blog/blob/master/_posts/2013-12-11-what-can-article-level-metrics-do-for-you.Rmd">here</a>) that has all R code embedded. Using R and <a href="http://yihui.name/knitr/">knitr</a> - and the <a href="http://blog.martinfenner.org/data/alm_report_plos_biology_2013-05-20.csv">CSV file with the ALM data</a> - everyone can now reproduce the figures from the paper by simply running the embedded code, and can dig deeper into the data.</p>
<figure>
<img src="http://blog.martinfenner.org/images/2013-12-11_figure_3.svg" class="kg-image" alt="Figure 3. Views vs. citations for PLOS Biology articles published in 2010." /><figcaption aria-hidden="true"><strong>Figure 3.</strong> Views vs. citations for PLOS Biology articles published in 2010.</figcaption>
</figure>
<p>This was a good opportunity to improve the accessibility of the article in other ways. Instead of the raster image formats PNG, JPEG and TIFF used by PLOS and almost every other publisher, I generated the figures in the vector format SVG. Not only does SVG produce images independent of device resolution and screen size (try to zoom in on the figure above), but SVG can also easily be manipulated in the browser since it is XML. This is beyond the scope of this blog post, but look at the <a href="http://d3js.org/">d3.js</a> Javascript library for great examples of how SVG can be dynamically generated and changed in the browser. <strong><strong>Figure 3</strong></strong> above could for example be enhanced so that the article title is displayed when you hover over one of the bubbles, or we could enable zooming to show more detail.</p>
<p>Like all content on this blog, the article was created using <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a>, and the bibliography was dynamically generated. This makes it easy to change the citation style, and I decided to use the <a href="http://www.apastyle.org/">APA Style</a> that shows the citations in the text as author-date rather than numbered as with the PLOS style (see the example citation in the first paragraph). The combined bibliography for all blog posts including the article can be downloaded in bibtex format <a href="http://blog.martinfenner.org/bibliography/references.bib">here</a>.</p>
<p>Lastly, I wanted to generate nicer HTML for a better online reading experience. I haven’t done anything fancy, but most publishers seem to focus on navigation around an article, so that very little screen real estate is left for the actual content of the article. I’ve tried to improve readability by reducing the navigation areas to a minimum, by using readable fonts in larger sizes: <a href="https://typekit.com/fonts/minion-pro">Adobe Minion Pro</a> for the body text and <a href="https://typekit.com/fonts/myriad-pro">Adobe Myriad Pro</a> for headings, tables and figure legends.</p>
<h2 id="references">References</h2>
<p>Fenner, M. (2013). What can article-level metrics do for you? <em>PLoS Biol</em>, <em>11</em>(10), e1001687. <a href="http://doi.org/10.1371/journal.pbio.1001687">doi:10.1371/journal.pbio.1001687</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Can Article-Level Metrics Do for You?]]></title>
        <id>4wnt9kr-1hd8ns8-thwadgr-zaed7</id>
        <link href="https://blog.front-matter.io/mfenner/what-can-article-level-metrics-do-for-you"/>
        <updated>2013-12-11T15:58:00.000Z</updated>
        <summary type="html"><![CDATA[<em>Article-level metrics (ALMs) provide a wide range of metrics about the uptake of an individual journal article by the scientific community after publication. They include citations, usage statistics, discussions in online comments and social media,...]]></summary>
        <content type="html"><![CDATA[<p><em>Article-level metrics (ALMs) provide a wide range of metrics about the uptake of an individual journal article by the scientific community after publication. They include citations, usage statistics, discussions in online comments and social media, social bookmarking, and recommendations. In this essay, we describe why article-level metrics are an important extension of traditional citation-based journal metrics and provide a number of example from ALM data collected for PLOS Biology.</em></p>
<p><em>This is an open-access article distributed under the terms of the Creative Commons Attribution License, authored by me and <a href="http://dx.doi.org/10.1371/journal.pbio.1001687">originally published Oct 22, 2013 in PLOS Biology</a>.</em></p>
<p>The scientific impact of a particular piece of research is reflected in how this work is taken up by the scientific community. The first systematic approach that was used to assess impact, based on the technology available at the time, was to track citations and aggregate them by journal. This strategy is not only no longer necessary â€” since now we can easily track citations for individual articles â€” but also, and more importantly, journal-based metrics are now considered a poor performance measure for individual articles (Campbell, 2008; Glänzel &amp; Wouters, 2013). One major problem with journal-based metrics is the variation in citations per article, which means that a small percentage of articles can skew, and are responsible for, the majority of the journal-based citation impact factor, as shown by Campbell (2008) for the 2004 <em>Nature</em> Journal Impact Factor. <strong><strong>Figure 1</strong></strong> further illustrates this point, showing the wide distribution of citation counts between <em>PLOS Biology</em> research articles published in 2010. <em>PLOS Biology</em> research articles published in 2010 have been cited a median 19 times to date in Scopus, but 10% of them have been cited 50 or more times, and two articles (Dickson, Wang, Krantz, Hakonarson, &amp; Goldstein, 2010; Narendra et al., 2010) more than 300 times. <em>PLOS Biology</em> metrics are used as examples throughout this essay, and the dataset is available in the supporting information (<strong><strong>Data S1</strong></strong>). Similar data are available for an increasing number of other publications and organizations.</p>
<pre><code># code for figure 1: density plots for citation counts for PLOS Biology
# articles published in 2010

# load May 20, 2013 ALM report
alm &lt;- read.csv(&quot;data/alm_report_plos_biology_2013-05-20.csv&quot;, stringsAsFactors = FALSE)

# only look at research articles
alm &lt;- subset(alm, alm$article_type == &quot;Research Article&quot;)

# only look at papers published in 2010
alm$publication_date &lt;- as.Date(alm$publication_date)
alm &lt;- subset(alm, alm$publication_date &gt; &quot;2010-01-01&quot; &amp; alm$publication_date &lt;=
    &quot;2010-12-31&quot;)

# labels
colnames &lt;- dimnames(alm)[[2]]
plos.color &lt;- &quot;#1ebd21&quot;
plos.source &lt;- &quot;scopus&quot;

plos.xlab &lt;- &quot;Scopus Citations&quot;
plos.ylab &lt;- &quot;Probability&quot;

quantile &lt;- quantile(alm[, plos.source], c(0.1, 0.5, 0.9), na.rm = TRUE)

# plot the chart
opar &lt;- par(mai = c(0.5, 0.75, 0.5, 0.5), omi = c(0.25, 0.1, 0.25, 0.1), mgp = c(3,
    0.5, 0.5), fg = &quot;black&quot;, cex.main = 2, cex.lab = 1.5, col = plos.color,
    col.main = plos.color, col.lab = plos.color, xaxs = &quot;i&quot;, yaxs = &quot;i&quot;)

d &lt;- density(alm[, plos.source], from = 0, to = 100)
d$x &lt;- append(d$x, 0)
d$y &lt;- append(d$y, 0)
plot(d, type = &quot;n&quot;, main = NA, xlab = NA, ylab = NA, xlim = c(0, 100), frame.plot = FALSE)
polygon(d, col = plos.color, border = NA)
mtext(plos.xlab, side = 1, col = plos.color, cex = 1.25, outer = TRUE, adj = 1,
    at = 1)
mtext(plos.ylab, side = 2, col = plos.color, cex = 1.25, outer = TRUE, adj = 0,
    at = 1, las = 1)

par(opar)</code></pre>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/2013-12-11_figure_1.svg" class="kg-image" width="960" height="672" alt="Figure 1. Citation counts for PLOS Biology articles published in 2010. Scopus citation counts plotted as a probability distribution for all 197 PLOS Biology research articles published in 2010. Data collected May 20, 2013. Median 19 citations; 10% of papers have at least 50 citations." /><figcaption aria-hidden="true"><strong>Figure 1. Citation counts for PLOS Biology articles published in 2010.</strong> Scopus citation counts plotted as a probability distribution for all 197 <em>PLOS Biology</em> research articles published in 2010. Data collected May 20, 2013. Median 19 citations; 10% of papers have at least 50 citations.</figcaption>
</figure>
<p>Scientific impact is a multi-dimensional construct that can not be adequately measured by any single indicator (Bollen, Sompel, Hagberg, &amp; Chute, 2009; Glänzel &amp; Wouters, 2013; Schekman &amp; Patterson, 2013). To this end, PLOS has collected and displayed a variety of metrics for all its articles since 2009. The array of different categorised article-level metrics (ALMs) used and provided by PLOS as of August 2013 are shown in <strong><strong>Figure 2</strong></strong>. In addition to citations and usage statistics, i.e., how often an article has been viewed and downloaded, PLOS also collects metrics about: how often an article has been saved in online reference managers, such as Mendeley; how often an article has been discussed in its comments section online, and also in science blogs or in social media; and how often an article has been recommended by other scientists. These additional metrics provide valuable information that we would miss if we only consider citations. Two important shortcomings of citation-based metrics are that (1) they take years to accumulate and (2) citation analysis is not always the best indicator of impact in more practical fields, such as clinical medicine (Eck, Waltman, Raan, Klautz, &amp; Peul, 2013). Usage statistics often better reflect the impact of work in more practical fields, and they also sometimes better highlight articles of general interest (for example, the 2006 <em>PLOS Biology</em> article on the citation advantage of Open Access articles (Eysenbach, 2006), one of the 10 most-viewed articles published in <em>PLOS Biology</em>).</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/2013-12-11_figure_2.jpg" class="kg-image" width="2007" height="846" alt="Figure 2. Article-level metrics used by PLOS in August 2013 and their categories. Taken from (Lin &amp; Fenner, 2013) with permission by the authors." /><figcaption aria-hidden="true"><strong>Figure 2. Article-level metrics used by PLOS in August 2013 and their categories.</strong> Taken from <span class="citation" data-cites="Lin2013" style="box-sizing: border-box;">(Lin &amp; Fenner, 2013)</span> with permission by the authors.</figcaption>
</figure>
<p>A bubble chart showing all 2010 <em>PLOS Biology</em> articles (<strong><strong>Figure 3</strong></strong>) gives a good overview of the year’s views and citations, plus it shows the influence that the article type (as indicated by dot color) has on an article’s performance as measured by these metrics. The weekly <em>PLOS Biology</em> publication schedule is reflected in this figure, with articles published on the same day present in a vertical line. <strong><strong>Figure 3</strong></strong> also shows that the two most highly cited 2010 <em>PLOS Biology</em> research articles are also among the most viewed (indicated by the red arrows), but overall there isn’t a strong correlation between citations and views. The most-viewed article published in 2010 in <em>PLOS Biology</em> is an essay on Darwinian selection in robots (Floreano &amp; Keller, 2010). Detailed usage statistics also allow speculatulation about the different ways that readers access and make use of published literature; some articles are browsed or read online due to general interest while others that are downloaded (and perhaps also printed) may reflect the reader’s intention to look at the data and results in detail and to return to the article more than once.</p>
<pre><code># code for figure 3: Bubblechart views vs. citations for PLOS Biology
# articles published in 2010.

# Load required libraries
library(plyr)

# load May 20, 2013 ALM report
alm &lt;- read.csv(&quot;../data/alm_report_plos_biology_2013-05-20.csv&quot;, stringsAsFactors = FALSE,
    na.strings = c(&quot;0&quot;))

# only look at papers published in 2010
alm$publication_date &lt;- as.Date(alm$publication_date)
alm &lt;- subset(alm, alm$publication_date &gt; &quot;2010-01-01&quot; &amp; alm$publication_date &lt;=
    &quot;2010-12-31&quot;)

# make sure counter values are numbers
alm$counter_html &lt;- as.numeric(alm$counter_html)

# lump all papers together that are not research articles
reassignType &lt;- function(x) if (x == &quot;Research Article&quot;) 1 else 0
alm$article_group &lt;- aaply(alm$article_type, 1, reassignType)

# calculate article age in months
alm$age_in_months &lt;- (Sys.Date() - alm$publication_date)/365.25 * 12
start_age_in_months &lt;- floor(as.numeric(Sys.Date() - as.Date(strptime(&quot;2010-12-31&quot;,
    format = &quot;%Y-%m-%d&quot;)))/365.25 * 12)

# chart variables
x &lt;- alm$age_in_months
y &lt;- alm$counter
z &lt;- alm$scopus

xlab &lt;- &quot;Age in Months&quot;
ylab &lt;- &quot;Total Views&quot;

labels &lt;- alm$article_group
col.main &lt;- &quot;#1ebd21&quot;
col &lt;- &quot;#666358&quot;

# calculate bubble diameter
z &lt;- sqrt(z/pi)

# calculate bubble color
getColor &lt;- function(x) c(&quot;#c9c9c7&quot;, &quot;#1ebd21&quot;)[x + 1]
colors &lt;- aaply(labels, 1, getColor)

# plot the chart
opar &lt;- par(mai = c(0.5, 0.75, 0.5, 0.5), omi = c(0.25, 0.1, 0.25, 0.1), mgp = c(3,
    0.5, 0.5), fg = &quot;black&quot;, cex = 1, cex.main = 2, cex.lab = 1.5, col = &quot;white&quot;,
    col.main = col.main, col.lab = col)

plot(x, y, type = &quot;n&quot;, xlim = c(start_age_in_months, start_age_in_months + 13),
    ylim = c(0, 60000), xlab = NA, ylab = NA, las = 1)
symbols(x, y, circles = z, inches = exp(1.3)/15, bg = colors, xlim = c(start_age_in_months,
    start_age_in_months + 13), ylim = c(0, ymax), xlab = NA, ylab = NA, las = 1,
    add = TRUE)
mtext(xlab, side = 1, col = col.main, cex = 1.25, outer = TRUE, adj = 1, at = 1)
mtext(ylab, side = 2, col = col.main, cex = 1.25, outer = TRUE, adj = 0, at = 1,
    las = 1)

par(opar)</code></pre>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/2013-12-11_figure_3.svg" class="kg-image" width="960" height="672" alt="Figure 3. Views vs. citations for PLOS Biology articles published in 2010. All 304 PLOS Biology articles published in 2010. Bubble size correlates with number of Scopus citations. Research articles are labeled green; all other articles are grey. Red arrows indicate the two most highly cited papers. Data collected May 20, 2013." /><figcaption aria-hidden="true"><strong>Figure 3. Views vs. citations for PLOS Biology articles published in 2010.</strong> All 304 <em>PLOS Biology</em> articles published in 2010. Bubble size correlates with number of Scopus citations. Research articles are labeled green; all other articles are grey. Red arrows indicate the two most highly cited papers. Data collected May 20, 2013.</figcaption>
</figure>
<p>When readers first see an interesting article, their response is often to view or download it. By contrast, a citation may be one of the last outcomes of their interest, occuring only about 1 in 300 times a PLOS paper is viewed online. A lot of things happen in between these potential responses, ranging from discussions in comments, social media, and blogs, to bookmarking, to linking from websites. These activities are usually subsumed under the term â€œaltmetrics,â€? and their variety can be overwhelming. Therefore, it helps to group them together into categories, and several organizations, including PLOS, are using the category labels of Viewed, Cited, Saved, Discussed, and Recommended (<strong><strong>Figures 2 and 4</strong></strong>, see also (Lin &amp; Fenner, 2013)).</p>
<pre><code># code for figure 4: bar plot for Article-level metrics for PLOS Biology

# Load required libraries
library(reshape2)

# load May 20, 2013 ALM report
alm &lt;- read.csv(&quot;../data/alm_report_plos_biology_2013-05-20.csv&quot;, stringsAsFactors = FALSE,
    na.strings = c(0, &quot;0&quot;))

# only look at research articles
alm &lt;- subset(alm, alm$article_type == &quot;Research Article&quot;)

# make sure columns are in the right format
alm$counter_html &lt;- as.numeric(alm$counter_html)
alm$mendeley &lt;- as.numeric(alm$mendeley)

# options
plos.color &lt;- &quot;#1ebd21&quot;
plos.colors &lt;- c(&quot;#a17f78&quot;, &quot;#ad9a27&quot;, &quot;#ad9a27&quot;, &quot;#ad9a27&quot;, &quot;#ad9a27&quot;, &quot;#ad9a27&quot;,
    &quot;#dcebdd&quot;, &quot;#dcebdd&quot;, &quot;#789aa1&quot;, &quot;#789aa1&quot;, &quot;#789aa1&quot;, &quot;#304345&quot;, &quot;#304345&quot;)

# use subset of columns
alm &lt;- subset(alm, select = c(&quot;f1000&quot;, &quot;wikipedia&quot;, &quot;researchblogging&quot;, &quot;comments&quot;,
    &quot;facebook&quot;, &quot;twitter&quot;, &quot;citeulike&quot;, &quot;mendeley&quot;, &quot;pubmed&quot;, &quot;crossref&quot;, &quot;scopus&quot;,
    &quot;pmc_html&quot;, &quot;counter_html&quot;))

# calculate percentage of values that are not missing (i.e. have a count of
# at least 1)
colSums &lt;- colSums(!is.na(alm)) * 100/length(alm$counter_html)
exactSums &lt;- sum(as.numeric(alm$pmc_html), na.rm = TRUE)

# plot the chart
opar &lt;- par(mar = c(0.1, 7.25, 0.1, 0.1) + 0.1, omi = c(0.1, 0.25, 0.1, 0.1),
    col.main = plos.color)

plos.names &lt;- c(&quot;F1000Prime&quot;, &quot;Wikipedia&quot;, &quot;Research Blogging&quot;, &quot;PLOS Comments&quot;,
    &quot;Facebook&quot;, &quot;Twitter&quot;, &quot;CiteULike&quot;, &quot;Mendeley&quot;, &quot;PubMed Citations&quot;, &quot;CrossRef&quot;,
    &quot;Scopus&quot;, &quot;PMC HTML Views&quot;, &quot;PLOS HTML Views&quot;)
y &lt;- barplot(colSums, horiz = TRUE, col = plos.colors, border = NA, xlab = plos.names,
    xlim = c(0, 120), axes = FALSE, names.arg = plos.names, las = 1, adj = 0)
text(colSums + 6, y, labels = sprintf(&quot;%1.0f%%&quot;, colSums))

par(opar)</code></pre>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/2013-12-11_figure_4.svg" class="kg-image" alt="Figure 4. Article-level metrics for PLOS Biology. Proportion of all 1,706 PLOS Biology research articles published up to May 20, 2013 mentioned by particular article-level metrics source. Colors indicate categories (Viewed, Cited, Saved, Discussed, Recommended), as used on the PLOS website." /><figcaption aria-hidden="true"><strong>Figure 4.</strong> Article-level metrics for PLOS Biology. Proportion of all 1,706 <em>PLOS Biology</em> research articles published up to May 20, 2013 mentioned by particular article-level metrics source. Colors indicate categories (Viewed, Cited, Saved, Discussed, Recommended), as used on the PLOS website.</figcaption>
</figure>
<p>All <em>PLOS Biology</em> articles are viewed and downloaded, and almost all of them (all research articles and nearly all front matter) will be cited sooner or later. Almost all of them will also be bookmarked in online reference managers, such as Mendeley, but the percentage of articles that are discussed online is much smaller. Some of these percentages are time dependent; the use of social media discussion platforms, such as Twitter and Facebook for example, has increased in recent years (93% of <em>PLOS Biology</em> research articles published since June 2012 have been discussed on Twitter, and 63% mentioned on Facebook). These are the locations where most of the online discussion around published articles currently seems to take place; the percentage of papers with comments on the PLOS website or that have science blog posts written about them is much smaller. Not all of this online discussion is about research articles, and perhaps, not surprisingly, the most-tweeted PLOS article overall (with more than 1,100 tweets) is a <em>PLOS Biology</em> perspective on the use of social media for scientists (Bik &amp; Goldstein, 2013).</p>
<p>Some metrics are not so much indicators of a broad online discussion, but rather focus on highlighting articles of particular interest. For example, science blogs allow a more detailed discussion of an article as compared to comments or tweets, and journals themselves sometimes choose to highlight a paper on their own blogs, allowing for a more digestible explanation of the science for the non-expert reader (Fausto et al., 2012). Coverage by other bloggers also serves the same purpose; a good example of this is one recent post on the OpenHelix Blog (“Video Tip of the Week: Turkeys and their genomes,” 2012) that contains video footage of the second author of a 2010 <em>PLOS Biology</em> article (Dalloul et al., 2010) discussing the turkey genome.</p>
<p>F1000Prime, a commercial service of recommendations by expert scientists, was added to the PLOS Article-Level Metrics in August 2013. We now highlight on the PLOS website when any articles have received at least one recommendation within F1000Prime. We also monitor when an article has been cited within the widely used modern-day online encyclopedia, Wikipedia. A good example of the latter is the Tasmanian devil Wikipedia page (“Tasmanian devil,” 2013) that links to a <em>PLOS Biology</em> research article published in 2010 (Nilsson et al., 2010). While a F1000Prime recommendation is a strong endorsement from peer(s) in the scientific community, being included in a Wikipedia page is akin to making it into a textbook about the subject area and being read by a much wider audience that goes beyond the scientific community.</p>
<p><em>PLOS Biology</em> is the PLOS journal with the highest percentage of articles recommended in F1000Prime and mentioned in Wikipedia, but there is only partial overlap between the two groups of articles because they focus on different audiences (<strong><strong>Figure 5</strong></strong>). These recommendations and mentions in turn show correlations with other metrics, but not simple ones; you can’t assume, for example, that highly cited articles are more likely to be recommended by F1000Prime, so it will be interesting to monitor these trends now that we include this information.</p>
<pre><code># code for figure 5: Venn diagram F1000 vs. Wikipedia for PLOS Biology
# articles

# load required libraries
library(&quot;plyr&quot;)
library(&quot;VennDiagram&quot;)

# load May 20, 2013 ALM report
alm &lt;- read.csv(&quot;../data/alm_report_plos_biology_2013-05-20.csv&quot;, stringsAsFactors = FALSE)

# only look at research articles
alm &lt;- subset(alm, alm$article_type == &quot;Research Article&quot;)

# group articles based on values in Wikipedia and F1000
reassignWikipedia &lt;- function(x) if (x &gt; 0) 1 else 0
alm$wikipedia_bin &lt;- aaply(alm$wikipedia, 1, reassignWikipedia)
reassignF1000 &lt;- function(x) if (x &gt; 0) 2 else 0
alm$f1000_bin &lt;- aaply(alm$f1000, 1, reassignF1000)
alm$article_group = alm$wikipedia_bin + alm$f1000_bin
reassignCombined &lt;- function(x) if (x == 3) 1 else 0
alm$combined_bin &lt;- aaply(alm$article_group, 1, reassignCombined)
reassignNo &lt;- function(x) if (x == 0) 1 else 0
alm$no_bin &lt;- aaply(alm$article_group, 1, reassignNo)

# remember to divide f1000_bin by 2, as this is the default value
summary &lt;- colSums(subset(alm, select = c(&quot;wikipedia_bin&quot;, &quot;f1000_bin&quot;, &quot;combined_bin&quot;,
    &quot;no_bin&quot;)), na.rm = TRUE)
rows &lt;- nrow(alm)

# options
plos.colors &lt;- c(&quot;#c9c9c7&quot;, &quot;#0000ff&quot;, &quot;#ff0000&quot;)

# plot the chart
opar &lt;- par(mai = c(0.5, 0.75, 3.5, 0.5), omi = c(0.5, 0.5, 1.5, 0.5), mgp = c(3,
    0.5, 0.5), fg = &quot;black&quot;, cex.main = 2, cex.lab = 1.5, col = plos.color,
    col.main = plos.color, col.lab = plos.color, xaxs = &quot;i&quot;, yaxs = &quot;i&quot;)

venn.plot &lt;- draw.triple.venn(area1 = rows, area2 = summary[1], area3 = summary[2]/2,
    n12 = summary[1], n23 = summary[3], n13 = summary[2]/2, n123 = summary[3],
    euler.d = TRUE, scaled = TRUE, fill = plos.colors, cex = 2, fontfamily = rep(&quot;sans&quot;,
        7))

par(opar)</code></pre>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/2013-12-11_figure_5.svg" class="kg-image" width="480" height="480" alt="Figure 5. PLOS Biology articles: sites of recommendation and discussion. Number of PLOS Biology research articles published up to May 20, 2013 that have been recommended by F1000Prime (red) or mentioned in Wikipedia (blue)." /><figcaption aria-hidden="true"><strong>Figure 5.</strong> PLOS Biology articles: sites of recommendation and discussion. Number of <em>PLOS Biology</em> research articles published up to May 20, 2013 that have been recommended by F1000Prime (red) or mentioned in Wikipedia (blue).</figcaption>
</figure>
<p>With the increasing availability of ALM data, there comes a growing need to provide tools that will allow the community to interrogate them. A good first step for researchers, research administrators, and others interested in looking at the metrics of a larger set of PLOS articles is the recently launched ALM Reports tool (“ALM Reports,” 2013). There are also a growing number of service providers, including <a href="http://altmetric.com/">Altmetric.com</a> (“<a href="http://altmetric.com/">Altmetric.com</a>,” 2013), ImpactStory (“ImpactStory,” 2013), and Plum Analytics (“Plum Analytics,” 2013) that provide similar services for articles from other publishers.</p>
<p>As article-level metrics become increasingly used by publishers, funders, universities, and researchers, one of the major challenges to overcome is ensuring that standards and best practices are widely adopted and understood. The National Information Standards Organization (NISO) was recently awarded a grant by the Alfred P. Sloan Foundation to work on this (“NISO Alternative Assessment Metrics (Altmetrics) Project,” 2013), and PLOS is actively involved in this project. We look forward to further developing our article-level metrics and to having them adopted by other publishers, which hopefully will pave the way to their wide incorporation into research and researcher assessments.</p>
<h3 id="supporting-information">Supporting Information</h3>
<p><strong><strong><a href="http://dx.doi.org/10.1371/journal.pbio.1001687.s001">Data S1</a>. Dataset of ALM for PLOS Biology articles used in the text, and R scripts that were used to produce figures.</strong></strong> The data were collected on May 20, 2013 and include all <em>PLOS Biology</em> articles published up to that day. Data for F1000Prime were collected on August 15, 2013. All charts were produced with R version 3.0.0.</p>
<h2 id="references">References</h2>
<p>ALM Reports. (2013). Retrieved from <a href="http://almreports.plos.org/">http://almreports.plos.org</a></p>
<p><a href="http://altmetric.com/">Altmetric.com</a>. (2013). Retrieved from <a href="http://www.altmetric.com/">http://www.altmetric.com/</a></p>
<p>Bik, H. M., &amp; Goldstein, M. C. (2013). An introduction to social media for scientists. <em>PLOS Biology</em>, <em>11</em>(4), e1001535. <a href="http://doi.org/10.1371/journal.pbio.1001535">doi:10.1371/journal.pbio.1001535</a></p>
<p>Bollen, J., Sompel, H. de, Hagberg, A., &amp; Chute, R. (2009). A Principal Component Analysis of 39 Scientific Impact Measures. <em>PLoS ONE</em>, <em>4</em>(6), e6022. <a href="http://doi.org/10.1371/journal.pone.0006022">doi:10.1371/journal.pone.0006022</a></p>
<p>Campbell, P. (2008). Escape from the impact factor. <em>Ethics in Science and Environmental Politics</em>, <em>8</em>, 5–7. Journal article. <a href="http://doi.org/10.3354/esep00078">doi:10.3354/esep00078</a></p>
<p>Dalloul, R. A., Long, J. A., Zimin, A. V., Aslam, L., Beal, K., Blomberg, L. A., … Reed, K. M. (2010). Multi-platform next-generation sequencing of the domestic turkey (Meleagris gallopavo): genome assembly and analysis. <em>PLOS Biology</em>, <em>8</em>(9). <a href="http://doi.org/10.1371/journal.pbio.1000475">doi:10.1371/journal.pbio.1000475</a></p>
<p>Dickson, S. P., Wang, K., Krantz, I., Hakonarson, H., &amp; Goldstein, D. B. (2010). Rare variants create synthetic genome-wide associations. <em>PLOS Biology</em>, <em>8</em>(1), e1000294. <a href="http://doi.org/10.1371/journal.pbio.1000294">doi:10.1371/journal.pbio.1000294</a></p>
<p>Eck, N. J. van, Waltman, L., Raan, A. F. J. van, Klautz, R. J. M., &amp; Peul, W. C. (2013). Citation analysis may severely underestimate the impact of clinical research as compared to basic research. <em>PLOS ONE</em>, <em>8</em>(4), e62395. <a href="http://doi.org/10.1371/journal.pone.0062395">doi:10.1371/journal.pone.0062395</a></p>
<p>Eysenbach, G. (2006). Citation advantage of open access articles. <em>PLOS Biology</em>, <em>4</em>(5), e157. <a href="http://doi.org/10.1371/journal.pbio.0040157">doi:10.1371/journal.pbio.0040157</a></p>
<p>Fausto, S., Machado, F. A., Bento, L. F. J., Iamarino, A., Nahas, T. R., &amp; Munger, D. S. (2012). Research blogging: indexing and registering the change in science 2.0. <em>PLOS ONE</em>, <em>7</em>(12), e50109. <a href="http://doi.org/10.1371/journal.pone.0050109">doi:10.1371/journal.pone.0050109</a></p>
<p>Floreano, D., &amp; Keller, L. (2010). Evolution of adaptive behaviour in robots by means of Darwinian selection. <em>PLOS Biology</em>, <em>8</em>(1), e1000292. <a href="http://doi.org/10.1371/journal.pbio.1000292">doi:10.1371/journal.pbio.1000292</a></p>
<p>Glänzel, W., &amp; Wouters, P. (2013). The dos and don’ts in individudal level bibliometrics. Retrieved from <a href="http://de.slideshare.net/paulwouters1/issi2013-wg-pw">http://de.slideshare.net/paulwouters1/issi2013-wg-pw</a></p>
<p>ImpactStory. (2013). Retrieved from <a href="http://impactstory.org/">http://impactstory.org/</a></p>
<p>Lin, J., &amp; Fenner, M. (2013). Altmetrics in Evolution: Defining and Redefining the Ontology of Article-Level Metrics. <em>Information Standards Quarterly</em>, <em>25</em>(2), 20. <a href="http://doi.org/10.3789/isqv25no2.2013.04">doi:10.3789/isqv25no2.2013.04</a></p>
<p>Narendra, D. P., Jin, S. M., Tanaka, A., Suen, D.-F., Gautier, C. A., Shen, J., … Youle, R. J. (2010). PINK1 is selectively stabilized on impaired mitochondria to activate Parkin. <em>PLOS Biology</em>, <em>8</em>(1), e1000298. <a href="http://doi.org/10.1371/journal.pbio.1000298">doi:10.1371/journal.pbio.1000298</a></p>
<p>Nilsson, M. A., Churakov, G., Sommer, M., Tran, N. V., Zemann, A., Brosius, J., &amp; Schmitz, J. (2010). Tracking marsupial evolution using archaic genomic retroposon insertions. <em>PLOS Biology</em>, <em>8</em>(7), e1000436. <a href="http://doi.org/10.1371/journal.pbio.1000436">doi:10.1371/journal.pbio.1000436</a></p>
<p>NISO Alternative Assessment Metrics (Altmetrics) Project. (2013). Retrieved from <a href="http://www.niso.org/topics/tl/altmetrics/initiative">http://www.niso.org/topics/tl/altmetrics/initiative</a></p>
<p>Plum Analytics. (2013). Retrieved from <a href="http://www.plumanalytics.com/">http://www.plumanalytics.com/</a></p>
<p>Schekman, R., &amp; Patterson, M. (2013). Reforming research assessment. <em>eLife</em>, <em>2</em>, e00855. <a href="http://doi.org/10.7554/eLife.00855">doi:10.7554/eLife.00855</a></p>
<p>Tasmanian devil. (2013). Retrieved from <a href="http://en.wikipedia.org/wiki/Tasmanian%5Cdevil">http://en.wikipedia.org/wiki/Tasmanian\devil</a></p>
<p>Video Tip of the Week: Turkeys and their genomes. (2012). Retrieved from <a href="http://blog.openhelix.eu/?p=14388">http://blog.openhelix.eu/?p=14388</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Opening Science - the Book]]></title>
        <id>4rt4r9x-k0d9wpr-tm36qrt-zfpdd</id>
        <link href="https://blog.front-matter.io/mfenner/opening-science-the-book"/>
        <updated>2013-12-05T16:06:00.000Z</updated>
        <summary type="html"><![CDATA[Opening Science: The Evolving Guide on How the Internet is Changing Research, Collaboration and Scholarly Publishing is a SpringerOpen book (using a Creative Commons Attribution-NonCommercial license) that will be published in a few weeks....]]></summary>
        <content type="html"><![CDATA[<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/17th/B00HD102QG.01._SCLZZZZZZZ_SX500_.jpg" class="kg-image" width="332" height="500" />
</figure>
<p><a href="http://www.openingscience.org/get-the-book/">Opening Science: The Evolving Guide on How the Internet is Changing Research, Collaboration and Scholarly Publishing</a> is a SpringerOpen book (using a <a href="http://book.openingscience.org/cases_recipes_howtos/creative_commons_licences">Creative Commons Attribution-NonCommercial license</a>) that will be published in a few weeks. If you can’t wait for the book to be published and/or you want to make comments or suggestions, go to the dynamic book online version at <a href="http://book.openingscience.org/">http://book.openingscience.org</a>. I am an author or co-author of three chapters (<a href="http://book.openingscience.org/tools/reference_management">Reference Management</a>, <a href="http://book.openingscience.org/vision/altmetrics">Altmetrics and Other Novel Measures for Scientific Impact</a>, <a href="http://book.openingscience.org/cases_recipes_howtos/unique_identifiers_for_researchers">Unique Identifiers for Researchers</a>) and have helped put the dynamic book together. The book is generated from markdown files hosted in a <a href="https://github.com/openingscience/book/">public Github repo</a> using <a href="http://jekyllrb.com/">Jekyll</a> and <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a>, and we use <a href="http://prose.io/">Prose</a> to enable online editing of the content.</p>
<p>Using markdown, github, jekyll and pandoc is nothing new for blogs, but this is probably one of the first scholarly books using this workflow. The dynamic book is therefore still very much work in progress and feedback is greatly appreciated.</p>
<p>Another great example using a very similar workflow is the upcoming book <a href="http://adv-r.had.co.nz/">Advanced R Programming</a> by Hadley Wickham, but he is of course using R and <a href="http://yihui.name/knitr/">knitr</a> to create most of the markdown. In contrast to Hadley we stored the individual chapters as Jekyll posts rather than pages, as this better integrates with other Jekyll functionality, e.g. tags.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Grammar of Scholarly Communication]]></title>
        <id>1hywb10-a4z994r-j7nmkvc-h2bad</id>
        <link href="https://blog.front-matter.io/mfenner/the-grammar-of-scholarly-communication"/>
        <updated>2013-11-17T16:10:00.000Z</updated>
        <summary type="html"><![CDATA[Authoring of scholarly articles is a recurring theme in this blog since it started in 2008. Authoring is still in desperate need for improvement, and nobody has convincingly figured out how to solve this problem. Authoring involves several steps,...]]></summary>
        <content type="html"><![CDATA[<p>Authoring of scholarly articles is a recurring theme in this blog since it started in 2008. Authoring is still in desperate need for improvement, and nobody has convincingly figured out how to solve this problem. Authoring involves several steps, and it helps to think about them separately:</p>
<ul>
<li><strong><strong>Writing</strong></strong>. Manuscript writing, including formatting, collaborative authoring</li>
<li><strong><strong>Submission</strong></strong>. Formatting a manuscript according to a publisher’s author guidelines, and handing it over to a publishing platform</li>
<li><strong><strong>Revision</strong></strong>. Changes made to a manuscript in the peer review process, or after publication</li>
</ul>
<p>Although authoring typically involves text, similar issues arise for other research outputs, e.g research data. And these considerations are also relevant for other forms of publishing, whether it is self-publication on a blog or website, or publishing of preprints and white papers.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/12th/1278021067_0bf5c8aa82_c.jpeg" class="kg-image" width="800" height="595" alt="Flickr photo by citnaj." /><figcaption aria-hidden="true">Flickr photo by <a href="http://www.flickr.com/photos/citnaj/1278021067/">citnaj</a>.</figcaption>
</figure>
<p>For me the main challenge in authoring is to go from human-readable unstructured content to highly structured machine-readable content. We could make authoring simpler by either forgoing any structure and just publishing in any format we want, or we can force authors to structure their manuscripts according to a very specific set of rules. The former doesn’t seem to be an option, not only do we have a set of community standards that have evolved for a very long time (research articles for example have title, authors, results, references, etc.), but it also makes it hard to find and reuse scholarly research by others.</p>
<p>The latter option is also not really viable since most researchers haven’t learned to produce their research outputs in machine-readable highly standardized formats. There are some exceptions, e.g. <a href="http://www.consort-statement.org/">CONSORT</a> and other reporting standards in clinical medicine or the <a href="http://blogs.ch.cam.ac.uk/pmr/2012/01/23/brian-mcmahon-publishing-semantic-crystallography-every-science-data-publisher-should-watch-this-all-the-way-through/">semantic publishing in Crystallography</a>, but for the most part research outputs are too diverse to easily find a format that works for all of them. The current trend is certainly towards machine-readable rather than towards human-readable, but there is still a significant gap - scholarly articles are transformed from documents in Microsoft Word (or sometimes LaTeX) format into XML (for most biomedical research that means <a href="http://jats.nlm.nih.gov/publishing/">JATS</a>) using kludgy tools and lots of manual labor.</p>
<p>What solutions have been tried to overcome the limitations of our current authoring tools, and to make the process more enjoyable for authors and more productive for publishers?</p>
<ol>
<li>Do the conversion manually, still a common workflow.</li>
<li>Tools for publishers such as <a href="http://blogs.plos.org/mfenner/2009/05/01/extyles_interview_with_elizabeth_blake_and_bruce_rosenblum/">eXtyles</a>, <a href="http://www.shabash.net/merops/">Merops</a> - both commercial - or the evolving Open Source <a href="http://www.lib.umich.edu/mpach/modules">mPach</a> that convert Microsoft Word documents into JATS XML and do a lot of automated checks along the way.</li>
<li>Tools for authors that directly generate JATS XML, either as a Microsoft Word plugin (the <a href="http://blogs.nature.com/mfenner/2008/11/07/interview-with-pablo-fernicola">Article Authoring Add-In</a>, not actively maintained) in the browser (e.g. <a href="http://blogs.plos.org/mfenner/2009/02/27/lemon8_xml_interview_with_mj_suhonos/">Lemon8-XML</a>, not actively maintained), or directly in a publishing platform such as Wordpress (<a href="http://annotum.org/">Annotum</a>).</li>
<li>Forget about XML and use HTML5 has the canonical file format, e.g. as <a href="http://blogs.plos.org/mfenner/2011/03/19/a-very-brief-history-of-scholarly-html/">Scholarly HTML</a> or HTML5 specifications such as <a href="https://github.com/oreillymedia/HTMLBook/blob/master/specification.asciidoc">HTMLBook</a>. Please read Molly Sharp’s <a href="http://blogs.plos.org/tech/structured-documents-for-science-jats-xml-as-canonical-content-format/">blog post</a> for background information about HTML as an alternative to XML.</li>
<li>Use file formats for authoring that are a better fit for the requirements of scholarly authors, in particular <a href="http://sensiblescience.io/mfenner/a-call-for-scholarly-markdown/">Scholarly Markdown</a>.</li>
<li>Build online editors for scientific content that hide the underlying file format, and guide users towards a structured format, e.g. by not allowing input that doesn’t conform to specifications.</li>
</ol>
<p><strong><strong>Solution 1.</strong></strong> isn’t really an option, as it makes scholarly publishing unnecessarily slow and expensive. Typesetter Kaveh Bazergan has gone on record at the <a href="http://www.nature.com/spoton/2012/11/spoton-london-2012-a-global-conference/">SpotOn London Conference 2012</a> by saying that the current process is insane and that he wants to be “put out of business”.</p>
<p><strong><strong>Solution 2.</strong></strong> is probably the most commonly used workflow used by larger publishers today, but is very much centered around a Microsoft Word to XML workflow. LaTeX is a popular authoring environment in some disciplines, but still requires work to convert documents into web-friendly formats such as HTML and XML.</p>
<p><strong><strong>Solutions 3. to 5.</strong></strong> have never picked up any significant traction. Overall the progress in this area has been modest at best, and the mainstream of authoring today isn’t too different from 20 years ago. Although I have gone on record for saying that <a href="http://blog.martinfenner.org/tags.html#markdown-ref">Scholarly Markdown</a> has a lot of potential, the problem is much bigger than finding a single file format, and markdown will never be the solution for all authoring needs.</p>
<p><strong><strong>Solution 6.</strong></strong> is an area where a lot of exciting development is currently happening, examples include <a href="https://www.authorea.com/">Authorea</a>, <a href="https://www.writelatex.com/">WriteLateX</a>, <a href="https://www.sharelatex.com/">ShareLaTeX</a>. Although the future of scholarly authoring will certainly include online authoring tools (making it much easier to collaborate, one of the authoring pain points), we run the risk of locking in users into one particular authoring environment.</p>
<h3 id="going-forward">Going Forward</h3>
<p>How can we move forward? I would suggest the following:</p>
<ol>
<li>Publishers should accept manuscripts in any reasonable file format, which means at least Microsoft Word, Open Office, LaTeX, Markdown, HTML and PDF, but possibly more. This will create a lot of extra work for publishers, but will open the doors for innovation, both in the academic and commercial sector. We will never see significant progress in scholarly authoring tools if the submission step requires manuscripts to be in a single file format (Microsoft Word) - in particular since this file format is a general purpose word processsing format and not something designed specifically for scholarly content. And we want researchers to spend their time doing research and writing up their research, not formatting documents.</li>
<li>To handle this avalanche of unstructured documents, publishers need conversion tools that can transform all these documents into a format that can feed into their editorial and publishing workflows. A limited number of these tools exist already, but this will require a significant development effort. Again, opening up submissions to a variety of file formats will not only foster innovation in authoring tools, but also in document conversion tools.</li>
<li>We should think beyond XML. Many of the workflows designed today center around conversions from one XML format to another, e.g. Microsoft Word to JATS or <a href="http://www.tei-c.org/index.xml">TEI</a> (popular in the humanities), often using XLST transforms. Not only is XML difficult for humans to read or edit, but the web and many of the technologies built around it are moving away from XML towards HTML5 and JSON. XML is fine as an important output format for publishing, but maybe not the best format to hold everything together.</li>
<li>As we haven’t come up with a canonical file format for scholarly documents by now, we should give up that idea. XML is great for publisher workflows, but is not something humans can easily edit or read. PDF is still the most widely read format by humans, but is not a good intermediary format. LaTeX is too complex for authors outside of mathematics, physics and related fields, and is not built with web standards in mind. Markdown is promising, but doesn’t easily support highly structured content. And HTML5 and the related ePub are widely popular, but can be hard to edit without a visual editor, and currently don’t include enough standard metadata to support scholarly content out of the box.</li>
<li>The focus should not be on canonical file formats for scholarly documents, but on tools that understand the manuscripts created by researchers and can transform them into something more structured. As we have learned from document conversion tools such as <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a>, we can’t do this with a simple find and replace using regular expressions, but need a more structured approach. Pandoc is taking the input document (markdown, LaTeX or HTML) apart and is constructing an abstract syntax tree (<a href="http://en.wikipedia.org/wiki/Abstract_syntax_tree">AST</a>) of the document, using parsing expression grammar (<a href="http://en.wikipedia.org/wiki/Parsing_expression_grammar">PEG</a>), which includes a set of parsing rules. Parsing expression grammars are fairly new, <a href="http://bford.info/pub/lang/peg">first described by Bryan Ford</a> about 10 years ago, but in my mind are a very good fit for the formal grammar of scientific documents. It should be fairly straightforward to generate a variety of output formats from the AST (Pandoc can convert into more than 30 document formats), the hard part is the parsing of the input.</li>
</ol>
<p>All this requires a lot of work. Pandoc is a good model to start, but is written in Haskell, a functional programming language that not many people are familar with. For small changes Pandoc allows you to directly manipulate the AST (represented as JSON) using <a href="http://johnmacfarlane.net/pandoc/scripting.html">filters</a> written in Haskell or Python. And <a href="https://github.com/jgm/pandoc">custom writers</a> for other document formats can be written using <a href="http://www.lua.org/">Lua</a>, another interesting programming language that not many people know about. Lua is a fast and relatively easy to learn scripting language that can be easily embedded into other languages, and for similar reasons is also used to <a href="http://en.wikipedia.org/wiki/Wikipedia:Lua">extend the functionality of Wikipedia</a>. PEG parsers in other languages include <a href="http://treetop.rubyforge.org/">Treetop</a> (Ruby), <a href="http://pegjs.majda.cz/">PEG.js</a> (Javascript), and <a href="http://www.antlr.org/">ANTLR</a>, a popular parser generator that also includes PEG features.</p>
<p>But I think the effort to build a solid open source conversion tool for scholarly documents is worth it, in particular for smaller publishers and publishing platforms who can’t afford the commercial Microsoft Word to JATS conversion tools. We shouldn’t take any shortcuts - e.g. by focussing on XML and XLST transforms - and we can improve this tool over time, e.g. by starting with a few input and output formats. This tool will be valuable beyond authoring, as it can also be very helpful to convert published scholarly content into other formats such as ePub, and in text mining, which in many ways tries to solve many of the same problems. The <a href="http://johnmacfarlane.net/pandoc/scripting.html">Pandoc documentation</a> includes an example of extracting all URLs out of a document, and this can be modified to extract other content. In case you wonder whether I gave up on the idea of <a href="http://blog.martinfenner.org/tags.html#markdown-ref">Scholarly Markdown</a> - not at all. To me this is a logical next step, opening up journal submission systems to Scholarly Markdown and other evolving file formats. And Pandoc, one of the most interesting tools in this space, is a markdown conversion tool at its heart. The next steps could be the following:</p>
<ul>
<li>write a custom writer in Lua that generates JATS output from Pandoc</li>
<li>explore how difficult it would be to add Microsoft Word .docx as Pandoc input format</li>
<li>develop Pandoc filters relevant for scholarly documents (e.g. <a href="http://sensiblescience.io/mfenner/auto-generating-links-to-data-and-resources/">auto-linking accession numbers of biomedical databases</a>)</li>
</ul>
<hr />
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is holding us back?]]></title>
        <id>6p0f4v6-8498gjt-d073r17-wqxee</id>
        <link href="https://blog.front-matter.io/mfenner/what-is-holding-us-back"/>
        <updated>2013-11-11T16:14:00.000Z</updated>
        <summary type="html"><![CDATA[Last Friday and Saturday the 6th SpotOn London conference tool place at the British Library. I had a great time with many interesting sessions and good conversations both in and between sessions. But I might be biased, since I helped organize the event,...]]></summary>
        <content type="html"><![CDATA[<p>Last Friday and Saturday the 6th <a href="http://www.nature.com/spoton/event/spoton-london2013/">SpotOn London conference</a> tool place at the British Library. I had a great time with many interesting sessions and good conversations both in and between sessions. But I might be biased, since I helped organize the event, and in particular did help put the <a href="http://www.nature.com/spoton/?cat=11">sessions for the Tools strand</a> together.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/11th/10739125344_fb8533423a_o.jpg" class="kg-image" width="640" height="640" alt="SpotOn London name tags. Flickr photo by keatl." /><figcaption aria-hidden="true"><strong>SpotOn London name tags</strong>. Flickr photo by <a href="http://www.flickr.com/photos/keatl/10739125344/in/photolist-hmYPPw-9CVkfd/">keatl</a>.</figcaption>
</figure>
<p>The following blog post summarizes some of my thoughts before, during and after the conference, and I want to focus on innovation in scholarly publishing, or rather: what is holding us back?</p>
<h2 id="reason-1">Reason #1</h2>
<p>The <a href="http://www.nature.com/spoton/event/spoton-london-2013-whats-your-number-altmetrics-session/">#solo13alt</a> session on Saturday looked at <em>the role of altmetrics in the evaluation of scientific research</em>. I was one of the panelists and had summarized my ideas prior to the session in a <a href="http://blogs.plos.org/tech/evaluating-impact-whats-your-number/">blog post</a> written together with Jennifer Lin. It was an interesting session, although a bit too controversial for my taste. But it became obvious to me in this and a few other sessions that other obsession with quantitative assessment of science is increasingly dangerous. Other people have said this more eloquently:</p>
<ul>
<li>The mania for measurement - Stephen Curry in the <a href="http://www.nature.com/spoton/event/spoton-london-2013-whats-your-number-altmetrics-session/">#solo13alt</a> session</li>
<li>Why research assessment is out of control - <a href="http://www.theguardian.com/education/2013/nov/04/peter-scott-research-excellence-framework">Peter Scott</a></li>
<li>Universities are becoming metrics factories, driven by large corporates - <a href="http://blogs.ch.cam.ac.uk/pmr/2013/11/10/spoton2013-yet-another-wonderful-meeting/">Peter Murray-Rust</a></li>
<li>The ‘real’ revolution in science will come when the scientific egosystem gets rid of the credit-imperative - <a href="https://twitter.com/Villavelius/status/399157271793762304">Jan Velterop</a></li>
<li>Excellence by Nonsense: The Competition for Publications in Modern Science - <a href="http://book.openingscience.org/basics_background/excellence_by_nonsense/">Mathias Binswanger</a></li>
</ul>
<p>My job title is <em>Technical Lead Article-Level Metrics</em> so it might sound surprising that I say this. But we have to differentiate of what we do now and in the next few years - which is mainly to get away from the Journal Impact Factor to more reasonable metrics that look at individual articles and include other metrics besides citations - to where we want to be in 10 or more years. And for the latter it is essential that journal articles and other research outputs are valued for the research they contain, rather than serving as a currency for <em>merit</em> that can be exchanged into grants and acadmic advancement. This is a very difficult problem to solve and I have no answers yet. Going back to how science was conducted until about 50 years ago - as a small elite club that worked based on closed personal networks - is definitely not the answer.</p>
<h2 id="reason-2">Reason #2</h2>
<p>In <a href="http://www.nature.com/spoton/event/spoton-london-2013-keynote-1-boson-50-years-50003-scientists-understanding-our-universe-through-global-scientific-collaboration-and-open-access/">his keynote</a> Salvatore Mele from CERN explained to us that Open Access in High Energy Phsics is 50 years old, and that the culture of sharing preprints preceeded the <a href="http://arxiv.org/">ArXiv</a> e-prints service - scientists were mailing their manuscripts to each other at least 20 years before ArXiV launched in 1991. A similar culture doesn’t exist in the life sciences and therefore the preprint services for biologists launched this year (e.g. <a href="https://peerj.com/preprints/">PeerJ Preprints</a> and <a href="http://biorxiv.org/">bioRxiv</a>) will have a hard time gaining traction.</p>
<p>Email is one of those services that every researcher uses, and we should think much more about how we can create innovative services around email rather than only considering new tools and services that are still used only by early adopters. AJ Cann had coordinated a workshop around email at SpotOn London that he called <a href="http://www.nature.com/spoton/event/spoton-london-2013-the-dark-art-of-dark-social-email-the-antisocial-medium-which-will-not-die-workshop/">the dark art of dark social: email, the antisocial medium that will not die</a>. I am still puzzled why most researchers prefer to receive tables of content by email rather than as a RSS feed, but we shouldn’t confuse what we get excited about as software developers and early adopters of online tools with what the mainstream scientist would be likely to use.</p>
<p>Another good example is <a href="http://royalsociety.org/policy/projects/science-public-enterprise/report/">data sharing</a>, a topic that was discussed in at least three SpotOn sessions. Even though most attendees at SpotOn London agreed that sharing of research data is important, it is obvious that this is currently not common practice in most scientific disciplines. Funders have created data sharing policies (e.g. <a href="http://www.nsf.gov/bfa/dias/policy/dmp.jsp">NSF</a> or the <a href="http://www.wellcome.ac.uk/About-us/Policy/Spotlight-issues/Data-sharing/">Wellcome Trust</a>), as <a href="http://dx.doi.org/10.1371/journal.pone.0067111">have publishers</a>, and many organizations are thinking about incentives for data sharing, including data journals such as <a href="http://www.nature.com/scientificdata/">Scientific Data</a> that will launch in 2014 and was presented by Ruth Wilson in the <a href="http://www.nature.com/spoton/event/spoton-london-2013-how-can-we-encourage-data-sharing-discussion/">motivations for data sharing</a> session. Even though incentives can help promote changes, I am pessimistic that something as central to the conduct of science as data sharing can be changed without more scientists being intrinsically motivated to do so. This is a much slower process that should start as early as possible during training, as pointed out by Kaitlin Thaney in the <a href="http://www.nature.com/spoton/event/spoton-london-2013-how-can-we-encourage-data-sharing-discussion/">#solo13carrot</a> session.</p>
<h2 id="reason-3">Reason #3</h2>
<p>In terms of the technology that is holding us back, I increasingly think that publisher manuscript submission systems may be the single most important place that is slowing down innovation. I participated in the first <a href="https://sites.google.com/site/beyondthepdf/">Beyond the PDF</a> workshop in 2011, and I think now that <strong><strong>Beyond the MTS (or manuscript tracking system)</strong></strong> might have been a better motto than <strong><strong>Beyond the PDF</strong></strong>, as many of the problems we discussed relate to typical editorial workflows we use today. These systems need to implement many of the ideas discussed at SpotOn London and other places, from opening up peer review (<a href="http://www.nature.com/spoton/event/spoton-london-2013-how-should-peer-review-evolve/">#solo13peer</a>) to making it easier to integrate research data into manuscripts (<a href="http://www.nature.com/spoton/event/spoton-london-2013-how-should-peer-review-evolve/">#solo13carrot</a>) and to ideas of how the scientific record should like in the digital age (<a href="http://www.nature.com/spoton/event/spoton-london-2013-what-should-the-scientific-record-look-like-in-the-digital-age-discussion/">#solo13digital</a>). In the latter panel we discussed both new authoring tools such as <a href="https://www.writelatex.com/">WriteLaTeX</a>, and new ideas of what a research object should look like and how the different parts are linked to each other. A major theme here was reproducibility highlighted both by Carol Goble (also see her <a href="http://www.slideshare.net/carolegoble/ismb2013-keynotecleangoble">ISMB/ECCB 2013 Keynote</a>) and Peter Kraker (see also his <a href="http://science.okfn.org/2013/10/18/its-not-only-peer-reviewed-its-reproducible/">Open Knowledge Foundation blog post</a>).</p>
<p>The problem with today’s manuscript submission systems is that they have grown so big and complex that any change is slow and cumbersome, rather than iterative and part of an ongoing dialogue. I don’t want to blame any single vendor of these systems, but rather suggest that we carefully re-evaluate the workflow from the manuscript written by one or more authors to the accepted manuscript. My personal interest is mainly in authoring tools, and I have recently written about and experimented with <a href="http://localhost:4000/tags.html#markdown-ref">Markdown</a>. This process of re-evaluating manuscript tracking systems is not simply about technology, but is rather about how we approach this problem as author, publisher, tool vendor and as a community.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is the Value of Hack Days?]]></title>
        <id>7bjvqn4-ghc976r-dtww5n4-scx31</id>
        <link href="https://blog.front-matter.io/mfenner/what-is-the-value-of-hack-days"/>
        <updated>2013-11-04T16:18:00.000Z</updated>
        <summary type="html"><![CDATA[This Friday and Saturday the SpotOn London Conference will take place at the British Library in London. I am very excited, as I have come to this conference since the first one in 2008, and have helped organize the event since 2009....]]></summary>
        <content type="html"><![CDATA[<p>This Friday and Saturday the <a href="http://www.nature.com/spoton/event/spoton-london2013/">SpotOn London Conference</a> will take place at the British Library in London. I am very excited, as I have come to this conference since the <a href="https://twitter.com/McDawg/status/397068628102610945">first one in 2008</a>, and have helped organize the event since 2009. The conference is about science communication in the broadest sense, and has three strands that focus on <em>science communication, science policy and tools</em>. Equally important as the sessions are of course the many highly engaging informal discussions of the 250 participants that take place between and after the sessions.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/spoton12_hack.jpg" class="kg-image" width="640" height="426" alt="Presenting from SpotOn London 2012 hackathon. One of the projects in 2012 was a collaborative commenting system. Picture from Flickr, taken by Lou Woodley." /><figcaption aria-hidden="true"><strong>Presenting from SpotOn London 2012 hackathon</strong>. One of the projects in 2012 was a collaborative commenting system. Picture from <a href="http://www.flickr.com/photos/25467658@N00/8252989528/">Flickr</a>, taken by Lou Woodley.</figcaption>
</figure>
<p>SpotOn London sessions are also more conversations than presentations, as they usually have 2-4 panelists with ample time for discussion with the audience. I will take part in two panels:</p>
<ul>
<li><a href="http://www.nature.com/spoton/event/spoton-london-2013-what-the-hack-part-one-hackdays-session/">What the hack?!</a> (Friday 10:30 AM, hashtag <a href="https://twitter.com/search?q=%23solo13hack">#solo13hack</a>), with Peter Murray-Rust, Ross Mounce and Helen Jackson</li>
<li><a href="http://www.nature.com/spoton/event/spoton-london-2013-whats-your-number-altmetrics-session/">What’s your number? - Altmetrics session</a> (Saturday 10:30 AM, hashtag <a href="https://twitter.com/search?q=%23solo13alt">#solo13alt</a>), with Marie Boran, David Colquhoun, Jean Liu and Stephen Curry</li>
</ul>
<p>I will summarize my thoughts regarding the altmetrics session in another post, but want to talk about the first session in more detail. According to the <a href="http://en.wikipedia.org/wiki/Hackathon">English Wikipedia</a></p>
<blockquote>
A <strong><strong>hackathon</strong></strong> (also known as a <strong><strong>hack day</strong></strong>, <strong><strong>hackfest</strong></strong> or <strong><strong>codefest</strong></strong>) is an event in which computer programmers and others involved in software development, including graphic designers, interface designers and project managers, collaborate intensively on software projects.
</blockquote>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/Wikimedia_hackathon_020_-_Berlin_2012_03.jpg" class="kg-image" width="512" height="769" alt="Wikimedia Hackathon Berlin June 2012. Largest hackathon I have attended so far with 100 people. Photo by Gulliaume Paumier, CC-BY license." /><figcaption aria-hidden="true"><strong>Wikimedia Hackathon Berlin June 2012</strong>. Largest hackathon I have attended so far with 100 people. Photo by Gulliaume Paumier, CC-BY license.</figcaption>
</figure>
<p>It is too bad that we will have no hackathon at year’s SpotOn London for logistical reasons, but the session is a great opportunity to reflect on the value of science hackdays. It is clear that hackdays for scientific software have become popular, with almost too many opportunities to participate.</p>
<h3 id="what-i-like">What I like</h3>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/group_foto.jpg" class="kg-image" width="800" height="600" alt="#hack4ac. Our team working on PLOS Author Contributions." /><figcaption aria-hidden="true"><strong>#hack4ac</strong>. Our team working on <a href="http://hack4ac.com/plos-author-contributions/">PLOS Author Contributions</a>.</figcaption>
</figure>
<ul>
<li>Do stuff. And have plenty of time to do stuff instead sessions in short intervals</li>
<li>Hackdays let you do great team work</li>
<li>Learn about other interesting projects and meet people doing cool work</li>
</ul>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/alm12_anti_gaming.png" class="kg-image" width="720" height="341" alt="ALM 2012 hackathon. Brainstorming board from anti-gaming group." /><figcaption aria-hidden="true"><a href="http://article-level-metrics.plos.org/alm-workshop-2012/hackathon/"><strong>ALM 2012 hackathon</strong></a>. Brainstorming board from anti-gaming group.</figcaption>
</figure>
<h3 id="what-i-don-t-like">What I don’t like</h3>
<ul>
<li>Hackdays are very much targeted at intermediate to advanced software developers, and it is sometimes not easy for beginners to participate</li>
<li>Too much time spent setting up stuff</li>
<li>Some of the work done at hackdays can be better done in virtual collaborations over weeks or months</li>
<li>Not many projects make it beyond the hackday and actually turn into a useable product. One example where this is not true are the visualizations started at the ALM 2012 hackathon that were implemented by OJS in 2013 (<a href="http://dx.doi.org/10.3402/gha.v6i0.19283">see article for more</a>), and of course <a href="http://impactstory.org/">ImpactStory</a> that started at a hackathon at the <a href="http://beyond-impact.org/">Beyond Impact</a> conference in May 2011.</li>
</ul>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/alm_d3.png" class="kg-image" width="542" height="368" alt="ALM 2012 hackathon. Sparkline visualization implemented by OJS based on work at the workshop." /><figcaption aria-hidden="true"><a href="http://article-level-metrics.plos.org/alm-workshop-2012/hackathon/"><strong>ALM 2012 hackathon</strong></a>. Sparkline visualization implemented by OJS based on work at the workshop.</figcaption>
</figure>
<h3 id="some-of-the-challenges">Some of the challenges</h3>
<ul>
<li>Coming up with projects where progress can be made in a day or two</li>
<li>Technology: WiFi access, access to servers for code deployment, collaboration tools, etc.</li>
<li>Come up with a good unifying theme, so that the various projects during the hackday relate to each other. The theme at <a href="http://hack4ac.com/">#hack4ac</a> was to demonstrate the value of the CC-BY license within academia.</li>
</ul>
<h3 id="some-ideas-to-improve-science-hackdays">Some ideas to improve science hackdays</h3>
<ul>
<li>Go beyond software development. We <a href="http://blogs.plos.org/tech/alm-data-challenge-metrics-for-a-standard-set-of-dois/">recently tried a data challenge using Altmetrics data</a>, and at a <a href="http://blog.martinfenner.org/2013/07/02/auto-generating-links-to-data-and-resources/">hackathon between IGSN, DataCite, PANGAEA and ORCID in July</a> we focussed on a high-level discussion of technical issues. There is a continuum towards the <a href="http://en.wikipedia.org/wiki/BarCamp">BarCamp</a> format, although I don’t like to drift too much from <em>doing</em> to <em>talking</em>. A good example of a workshop open to everyone and not just software developers is the SpotOn London workshop this Saturday on <a href="http://www.nature.com/spoton/event/spoton-london-2013-wikipedia-editing-workshop/">Wikipedia Editing</a> run by Brian Kelly and Toni Sant.</li>
<li>Meet before and after the hackathon. This can be done online, but it helps to focus on what can be achieved in the limited time available for a hackathon, and to follow up on projects that have just been started. But a hackathon is also a great opportunity to meet new people and new ideas, so meeting afterwards is more important than before.</li>
<li>Involve remote people. A lot of the fun of hackdays comes from sitting around a table and doing something together. But sometimes this is not possible for everyone, so think about remote participation where it makes sense.</li>
</ul>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Commenting on scientific papers]]></title>
        <id>7fnjysv-z6s9v2t-4f87bw6-1z26p</id>
        <link href="https://blog.front-matter.io/mfenner/commenting-on-scientific-papers"/>
        <updated>2013-10-25T16:21:00.000Z</updated>
        <summary type="html"><![CDATA[I think it is fair to say that commenting on scientific papers is broken. And with commenting I mean online comments that are publicly available, not informal discussions in journal clubs or at meetings....]]></summary>
        <content type="html"><![CDATA[<p>I think it is fair to say that commenting on scientific papers is broken. And with commenting I mean online comments that are publicly available, not informal discussions in journal clubs or at meetings. This definition would include discussions of papers on social media such as Twitter or Facebook. Why do I think that commenting is broken?</p>
<ul>
<li>the number of papers with online comments is low. <a href="https://doi.org/10.1371/journal.pbio.1001687">For PLOS Biology</a> we have comments on the journal platform for 11% of articles, tweets for 14% of articles and Facebook activity for 22% of articles. The numbers for Twitter and Facebook are much higher for more recently published articles, but are nowhere close to every article having at least one comment.</li>
<li>even though there is a fair amount of social media activity around articles, the quality of the discussion is varied. Twitter for example seems to work mostly as an alerting service for interesting articles with little more than the title of the article in the tweet text and not much discussion.</li>
<li>when comments are made, they are really hard to find coming from the article. Unless they are made on the journal platform, or the publisher tracks article-level metrics and links out to these comments.</li>
</ul>
<p>What can be done to address these issues, i.e. increase the number of comments, increase the depth of the discussion, and make it easier to link comments to articles? Some of the thoughts that I and others have had include the following:</p>
<ul>
<li>lower the technical barriers for commenting by providing a common and familiar commenting platform with an attractive user interface. Many blogs (including this one) and <a href="http://elife.elifesciences.org/">some publishers</a> use Disqus, which is arguably the most popular third-party commenting platform.</li>
<li>develop new features that make commenting more attractive, including comments linked to specific sections of the text and notes that can be public, semi-public or private. See for example <a href="https://medium.com/about/5972c72b18f2">what Medium is doing</a>, check out <a href="http://hypothes.is/">Hypothes.is</a> and <a href="http://blog.peerj.com/post/62886292466/peerj-questions-a-new-way-to-never-publish-forget">PeerJ Questions</a>, or study what services such as <a href="http://stackoverflow.com/">Stackoverflow</a> are doing.</li>
<li>Link comments made in different places about the same object together, e.g. through Article-Level Metrics services.</li>
<li>create incentives for scientists to comment, e.g. through <a href="http://openbadges.org/">Mozilla Open Badges</a> or by making them part of a community.</li>
</ul>
<p>On Tuesday the US National Library of Medicine launched <a href="http://ncbiinsights.ncbi.nlm.nih.gov/2013/10/22/pubmed-commons-a-new-forum-for-scientific-discourse/">PubMed Commons</a> as a <em>New Forum for Scientific Discourse</em>:</p>
<blockquote>
We hope that PubMed Commons will leverage the social power of the internet to encourage constructive criticism and high quality discussions of scientific issues that will both enhance understanding and provide new avenues of collaboration within the community.
</blockquote>
<p>PubMed Commons is still a pilot project and in order to read or write comments you have to be a PubMed Commons participant and be signed in with your My NCBI account. PubMed Commons has some important features:</p>
<ul>
<li>PubMed is probably the place where most life sciences researchers search for literature. Having comments and discussion there makes perfect sense, and is probably a better place than a publisher platform that only targets particular journals. PubMed also has a reputation that is very different from social media tools that are popular, but not really familiar to most scientists.</li>
<li>Access to PubMed Commons is restricted to researchers, and this is one strategy to have the comments focus on scientific discourse. It has to be seen whether the process of registering for PubMed Commons (which currently is a bit more involved than most commenting systems) is a barrier for scientists to take part in the discussion, or whether it generates an audience that makes it more likely that scientists contribute.</li>
<li>For people signing in with their My NCBI account (I don’t know the percentage of PubMed users that do that on a regular basis), commenting is really easy and the interface is straightforward. The comment editor uses markdown, which makes it easy to format comments and to include links.</li>
</ul>
<p><em>10/26/13: added link to the recently launched <a href="http://blog.peerj.com/post/62886292466/peerj-questions-a-new-way-to-never-publish-forget">PeerJ Questions</a> which uses a question and answer format (thanks to Jason Hoyt for reminding me).</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Can Article-Level Metrics Do for You?]]></title>
        <id>4ztpehs-39491ws-7gyd6z4-zc3pf</id>
        <link href="https://blog.front-matter.io/mfenner/what-can-article-level-metrics-do-for-you-2"/>
        <updated>2013-10-23T16:23:00.000Z</updated>
        <summary type="html"><![CDATA[Yesterday PLOS Biology published an essay by me: What Can Article Level Metrics Do for You? (Fenner, 2013). I had help from many others in writing the essay, in particular PLOS Biology editor Emma Ganley. I hope that the essay can help researchers get introduced to article-level metrics,...]]></summary>
        <content type="html"><![CDATA[<p>Yesterday PLOS Biology published an essay by me: <a href="http://dx.doi.org/10.1371/journal.pbio.1001687">What Can Article Level Metrics Do for You?</a> (Fenner, 2013). I had help from many others in writing the essay, in particular PLOS Biology editor Emma Ganley. I hope that the essay can help researchers get introduced to article-level metrics, and I am honored that the essay is part of the <a href="http://dx.doi.org/10.1371/journal.pbio.1001688">PLOS Biology 10th anniversary collection</a>.</p>
<p>The essay is an Open Access article published under a CC-BY license, so not only can everyone read it, but the text and figures can be freely reused, as long as proper attribution is provided, e.g. Fig. 5:</p>
<figure>
<img src="http://blog.martinfenner.org/images/venndiagram_plos_biology.png" class="kg-image" alt="PLOS Biology articles: sites of recommendation and discussion. Number of PLOS Biology research articles published until May 20, 2013 that have been recommended by F1000Prime (red) and/or mentioned in Wikipedia (blue). Taken from doi:10.1371/journal.pbio.1001687.g005" /><figcaption aria-hidden="true"><strong>PLOS Biology articles: sites of recommendation and discussion</strong>. Number of PLOS Biology research articles published until May 20, 2013 that have been recommended by F1000Prime (red) and/or mentioned in Wikipedia (blue). Taken from <a href="http://dx.doi.org/10.1371/journal.pbio.1001687.g005">doi:10.1371/journal.pbio.1001687.g005</a></figcaption>
</figure>
<p>Although this is an essay and not a research article, I’ve added the data and R scripts used to generate the figures (1, 3-5) as <a href="http://dx.doi.org/10.1371/journal.pbio.1001687.s001">supporting information</a>. As I <a href="http://blog.martinfenner.org/2013/10/20/the-complete-article/">have said earlier</a>, I think it is important that an article contains more than the text. With the open source software <a href="http://www.r-project.org/">R</a> or <a href="http://www.rstudio.com/">RStudio</a> everyone can recreate the figures, and can look at the data underlying the figures in the essay. One can for example look into the data behind Fig. 5 to better understand how articles with F1000Prime recommendations <strong><strong>and</strong></strong> Wikipedia mentions differ from those <strong><strong>only</strong></strong> recommended in F1000Prime. Feel free to ask for help getting started in the comments.</p>
<p>Incidentally this is also my first PLOS article (my wife is way ahead of me with 5 research articles), so that I can finally look at PLOS article-level metrics as an author - after being the technical lead for this project since May 2012.</p>
<h2 id="references">References</h2>
<p>Fenner, M. (2013). What can article-level metrics do for you? <em>PLoS Biol</em>, <em>11</em>(10), e1001687. <a href="http://doi.org/10.1371/journal.pbio.1001687">doi:10.1371/journal.pbio.1001687</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Complete Article]]></title>
        <id>5k400sa-tdx8j4b-m6b70tg-tx7ds</id>
        <link href="https://blog.front-matter.io/mfenner/the-complete-article"/>
        <updated>2013-10-20T16:26:00.000Z</updated>
        <summary type="html"><![CDATA[Open access to research data is becoming increasingly important, as manifested by memos or press releases from the Wellcome Trust, the European Commission, and the the Office of Science and Technology Policy (OSTP)...]]></summary>
        <content type="html"><![CDATA[<p>Open access to research data is becoming increasingly important, as manifested by memos or press releases from the <a href="http://www.wellcome.ac.uk/About-us/Policy/Policy-and-position-statements/WTX035043.htm">Wellcome Trust</a>, the <a href="http://europa.eu/rapid/press-release_IP-12-790_en.htm">European Commission</a>, and the <a href="http://www.whitehouse.gov/blog/2013/02/22/expanding-public-access-results-federally-funded-research">the Office of Science and Technology Policy</a> (OSTP) from the White House.</p>
<p>Open access to research data is important as this makes it easier for other researchers to reproduce the research, and to build upon the research by others by re-analysis of data or combination with other research data. In other words, <a href="http://royalsociety.org/policy/projects/science-public-enterprise/report/">Science as an open enterprise</a>.</p>
<p>The major challenge to open access to research data is that data sharing is not a widespread practice. Several strategies have been developed to create incentives for researchers to share research data, including services that make it easier to share research data (e.g. <a href="http://figshare.com/">figshare</a>, <a href="http://dataup.cdlib.org/">DataUp</a> and <a href="http://www.zenodo.org/">Zenodo</a>), <a href="http://www.knowledge-exchange.info/Default.aspx?ID=586">metrics for research data</a>, and data journals such as <a href="http://www.earth-system-science-data.net/">Earth System Science Data</a>, <a href="http://www.gigasciencejournal.com/">GigaScience</a> or the <a href="http://openarchaeologydata.metajnl.com/">Journal of open archaeology data</a>. Some of the sticks that have been tried in addition to the carrots above include data management plan requirements such as those <a href="http://www.nsf.gov/bfa/dias/policy/dmp.jsp">set forth by the National Science Foundation (NSF)</a> in 2011.</p>
<p>I would argue that all these carrots and sticks will eventually fall short, unless we redefine what the journal article (and similarly monograph) in the digital age should be about. Research data should become a required part of any research article, rather than an optional afterthought, or taking on a life on their own in a separate data journal.</p>
<p>The <em>complete article</em> - as I would like to call this journal article made fit for the digital age - should not only include the research data used to create figures and tables and reportes as results. Equally important are descriptions of reagents, workflows and software tools that go into much more detail compared to what is common practice today.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/complete_paper.png" class="kg-image" width="567" height="495" alt="Ingredients of the complete article" /><figcaption aria-hidden="true"><strong>Ingredients of the complete article</strong></figcaption>
</figure>
<p>The <em>complete article</em> does not have to come as one big file. More likely the research data will be hosted at one or more data centers elsewhere. Authorship will turn into contributorship and will include all roles required to put the <em>complete article</em> together, including for example data collection and -analysis, and writing software needed to analyze the data. The <em>complete article</em> can be shorter or longer than the typical article today, important is not article length, but the combination of text, data, and description of reagents and analysis tools.</p>
<p>The <em>complete article</em> should also include (or link to) the text of the peer reviews and previous article versions, including preprints. This makes it much easier to understand the article (and the data) in context. The <em>complete article</em> should also link to article-level metrics post-publication for similar reasons.</p>
<p>This idea of a <em>complete article</em> is not too far away from the best practices used today, but it is important to make it the default for scientific publication. Too much of what we publish today is still centered around the concept of what can be printed on paper, and telling exciting stories that have impact counts more than telling complete stories that can be reproduced.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Challenges in automated DOI resolution]]></title>
        <id>32kfege-w759rsb-rxhvycf-cbg31</id>
        <link href="https://blog.front-matter.io/mfenner/challenges-in-automated-doi-resolution"/>
        <updated>2013-10-13T16:29:00.000Z</updated>
        <summary type="html"><![CDATA[Yesterday we created a set of roughly 10,000 DOIs for journal articles published in 2011 or 2012. We used these DOIs as a reference set in a data hackathon around article-level metrics/altmetrics - material for another blog post.The random DOis were generated using the CrossRef RanDOIm service,...]]></summary>
        <content type="html"><![CDATA[<p>Yesterday we created a set of roughly 10,000 DOIs for journal articles published in 2011 or 2012. We used these DOIs as a reference set in a <a href="http://almdatachallenge.eventbrite.com/">data hackathon</a> around article-level metrics/altmetrics - material for another blog post.</p>
<p>The random DOis were generated using the <a href="http://random.labs.crossref.org/">CrossRef RanDOIm service</a>, with article titles fetched from the <a href="http://labs.crossref.org/openurl/">CrossRef OpenURL API</a>. We didn’t have time to properly parse the publication date and only used the publication year. We used the <code>crossref_r</code> and <code>crossref</code> functions from the rOpenSci <a href="http://ropensci.github.io/rplos/">rplos package</a> (and some extra help from Scott Chamberlain) to achieve this, the datasets were deposited to figshare and can be found <a href="https://doi.org/10.6084/m9.figshare.821209">here</a> (2011) and <a href="https://doi.org/10.6084/m9.figshare.821213">here</a> (2012).</p>
<p>The basic idea behind DOI names is summarized well in the <a href="http://en.wikipedia.org/wiki/Digital_object_identifier">Wikipedia entry</a>:</p>
<blockquote>
A digital object identifier (DOI) is a character string (a “digital identifier”) used to uniquely identify an object such as an electronic document. Metadata about the object is stored in association with the DOI name and this metadata may include a location, such as a URL, where the object can be found. The DOI for a document is permanent, whereas its location and other metadata may change. Referring to an online document by its DOI provides more stable linking than simply referring to it by its URL, because if its URL changes, the publisher need only update the metadata for the DOI to link to the new URL.
</blockquote>
<p>DOIs for journal articles should provide users with a URL specific for that journal article. This URL could point to a digital copy of the journal article in HTML or PDF format, or could point to a landing page (with an abstract or other basic metadata) for journal articles that require a subscription. This should work not only for humans using a web browser, but also for automated services using command line tools such as <a href="http://curl.haxx.se/">curl</a> as scientific infrastructure depends heavily on automation and computers talking to each other. In our use case we want to find content linking to a specific article, and as some services (e.g. social media) will use the URL and not DOI of an article, we need to find out that URL.</p>
<p>Unfortunately it was difficult to find a URL for many DOIs in our reference set using automated tools. All these DOIs resolve to URLs for human users using a web browser, but for automated tools there are a number of challenges:</p>
<h3 id="requiring-a-cookie">Requiring a cookie</h3>
<p>Some publishers require a cookie, and that can cause problems for automated tools. We can use the popular command line tool <code>curl</code> with the options <code>-L</code> to follow redirects and <code>-I</code> to only send the header (as we care about the location and not the content of the page).</p>
<pre><code>curl -I -L &quot;http://dx.doi.org/10.1080/13658816.2010.531020&quot;</code></pre>
<p>This command will lead us not to a page specific for that article, but to a “Cookie absent” page. You can work around this by having curl accept cookies:</p>
<pre><code>curl -I -L --cookie &quot;tmp&quot; &quot;http://dx.doi.org/10.1080/13658816.2010.531020&quot;</code></pre>
<p>Unfortunately not all tools do this. The way Facebook tracks likes, shares, comments, etc. is a prominent example.</p>
<h3 id="too-many-redirects">Too many redirects</h3>
<p>Some DOIs never resolve using a HEAD request, and curl stops after 50 redirects:</p>
<pre><code>curl -I -L &quot;http://dx.doi.org/10.1097/SLA.0b013e318235e525&quot;</code></pre>
<p>This error may relate to the “requiring a cookie” error above.</p>
<h3 id="method-not-allowed">Method not allowed</h3>
<p>Some DOis HEAD requests result in a “405 Method Not Allowed” error. The reason is that the journal platform doesn’t accept the HEAD request, but wants a GET instead.</p>
<pre><code>curl -I -L &quot;http://dx.doi.org/10.1002/sam.10120&quot;</code></pre>
<p>The <a href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html">HTTP 1.1 protocol</a> says about HEAD:</p>
<blockquote>
The HEAD method is identical to GET except that the server MUST NOT return a message-body in the response. … This method is often used for testing hypertext links for validity, accessibility, and recent modification.
</blockquote>
<p>We can work around this error by using a GET request, which unfortunately creates extra overhead and is not the recommended way to obtain this kind of information.</p>
<h3 id="empty-reply-from-server">Empty reply from server</h3>
<p>Some DOIs never resolve using a HEAD because curl reports “Empty reply from server” and we don’t get a HTTP 200 status code.</p>
<pre><code>curl -I -L &quot;http://dx.doi.org/10.1016/j.cca.2011.04.012&quot;</code></pre>
<p>You can again work around this by using the location information before the last redirect, but maybe resolving a DOI should not result in curl routinely throwing an error. It looks as if this error is related to “method not allowed”, as a GET request resolves to a landing page.</p>
<p>This problem is not specific to the <code>curl</code> tool, we get exactly the same error with <code>wget</code>:</p>
<pre><code>wget -S --spider &quot;http://dx.doi.org/10.1016/j.cca.2011.04.012&quot;</code></pre>
<h3 id="timeout-errors">Timeout errors</h3>
<p>Some DOI resolutions resulted in timeout errors, but this was temporary and much less frequent than the errors above.</p>
<h3 id="resource-not-found">Resource not found</h3>
<p>We didn’t specifically look into this error, which is a well-known problem with URLs. The DOI names we used were from 2011 and 2012, and it is known that <a href="http://en.wikipedia.org/wiki/Link_rot">link rot</a> is more common the older the resource is.</p>
<h3 id="content-negotiation">Content negotiation</h3>
<p>As Karl Ward has pointed out in the comments there are other ways to get to the URL from the DOI name, e.g. using content negotiation:</p>
<pre><code>curl -LH &quot;Accept: application/vnd.crossref.unixref+xml&quot; &quot;http://dx.doi.org/10.1016/j.cca.2011.04.012&quot;</code></pre>
<p>The URL is stored in the <code>doi_data/resource</code> attribute. The URL stored there is unfortunately not always the final landing page for the article, e.g. for the DOI name used in the example above.</p>
<h3 id="conclusions">Conclusions</h3>
<p>We created a reference set of 10,000 DOIs to collect metrics around them. The first conclusion from this exercise is that getting the URL for these articles is a challenge in many cases. This does not seem to relate to a permission problem for subscription content, but rather how the HTTP HEAD request is handled. Content negotiation is one alternative, but sometimes leads to different URLs for the landing page than where the user would get via the browser. We therefore have to rewrite our code to use GET requests and to better handle the scenarios above.</p>
<p><em>Update 10/13/13: Updated the title and the text to make it clear that I am not talking about DOIs that don’t resolve for human users, but rather about the problems automating this process using command-line tools.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Altmetrics coming of age? Not for Wikipedia]]></title>
        <id>1f9c8vp-axj96c8-vyptv66-q0vpd</id>
        <link href="https://blog.front-matter.io/mfenner/altmetrics-coming-of-age-not-for-wikipedia"/>
        <updated>2013-08-10T16:31:00.000Z</updated>
        <summary type="html"><![CDATA[Ten days ago Information Standards Quarterly (ISQ) published a special issue on altmetrics. I was the guest editor for the five altmetrics articles, and in the editorial that I titled <strong><strong>Altmetrics have come of age</strong></strong> I...]]></summary>
        <content type="html"><![CDATA[<p>Ten days ago Information Standards Quarterly (ISQ) published a <a href="http://www.niso.org/publications/isq/2013/v25no2/">special issue on altmetrics</a>. I was the guest editor for the five altmetrics articles, and in the <a href="https://doi.org/10.3789/isqv25no2.2013.01">editorial</a> that I titled <strong><strong>Altmetrics have come of age</strong></strong> I argued that</p>
<blockquote>
We no longer need to talk about whether it is possible to reliably collect altmetrics, or whether this is valuable information that can complement citations and usage statistics.
</blockquote>
<p>In June we have seen that the National Information Standards Organization (<a href="http://www.niso.org/home/">NISO</a>) was <a href="https://doi.org/10.3789/isqv25no2.2013.07">awarded a grant</a> by the <a href="http://www.sloan.org/">Sloan Foundation</a> to develop standards and recommended best practices for altmetrics.</p>
<p>Unfortunately Wikipedia - which is of course an important source of altmetrics information and was also mentioned in the editorial - doesn’t think so. When you try to go to the <a href="https://en.wikipedia.org/w/index.php?title=Altmetrics&amp;redirect=no">Altmetrics</a> page on the English Wikipedia, you get this:</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/13th/wikipedia_redirect.png" class="kg-image" width="700" height="349" alt="Wikipedia doesn’t think Altmetrics need their own page" /><figcaption aria-hidden="true">Wikipedia doesn’t think Altmetrics need their own page</figcaption>
</figure>
<p>In other words, you are redirected to a short section on the <a href="https://en.wikipedia.org/wiki/Impact_factor#Article_level_metrics_and_altmetrics">Impact Factor</a> page. I would go and start an altmetrics (and article-level metrics) page, but with my professional involvement in altmetrics it is difficult to write from a <a href="http://en.wikipedia.org/wiki/Wikipedia:Neutral_point_of_view">neutral point of view</a>, one of the core Wikipedia policies.</p>
<p><em>Update August 13, 2013: We now have a nice <a href="http://en.wikipedia.org/wiki/Altmetrics">altmetrics</a> Wikipedia page thanks to the hard work of <a href="http://en.wikipedia.org/wiki/User:Egonw">Egon Willighagen</a> and others.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CSL is more than citation styles]]></title>
        <id>3tctyfk-5cd955b-9hdegxe-va8rt</id>
        <link href="https://blog.front-matter.io/mfenner/csl-is-more-than-citation-styles"/>
        <updated>2013-08-08T16:33:00.000Z</updated>
        <summary type="html"><![CDATA[According to the description on the Citation Style Language (CSL) website, CSL <em>is an open XML-based language to describe the formatting of citations and bibliographies</em>. We use reference managers such as <strong>Zotero</strong>,...]]></summary>
        <content type="html"><![CDATA[<p>According to the <a href="http://citationstyles.org/">description</a> on the Citation Style Language (CSL) website, CSL <em>is an open XML-based language to describe the formatting of citations and bibliographies</em>. We use reference managers such as <strong>Zotero</strong>, <strong><strong>Mendeley</strong></strong>, or <strong><strong>Papers</strong></strong> to format our references in manuscripts we submit for publication, and underneath a CSL processor such as <a href="https://bitbucket.org/fbennett/citeproc-js/wiki/Home">Citeproc-js</a> - together with a CSL file for a particular citation style - is doing the work:</p>
<figure>
<img src="https://editor.front-matter.io/content/images/2021/02/csl.png" class="kg-image" width="394" height="577" alt="Citation processing during manuscript writing" /><figcaption aria-hidden="true">Citation processing during manuscript writing</figcaption>
</figure>
<p>When the journal article is accepted the publisher takes the text with the formatted text citation and turns it into XML, a process that is error-prone and takes time:</p>
<figure>
<img src="https://editor.front-matter.io/content/images/2021/02/csl2.png" class="kg-image" width="454" height="414" alt="Citation processing by the publisher" /><figcaption aria-hidden="true">Citation processing by the publisher</figcaption>
</figure>
<p>It is not hard to see that something is very wrong here:</p>
<ul>
<li>Authors are required to use a specific citation style (there are probably about 1,000 different citation styles and many more dependent styles) even though the publisher doesn’t directly use the formatted text. The publisher eLife <a href="http://www.elifesciences.org/elife-references/">accepts references in any format</a>.</li>
<li>Turning structured information into plain text and back into structured XML is always a bad idea. <a href="http://twitter.com/kaveh1000">Kaveh Bazargan</a> is a typesetter who has gone on record for saying that we should stop this nonsense and put him out of business.</li>
</ul>
<p>It is also obvious how the ideal workflow should look like:</p>
<figure>
<img src="https://editor.front-matter.io/content/images/2021/02/csl3.png" class="kg-image" width="768" height="908" alt="Ideal workflow of citation processing" /><figcaption aria-hidden="true">Ideal workflow of citation processing</figcaption>
</figure>
<p>We go from structured content to structured content, and never use citations formatted as text as intermediary steps in the workflow.</p>
<p>What is surprising is that this is an ideal workflow and not something that publishers actually do. Most journal author instructions don’t even mention CSL styles (I work for PLOS and they are no exception). There are some issues to be solved, but they are all minor:</p>
<ul>
<li>The Citeproc JSON citation format isn’t really an official standard, but rather something invented for the most popular CSL processor, Citeproc-js.</li>
<li>People like to fight over standards, and there are always people you prefer bibtex, RIS, MODS or BibJSON over Citeproc JSON, or want authors to to use JATS XML.</li>
</ul>
<p>I would really like to push Citeproc JSON as a standard bibliographic exchange format for authors. There are several things I like about Citeproc JSON:</p>
<ul>
<li>It is the native format to format citations, so it is used internally by many reference managers anyway.</li>
<li>Citeproc JSON is really good in handling all the possible variations of author names. Putting all authors into a single text field as in bibtex requires a lot of trickery to get it right.</li>
<li>JSON is a standard serialization format and there are a kinds of libraries in different programming languages to do things like searching, sorting or finding of duplicates. And JSON is easily extensible, e.g. if we would want to add ORCID identifiers for authors.</li>
</ul>
<p>I have five suggestions to move forward:</p>
<ul>
<li>Make a specification for Citeproc JSON that is as clear as the CSL specification.</li>
<li>Consider extending the specification to include content other than citations. Ideally we should be able to add arbitrary <a href="http://blog.martinfenner.org/2013/06/29/metadata-in-scholarly-markdown/">metadata about a manuscript</a>.</li>
<li>Consider other serialization formats besides JSON. I particularly <a href="http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/">like YAML</a> as it is very similar to JSON, but human-readable, but other people might prefer XML. It is relatively easy to transform data between these serialization formats, in particular between JSON and YAML. In my <a href="http://blog.martinfenner.org/about.html">About page</a> I only need the <a href="https://github.com/nodeca/js-yaml">js-yaml</a> library and one extra line of code to use Citeproc YAML instead of Citeproc JSON (in the d3.js visualization).</li>
<li>Add Citeproc JSON (and YAML) support to reference managers. Zotero is already doing this, but it should be an easy to add feature if the reference manager is already using CSL internally (Mendeley and Papers).</li>
<li>Push publishers to accept Citeproc JSON with manuscript submissions.</li>
</ul>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What a publication timeline can tell you]]></title>
        <id>1fv83cb-9c39n2r-6xhyb06-vkh48</id>
        <link href="https://blog.front-matter.io/mfenner/what-a-publication-timeline-can-tell-you"/>
        <updated>2013-08-06T16:35:00.000Z</updated>
        <summary type="html"><![CDATA[Now that I can automatically import my publications from my ORCID profile and display them in this blog, I also want to visualize them. I have started with d3.js code that displays the number of publications per year - using the list of my publications in Citeproc JSON format....]]></summary>
        <content type="html"><![CDATA[<p>Now that I can <a href="http://blog.martinfenner.org/2013/08/04/automatically-list-all-your-publications-in-your-blog/">automatically import my publications from my ORCID profile and display them</a> in this blog, I also want to visualize them. I have started with <a href="https://github.com/mfenner/blog/blob/master/_includes/by_year.js">d3.js code</a> that displays the number of publications per year - using the list of my publications in Citeproc JSON format. The chart is displayed on my <a href="http://blog.martinfenner.org/about.html">About page</a>, but I have also embedded the Javascript here:</p>
<p>I am a big fan of data visualizations because they can highlight something that you would otherwise miss. In this case I was really surprised to see how well my different academic jobs over the years (1991-1993, 1994-1998, 1998-2000, 2000-2005, 2005-2012) are reflected in my publication pattern. You clearly see the gaps between the jobs, indicating that I not only switched jobs, but also changed the research focus every time. The publications are listed chronologically on the <a href="http://blog.martinfenner.org/about.html">About page</a> page and you can look at the papers I wrote since my first publication in 1993. My publication pattern seems to indicate that I was never really on track for a typical academic career, so it should not be a surprise that I left academia in 2012.</p>
<p>There are at least two other visualizations I want to do: publications by type (journal article, book chapter, dataset, etc.), and author position with number of co-authors. You can reuse the Javascript code with small modifications (CSS and the JSON query) even if you are not running a Jekyll blog.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatically list all your publications in your blog]]></title>
        <id>6pxbtbd-xk48cx9-ckf1x6v-486xb</id>
        <link href="https://blog.front-matter.io/mfenner/automatically-list-all-your-publications-in-your-blog"/>
        <updated>2013-08-04T16:37:00.000Z</updated>
        <summary type="html"><![CDATA[A common feature of blogs written by scientists is a listing of all their publications. Publication lists are a great way to provide background information about your research. Publication lists should provide links to the fulltext versions of these publications,...]]></summary>
        <content type="html"><![CDATA[<p>A common feature of blogs written by scientists is a listing of all their publications. Publication lists are a great way to provide background information about your research. Publication lists should provide links to the fulltext versions of these publications, should be nicely formatted - e.g. using a common citation style such as APA - and should be easy to maintain. A number of tools for a variety of blogging platforms (including Wordpress and Jekyll) are available to help with this task, but maintaining the list of publications has remained difficult.</p>
<p>Publication lists are best maintained in a system built for this purpose. This could be either a reference manager, or a profile page in a social network for scientists. Even better suited for this task is your Open Researcher &amp; Contributor ID (<a href="http://orcid.org/">ORCID</a>) profile, as this service (<strong><strong>???</strong></strong>) directly integrates with a number of bibliographic databases and makes the profile information available via an open API.</p>
<h3 id="orcid-feed">ORCID Feed</h3>
<p>Last week I have started work on <a href="http://feed.labs.orcid-eu.org/">ORCID Feed</a>, a service that reformats the API response from ORCID into RSS, bibtex and formattted citations, making it easier for scientists to reuse the content stored in their ORCID profile. This service is still experimental, so please report any issues <a href="https://github.com/orcid-eu-labs/orcid-feed/issues">here</a>.</p>
<h3 id="jekyll-orcid">jekyll-orcid</h3>
<p>I have now added the final piece to automatically import my publications into this blog. <a href="https://github.com/mfenner/jekyll-orcid">jekyll-orcid</a> is a Jekyll plugin that automatically downloads all my publications from my ORCID profile via <strong><strong>ORCID Feed</strong></strong> and stores them in a subfolder of this blog, both in bibtex and Citeproc JSON format. It does this every time you regenerate your blog, so that the publication list will be automatically updated with new content. I can then use <a href="https://github.com/inukshuk/jekyll-scholar">jekyll-scholar</a>, a popular Jekyll plugin written by Sylvester Keil to generate a bibliography (<code>jekyll-orcid</code> automatically adds a YAML frontmatter section to the files so that jekyll-scholar can process it). I can format this auto-generated bibliography in a variety of ways - you can see the result in my <a href="http://blog.martinfenner.org/about.html">About</a> page where I also provide a download link of the bibtex file.</p>
<p>My publications are of course also available if I want to cite them in the text, e.g. our recent publication summarizing the main findings from the 2011 European Consensus Conference on germ-cell cancer (<strong><strong>???</strong></strong>), or last year’s case report on liver toxicity induced by the cancer drug imatinib (<strong><strong>???</strong></strong>).</p>
<p>Similar tools also exist for Wordpress, e.g. <a href="http://wordpress.org/plugins/papercite/">Papercite</a>, which can import the bibtex file directly from ORCID Feed.</p>
<h3 id="next">Next</h3>
<p>Now there is only one step missing to have your paper that was just published automatically appear in your publication list. Assuming you have provided your ORCID identifier when you submitted the paper, and the publisher has included your ORCID identifier in the metadata sent to CrossRef (both are already common practices), we only need CrossRef to automatically push that paper into your ORCID profile.</p>
<p>And once we have this workflow in place, we can automatically add additional information, including links to the fulltext paper in the institutional repository, copyright information, and metrics.</p>
<h2 id="references">References</h2>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Citeproc YAML for bibliographies]]></title>
        <id>1mvx4pm-61n874b-p2qgv5h-pbg9g</id>
        <link href="https://blog.front-matter.io/mfenner/citeproc-yaml-for-bibliographies"/>
        <updated>2013-07-30T16:39:00.000Z</updated>
        <summary type="html"><![CDATA[The standard local file formats for bibliographic data are probably bibtex and RIS. They have been around for a long time, and are supported by all reference managers and many other tools and services. Unfortunately these formats are far from perfect:neither...]]></summary>
        <content type="html"><![CDATA[<p>The standard local file formats for bibliographic data are probably bibtex and RIS. They have been around for a long time, and are supported by all reference managers and many other tools and services. Unfortunately these formats are far from perfect:</p>
<ul>
<li>neither bibtex nor RIS use a web-friendly data interchange format such as XML or JSON, which makes it harder to work with these formats</li>
<li>bibtex - and to a lesser extend RIS - don’t support all entry types that we need, e.g. datasets, or new standards such as ORCID author identifiers</li>
<li>bibtex stores all authors in a single field, which makes author names hard to parse</li>
</ul>
<h3 id="bibtex">bibtex</h3>
<pre><code>@article{fenner2012a,
  title = {One-click science marketing},
  volume = {11},
  url = {http://dx.doi.org/10.1038/nmat3283},
  doi = {10.1038/nmat3283},
  number = {4},
  journal = {Nature Materials},
  publisher = {Nature Publishing Group},
  author = {Fenner, Martin},
  year = {2012},
  month = {mar},
  pages = {261-263}
}</code></pre>
<p>One obvious solution would be to store bibliographic data in XML or JSON. These formats have very good support in all programming languages, and they are the formats used by APIs on the web. There have been some efforts to standardize these formats for bibliographic data, e.g. <a href="http://www.bibjson.org/">BibJSON</a>, <a href="http://www.loc.gov/standards/mods/">MODS</a>, <a href="http://bibtexml.sourceforge.net/">BibTeX XML</a> or Endnote XML.</p>
<h3 id="bibtex-xml">BibTeX XML</h3>
<pre><code>&lt;bibtex:entry id=&#39;fenner2012a&#39;&gt;
  &lt;bibtex:article&gt;
    &lt;bibtex:title&gt;One-click science marketing&lt;/bibtex:title&gt;
    &lt;bibtex:volume&gt;11&lt;/bibtex:volume&gt;
    &lt;bibtex:url&gt;http://dx.doi.org/10.1038/nmat3283&lt;/bibtex:url&gt;
    &lt;bibtex:doi&gt;10.1038/nmat3283&lt;/bibtex:doi&gt;
    &lt;bibtex:number&gt;4&lt;/bibtex:number&gt;
    &lt;bibtex:journal&gt;Nature Materials&lt;/bibtex:journal&gt;
    &lt;bibtex:publisher&gt;Nature Publishing Group&lt;/bibtex:publisher&gt;
    &lt;bibtex:person&gt;
      &lt;bibtex:first&gt;Martin&lt;/bibtex:first&gt;
      &lt;bibtex:last&gt;Fenner&lt;/bibtex:last&gt;
    &lt;bibtex:person&gt;&lt;bibtex:author/&gt;
    &lt;bibtex:year&gt;2012&lt;/bibtex:year&gt;
    &lt;bibtex:month&gt;mar&lt;/bibtex:month&gt;
    &lt;bibtex:pages&gt;261-263&lt;/bibtex:pages&gt;
  &lt;/bibtex:article&gt;
&lt;/bibtex:entry&gt;</code></pre>
<p>My problem with these formats is that they are made for computers talking to each other and not humans. I personally think that a file with bibliographic data should be human-readable, similar to why <a href="http://blog.martinfenner.org/2012/12/13/a-call-for-scholarly-markdown/">I like markdown</a> for writing scientific documents.</p>
<p>When you have too many standards and are not happy with any of them, you of course create a new standard.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/13th/standards.png" class="kg-image" width="500" height="283" alt="How Standards Proliferate. Taken from http://xkcd.com/927/" /><figcaption aria-hidden="true"><strong>How Standards Proliferate</strong>. Taken from <a href="http://xkcd.com/927/" class="uri">http://xkcd.com/927/</a></figcaption>
</figure>
<p>My suggestion for a new bibliographic file format is twofold: a) use YAML for data serialization and b) use CSL as data format. <a href="http://www.yaml.org/spec/1.2/spec.html">YAML</a> is a data format popular with Ruby Developers and is described on the <a href="http://yaml.org/">YAML website</a> as</p>
<blockquote>
YAML is a human friendly data serialization standard for all programming languages.
</blockquote>
<p>Something that not may people seem to know is that YAML is a superset of JSON and that <a href="http://yaml.org/spec/1.2/spec.html#id2759572">every JSON file is also a valid YAML file</a>. The main difference is the better human readability of YAML.</p>
<p><strong><strong>Citation Style Language</strong></strong> is described on the <a href="http://citationstyles.org/">CSL website</a> as</p>
<blockquote>
CSL is an open XML-based language to describe the formatting of citations and bibliographies.
</blockquote>
<p>Although some commercial applications still use proprietary citation styles, CSL has become the de facto standard, and is used by the reference managers <strong><strong>Zotero</strong></strong>, <strong><strong>Mendeley</strong></strong>, <strong><strong>Papers</strong></strong>, and others. This blog uses CSL via Pandoc and the <a href="http://code.google.com/p/citeproc-hs/">citeproc-hs</a> library. CSL processors need bibliographic data in a standard format. The popular <a href="https://bitbucket.org/fbennett/citeproc-js/wiki/Home">Citeproc-js</a> Javascript CSL processor by Frank Bennett for example uses JSON, but we might as well use YAML:</p>
<h3 id="citeproc-yaml">Citeproc YAML</h3>
<pre><code>- title: One-click science marketing
  volume: &#39;11&#39;
  URL: http://dx.doi.org/10.1038/nmat3283
  DOI: 10.1038/nmat3283
  issue: &#39;4&#39;
  container-title: Nature Materials
  publisher: Nature Publishing Group
  author:
  - family: Fenner
    given: Martin
    orcid: 0000-0003-1419-2405
  page: 261-263
  id: fenner2012a
  type: article-journal
  issued:
    date-parts:
      - 2012
      - 3</code></pre>
<p>I hope you agree that this format is not only structured and can be understood by computers, but is also very readable by humans. You may have noticed that I have inserted my ORCID, something that is very difficult to do with bibtex where all authors are stored in one text string (see above).</p>
<p>Careful readers of this blog will of course remember that <a href="http://blog.martinfenner.org/2013/06/29/metadata-in-scholarly-markdown/">I have written about</a> using YAML to store metadata about a blog post. We could now add bibliographic information to these metadata, either in the YAML frontmatter (if it is a Jekyll blog), or in a separate file. It should be straightforward to adapt the existing CSL processors to understand YAML since YAML and JSON are so similar. To get started with some Citeproc YAML, use the new (and still experimental) <strong><strong>ORCID Feed</strong></strong> Webservice with your ORCID and specify the <code>yml</code> format, e.g. <a href="http://feed.labs.orcid-eu.org/0000-0003-1419-2405.yml">http://feed.labs.orcid-eu.org/0000-0003-1419-2405.yml</a> for my publications.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RSS Feeds for Scholarly Authors]]></title>
        <id>2g5g9rz-qhn853t-x5524vr-prmf3</id>
        <link href="https://blog.front-matter.io/mfenner/rss-feeds-for-scholarly-authors"/>
        <updated>2013-07-26T16:48:00.000Z</updated>
        <summary type="html"><![CDATA[Open Researcher &amp; Contributor ID (ORCID) provides a persistent identifier for researchers and lets them claim their research outputs in the ORCID Registry. I have been involved with ORCID since early 2010 and I am happy to see that nine months after...]]></summary>
        <content type="html"><![CDATA[<p>Open Researcher &amp; Contributor ID (<a href="https://speakerdeck.com/mfenner/orcid-connecting-research-and-researchers-1">ORCID</a>) provides a persistent identifier for researchers and lets them claim their research outputs in the ORCID Registry. I have been involved with ORCID since early 2010 and I am happy to see that nine months after launch 200,000 researchers have signed up for the service, and the organization has more than <a href="http://orcid.org/about/community/members">70 member organizations</a>.</p>
<div class="iframe">

</div>
<p><a href="https://orcid.org/register">Registering for an ORCID identifier</a> is easy, and can be done in a few minutes. Claiming works in the profile is also straightforward, and works by integration with CrossRef Search, Scopus, Web of Science, DataCite Metadata Search, and other services. Even though about 1.5 million works have been claimed by now, many users have still not claimed any works or added profile information in other ways.</p>
<p>These numbers should go up as more academic institutions sign up for ORCID and help their researchers create ORCIDs and claim works. In the meantime we need more incentives for researchers to add publications to their ORCID profile. Publication lists are a very good reason to add your papers and other research outputs to your ORCID profile.</p>
<h3 id="publication-lists">Publication Lists</h3>
<p>Every researcher maintains a list of his publications in some form. These publication lists are used for grant and job applications, for academic websites to attract collaborators and students, and more. Publication lists can be generated in many different ways, but I have never heard that someone finds this process fun or easy. The challenge is multiplied when the publication list is not generated for an individual, but for a research group, department or institution (my university goes through this process every year uisng RefWorks and produces an <a href="http://www.refworks.com/RefShare2?site=047931198213200000/RWWS6A619751/2013%20Hochschulbibliografie">annual institutional bibliography</a>).</p>
<p>Although the library usually takes care of the larger publication lists and can help researchers setting up their own lists, there still is much that needs to be done by individual researchers, and the process needs to be easier. Some recommendations are:</p>
<ul>
<li>don’t reinvent the wheel</li>
<li>use persistent identifiers</li>
<li>use standards</li>
<li>don’t worry about citation styles</li>
<li>keep everything upstream, not locally</li>
</ul>
<p>Don’t try to invent a new way of managing publication lists. Other people have worked on this problem before, and there are many tools available. This doesn’t mean you shouldn’t try something new, but please build it on top of all the infrastructure and services we have already.</p>
<p>Managing publication lists becomes much easier when you use persistent identifiers such as DOIs. They make it much easier to obtain metadata (e.g. authors, title, journal) and the full-text version. Some disciplines use other identifiers, but a local identifier such as a URL is usually a bad idea.</p>
<p>Use standard protocols, standard file formats and standard metadata. BibTex and RIS are file formats for references that almost every piece of software handling references understands.</p>
<p>Citation styles come from a time when publications were printed on paper. They make no real sense anymore, and as a researcher you shouldn’t bother which one of 3000+ styles is the appropriate one.</p>
<p>The last recommendation is the most important one. Don’t try to manage publication lists in your local system, or your department, but rather do this as much upstream as possible. ORCID is an ideal service for this. But don’t try to manually add or edit publications in the ORCID registry, but rather claim them from CrossRef, DataCite or similar services, because these are the places that have authoritative information about publication. If you try to “fix” information (because all metadata can contain mistakes), nobody will notice. If something is wrong with your works, notify the publisher so that the CrossRef metadata can be updated.</p>
<h3 id="orcid-profiles-as-rss-feeds">ORCID Profiles as RSS Feeds</h3>
<p>ORCID is a good place to manage publication lists, but it is often not easy to get the information out of the system. The standard way is via a REST API (XML or JSON). This might work really well for a software developer who wants to connect his system to ORCID, but most researchers have other things to do.</p>
<p>RSS was invented to publish information about frequently updated works, and a good example are Tables of Content (TOC) for journals. RSS is also a great tool to manage publication lists, as it can be easily integrated into content management systems such as Wordpress or Drupal. There is a <a href="http://oxford.crossref.org/best_practice/rss/">Recommendation on RSS Feeds for Scholarly Publishers</a>, and we can apply the same guidelines to <strong><strong>RSS Feeds for Scholarly Authors</strong></strong>. With <a href="http://en.wikipedia.org/wiki/OPML">OPML</a> we also have a standard format to aggregate multiple RSS feeds, and this is true not only for journal RSS feeds, but also author RSS feeds.</p>
<p>Unfortunately there is one missing piece in this workflow: turning ORCID profiles into RSS feeds. At the <a href="http://occamstypewriter.org/trading-knowledge/2012/11/13/solo-hackday/">SpotOn London hackathon</a> last November I worked with <a href="http://twitter.com/easternblot">Eva Amsen</a> and <a href="http://twitter.com/graemedmoffat">Graeme Moffat</a> to hack this workflow together using available tools. But we really need a more mature solution. Until RSS feeds are provided by the core ORCID service - and there is so much other stuff to do right now that this will take time - the best solution might be a web service that turns ORCID profiles into scholarly RSS as described above for journal articles.</p>
<p>Today I finally came around implementing a first version of this - hacking together a Ruby Sinatra application hosted on Amazon Web Services (<a href="http://hack4ac.com/">#hack4ac</a> attendees know why). The application takes an ORCID ID (e.g. mine: <a href="http://feed.labs.orcid-eu.org/0000-0003-1419-2405.rss">http://feed.labs.orcid-eu.org/0000-0003-1419-2405.rss</a>) and returns an RSS feed. The first version just returns just the name and biography from the profile, but I only started working on this today. ORCID Feed can be found at <a href="http://feed.labs.orcid-eu.org/">http://feed.labs.orcid-eu.org</a> and the source code is available at <a href="https://github.com/mfenner/orcid-feed">Github</a>. Please add suggestions and comments to the Github issue tracker <a href="https://github.com/mfenner/orcid-feed/issues">here</a>.</p>
<p><strong><strong>Update 7/28/13</strong></strong>: <em>I’ve added publications to the output, and additional content types. Use them as extension (e.g. <code>.json</code>), as format parameter (e.g. <code>?format=rss</code>), or use an accept-header, e.g. <code>Accept: application/x-bibtex</code>. I’ve also added basic error checking with cleanup of names and removal of duplicates.</em></p>
<ul>
<li>html (the default): forward to profile on the ORCID website</li>
<li>rss - RSS feed</li>
<li>bib - bibtex file</li>
<li>json - Citeproc JSON</li>
</ul>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The trouble with keynotes]]></title>
        <id>47680s4-vrb8bjb-1dg3j1y-r8sqz</id>
        <link href="https://blog.front-matter.io/mfenner/the-trouble-with-keynotes"/>
        <updated>2013-07-23T16:50:00.000Z</updated>
        <summary type="html"><![CDATA[A keynote is a presentation typically given at a start of a conference that sets the central theme for the event. A keynote speaker usually has more time (45-60 min) than other presenters, and has the full attention of everyone attending the conference....]]></summary>
        <content type="html"><![CDATA[<p>A keynote is a presentation typically given at a start of a conference that sets the central theme for the event. A keynote speaker usually has more time (45-60 min) than other presenters, and has the full attention of everyone attending the conference. The keynotes at the conferences I attended the last several years (mostly scholarly communication conferences) seem to work like this:</p>
<ul>
<li>find a prominent speaker, ideally not a core member of the community attending the conference</li>
<li>tell him to talk about something he knows a lot about, not necessarily a central theme of the conference</li>
<li>the keynote should be inspiring and eye-opening, instead of focussing on the conference</li>
</ul>
<p>The problem with this approach is that it focusses too much on the <em>prominent speaker</em> and it runs the risk of the keynote speaker talking about what he always talks about. Meaning that we don’t learn much if we have heard the keynote speaker before. Which is too bad, because keynotes should contain things that are unexpected and exciting.</p>
<p>One of the best keynotes I had the pleasure of listening to in the last several years was the one given by <a href="http://michaelnielsen.org/blog/michael-a-nielsen/">Michael Nielsen</a> at <a href="http://www.nature.com/spoton/">Science Online London 2011</a> (disclaimer: I was one of the conference organizers). Not only is Michael a very good speaker, but his presentation about <strong><strong>Open Science</strong></strong> fit perfectly into the conference, and it was clear that he had made the presentation specifically for this conference (with an audience that knows a lot about Open Science). One of the main themes of his presentation – the <em>collective action problem</em>, or to get started with something that benefits everyone, but where there is a cost doing the first step - is something I later picked up <a href="https://doi.org/10.1629/24277">in a publication</a> about the Open Researcher &amp; Contributor ID (Fenner, Gomez, &amp; Thorisson, 2011).</p>
<div class="iframe">
<a href="https://vimeo.com/29784152"></a>
<div id="crawler_player">
Play
<img src="https://f.vimeocdn.com/p/images/crawler_logo.png" class="logo" alt="Vimeo" />
</div>
</div>
<p><em>Keynote by Michael Nielsen at the <a href="http://www.nature.com/spoton/">Science Online London 2011 Conference</a>, video recording and editing by <a href="http://river-valley.tv/keynote-solo2011/">River Valley TV</a>.</em></p>
<p>Luckily we increasingly have video recordings of keynote presentations available online, making it easier to listen to the good presentations. <a href="http://www.ted.com/tedx">TED and TEDx</a> have of course made the format of recordings of carefully prepared talks popular. For large scholarly and academic conferences the best starting point is <a href="http://river-valley.tv/">River Valley TV</a>. The <a href="http://www.mediatheque.lindau-nobel.org/">Lindau Nobel Laureate Meeting</a> has hundreds of presentations by Nobel laureates. And as video recording and streaming has become easier technically (e.g. with <a href="http://googleblog.blogspot.de/2011/09/google-92-93-94-95-96-97-98-99-100.html">Google Hangouts on Air</a>), recording good keynotes should become the norm and not the exception.</p>
<p><em>After several hundred blog posts here and elsewhere, this may well be my first blog post with embedded video.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Creating charts with Datawrapper]]></title>
        <id>1xp3778-w2k9wna-270veya-99gt1</id>
        <link href="https://blog.front-matter.io/mfenner/creating-charts-with-datawrapper"/>
        <updated>2013-07-19T16:54:00.000Z</updated>
        <summary type="html"><![CDATA[Figures are an important part of any scientific document. While the kind of figure commonly used obviously varies between disciplines, charts are an important part of many publications. There are two problems in how charts are currently used:the data...]]></summary>
        <content type="html"><![CDATA[<p>Figures are an important part of any scientific document. While the kind of figure commonly used obviously varies between disciplines, charts are an important part of many publications. There are two problems in how charts are currently used:</p>
<ul>
<li>the data used to draw the chart are not available or difficult to obtain</li>
<li>charts are drawn as static images with no interactivity, e.g. to see the values of individual data points</li>
</ul>
<p>Ross Mounce and others did a <strong><strong>Figures → Data</strong></strong> project at the recent <a href="http://hacka4ac.com/">hack4ac</a> to extract data from figures, described in a <a href="http://rossmounce.co.uk/2013/07/09/hack4ac-recap/">blog post</a>. The experience was painful, even though they started with a <em>really</em> simple chart.</p>
<p>While we should of course <a href="http://datadryad.org/">publish all data associated with a paper</a>, the smarter strategy to overcome the two limitations above would be to embed the data used for a chart directly into the document. We have many tools that can accomplish this, and I have given an example using R in an <a href="http://blog.martinfenner.org/2013/06/17/what-is-scholarly-markdown/">earlier blog post</a>. The problem is the sometimes steep learning curve.</p>
<p>One approach is to build an easy-to use online tool, and <a href="http://datawrapper.de/">Datawrapper</a> is exactly that:</p>
<blockquote>
An open source tool helping anyone to create simple, correct and embeddable charts in minutes.
</blockquote>
<p>Datawrapper uses the <strong><strong>d3.js</strong></strong> and <strong><strong>Highcharts</strong></strong> Javascript libraries for data visualizations, and the service is easy to use. It took me for example about 15 min to generate the chart below. The data used for the chart are embedded (click <strong><strong>Get the data</strong></strong>) and you can hover over the chart to see the actual numbers by month.</p>
<div class="iframe">
<div id="__svelte-dw" class="dw-chart chart vis-height-fit theme-datawrapper-data vis-d3-lines">
<div id="header" class="dw-chart-header">
<h1 class="block headline-block" id="monthly-article-views-over-time"><span class="block-inner">Monthly Article Views Over Time</span></h1>
<p><span class="block-inner">Monthly HTML views from the PLOS website for two PLOS Medicine articles published July 21, 2009. The striking difference in usage between "Preferred Reporting Items for Systematic Reviews and Meta-Analyses: The PRISMA Statement" (https://doi.org/10.1371/journal.pmed.1000097) and "Can the Relationship between Doctors and Drug Companies Ever Be a Healthy One?" (https://doi.org/<a href="https://doi.org/10.1371/journal.pmed.1000075" class="id-link">10.1371/journal.pmed.1000075</a>) becomes apparent only 18 months after publication.<br />
</span></p>
</div>
<div id="chart" class="dw-chart-body" aria-hidden="false">

</div>
<div id="footer" class="dw-chart-footer">
<div class="footer-left">
<span class="footer-block byline-block"> <span class="block-inner"><span class="byline-caption">Chart:</span> Martin Fenner </span> </span><span class="separator separator-before-source"></span> <span class="footer-block source-block"> <span class="block-inner"><span class="source-caption">Source:</span> <a href="http://www.plosmedicine.org/" class="source">PLOS</a></span> </span><span class="separator separator-before-get-the-data"></span> <span class="footer-block get-the-data-block"> <span class="block-inner"><a href="data" class="dw-data-link">Get the data</a></span> </span>
</div>
<div class="footer-center">

</div>
<div class="footer-right">

</div>
</div>
<div class="dw-after-body">

</div>
</div>
</div>
<p>Most journal articles see the highest usage immediately after publication, and the light purple line shows this pattern for Darcy et al. (2009). The dark purple line for Moher et al. (2009) – published on the same day – on the other hand shows a highly unusual usage pattern, as the usage actually increases over time, starting about 1 1/2 years after publication. The article is a guideline for reporting systematic reviews and meta-analyses, and is now viewed more often than directly after publication four years ago.</p>
<p>Datawrapper does three things: it makes it easy to generate charts, it allows you to embed them directly into your webpage (using an <code>&lt;iframe&gt;</code> tag), and it is Open Source software (MIT license, Github repo <a href="https://github.com/datawrapper/datawrapper">here</a>) so that you can help improve the code and host this service on your own. DataWrapper was written in Javascript and PHP by a group of German journalists, and the main focus is data journalism where the service has become really <a href="http://blog.datawrapper.de/2013/datawrapper-crosses-mark-of-10-million-visits/">popular</a> with more than 3.5 million views of embedded charts in May 2013 alone.</p>
<p>Datawrapper is a perfect tool for science blogs and websites with scientific content, but it can also enhance the charts in scientific articles. We need a few additional chart types, error bars and more flexible labeling. And we might want to add a license picker, making it easy to add a Creative Commons license so that it is clear how the chart can be reused. Datawrapper is intended for online use, but the service can also save the charts as PNG or PDF. We would want to add saving to SVG (already used for online rendering) for easier embedding into the XML and ePub versions of articles.</p>
<h2 id="references">References</h2>
<p>D’Arcy, E., &amp; Moynihan, R. (2009). Can the relationship between doctors and drug companies ever be a healthy one? <em>PLoS Medicine</em>, <em>6</em>(7), e1000075. Retrieved from <a href="http://doi.org/10.1371/journal.pmed.1000075">http://doi.org/10.1371/journal.pmed.1000075</a></p>
<p>Moher, D., Liberati, A., Tetzlaff, J., &amp; Altman, D. G. (2009). Preferred reporting items for systematic reviews and meta-analyses: The pRISMA statement. <em>PLoS Medicine</em>, <em>6</em>(7), e1000097. Retrieved from <a href="http://doi.org/10.1371/journal.pmed.1000097">http://doi.org/10.1371/journal.pmed.1000097</a></p>
<hr />
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Altmetrics: first we need the for what? and only then the how? OK?]]></title>
        <id>67zrt5q-4wc91nv-76378d2-yfs9f</id>
        <link href="https://blog.front-matter.io/mfenner/altmetrics-first-we-need-the-for-what-and-only-then-the-how-ok"/>
        <updated>2013-07-09T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Altmetrics track the impact of scholarly works in the social web. Article-Level Metrics focuses on articles, but also looks at traditional citations and usage statistics. The PLOS Article-Level Metrics project was started in 2008....]]></summary>
        <content type="html"><![CDATA[<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/cute-500x132.png" class="kg-image" width="500" height="132" />
</figure>
<p>Altmetrics track the impact of scholarly works in the social web. Article-Level Metrics focuses on articles, but also looks at traditional citations and usage statistics. The <a href="https://web.archive.org/web/20170913082053/http://article-level-metrics.plos.org/">PLOS Article-Level Metrics</a> project was started in 2008. The <a href="https://web.archive.org/web/20170913082053/http://altmetrics.org/manifesto/">altmetrics manifesto</a> was published in October 2010 and described the fundamental ideas. By October 2011 we had a number of altmetrics tools, fueled by the Mendeley/PLOS API <a href="https://web.archive.org/web/20170913082053/http://blog.mendeley.com/design-research-tools/winners-of-the-first-binary-battle-apps-for-science-contest/">programming contest</a>. In 2012 the focus shifted from the fact that we can provide these numbers to a discussion of the many open questions. We could see this at the <a href="https://web.archive.org/web/20170913082053/http://blogs.plos.org/mfenner/2012/06/25/random-notes-from-the-altmetrics12-conference/">altmetrics12 conference</a> in June, and even more so at the <a href="https://web.archive.org/web/20170913082053/https://sites.google.com/site/altmetricsworkshop/">altmetrics workshop</a> hosted by PLOS last week in San Francisco.</p>
<p>Altmetrics can provide a large amount of information about the post-publication activity around an article (and other scholarly content), and this is exciting, but at the same time also somewhat overwhelming and scary. Some of the things that we as a community have to figure out include standards for collecting, aggregating and displaying altmetrics data, strategies to combat attempts to game these metrics, and finding appropriate ways for the different organizations providing altmetrics to work together as a community. These and other topics were discussed in great detail at the PLOS altmetrics workshop, and we made excellent progress not least thanks to the excellent moderation by <em>Cameron Neylon</em>. The third day of the workshop was a <a href="https://web.archive.org/web/20170913082053/https://sites.google.com/site/altmetricsworkshop/altmetrics-hackathon">hackathon</a>, and we were able to translate some of the ideas into prototypes of new tools.</p>
<p>The most important conclusion from the workshop for me personally was that we should really should focus on use cases. Altmetrics should help answer questions that we can’t answer today, and despite the promise, the various altmetrics tools still have a log way to go. A case in point is the promise that altmetrics can make it easier to find relevant scholarly content. We all use social media to help us find papers and other stuff, but integration of altmetrics into the traditional scholarly search tools is still missing. <a href="https://web.archive.org/web/20170913082053/http://rerank.it/">ReRank</a> is a cool prototype developed during the hackathon last Saturday, but we are still a long way from having altmetrics feeding directly into the relevance sorting of search results.</p>
<p>With these thoughts in the back of mind, I look forward to the <a href="https://web.archive.org/web/20170913082053/http://www.nature.com/spoton/event/spoton-london-2012-altmetrics-beyond-the-numbers/">altmetrics session</a> at the <a href="https://web.archive.org/web/20170913082053/http://www.nature.com/spoton/in/london/">SpotOn London conference</a> this Sunday afternoon. <em>Sarah Venis</em> from <a href="https://web.archive.org/web/20170913082053/http://www.msf.org/">Médecins sans Frontières</a> (MSF) will talk about the questions that she hopes altmetrics can answer for her organization. MSF is very interested to look beyond citations for the impact of their publications, as their primary target audience is not really the scholarly community, but rather people in need in various parts of the world. <em>Marie Boran</em> from the <a href="https://web.archive.org/web/20170913082053/http://www.deri.ie/about/team/member/marie_boran/">Digital Research Enterprise Institute</a> (DERI) is interested in using altmetrics as a recommendation tool to find researchers with similar interests. <em>Euan Adie</em> from <a href="https://web.archive.org/web/20170913082053/http://altmetric.com/">altmetric.com</a> and I (technical lead for the <a href="https://web.archive.org/web/20170913082053/http://article-level-metrics.plos.org/">PLOS Article-Level Metrics project</a>) will use our respective tools to try to answer some of these questions. For me altmetrics are primarily tools to tell a good story, and that is one reason why we picked the title <em>Altmetrics beyond the Numbers</em> for this session. The focus of the session will then shift to an open discussion, and I hope we can get some good answers to this and other questions.</p>
<p>A clear focus on use cases should go a long way to reduce that feeling of being overwhelmed by all the numbers that altmetrics can provide. If we have specific goals for which we need altmetrics, it becomes much easier to decide what numbers work best for us, what standards we need and whom to ask to collect this information. <a href="https://web.archive.org/web/20170913082053/http://www.nature.com/spoton/2012/11/spoton-london-2012-altmetrics-everywhere-but-what-are-we-missing-solo12impact/">AJ Cann</a> and <a href="https://web.archive.org/web/20170913082053/http://ukwebfocus.wordpress.com/2012/11/08/understanding-the-limits-of-altmetrics-slideshare-statistics/">Brian Kelly</a> have written two excellent blog post about the confusion that too many altmetrics numbers can create, and the workshop <a href="https://web.archive.org/web/20170913082053/http://www.nature.com/spoton/event/spoton-london-2012-assessing-social-media-impact/">Assessing Social Media Impact</a> during SpotOn London addresses some of these questions. Hackathons have played an important role in the history of altmetrics. I invite you to come to the <a href="https://web.archive.org/web/20170913082053/http://www.nature.com/spoton/event/spoton-london-2012-fringe-event-hackday/">SpotOn London hackathon</a> this Saturday if you have some cool ideas and want to get started with the help of others.</p>
<h3 id="other-reports-from-the-plos-article-level-metrics-aka-altmetrics-workshop">Other reports from the PLOS Article-Level Metrics (aka Altmetrics) Workshop</h3>
<ul>
<li><strong>Paul Groth</strong>: <a href="https://web.archive.org/web/20170913082053/http://thinklinks.wordpress.com/2012/11/05/trip-report-plos-article-level-metrics-workshop-and-hackathon/">Trip Report: PLOS Article Level Metrics Workshop and Hackathon</a></li>
<li><strong>Karthik Ram</strong>: <a href="https://web.archive.org/web/20170913082053/http://inundata.org/2012/11/08/plos-altmetrics-workshop/">PLOS Altmetrics workshop</a></li>
<li><strong>Carl Boettiger</strong>: <a href="https://web.archive.org/web/20170913082053/http://www.carlboettiger.info/2012/11/03/altmetrics-conference.html">Altmetrics Conference</a></li>
<li><strong>Pedro Beltrao</strong>: <a href="https://web.archive.org/web/20170913082053/http://pbeltrao.blogspot.de/2012/11/scholarly-metrics-with-heart.html">Scholarly metrics with a heart</a></li>
<li><strong>Ian Mulvany</strong>: <a href="https://web.archive.org/web/20170913082053/https://plus.google.com/u/0/photos/102755743034732738536/albums/5807181863066123265">a photo post</a></li>
<li><strong>Euan Adie</strong> (who couldn’t attend in person but followed remotely): <a href="https://web.archive.org/web/20170913082053/http://altmetric.com/blog/?p=316">Want some hackathon friendly altmetrics data? arXiv tweets dataset now up on figshare</a></li>
</ul>
<p><em>Please let me know if you see other reports of the workshop that I have missed.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto generating links to data and resources]]></title>
        <id>44vrj89-y059qsb-ggqsg0y-8sp7a</id>
        <link href="https://blog.front-matter.io/mfenner/auto-generating-links-to-data-and-resources"/>
        <updated>2013-07-02T16:58:00.000Z</updated>
        <summary type="html"><![CDATA[A few weeks ago Kafkas et al. (2013) published a paper looking at current patterns of how datasets o biological databases are cited in research articles, based on an analysis of the full text Open Access articles available from Europe PMC....]]></summary>
        <content type="html"><![CDATA[<p>A few weeks ago Kafkas et al. (2013) published a paper looking at current patterns of how datasets o biological databases are cited in research articles, based on an analysis of the full text Open Access articles available from Europe PMC. They identified data citations by:</p>
<ol>
<li>Accession numbers available in articles as publisher-supplied, structured content;</li>
<li>Accession numbers identified in articles by text mining;</li>
<li>References to articles from the ENA, UniProt and PDBe records.</li>
</ol>
<p>They could show that text mining doubles the number of structured annotations available in journal articles (from 2.26% to 5.15%), and that these structured annotations should be extended beyond the ENA, UniProt and PDB identifiers that their analysis focused on. ENA identifiers (for nucleotide sequences in GenBank, EMBL or DDBJ) make up the largest group, with 160,112 identifiers found in the 410,364 articles that were analyzed.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/12th/journal.pone.0063184.g003.png" class="kg-image" width="500" height="419" alt="Database Citation in Full Text Biomedical Articles. Fig. 3 from (Kafkas et al., 2013)." /><figcaption aria-hidden="true"><strong>Database Citation in Full Text Biomedical Articles</strong>. Fig. 3 from <span class="citation" data-cites="Kafkas:2013fp" style="box-sizing: border-box;">(Kafkas et al., 2013)</span>.</figcaption>
</figure>
<p>Another result in the paper is that references to articles in these databases show little overlap with database links found in articles. One of the conclusions drawn by the author is that</p>
<blockquote>
Text-mining can be used to extend structured data citation, and could be a basis for the development of services to help authors or editors to add structured content at the beginning of the publication process, rather than after the fact.
</blockquote>
<p>Adding structured data citations during the authoring phase of a manuscript requires tools that make this process easier, providing auto-linking and verification of the without requiring extra input from the author. Scholarly Markdown is an ideal platform for these tools, as it is easier to extend than traditional word processors such as Microsoft Word. During a small workshop around persistent identifiers for data (<a href="http://datacite.org/">DataCite</a>), people (<a href="http://orcid.org/">ORCID</a>) and geological samples (<a href="http://www.geosamples.org/igsnabout">IGSN</a>) that took place yesterday and today at the <a href="http://www.gfz-potsdam.de/portal/gfz/cegit">GFZ Potsdam</a> I worked on a tool that does auto-linking for these identifiers:</p>
<ul>
<li>IGSN. <a href="http://www.geosamples.org/igsnabout">International Geosample Number</a></li>
<li>MGI identifiers for genetically modified mouse strains in the <a href="http://www.findmice.org/about">Internal Mouse Strain Resource</a></li>
<li>ENA. <a href="http://www.ebi.ac.uk/ena/about/about">Genbank / ENA / DDBJ nucleotide sequences</a></li>
<li>UniProt protein sequences from the <a href="http://www.uniprot.org/help/about">UniProt database</a></li>
<li>PDB. <a href="http://www.rcsb.org/pdb/static.do?p=home/faq.html">Protein Data Bank protein structure information</a></li>
</ul>
<p>The list includes the IGSN, the database identifiers studied by Kafkas et al (2013), and the MGI identifier for genetically altered mice. In the life sciences there is a long tradition - and requirement by journals - to use database identifiers for data, but identifiers for resources such as genetically modified mice are unfortunately not in common use.</p>
<p>This blog uses the Pandoc markdown processor and the Jekyll static website generator. The easiest way to implement this functionality was by writing a filter for the liquid templating engine used by Jekyll, and provide this filter as a Jekyll plugin. The Jekyll plugin can be found at <a href="https://github.com/mfenner/jekyll-scholmd">mfenner/jekyll-scholmd</a>. The plugin expects the name of the identifier, followed by a colon and optional space, followed by the identifier:</p>
<pre><code>GenBank:  M10090
IGSN:  JRH964436
MGI:  96922
UniProt:  P02144
PDB:  1mbn</code></pre>
<p>This input is automatically translated into <a href="http://www.ebi.ac.uk/ena/data/view/M10090">GenBank:M10090</a>, <a href="http://hdl.handle.net/10273/JRH964436">IGSN:JRH964436</a>, <a href="http://www.findmice.org/summary?gaccid/96922">MGI:96922</a>, and information about the human myoglobin protein (<a href="http://www.uniprot.org/uniprot/P02144">UniProt:P02144</a>, <a href="http://www.rcsb.org/pdb/explore/explore.do?structureId=1mbn">PDB:1mbn</a>) is generated in a similar fashion.</p>
<p>The plugin was written in a few hours today, and is my first Jekyll plugin. There is room for improvement, e.g. support for more identifiers, better regex matching, validation of the resulting links, and automated tag generation if an identifier is found. Ideally the auto-linking should happen in the markdown and not the HTML output, so that these structured database links are also available in other markdown outputs such as PDF. But this is another example how Scholarly Markdown can make it easier for researchers to author documents without requiring a fancy web-based user interface.</p>
<h2 id="references">References</h2>
<p>Kafkas, Ş., Kim, J.-H., &amp; McEntyre, J. R. (2013). Database Citation in Full Text Biomedical Articles. <em>PLoS ONE</em>. <a href="http://doi.org/10.1371/journal.pone.0063184">doi:10.1371/journal.pone.0063184</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Metadata in Scholarly Markdown]]></title>
        <id>790b7wb-xms99jv-jx1dajd-dphkz</id>
        <link href="https://blog.front-matter.io/mfenner/metadata-in-scholarly-markdown"/>
        <updated>2013-06-29T17:01:00.000Z</updated>
        <summary type="html"><![CDATA[Scholarly documents often need metadata that describe them: typically author(s), title and location (DOI or URL), but possibly many other things. For some metadata it makes sense to store them in the document text, e.g. as is typically done for citations....]]></summary>
        <content type="html"><![CDATA[<p>Scholarly documents often need metadata that describe them: typically author(s), title and location (DOI or URL), but possibly many other things. For some metadata it makes sense to store them in the document text, e.g. as is typically done for citations. The problem is that this can make it hard to make the metadata machine-readable. The worst place for metadata is of course outside of the document, and unfortunately that it is the most common way of doing this. Two examples:</p>
<ul>
<li>Manuscript submission. Papers submitted to scholarly journals contain the metadata in the text, but authors are required to enter the information again into a webform. You can add metadata (<a href="http://office.microsoft.com/en-001/word-help/add-property-information-to-a-document-HA010163766.aspx">property information</a>) to Microsoft Word documents, but it seems that nobody is doing it.</li>
<li>PDFs and image files. Even though we have at least one good standard with <a href="http://blogs.plos.org/mfenner/2008/12/22/just_doi_it/">XMP</a> to store metadata in these documents, it is not a common practice. Information about these documents is therefore stored somewhere else and doesn’t automatically travel with them.</li>
</ul>
<p>The best place for metadata is the document itself, and the metadata should be stored in machine-readable format. Another requirement is flexibility in what we can store, and we shouldn’t limit ourselves to a predefined list. Pandoc for example allows only three attributes in the <a href="http://johnmacfarlane.net/pandoc/README.html">title block</a>:</p>
<pre><code>% title
% author(s) (separated by semicolons)
% date</code></pre>
<p>For Scholarly Markdown we have another requirement: the metadata should be writeable and readable by humans. <a href="http://en.wikipedia.org/wiki/YAML">YAML</a> is the perfect format for this. JSON is closely related to YAML (and is in fact a subset of YAML 1.2), but YAML can also be written with whitespace instead of curly braces. The static website generator Jekyll - which I use to parse the markdown for this blog into HTML - uses YAML at the beginning of markdown documents to store metadata, and we can easily extend this functionality. Carl Boettinger posted a comment yesterday saying that YAML support is on the Pandoc development roadmap.</p>
<p>Below is the YAML for (Ethan P. White, 2013), where I reposted a paper written in markdown:</p>
<pre><code>---
layout: post
title: &quot;Nine simple ways to make it easier to (re)use your data&quot;
tags: [example, citation]
authors:
 - name: Ethan P. White
   orcid: 0000-0001-6728-7745
   affiliation: Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341
 - name: Elita Baldrige
   orcid: 0000-0003-1639-5951
   affiliation: Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341
 - name: Zachary T. Brym
   affiliation: Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341
 - name: Kenneth J. Locey
   affiliation: Dept. of Biology, Utah State University, Logan, UT, USA, 84341
 - name: Daniel J. McGlinn
   affiliation: Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341
 - name: Sarah R. Supp
   affiliation: Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341
---</code></pre>
<p>In JSON the same information would look like this (and Jekyll is able to parse it, since JSON is a subset of YAML 1.2):</p>
<pre><code>---
{
  &quot;layout&quot;: &quot;post&quot;,
  &quot;title&quot;: &quot;Nine simple ways to make it easier to (re)use your data&quot;,
  &quot;tags&quot;: [
    &quot;example&quot;,
    &quot;citation&quot;
  ],
  &quot;authors&quot;: [
    {
      &quot;name&quot;: &quot;Ethan P. White&quot;,
      &quot;orcid&quot;: &quot;0000-0001-6728-7745&quot;,
      &quot;affiliation&quot;: &quot;Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341&quot;
    },
    {
      &quot;name&quot;: &quot;Elita Baldrige&quot;,
      &quot;orcid&quot;: &quot;0000-0003-1639-5951&quot;,
      &quot;affiliation&quot;: &quot;Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341&quot;
    },
    {
      &quot;name&quot;: &quot;Zachary T. Brym&quot;,
      &quot;affiliation&quot;: &quot;Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341&quot;
    },
    {
      &quot;name&quot;: &quot;Kenneth J. Locey&quot;,
      &quot;affiliation&quot;: &quot;Dept. of Biology, Utah State University, Logan, UT, USA, 84341&quot;
    },
    {
      &quot;name&quot;: &quot;Daniel J. McGlinn&quot;,
      &quot;affiliation&quot;: &quot;Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341&quot;
    },
    {
      &quot;name&quot;: &quot;Sarah R. Supp&quot;,
      &quot;affiliation&quot;: &quot;Dept. of Biology and the Ecology Center, Utah State University, Logan, UT, USA, 84341&quot;
    }
  ]
}
---</code></pre>
<p>You can see that the author information required for manuscript submission can easily be written in YAML (email addresses were removed to protect privacy). JSON is also possible for people where this is a better fit into their workflow, but it is more difficult to write for humans because of the curly braces, and because all strings need to be in double quotes.</p>
<p>Once the ORCID Registry <a href="http://orcid.org/blog/2013/06/27/orcid-plans-launch-affiliation-module-using-isni-and-ringgold-organization">adds affiliation</a> information, we no longer need to provide email and affiliation when submitting manuscripts. I have stored my own name, orcid, email and affiliation in my site configuration file so that I don’t have to provide this info for every blog post.</p>
<p>In this blog markdown files are currently only processed to HTML, and I store the metadata in HTML <code>meta</code> tags in a <a href="http://www.monperrus.net/martin/accurate+bibliographic+metadata+and+google+scholar">format</a> used by many sites and services, including Google Scholar - look at the source code of Ethan P. White et al. (2013) for an example. These metadata are also understood by the <a href="http://www.russet.org.uk/blog/2071">Greycite service</a> built by Phil Lord and Lindsay Marshall that generates citation information for weblinks, adding important metadata such as title, authors and publication_date so that we can properly cite our blog post (Ethan P. White, 2013).</p>
<p>And I use the metadata to link the author names to their ORCID profile (if they have an ORCID) or email address, with the affiliation visible when you hover over the name. My own name is linked to the <a href="http://blog.martinfenner.org/about.html">About</a> page of this site, but with a little development effort I could automatically add all my publications (and other works) in my ORCID profile to that page.</p>
<p>Metadata are important, and Scholarly Markdown makes it easy to embed them.</p>
<p><em>Update 06/30/13: added JSON example to demonstrate the differences to YAML, and to show that Jekyll also works with JSON (used in this blog post, and tested with the examples above which produce identical HTML output). Also added two references, using the embedded HTML metadata and the Greycite service to generate citations in bibtex.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nine simple ways to make it easier to (re)use your data]]></title>
        <id>4mvww4t-31082p9-8q2yvs0-44xht</id>
        <link href="https://blog.front-matter.io/mfenner/nine-simple-ways-to-make-it-easier-to-re-use-your-data"/>
        <updated>2013-06-25T17:04:00.000Z</updated>
        <summary type="html"><![CDATA[This paper in markdown format was written by Ethan White et al. The markdown file and the associated bibliogaphy and figure files are available from the Github repository of the paper.I used this version, an earlier version was published as PeerJ Preprint....]]></summary>
        <content type="html"><![CDATA[<blockquote>
This paper in markdown format was written by Ethan White et al. The markdown file and the associated bibliogaphy and figure files are available from the <a href="https://github.com/weecology/data-sharing-paper">Github repository of the paper</a>.
</blockquote>
<blockquote>
I used <a href="https://github.com/weecology/data-sharing-paper/commit/b5a73eb0942a18bb29810025a528aea48a8465e7">this</a> version, an earlier version was published as <a href="http://dx.doi.org/10.7287/peerj.preprints.7v1">PeerJ Preprint</a>. Special thanks to Ethan White for allowing me to reuse this paper. The paper is used here as an example document to show how markdown can handle scholarly documents, in particular tables, figures and citations. The document was slightly modified from the orginal: added YAML frontmatter (needed by jekyll, author names are also stored there), and changed the anchor text for some links. This post is using the APA citation style. Please restrict your comments to issues related to Scholarly Markdown, for the content of the article contact Ethan directly.
</blockquote>
<h2 id="abstract">Abstract</h2>
<p>Sharing data is increasingly considered to be an important part of the scientific process. Making your data publicly available allows original results to be reproduced and new analyses to be conducted. While sharing your data is the first step in allowing reuse, it is also important that the data be easy to understand and use. We describe nine simple ways to make it easy to reuse the data that you share and also make it easier to work with it yourself. Our recommendations focus on making your data understandable, easy to analyze, and readily available to the wider community of scientists.</p>
<h2 id="introduction">Introduction</h2>
<p>Sharing data is increasingly recognized as an important component of the scientific process (Whitlock, McPeek, Rausher, Rieseberg, &amp; Moore, 2010). The sharing of scientific data is beneficial because it allows replication of research results and reuse in meta-analyses and projects not originally intended by the data collectors (Poisot, Mounce, &amp; Gravel, 2013). In ecology and evolutionary biology, sharing occurs through a combination of formal data repositories like <a href="http://www.ncbi.nlm.nih.gov/genbank/">GenBank</a> and <a href="http://datadryad.org/">Dryad</a>, and through individual and institutional websites.</p>
<p>While data sharing is increasingly common and straightforward, much of the shared data in ecology and evolutionary biology are not easily reused because they do not follow best practices in terms of data structure, metadata, and licensing (M. B. Jones, Schildhauer, Reichman, &amp; Bowers, 2006). This makes it more difficult to work with existing data and therefore makes the data less useful than it could be (M. B. Jones et al., 2006; O. J. Reichman, Jones, &amp; Schildhauer, 2011). Here we provide a list of 9 simple ways to make it easier to reuse the data that you share.</p>
<p>Our recommendations focus on making your data understandable, easy to work with, and available to the wider community of scientists. They are designed to be simple and straightforward to implement, and as such represent an introduction to good data practices rather than a comprehensive treatment. We contextualize our recommendations with examples from ecology and evolutionary biology, though many of the recommendations apply broadly across scientific disciplines. Following these recommendations makes it easier for anyone to reuse your data including other members of your lab and even yourself.</p>
<h2 id="1-share-your-data">1. Share your data</h2>
<p>The first and most important step in sharing your data is to share your data. The recommendations below will help make your data more useful, but sharing it in any form is a big step forward. So, why should you share your data?</p>
<p>Data sharing provides substantial benefits to the scientific community (Fienberg &amp; Martin, 1985). It allows</p>
<ol>
<li>the results of existing analyses to be reproduced and improved upon (Fienberg &amp; Martin, 1985; Poisot et al., 2013),</li>
<li>data to be combined in meta-analyses to reach general conclusions (Fienberg &amp; Martin, 1985),</li>
<li>new approaches to be applied to the data and new questions asked using it (Fienberg &amp; Martin, 1985), and</li>
<li>approaches to scientific inquiry that couldn’t even be considered without broad scale data sharing (Hampton et al., 2013).</li>
</ol>
<p>As a result, data sharing is increasingly required by funding agencies (Poisot et al. (2013); e.g., <a href="http://www.nsf.gov/bfa/dias/policy/dmp.jsp">NSF</a>, <a href="http://grants.nih.gov/grants/guide/notice-files/NOT-OD-03-032.html">NIH</a>, <a href="http://www.nserc-crsng.gc.ca/Professors-Professeurs/FinancialAdminGuide-GuideAdminFinancier/Responsibilities-Responsabilites_eng.asp">NSERC</a>, <a href="http://www.fwf.ac.at/en/public_relations/oai/index.html">FWF</a>), journals (Whitlock et al., 2010), and potentially by law (e.g. <a href="http://doyle.house.gov/sites/doyle.house.gov/files/documents/2013%2002%2014%20DOYLE%20FASTR%20FINAL.pdf">FASTR</a>).</p>
<p>Despite these potential benefits to the community, many scientists are still reluctant to share data. This reluctance is largely due to perceived fears of 1) competition for publications based on the shared data, 2) technical barriers, and 3) a lack of recognition for sharing data (Hampton et al., 2013; Palmer et al., 2004). These concerns are often not as serious as they first appear, and the minimal costs associated with data sharing are frequently offset by individual benefits to the data sharer (Hampton et al., 2013; Parr &amp; Cummings, 2005). Many data sharing initiatives allow for data embargoes or limitations on direct competition that can last for several years while the authors develop their publications and thus avoid competition for deriving publications from the data. Also, logistical barriers to data sharing are diminishing as data archives become increasingly common and easy to use (Hampton et al., 2013; Parr &amp; Cummings, 2005). Datasets are now considered citable entities and data providers receive recognition in the form of increased citation metrics and credit on CVs and grant applications (Heather A Piwowar &amp; Vision, 2013; Heather A. Piwowar, Day, &amp; Fridsma, 2007; Poisot et al., 2013). In addition to increased citation rates, shared datasets that are documented and standardized are also more easily reused in the future by the original investigator. As a result, it is increasingly beneficial to the individual researcher to share data in the most useful manner possible.</p>
<h2 id="2-provide-metadata">2. Provide metadata</h2>
<p>The first key to using data is understanding it. Metadata is information about the data including how it was collected, what the units of measurement are, and descriptions of how to best use the data. Clear metadata makes it easier to figure out if a dataset is appropriate for a project. It also makes data easier to use by both the original investigators and by other scientists by making it easy to figure out how to work with the data. Without clear metadata, datasets can be overlooked or not used due to the difficulty of understanding the data (Fraser &amp; Gluck, 1999; A. S. Zimmerman, 2003), and the data becomes less useful over time (Michener, Brunt, Helly, Kirchner, &amp; Stafford, 1997).</p>
<p>Metadata can take several forms, including descriptive file and column names, a written description of the data, images (<em>i.e.,</em> maps, photographs), and specially structured information that can be read by computers. Good metadata should provide 1) the what, when, where, and how of data collection, 2) how to find and access the data, 3) suggestions on the suitability of the data for answering specific questions, 4) warnings about known problems or inconsistencies in the data, and 5) information to check that the data are properly imported, such as the number of rows and columns in the dataset and the total sum of numerical columns (Michener et al., 1997; Strasser, Cook, Michener, &amp; Budden, 2012; A. S. Zimmerman, 2003).</p>
<p>Just like any other scientific publication, metadata should be logically organized, complete, and clear enough to enable interpretation and use of the data (A. Zimmerman, 2007). Specific metadata standards exist (<em>e.g.,</em> Ecological Metadata Language <a href="http://knb.ecoinformatics.org/software/eml/">EML</a>, Directory Interchange Format <a href="http://gcmd.gsfc.nasa.gov/add/difguide/index.html">DIF</a>, Darwin Core <a href="http://rs.tdwg.org/dwc/">DWC</a> (Wieczorek et al., 2012), Dublin Core Metadata Initiative <a href="http://dublincore.org/metadata-basics/">DCMI</a>, Federal Geographic Data Committee <a href="http://www.fgdc.gov/metadata/geospatial-metadata-standards">FGDC</a> (O. J. Reichman et al., 2011; Whitlock, 2011). These standards are designed to provide consistency in metadata across different datasets and also to allow computers to interpret the metadata automatically. This allows broader and more efficient use of shared data (Brunt, McCartney, Baker, &amp; Stafford, 2002; M. B. Jones et al., 2006). While following these standards is valuable, the most important thing is to have metadata at all.</p>
<p>You don’t need to spend a lot of extra time to write good metadata. The easiest way to develop metadata is to start describing your data during the planning and data collection stages. This will help you stay organized, make it easier to work with your data after it has been collected, and make eventual publication of the data easier. If you decide to take the extra step and follow metadata standards, there are tools designed to make this easier including: <a href="http://knb.ecoinformatics.org/morpho%20portal.jsp">KNB Morpho</a>, <a href="http://geology.usgs.gov/tools/metadata/tools/doc/xtme.html">USGS xtme</a>, and <a href="http://www.fgdc.gov/metadata/documents/workbook_0501_bmk.pdf">FGDC workbook</a>.</p>
<h2 id="3-provide-an-unprocessed-form-of-the-data">3. Provide an unprocessed form of the data</h2>
<p>Often, the data used in scientific analyses are modified in some way from the original form in which they were collected. This is done to address the questions of interest in the best manner possible and to address common limitations associated with the raw data. However, the best way to process data depends on the question being asked and corrections for common data limitations often change as better approaches are developed. It can also be very difficult to combine data from multiple sources that have each been processed in different ways. Therefore, to make your data as useful as possible it is best to share the data in as raw a form as possible.</p>
<p>This is not to say that your data are best suited for analysis in the raw form, but providing it in the raw form gives data users the most flexibility. Of course, your work to develop and process the data is also very important and can be quite valuable for other scientists using your data. This is particularly true when correcting data for common limitations. Providing both the raw and processed forms of the data, and clearly explaining the differences between them in the metadata, is an easy way to include the benefits of both data forms. An alternate approach is to share the unprocessed data along with the code that process the data to the form you used for analysis. This allows other scientists to assess and potentially modify the process by which you arrived at the values used in your analysis.</p>
<h2 id="4-use-standard-data-formats">4. Use standard data formats</h2>
<p>Everyone has their own favorite tools for storing and analyzing data. To make it easy to use your data it is best to store it in a standard format that can be used by many different kinds of software. Good standard formats include the type of file, the overall structure of the data, and the specific contents of the file.</p>
<h3 id="use-standard-file-formats">Use standard file formats</h3>
<p>You should use file formats that are readable by most software and, when possible, are non-proprietary (Borer, Seabloom, Jones, &amp; Schildhauer, 2009; Strasser, Cook, Michener, Budden, &amp; Koskela, 2011; Strasser et al., 2012). Certain kinds of data in ecology and evolution have well established standard formats such as <a href="http://zhanglab.ccmb.med.umich.edu/FASTA/">FASTA</a> files for nucleotide or peptide sequences and the <a href="http://evolution.genetics.washington.edu/phylip/newicktree.html">Newick files</a> for phylogenetic trees. Use these well defined formats when they exist, because that is what other scientists and most existing software will be able to work with most easily.</p>
<p>Data that does not have a well defined standard format is often stored in tables. Tabular data should be stored in a format that can be opened by any type of software to increase reuseability of the data, i.e. text files. These text files use delimiters to indicate different columns. Commas are the most commonly used delimiter (i.e., comma-delimited text files with the .csv extension). Tabs can also be used as a delimiter, although problems can occur in displaying the data correctly when importing data from one program to another. In contrast to plain text files, proprietary formats such as those used by Microsoft Excel (e.g, .xls, .xlsx) can be difficult to load into other programs. In addition, these types of files can become obsolete, eventually making it difficult to open the data files at all if the newer versions of the software no longer support the original format (Borer et al., 2009; Strasser et al., 2011, 2012).</p>
<p>When naming files you should use descriptive names so that it is easy to keep track of what data they contain (Borer et al., 2009; Strasser et al., 2011, 2012). If there are multiple files in a dataset, name them in a consistent manner to make it easier to automate working with them. You should also avoid spaces in file names, which can cause problems for some software (Borer et al., 2009). Spaces in file names can be avoided by using camel case (e.g, RainAvg) or by separating the words with underscores (e.g., rain_avg).</p>
<h3 id="use-standard-table-formats">Use standard table formats</h3>
<p>Data tables are ubiquitous in ecology and evolution. Tabular data provides a great deal of flexibility in how to structure the data, which makes it easy to structure the data in a way that is difficult to (re)use. We provide three simple recommendations to help ensure that tabular data are properly structured to allow the data to be easily imported and analyzed by most data management systems and common analysis software, such as R and Python.</p>
<ul>
<li>Each row should represent a single observation (i.e., a record) and each column should represent a single variable or type of measurement (i.e., a field) (Borer et al., 2009; Strasser et al., 2011, 2012). This is the standard format for tables in the most commonly used database management systems and analysis packages and makes the data easy to work with in the most general way.</li>
<li>Every cell should contain only a single value (Strasser et al., 2012). For example, do not include units in the cell with the values (Figure 1) or include multiple measurements in a single cell, and break taxonomic information up into single components with one column each for family, genus, species, subspecies, etc. Violating this rule makes it difficult to process or analyze your data using standard tools, because there is no easy way for the software to treat the items within a cell as separate pieces of information.</li>
<li>There should only be one column for each type of information (Borer et al., 2009; Strasser et al., 2011, 2012). The most common violation of this rule is <a href="http://en.wikipedia.org/wiki/Cross_tabulation">cross-tab structured data</a>, where different columns contain measurements of the same variable (e.g., in different sites, treatments, etc.; Figure 1).</li>
</ul>
<figure>
<img src="http://blog.martinfenner.org/images/Data_formatting.jpg" class="kg-image" alt="Figure 1. Examples of how to restructure two common issues with tabular data. (a) Each cell should only contain a single value. If more than one value is present then the data should be split into multiple columns. (b) There should be only one column for each type of information. If there are multiple columns then the column header should be stored in one column and the values from each column should be stored in a single column." /><figcaption aria-hidden="true"><strong>Figure 1. Examples of how to restructure two common issues with tabular data</strong>. (a) Each cell should only contain a single value. If more than one value is present then the data should be split into multiple columns. (b) There should be only one column for each type of information. If there are multiple columns then the column header should be stored in one column and the values from each column should be stored in a single column.</figcaption>
</figure>
<p>While cross-tab data can be useful for its readability, and may be appropriate for data collection, this format makes it difficult to link the records with additional data (e.g., the location and environmental conditions at a site) and it cannot be properly used by most common database management and analysis tools (e.g., relational databases, dataframes in R and Python, etc.). If tabular data are currently in a cross-tab structure, there are tools to help restructure the data including functions in Excel, R (e.g., melt() function in the R package reshape; Wickham (2007)), and Python (e.g., melt() function in the <a href="http://pandas.pydata.org/">Pandas</a> Python module.</p>
<p>In addition to following these basic rules you should also make sure to use descriptive column names (Borer et al., 2009). Descriptive column names make the data easier to understand and therefore make data interpretation errors less likely. As with file names, spaces can cause problems for some software and should be avoided.</p>
<h3 id="use-standard-formats-within-cells">Use standard formats within cells</h3>
<p>In addition to using standard table structures it is also important to ensure that the contents of each cell don’t cause problems for data management and analysis software. Specifically, we recommend:</p>
<ul>
<li>Be consistent. For example, be consistent in your capitalization of words, choice of delimiters, and naming conventions for variables.</li>
<li>Avoid special characters. Most software for storing and analyzing data works best on plain text, and accents and other special characters can make it difficult to import your data (Borer et al., 2009; Strasser et al., 2012).</li>
<li>Avoid using your delimiter in the data itself (e.g., commas in the notes filed of a comma-delimited file). This can make it difficult to import your data properly. This means that if you are using commas as the decimal separator (as is often done in continental Europe) then you should use a non-comma delimiter (e.g., a tab).</li>
<li>When working with dates use the YYYY-MM-DD format (i.e., follow the <a href="http://www.iso.org/iso/support/faqs/faqs_widely_used_standards/widely_used_standards_other/iso8601">ISO 8601</a> data standard).</li>
</ul>
<h2 id="5-use-good-null-values">5. Use good null values</h2>
<p>Most ecological and evolutionary datasets contain missing or empty data values. Working with this kind of “null” data can be difficult, especially when the null values are indicated in problematic ways. Unfortunately, there are many different ways to indicate a missing/empty value, and very little agreement on which approach to use.</p>
<p>We recommend choosing a null value that is both compatible with most software and unlikely to cause errors in analyses (Table 1). The null value that is most compatible with the software commonly used by biologists is the blank (i.e., nothing; Table 1). Blanks are automatically treated as null values by R, Python, SQL, and Excel. They are also easily spotted in a visual examination of the data. Note that a blank involves entering nothing, it is not a space, so if you use this option make sure there aren’t any hidden spaces. There are two potential issues with blanks that should be considered:</p>
<ol>
<li>It can be difficult to know if a value is missing or was overlooked during data entry.</li>
<li>They can be confusing when spaces or tabs are used as delimiters in text files.</li>
</ol>
<p>NA and NULL are reasonable null values, but they are only handled automatically by a subset of commonly used software (Table 1). NA can also be problematic if it is also used as an abbreviation (e.g., North America, Namibia, <em>Neotoma albigula</em>, sodium, etc.). We recommend against using numerical values to indicate nulls (e.g., 999, -999, etc.) because they typically require an extra step to remove from analyses and can be accidentally included in calculations. We also recommend against using non-standard text indications (e.g., No data, ND, missing, —) because they can cause issues with software that requires consistent data types within columns). Whichever null value that you use, only use one, use it consistently throughout the data set, and indicate it clearly in the metadata.</p>
<table style="box-sizing: border-box; border-collapse: collapse; border-spacing: 0px; max-width: 100%; background-color: rgb(255, 255, 255); font-size: 19px; font-family: ff-tisa-sans-web-pro, Arial, sans-serif; width: 750px; margin-bottom: 21px; color: rgb(0, 0, 0); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;">
<caption><strong>Tabel 1. Commonly used null values, limitations, compatibility with common software and a recommendation regarding whether or not it is a good option</strong>. Null values are indicated as being a null value for specific software if they work consistently and correctly with that software. For example, the null value “NULL” works correctly for certain applications in R, but does not work in others, so it is not presented as part of the table.</caption>
<thead style="box-sizing: border-box;">
<tr class="header header" style="box-sizing: border-box;">
<th style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: bottom; border-top: 0px; font-weight: bold">Null values</th>
<th style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: bottom; border-top: 0px; font-weight: bold">Problems</th>
<th style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: bottom; border-top: 0px; font-weight: bold">Compatibility</th>
<th style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: bottom; border-top: 0px; font-weight: bold">Recommendation</th>
</tr>
</thead>
<tbody style="box-sizing: border-box;">
<tr class="odd odd" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>0</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Indistinguishable from a true zero</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Never use</p></td>
</tr>
<tr class="even even" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>blank</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Hard to distinguish values that are missing from those overlooked on entry. Hard to distinguish blanks from spaces, which behave differently.</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>R, Python, SQL</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Best option</p></td>
</tr>
<tr class="odd odd" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>999, -999</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Not recognized as null by many programs without user input. Can be inadvertently entered into calculations.</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Avoid</p></td>
</tr>
<tr class="even even" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>NA, na</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Can also be an abbreviation (e.g., North America), can cause problems with data type (turn a numerical column into a text column). NA is more commonly recognized than na.</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>R</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Good option</p></td>
</tr>
<tr class="odd odd" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>N/A</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>An alternate form of NA, but often not compatible with software</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Avoid</p></td>
</tr>
<tr class="even even" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>NULL</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Can cause problems with data type</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>SQL</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Good option</p></td>
</tr>
<tr class="odd odd" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>None</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Can cause problems with data type</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Python</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Avoid</p></td>
</tr>
<tr class="even even" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>No data</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Can cause problems with data type, contains a space</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Avoid</p></td>
</tr>
<tr class="odd odd" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Missing</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Can cause problems with data type</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Avoid</p></td>
</tr>
<tr class="even even" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>-,+,.</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Can cause problems with data type</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Avoid</p></td>
</tr>
</tbody>
</table>
<h2 id="6-make-it-easy-to-combine-your-data-with-other-datasets">6. Make it easy to combine your data with other datasets</h2>
<p>Ecological and evolutionary data are often most valuable when combined with other kinds of data (e.g., taxonomic, environmental). You can make it easier to combine your data with other data sources by including the data that is common across many data sources (e.g., Latin binomials, latitudes and longitudes) It is common for data to include codes or abbreviations. For example, in ecology and evolution codes often appear in place of site locations or taxonomy. This is useful because it reduces data entry (e.g., DS instead of <em>Dipodomys spectabilis</em>) and redundancy (a single column for a species ID rather than separate columns for family, genus, and species). However, without clear definitions these codes can be difficult to understand and make it more difficult to connect your data with external sources. The easiest way to link your data to other datasets is to include additional tables that contain a column for the code and additional columns that describe the item in the standard way. For example, you might include a table with the species codes followed by their most current family, genus, and specific epithet. For site location, you could include a table with the site code followed by latitude and longitude. Linked tables can also be used to include additional information about your data, such as spatial extent, temporal duration, and other appropriate details.</p>
<h2 id="7-perform-basic-quality-control">7. Perform basic quality control</h2>
<p>Data, just like any other scientific product, should undergo some level of quality control (O. J. Reichman et al., 2011). This is true regardless of whether you plan to share the data because quality control will make it easier to analyze your own data and decrease the chance of making mistakes. However, it is particularly important for data that will be shared because scientists using the data won’t be familiar with quirks in the data and how to work around them.</p>
<p>At its most basic, quality control can consist of a few quick sanity checks of the data. More advanced quality control can include automated checks on data as it is entered and double-entry of data (Lampe &amp; Weiler, 1998; Paulsen, Overgaard, &amp; Lauritsen, 2012). This additional effort can be time consuming, but is valuable because it increases data accuracy by catching typographical errors, reader/recorder error, out-of-range values, and questionable data in general (Lampe &amp; Weiler, 1998; Paulsen et al., 2012).</p>
<p>Before sharing your data we recommend performing a quick “data review”. Start by performing some basic sanity checks on your data. For example:</p>
<ul>
<li>If a column should contain numeric values, check that there are no non-numeric values in the data.</li>
<li>Check that empty cells actually represent missing data, and not mistakes in data entry, and indicate that they are empty using the appropriate null values (see recommendation 6).</li>
<li>Check for consistency in unit of measurement, data type (e.g., numeric, character), naming scheme (e.g., taxonomy, location), etc.</li>
</ul>
<p>These checks can be performed by carefully looking at the data or can be automated using common programming and analysis tools like R or Python.</p>
<p>Then ask someone else to look over your metadata and data and provide you with feedback about anything they didn’t understand. In the same way that friendly reviews of papers can help catch mistakes and identify confusing sections of papers, a friendly review of data can help identify problems and things that are unclear in the data and metadata.</p>
<h2 id="8-use-an-established-repository">8. Use an established repository</h2>
<p>For data sharing to be effective, data should be easy to find, accessible, and stored where it will be preserved for a long time (Kowalczyk &amp; Shankar, 2011). To make your data (and associated code) visible and easily accessible, and to ensure a permanent link to a well maintained website, we suggest depositing your data in one of the major well-established repositories. This guarantees that the data will be available in the same location for a long time, in contrast to personal and institutional websites that do not guarantee the long-term persistence of the data. There are repositories available for sharing almost any type of biological or environmental data. Repositories that host specific data types, such as molecular sequences (e.g., DDBJ, GenBank, MG-RAST), are often highly standardized in data type, format, and quality control approaches. Other repositories host a wide array of data types and are less standardized (e.g., Dryad, KNB, PANGAEA). In addition to the repositories focused on the natural sciences there are also all purpose repositories where data of any kind can be shared (e.g., figshare).</p>
<p>When choosing a repository you should consider where other researchers in your discipline are sharing their data. This helps you quickly identify the community’s standard approach to sharing and increases the likelihood that other scientists will discover your data. In particular, if there is a centralized repository for a specific kind of data (e.g., GenBank for sequence data) then you should use that repository.</p>
<p>In cases where there is no <em>de facto</em> standard it is worth considering differences among repositories in terms of use, data rights, and licensing (Table 2) and whether your funding agency or journal has explicit requirements or restrictions related to repositories. We also recommend that you use a repository that allows your dataset to be easily cited. Most repositories will describe how this works, but an easy way to guarantee that your data are citable is to confirm that the repository associates it with a persistent identifier, the most popular of which is the digital object identifier (DOI). DOIs are permanent unique identifiers that are independent of physical location and site ownership. There are also online tools for finding good repositories for your data including <a href="http://databib.org/">Databib</a> and <a href="http://re3data.org/">re3data</a>.</p>
<table style="box-sizing: border-box; border-collapse: collapse; border-spacing: 0px; max-width: 100%; background-color: rgb(255, 255, 255); font-size: 19px; font-family: ff-tisa-sans-web-pro, Arial, sans-serif; width: 750px; margin-bottom: 21px; color: rgb(0, 0, 0); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;">
<caption><strong>Table 2. Popular repositories for scientific datasets</strong>. This table does not include well-known molecular repositories (e.g. GenBank, EMBL, MG-RAST) that have become<span> </span><em>de facto</em><span> </span>standards in molecular and evolutionary biology. Consequently, several of these primarily serve the ecological community. These repositories are not exclusively used by members of specific institutions or museums, but accept data from the general scientific community.</caption>
<thead style="box-sizing: border-box;">
<tr class="header header" style="box-sizing: border-box;">
<th style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: bottom; border-top: 0px; font-weight: bold">Repository</th>
<th style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: bottom; border-top: 0px; font-weight: bold">License</th>
<th style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: bottom; border-top: 0px; font-weight: bold">DOI</th>
<th style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: bottom; border-top: 0px; font-weight: bold">Metadata</th>
<th style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: bottom; border-top: 0px; font-weight: bold">Access</th>
<th style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: bottom; border-top: 0px; font-weight: bold">Notes</th>
</tr>
</thead>
<tbody style="box-sizing: border-box;">
<tr class="odd odd" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Dryad</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>CC0</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Yes</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Suggested</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Open</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Ecology &amp; evolution data associated with publications</p></td>
</tr>
<tr class="even even" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Ecological Archives</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>No</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Yes</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Required</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Open</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Publishes supplemental data for ESA journals and stand alone data papers</p></td>
</tr>
<tr class="odd odd" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Knowledge Network for Biocomplexity</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>No</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Yes</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Required</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Variable</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Partners with ESA, NCEAS, DataONE</p></td>
</tr>
<tr class="even even" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Paleobiology Database</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Various CC</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>No</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Optional</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Variable</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Paleontology specific</p></td>
</tr>
<tr class="odd odd" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Data Basin</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Various CC</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>No</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Optional</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Open</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>GIS data in ESRI files, limited free space</p></td>
</tr>
<tr class="even even" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Pangaea</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Various CC</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Yes</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Required</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Variable</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Editors participate in QA/QC</p></td>
</tr>
<tr class="odd odd" style="box-sizing: border-box;">
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>figshare</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>CC0</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Yes</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Optional</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Open</p></td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)"><p>Also allows deposition of other research outputs and private datasets</p></td>
</tr>
</tbody>
</table>
<h2 id="9-use-an-established-and-liberal-license">9. Use an established and liberal license</h2>
<p>Including an explicit license with your data is the best way to let others know exactly what they can and cannot do with the data you shared. Following the <a href="http://pantonprinciples.org/">Panton Principles</a> we recommend:</p>
<ol>
<li>Using well established licenses in order to clearly communicate the rights and responsibilities of both the people providing the data and the people using it.</li>
<li>Using the most open license possible, because even minor restrictions on data use can have unintended consequences for the reuse of the data (Poisot et al., 2013; Schofield et al., 2009).</li>
</ol>
<p>The Creative Commons Zero license (CC0) places no restrictions on data use and is considered by many to be one of the best license for sharing data (e.g., (Poisot et al., 2013; Schofield et al., 2009), <a href="http://blog.datadryad.org/2011/10/05/why-does-dryad-use-cc0/">Why does Dryad use CC0</a>). Having a clear and open license will increase the chance that other scientists will be comfortable using your data.</p>
<h2 id="concluding-remarks">Concluding remarks</h2>
<p>Data sharing has the potential to transform the way we conduct ecological and evolutionary research (Fienberg &amp; Martin, 1985; Poisot et al., 2013; Whitlock et al., 2010). As a result, there are an increasing number of initiatives at the federal, funding agency, and journal levels to encourage or require the sharing of the data associated with scientific research (Heather A Piwowar &amp; Chapman, 2008; Poisot et al., 2013; Whitlock et al., 2010). However, making the data available is only the first step. To make data sharing as useful as possible it is necessary to make the data usable with as little effort as possible (M. B. Jones et al., 2006; O. J. Reichman et al., 2011). This allows scientists to spend their time doing science rather than cleaning up data.</p>
<p>We have provided a list of 9 practices that require only a small additional time investment but substantially improve the usability of data. These practices can be broken down into three major groups.</p>
<ol>
<li>Well documented data are easier to understand.</li>
<li>Properly formatted data are easier to use in a variety of software.</li>
<li>Data that is shared in established repositories with open licenses is easier for others to find and use.</li>
</ol>
<p>Most of these recommendations are simply good practice for working with data regardless of whether that data are shared or not. This means that following these recommendations (2-7) make the data easier to work with for anyone, including you. This is particularly true when returning to your own data for further analysis months or years after you originally collected or analyzed it. In addition, data sharing often occurs within a lab or research group. Good data sharing practices make these in-house collaborations faster, easier, and less dependent on lab members who may have graduated or moved on to other things.</p>
<p>By following these practices we can assure that the data collected in ecology and evolution can be used to its full potential to improve our understanding of biological systems.</p>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>Thanks to Karthik Ram for organizing this special section and inviting us to contribute. Carly Strasser and Kara Woo recommended important references and David Harris and Carly Strasser provided valuable feedback on null values, all via Twitter. Carl Boettiger, Matt Davis, Daniel Hocking, Heinz Pampel, Karthik Ram, Thiago Silva, Carly Strasser, Tom Webb, and beroe (Twitter handle) provided value comments on the manuscript. Many of these comments were part of the informal review process facilitated by posting this manuscript as a preprint. The writing of this paper was supported by a CAREER grant from the U.S. National Science Foundation (DEB 0953694) to EPW.</p>
<h2 id="references">References</h2>
<p>Borer, E. T., Seabloom, E. W., Jones, M. B., &amp; Schildhauer, M. (2009). Some simple guidelines for effective data management. <em>Bulletin of the Ecological Society of America</em>, <em>90</em>(2), 205–214. Retrieved from <a href="http://dx.doi.org/10.1890/0012-9623-90.2.205">http://dx.doi.org/10.1890/0012-9623-90.2.205</a></p>
<p>Brunt, J. W., McCartney, P., Baker, K., &amp; Stafford, S. G. (2002). The future of ecoinformatics in long term ecological research. In <em>Proceedings of the 6th world multiconference on systemics, cybernetics and informatics: SCI</em> (pp. 14–18).</p>
<p>Fienberg, S. E., &amp; Martin, M. E. (1985). <em>Sharing research data</em>. Natl Academy Pr.</p>
<p>Fraser, B., &amp; Gluck, M. (1999). Usability of geospatial metadata or space-time matters. <em>Bulletin of the American Society for Information Science and Technology</em>, <em>25</em>(6), 24–28. Retrieved from <a href="http://dx.doi.org/10.1002/bult.134">http://dx.doi.org/10.1002/bult.134</a></p>
<p>Hampton, S. E., Strasser, C. A., Tewksbury, J. J., Gram, W. K., Budden, A. E., Batcheller, A. L., … Porter, J. H. (2013). Big data and the future of ecology. <em>Frontiers in Ecology and the Environment</em>, <em>11</em>(3), 156–162. Retrieved from <a href="http://dx.doi.org/10.1890/120103">http://dx.doi.org/10.1890/120103</a></p>
<p>Jones, M. B., Schildhauer, M. P., Reichman, O., &amp; Bowers, S. (2006). The new bioinformatics: Integrating ecological data from the gene to the biosphere. <em>Annual Review of Ecology, Evolution, and Systematics</em>, <em>37</em>(1), 519–54. Retrieved from <a href="http://dx.doi.org/10.1146/annurev.ecolsys.37.091305.110031">http://dx.doi.org/10.1146/annurev.ecolsys.37.091305.110031</a></p>
<p>Kowalczyk, S., &amp; Shankar, K. (2011). Data sharing in the sciences. <em>Annual Review of Information Science and Technology</em>, <em>45</em>(1), 247–294. Retrieved from <a href="http://dx.doi.org/10.1002/aris.2011.1440450113">http://dx.doi.org/10.1002/aris.2011.1440450113</a></p>
<p>Lampe, A., &amp; Weiler, J. (1998). Data capture from the sponsors’ and investigators’ perspectives: Balancing quality, speed, and cost. <em>Drug Information Journal</em>, <em>32</em>(4), 871–886.</p>
<p>Michener, W. K., Brunt, J. W., Helly, J. J., Kirchner, T. B., &amp; Stafford, S. G. (1997). Nongeospatial metadata for the ecological sciences. <em>Ecological Applications</em>, <em>7</em>(1), 330–342. Retrieved from <a href="http://dx.doi.org/10.1890/1051-0761(1997)007%5B0330:nmftes%5D2.0.co;2">http://dx.doi.org/10.1890/1051-0761(1997)007[0330:nmftes]2.0.co;2</a></p>
<p>Palmer, M. A., Bernhardt, E. S., Chornesky, E. A., Collins, S. L., Dobson, A. P., Duke, C. S., … Turner, M. G. (2004). Ecological science and sustainability for a crowded planet. Retrieved from <a href="http://www.esa.org/ecovisions/ppfiles/EcologicalVisionsReport.pdf">http://www.esa.org/ecovisions/ppfiles/EcologicalVisionsReport.pdf</a></p>
<p>Parr, C., &amp; Cummings, M. (2005). Data sharing in ecology and evolution. <em>Trends in Ecology &amp; Evolution</em>, <em>20</em>(7), 362–363. Retrieved from <a href="http://dx.doi.org/10.1016/j.tree.2005.04.023">http://dx.doi.org/10.1016/j.tree.2005.04.023</a></p>
<p>Paulsen, A., Overgaard, S., &amp; Lauritsen, J. M. (2012). Quality of data entry using single entry, double entry and automated forms processing–An example based on a study of patient-reported outcomes. <em>PloS ONE</em>, <em>7</em>(4), e35087. Retrieved from <a href="http://dx.doi.org/10.1371/journal.pone.0035087">http://dx.doi.org/10.1371/journal.pone.0035087</a></p>
<p>Piwowar, H. A., &amp; Chapman, W. W. (2008). A review of journal policies for sharing research data. In <em>ELPUB2008</em>.</p>
<p>Piwowar, H. A., &amp; Vision, T. J. (2013). Data reuse and the open data citation advantage. <em>PeerJ PrePrints</em>, <em>1</em>, e1. Retrieved from <a href="http://dx.doi.org/10.7287/peerj.preprints.1">http://dx.doi.org/10.7287/peerj.preprints.1</a></p>
<p>Piwowar, H. A., Day, R. S., &amp; Fridsma, D. B. (2007). Sharing detailed research data is associated with increased citation rate. <em>PLoS ONE</em>, <em>2</em>(3), e308. Retrieved from <a href="http://dx.doi.org/10.1371/journal.pone.0000308">http://dx.doi.org/10.1371/journal.pone.0000308</a></p>
<p>Poisot, T., Mounce, R., &amp; Gravel, D. (2013). Moving toward a sustainable ecological science: Don’t let data go to waste! Retrieved from <a href="https://github.com/tpoisot/DataSharingPaper/blob/master/DataSharing-MS.md">https://github.com/tpoisot/DataSharingPaper/blob/master/DataSharing-MS.md</a></p>
<p>Reichman, O. J., Jones, M. B., &amp; Schildhauer, M. P. (2011). Challenges and opportunities of open data in ecology. <em>Science</em>, <em>331</em>(6018), 703–705. Retrieved from <a href="http://dx.doi.org/10.1126/science.1197962">http://dx.doi.org/10.1126/science.1197962</a></p>
<p>Schofield, P. N., Bubela, T., Weaver, T., Portilla, L., Brown, S. D., Hancock, J. M., … Rosenthal, N. (2009). Post-publication sharing of data and tools. <em>Nature</em>, <em>461</em>(7261), 171–173. Retrieved from <a href="http://dx.doi.org/10.1038/461171a">http://dx.doi.org/10.1038/461171a</a></p>
<p>Strasser, C. A., Cook, R. B., Michener, W. K., Budden, A., &amp; Koskela, R. (2011). Promoting data stewardship through best practices. In <em>Proceedings of the environmental information management conference 2011 (eIM 2011)</em>. Oak Ridge National Laboratory (ORNL).</p>
<p>Strasser, C. A., Cook, R., Michener, W. K., &amp; Budden, A. (2012). Primer on data management: What you always wanted to know. DataONE. Retrieved from <a href="http://dx.doi.org/10.5060/D2251G48">http://dx.doi.org/10.5060/D2251G48</a></p>
<p>Whitlock, M. C. (2011). Data archiving in ecology and evolution: Best practices. <em>Trends in Ecology &amp; Evolution</em>, <em>26</em>(2), 61–65. Retrieved from <a href="http://dx.doi.org/10.1016/j.tree.2010.11.006">http://dx.doi.org/10.1016/j.tree.2010.11.006</a></p>
<p>Whitlock, M. C., McPeek, M. A., Rausher, M. D., Rieseberg, L., &amp; Moore, A. J. (2010). Data archiving. <em>The American Naturalist</em>, <em>175</em>(2), 145–146. <a href="http://doi.org/10.1086/650340">doi:10.1086/650340</a></p>
<p>Wickham, H. (2007). Reshaping data with the reshape package. <em>Journal of Statistical Software</em>, <em>21</em>(12). Retrieved from <a href="http://www.jstatsoft.org/v21/i12/paper">http://www.jstatsoft.org/v21/i12/paper</a></p>
<p>Wieczorek, J., Bloom, D., Guralnick, R., Blum, S., Döring, M., Giovanni, R., … Vieglais, D. (2012). Darwin core: An evolving community-developed biodiversity data standard. <em>PLoS ONE</em>, <em>7</em>(1), e29715. <a href="http://doi.org/10.1371/journal.pone.0029715">doi:10.1371/journal.pone.0029715</a></p>
<p>Zimmerman, A. (2007). Not by metadata alone: The use of diverse forms of knowledge to locate data for reuse. <em>International Journal on Digital Libraries</em>, <em>7</em>(1-2), 5–16. <a href="http://doi.org/10.1007/s00799-007-0015-8">doi:10.1007/s00799-007-0015-8</a></p>
<p>Zimmerman, A. S. (2003). <em>Data sharing and secondary use of scientific data: Experiences of ecologists</em> (PhD thesis). The University of Michigan.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Citations in Markdown Part 3]]></title>
        <id>6xfp4tf-1629scv-vpjhftb-py4er</id>
        <link href="https://blog.front-matter.io/mfenner/citations-in-markdown-part-3"/>
        <updated>2013-06-24T17:08:00.000Z</updated>
        <summary type="html"><![CDATA[After the post last week and the crazy discussion that followed I would understand that you feel you have heard enough about citations in markdown. But I had the feeling last week that something was still missing, and I have done some more thinking....]]></summary>
        <content type="html"><![CDATA[<p>After the <a href="http://blog.martinfenner.org/2013/06/19/citations-in-scholarly-markdown/">post last week</a> and the crazy discussion that followed I would understand that you feel you have heard enough about citations in markdown. But I had the feeling last week that something was still missing, and I have done some more thinking. What we have so far:</p>
<ul>
<li>Pandoc has nice support for citations, including Citation Style Language support (i.e. it is using the same 5000+ citation styles as Zotero, Mendeley and Papers).</li>
<li>Pandoc requires a separate file to store the citations, typically in bibtex format. This is fine for some people, but can make the workflow complicated for short documents or when several people work on the bibliography at the same time.</li>
<li>Citations are similar to links, and we can use links for almost all the functionality we need, making it much easier to add citations to a text. The problem is a) citations that don’t include a weblink, b) being able to do this offline, and c) where in the HTML to store the citation metadata.</li>
</ul>
<p>And I looked at how Wikipedia is <a href="http://en.wikipedia.org/wiki/Wikipedia:Citing_sources">doing this</a>, and they use a) links, b) citations and c) footnotes. If Wikipedia thinks that it can’t do without citations and do everything as links, then maybe we also shouldn’t enforce this for scholarly texts.</p>
<p>I think what we need is the best of both worlds. We should use the Pandoc citation workflow, as it is similar to what we are used to from other authoring environments, and we get good citation style support, including more complex formatting of references. Some reference managers already support copy/paste of Pandoc citation keys. The inclusion of a bibtex file with a scholarly markdown text is also a bonus, as it allows the automated extraction of citations, e.g. by manuscript submission systems.</p>
<p>We also want to support a simpler solution for shorter texts or when people don’t want to use a separate bibtex file. Here we would add the citations as links, ideally in a syntax very similar to Pandoc citation keys:</p>
<pre><code>Johnson [@Johnson2006] didn&#39;t agree with ...

[@Johnson2006]: http://dx.doi.org/10.1002/aris.201 &quot;Data sharing in the sciences&quot;</code></pre>
<p>We need to write a tool that parses the markdown before Pandoc, fetches the citation metadata for these links in bibtex format (e.g. using CrossRef Content Negotiation), and adds them to the existing bibtex file (or creates a new bibtex file). The next time the markdown is parsed, the citation is already “cached” in the bibtex file. Those people who don’t have such a tool would see the citation as link (<strong><strong>???</strong></strong>), with the essential information (DOI or URL) preserved so that a downstream tool can fetch the bibliographic information. Some people were worried about typos in DOIs and URLs. They can add additional information - e.g. the title of the paper - in double quotes to allow checking of the correct DOI.</p>
<p>This workflow now makes a lot of sense to me, as it uses existing solutions, but also allows for easy entering of citation information in a way similar to the <a href="http://carlboettiger.info/2012/05/30/knitcitations.html">knitcitations</a> and <a href="http://wordpress.org/plugins/kcite/">kcite</a> tools. As I use jekyll and am a Ruby developer, I will implement the citation parsing as a jekyll plugin.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Flavor is Scholarly Markdown?]]></title>
        <id>4q35v3j-njt8288-5pk3yf4-agm3y</id>
        <link href="https://blog.front-matter.io/mfenner/what-flavor-is-scholarly-markdown"/>
        <updated>2013-06-21T17:11:00.000Z</updated>
        <summary type="html"><![CDATA[One important outcome of the recent Markdown for Science workshop was an overall agreement that all the different implementations (or flavors) of markdown that currently exist are a big problem for the adoption of Scholarly Markdown and that we need:A...]]></summary>
        <content type="html"><![CDATA[<p>One important outcome of the recent <a href="https://github.com/scholmd/scholmd/wiki/workshop">Markdown for Science</a> workshop was an overall agreement that all the different implementations (or flavors) of markdown that currently exist are a big problem for the adoption of Scholarly Markdown and that we <a href="https://github.com/scholmd/scholmd/wiki/What-is-Markdown">need</a>:</p>
<blockquote>
A reference implementation with documentation and tests
</blockquote>
<p>As described by Karthik Ram (<a href="https://github.com/scholmd/scholmd/wiki/workshop">31 flavors is great for ice cream but not markdown</a>), <a href="https://blog.martinfenner.org/posts/what-flavor-is-scholarly-markdown/@flavor">me</a> and <a href="http://www.codinghorror.com/blog/2012/10/the-future-of-markdown.html">others</a>, there is really a large number of markdown implementations to choose from, including</p>
<ul>
<li>John Gruber’s <a href="http://daringfireball.net/projects/markdown/">original Markdown</a></li>
<li><a href="https://help.github.com/articles/github-flavored-markdown">Github-flavored Markdown</a></li>
<li><a href="http://michelf.ca/projects/php-markdown/extra/">PHP Markdown Extra</a></li>
<li><a href="http://johnmacfarlane.net/pandoc/">Pandoc</a></li>
<li><a href="http://fletcherpenney.net/multimarkdown/">MultiMarkdown</a></li>
</ul>
<p>These different flavors all serve their needs, but for Markdown to take off in the relatively small scholarly community it would be very helpful to come up with a reference implementation. But how do we get to that point?</p>
<ol>
<li>Think about the features we need for Scholarly Markdown and make this the reference implementation?</li>
<li>Organize a working group or committee that decides what is Scholarly Markdown?</li>
<li>Pick the Markdown flavor with the best developer support?</li>
<li>Figure out what markdown flavor has the widest support by tools relevant for scholars?</li>
<li>See what markdown flavor most scholars are currently using?</li>
</ol>
<p>I think as a starting point, and until we come up with something better, #5 makes the most sense. The number of markdown users among scholars is still small, but my guess would be that Pandoc is currently the most popular Markdown flavor among scholars. This blog uses Pandoc and the static site generator <a href="http://jekyllrb.com/">Jekyll</a>, and is hosted on <a href="http://pages.github.com/">Github Pages</a> - for the source code use the link in the footer. Please tell me in the comments what you are using (Markdown flavor and tools), and whether I am correct with my wild guess regarding Pandoc. And make sure your preferred tool is listed in the <a href="https://github.com/scholmd/scholmd/wiki/Tools-to-support-your-markdown-authoring">Tools to support your markdown authoring</a> wiki page.</p>
<p>A reference Markdown document is also very helpful to move forward, as we can see what outputs in HTML, PDF (or other formats) our specific Markdown tools produce, and how they differ. This reference document should include citations, tables, figures, and other features typical for scholarly content. Ideally this is a paper written in Markdown and accepted for publication - proving the concept -, or it can be a published paper transformed into markdown, e.g. a paper by <a href="http://www.elifesciences.org/elife-now-supports-content-negotiation/">eLife</a>. Feel free to suggest a paper in the comments.</p>
<p>The idea of tests that came up in the Markdown workshop is also great. Ideally we have a set of tests that we (or someone else, e.g. a publisher) can run to make sure that the markdown in the document conforms with the reference implementation. This could also include basic checks for required metadata (title, author, publication date, etc.), and could optionally validate the citations as well.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Citations in Scholarly Markdown]]></title>
        <id>2mpjg29-86q8wk8-2c2xtz0-ef9ht</id>
        <link href="https://blog.front-matter.io/mfenner/citations-in-scholarly-markdown"/>
        <updated>2013-06-19T17:13:00.000Z</updated>
        <summary type="html"><![CDATA[In the comments on Monday’s blog post about the Markdown for Science workshop, Carl Boettiger had some good arguments against the proposal for how to do citations that we came up with during the workshop. As this is a complex topic,...]]></summary>
        <content type="html"><![CDATA[<p>In the comments on <a href="https://front-matter.io/mfenner/what-is-scholarly-markdown/">Monday’s blog post</a> about the Markdown for Science workshop, <a href="http://carlboettiger.info/">Carl Boettiger</a> had some good arguments against the proposal for how to do <a href="https://github.com/scholmd/scholmd/wiki/citations">citations</a> that we came up with during the workshop. As this is a complex topic, I decided to write this blog post.</p>
<p>Citations of the scholarly literature are an essential part of scholarly texts and therefore have to be supported by scholarly markdown. Both the <a href="http://johnmacfarlane.net/pandoc/README.html">Pandoc</a> and <a href="https://github.com/fletcher/MultiMarkdown/wiki/MultiMarkdown-Syntax-Guide">Multimarkdown</a> flavors of markdown support citations, using a bibtex file that contains citations, placeholders for citekeys – <code>[@smith04]</code> for Pandoc and <code>[#smith04]</code> for Multimarkdown – and the <a href="http://citationstyles.org/">Citation Style Language</a> for citation formatting (Pandoc). A very reasonable approach would therefore be to use this functionality, with a preference for Pandoc because of the Citation Style Language support. All reference managers can export to the bibtex format, and some of them (e.g. <a href="http://www.papersapp.com/papers/">Papers</a>) make it very easy to copy and paste citekeys.</p>
<p>Ten days after the workshop I’m not so sure anymore this is the best approach. For four reasons:</p>
<ol>
<li><strong><strong>YFNS</strong></strong>. This approach failed the YFNS (your friendly neighborhood scientist) test. We came up with this term during the workshop and it means that our ideas about authoring should make sense to the workflow of the average scientist. I thought that using citekeys is a good idea, but my wife (my YFNS) tells me that she never uses citekeys because there are just too many <code>[@smith04]</code>, and it is too easy get out of sync with the reference manager. She therefore prefers to put the complete reference information into the text while writing.</li>
<li><strong><strong>Snippets</strong></strong>. As I said <a href="https://front-matter.io/mfenner/what-is-scholarly-markdown/">previously</a>, I think that scholarly markdown has great potential not so much for writing full papers, but for all the little scientific documents we write on a daily basis. For this reason the citation information should ideally be embedded in the document if it is short, and that is difficult with bibtex (which is not human-readable).</li>
<li><strong><strong>Citations as links</strong></strong>. Carl Boettiger reminded me that I wrote a <a href="https://front-matter.io/mfenner/citations-are-links-so-where-is-the-problem/">blog post in 2010</a> stating that citations are nothing else than links, and that we should treat them accordingly. He has written a tool (<a href="http://carlboettiger.info/2012/05/30/knitcitations.html">knitcitations</a>) for R that does just that, and Phil Lord and colleagues have written a similar tool (<a href="http://wordpress.org/plugins/kcite/">kcite</a>) for Wordpress. In 2010 I wrote a tool for Wordpress (<a href="http://wordpress.org/plugins/link-to-link/">Link to Link</a>) that takes a different approach but also treats citations as links. All that we need is the DOI (or URL) for the article.</li>
<li><strong><strong>Vendor lock-in</strong></strong>. Although a number of excellent reference managers are available now, users are still limited in their choices because everyone has to use the same reference manager when multiple authors work on the same document. This has always annoyed me. It would no longer be the case if we embed the citation information in the document in a standard format.</li>
</ol>
<p>Part of the motivation for using scholarly markdown is that we can come up with best practices that make sense for digital content and don’t need to support conventions from an era when articles were still printed on paper. Reference information in the form of volumes and pages, and 1000s of citation styles certainly have outlived their purpose. Citation styles are a particular pain point, as they are nothing more than a visual representation of a citation - we should care much more about the machine-readable metadata, in particular the DOI or other identifier.</p>
<p>The best practice for scholarly markdown could therefore be to treat citations as links, using DOIs or other standard identifiers (PMID, ArXiV, etc.) where possible. Because we typically want to list the citations as references at the end of the document, reference-style links should be preferred over inline links. From the <a href="http://daringfireball.net/projects/markdown/syntax#link">markdown syntax documentation</a>:</p>
<pre><code>This is [an example][id] reference-style link.

This is [an example](http://example.com/ &quot;Title&quot;) inline link.
[id]: http://example.com/  &quot;Optional Title Here&quot;</code></pre>
<p>It might be tempting to use sequential numbers as id for the reference-style links, but the order of links can of course change during writing. It may make sense to think of the id in reference-style links as a citekey, and people should be free use that functionality of their reference manager. The citekey is used to link to the reference list at the bottom of the document, different from linking to the citekey in a separate bibtex file.</p>
<p>All of the above can be done in any text editor. This also includes the text editor that scholars spend most of their time with - their email program. Reference-style citations in an email are very readable, and also actionable since they are links and not text with bibliographic information.</p>
<p>One problem with this approach is of course that all links are inline in the resulting HTML, without a references section at the end of the document. This may be fine, as we can provide citation information in the title attribute, available upon hovering over the link (try hovering over <a href="https://doi.org/10.1371/journal.pmed.0020124">this link</a>, the journal eLife is doing <a href="https://doi.org/10.7554/eLife.00633">something similar</a>). The markdown could look like this (using the <em>Vancouver</em> citation style):</p>
<pre><code>[@Ioannidis2005]: http://dx.doi.org/10.1371/journal.pmed.0020124 &quot;Ioannidis JPA. Why Most Published Research Findings Are False. PLoS Medicine. Public Library of Science; 2005;2(8):e124. Available from: http://dx.doi.org/10.1371/journal.pmed.0020124&quot;</code></pre>
<p>The title attribute now of course uses a citation style, but this is optional information and can easily be reformatted as we have the DOI.</p>
<p>Or we break away from standard markdown and display reference-style links at the end of the document - similar to <a href="http://rephrase.net/box/word/footnotes/syntax/">footnotes</a>, which are also not part of standard markdown. But this is just a display issue that can be solved, and the solution might look different depending on whether the output is HTML, PDF or XML. This document for example contains 14 reference-style citations.</p>
<p>There is obviously a need for tools that make adding citations to scholarly markdown easier. This could be accomplished by relatively small changes to existing reference managers (enabling copy/paste of citations in reference-style markdown format), or by tools similar to the <a href="http://carlboettiger.info/2012/05/30/knitcitations.html">knitcitations</a> and <a href="http://wordpress.org/plugins/kcite/">kcite</a> mentioned above.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is Scholarly Markdown?]]></title>
        <id>65rvr37-2wt9x0a-dssxbb8-fbf1g</id>
        <link href="https://blog.front-matter.io/mfenner/what-is-scholarly-markdown"/>
        <updated>2013-06-17T17:16:00.000Z</updated>
        <summary type="html"><![CDATA[One of the important discussions taking place at the Markdown for Science workshop last weekend was about the definition of Scholarly Markdown. We came up with this:Markdown that supports the requirements of scientific textsMarkdown as format that glues...]]></summary>
        <content type="html"><![CDATA[<p>One of the important discussions taking place at the <a href="https://github.com/scholmd/scholmd/wiki">Markdown for Science</a> workshop last weekend was about the definition of Scholarly Markdown. We came up with <a href="https://github.com/scholmd/scholmd/wiki/What-is-Markdown">this</a>:</p>
<ol>
<li>Markdown that supports the requirements of scientific texts</li>
<li>Markdown as format that glues open scientific text resources together</li>
<li>A reference implementation with documentation and tests</li>
<li>A community</li>
</ol>
<p>We also agreed that <strong><strong>Scholarly Markdown</strong></strong> is a better term than <strong><strong>Markdown for Science</strong></strong>, as it also includes the Social Sciences and Humanities. And we agreed on a hashtag, <a href="https://twitter.com/search?q=%23scholmd&amp;src=typd">#scholmd</a>.</p>
<p>I like #3 and #4, and I was not surprised to see #1. #2 is one of the most important outcomes of the workshop for me personally, and was reflected in the discussion we had in the breakout session on <em>What is needed for Markdown to be adopted by the scientific community?</em> One important strategy is the following:</p>
<blockquote>
We need an online tool that makes it easy for scholars to write scholarly markdown in a collaborative manner.
</blockquote>
<p>We called this the <strong><strong>Google Docs for Scientists</strong></strong> (Google Docs is a good collaborative tool, but is lacking some important features required for scientific documents, e.g. integrated citation management). <a href="https://www.authorea.com/">Authorea</a> was mentioned as a promising example of this concept. It was also noted that some previous efforts failed, because the tool looked too different from Microsoft Word. But building such a tool wouldn’t really require markdown as a file format, and could for example also be done directly in HTML5. This would be a reasonable strategy, but in my mind is falling short because I think the problem we need to solve is more complicated than making collaboration easier with tools that look like Microsoft Word. We therefore also discussed a different strategy:</p>
<blockquote>
We need multiple tools that make it easy for scholars to create scholarly markdown documents and openly share them. This collaborative work is not limited to authoring scholarly papers, but also includes shorter scholarly texts, e.g. experimental results, lab notebooks, lecture notes, blog posts and working papers. Ease of use is not only defined by the writing experience, but also how easy it is to share documents with others.
</blockquote>
<p>This definition almost sounds like a definition for Open Science, and assumes that data - and increasingly software - are an integral part of reporting science. This makes <a href="https://www.scienceexchange.com/reproducibility">reproducibility</a> of scientific results much easier, and one nice example how this can be done is the integration of markdown into the R statistical software, using the <a href="http://yihui.name/knitr/">knitr package</a>. Using the <a href="http://article-level-metrics.plos.org/plos-alm-data/">May 2013 PLOS article-level metrics data</a> which are freely available for download, the R code below can be embedded into a markdown file and will produce the bar plot below when the markdown file is run in R (to try this yourself, download the ALM data and <a href="https://github.com/articlemetrics/plosOpenR/blob/master/barPlotSummary.Rmd">markdown file for this article</a>).</p>
<pre><code># Load required libraries
library(reshape2)

# Load the data from the bulk download, filter out DOIs that are not from PLoS journals
alm &lt;- read.csv(&quot;data-alm/alm_report_2013-05-20.csv&quot;, encoding = &quot;UTF8&quot;, sep = &quot;,&quot;, stringsAsFactors=FALSE, na.strings=c(&quot;0&quot;))
alm &lt;- subset(alm, (substr(alm$doi,1,15) == &quot;10.1371/journal&quot;))
alm$publication_date &lt;- as.Date(alm$publication_date)
alm$counter_html &lt;- as.numeric(alm$counter_html)

# Options
plos.start_date &lt;- NA
plos.end_date &lt;- NA
plos.colors &lt;- c(&quot;#304345&quot;,&quot;#304345&quot;,&quot;#789aa1&quot;,&quot;#789aa1&quot;,&quot;#789aa1&quot;,&quot;#ad9a27&quot;,&quot;#ad9a27&quot;,&quot;#ad9a27&quot;,&quot;#ad9a27&quot;,&quot;#ad9a27&quot;,&quot;#ad9a27&quot;,&quot;#ad9a27&quot;)

# Aggregate notes and comments
alm$comments &lt;- as.numeric(alm$comments)

# Aggregate Mendeley
alm$mendeley &lt;- rowSums(subset(alm, select=c(&quot;mendeley_readers&quot;,&quot;mendeley_groups&quot;)), na.rm=TRUE)
alm$mendeley[alm$mendeley == 0] &lt;- NA

# Use subset of columns
alm &lt;- subset(alm, select=c(&quot;counter_html&quot;,&quot;pmc_html&quot;,&quot;crossref&quot;,&quot;scopus&quot;,&quot;pubmed&quot;,&quot;mendeley&quot;,&quot;citeulike&quot;,&quot;comments&quot;,&quot;researchblogging&quot;,&quot;facebook&quot;,&quot;twitter&quot;,&quot;wikipedia&quot;))

# Calculate percentage of values that are not missing (i.e. have a count of at least 1)
colSums &lt;- colSums(!is.na(alm)) * 100 / length(alm$counter_html)
exactSums &lt;- sum(as.numeric(alm$pmc_html),na.rm =TRUE)

# Plot the chart.
opar &lt;- par(mar=c(1,7,2,1)+0.1,omi=c(1,0.3,1,1))
plos.names &lt;- c(&quot;PLoS HTML Views&quot;, &quot;PMC HTML Views&quot;,&quot;CrossRef&quot;,&quot;Scopus&quot;,&quot;PubMed Citations&quot;, &quot;Mendeley&quot;,&quot;CiteULike&quot;,&quot;PLoS Comments&quot;,&quot;Research Blogging&quot;,&quot;Facebook&quot;,&quot;Twitter&quot;,&quot;Wikipedia&quot;)
y &lt;- barplot(colSums,horiz=TRUE,col=plos.colors, border = NA, xlab=plos.names, xlim=c(0,120), axes=FALSE, names.arg=plos.names,las=1, adj=0)
text(colSums+6,y,labels=sprintf(&quot;%1.0f%%&quot;, colSums))</code></pre>
<figure>
<img src="http://blog.martinfenner.org/images/barplot-2013-06-17.svg" class="kg-image" alt="Proportion of articles covered by source. Article-level metrics for all 80,602 PLOS journal articles published until May 20, 2013." /><figcaption aria-hidden="true"><strong>Proportion of articles covered by source</strong>. Article-level metrics for all 80,602 PLOS journal articles published until May 20, 2013.</figcaption>
</figure>
<p>In a way this approach to scholarly markdown is much more difficult than building a nice online collaborative writing tool. But for me scholarly markdown is not about competing with Microsoft Word, it is about building something new that scholars want to use because it allows them to do something that is impossible with the existing tools. For the same reason my todo item at the end of the workshop was <em>think about document type where markdown shines</em>. The R example above is a great example where markdown shines. If you can think of additional examples, please add them to the comments.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Goodbye PLOS Blogs, Welcome Github Pages]]></title>
        <id>7nv73e7-2tq8adb-9mfvvg8-sfze4</id>
        <link href="https://blog.front-matter.io/mfenner/goodbye-plos-blogs-welcome-github-pages"/>
        <updated>2013-06-15T17:18:00.000Z</updated>
        <summary type="html"><![CDATA[This is the last Gobbledygook post on PLOS Blogs, and at the same time the first post at the new Github blog location. I have been blogging at PLOS Blogs since the PLOS Blogs Network was launched in September 2010, so this step wasn’t easy....]]></summary>
        <content type="html"><![CDATA[<p>This is the last <a href="http://blogs.plos.org/mfenner">Gobbledygook</a> post on PLOS Blogs, and at the same time the first post at the <a href="http://blog.martinfenner.org/">new Github blog location</a>. I have been blogging at PLOS Blogs since the <a href="http://blogs.plos.org/blogosphere/">PLOS Blogs Network</a> was launched in September 2010, so this step wasn’t easy. But I have two good reasons.</p>
<p>In May 2012 I started to work as technical lead for the <a href="http://article-level-metrics.plos.org/">PLOS Article-Level Metrics</a> project. Although this is contract work, and I also do other things - including spending 5% of my time as clinical researcher at Hannover Medical School - this created the awkward situation that I was never quite sure whether I was blogging as Martin Fenner or as someone working for PLOS. This was all in my head, as I never had any restrictions in my blogging from PLOS. With the recent launch of the <a href="http://blogs.plos.org/tech/">PLOS Tech Blog</a> there is now a good venue for the kind of topics I like to write about, and I have started to work on two posts for this new blog.</p>
<p>There will always be topics for which the PLOS Tech Blog is not a good fit, and for these posts I have launched the new personal blog at Github. But the main reason for this new blog is a technical one: I’m moving away from blogging on Wordpress to writing my posts in <a href="http://daringfireball.net/projects/markdown/">markdown</a> (a lightweight markup language), that are then transformed into static HTML pages using <a href="http://jekyllrb.com/">Jekyll</a> and <a href="http://johnmacfarlane.net/pandoc/">Pandoc</a>. Last weekend I co-organized the workshop <a href="https://github.com/scholmd/markdown_science/wiki"><strong><strong>Scholarly Markdown</strong></strong></a> together with <a href="http://twitter.com/houshuang">Stian Håklev</a>. A full workshop report will follow in another post, but the discussions before, at and after the workshop convinced me that <strong><strong>Scholarly Markdown</strong></strong> has a bright future and that it is time to move more of my writing to markdown. At the end of the workshop each participant suggested a <a href="https://github.com/scholmd/markdown_science/wiki/Todo-list-from-workshop">todo item</a> that he/she would be working on, and my todo item was “Think about document type where MD shines”. Markdown might be good for writing scientific papers, but I think it really shines in shorter scientific documents that can easily be shared with others. And blog posts are a perfect fit.</p>
<p>The new site is work in progress. Over time I will copy over all old blog posts from PLOS Blogs, and will work on the layout as well as additional features. Special thanks to <a href="http://carlboettiger.info/">Carl Boettiger</a> for helping me to get started with Jekyll and Github pages.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[re3data.org: registry of research data repositories launched]]></title>
        <id>18tkj4d-v459668-1c2shs3-7r0x4</id>
        <link href="https://blog.front-matter.io/mfenner/re3data-org-registry-of-research-data-repositories-launched"/>
        <updated>2013-06-01T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Earlier this week re3data.org – the Registry of Research Data Repositories – officially launched. The registry is nicely described in a preprint also published this week.<em>re3data.org offers researchers, funding organizations,...]]></summary>
        <content type="html"><![CDATA[<p>Earlier this week re3data.org – the Registry of Research Data Repositories – <a href="https://web.archive.org/web/20171029050202/http://www.re3data.org/2013/05/re3data-org-launched/">officially launched</a>. The registry is nicely described in a <a href="https://dx.org/10.7287/peerj.preprints.21v1">preprint</a> also published this week.</p>
<p><em>re3data.org offers researchers, funding organizations, libraries and publishers and overview of the heterogeneous research data repository landscape. Information icons help researchers to identify an adequate repository for the storage and reuse of their data.</em></p>
<p>I really like re3data.org, and that is not because I personally know several of the people involved in this project, or because they <a href="https://front-matter.io/mfenner/figshare-interview-with-mark-hahnel/">cited this blog</a> in their preprint. I think that we are just at the beginning of building the infrastructure needed for research data management, and re3data.org fills an important need. In my opinion it is not enough to provide lists of research data repositories, we need additional information that can help guide researchers in selecting an appropriate research data repository. re3data.org has addressed this nicely by providing a <a href="https://front-matter.io/mfenner/re3data-org-registry-of-research-data-repositories-launched/10.2312/re3.002">vocabulary for the registration and description of research data repositories</a>, and by creating a simple icon system:</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/figure2.png" class="kg-image" width="500" height="569" />
</figure>
<p><em>Possible values for each icon. From <a href="https://doi.org/10.7287/peerj.preprints.21v1">https://doi.org/10.7287/peerj.preprints.21v1</a></em></p>
<p>Future directions I would like re3data.org to take include:</p>
<ul>
<li><strong>Training and education.</strong> Researchers probably pick research data repositories mainly based on the familiarity of the repository within their community rather than the criteria developed by re3data.org. A lot more training and education is needed before researchers understand the importance of persistent identifiers, licenses and other criteria.</li>
<li><strong>Integration.</strong> re3data.org can make it easier to integrate into existing scientific infrastructure, e.g. by using persistent identifiers such as DOIs for research data repositories, or by providing an API that makes it easier for other services to integrate re3data.org.</li>
<li><strong>Governance</strong>. Whether or not scientific infrastructure such as re3data.org is accepted and used by the community depends on many factors, and governance is one of the most important ones. re3data.org should seek the support of other organizations, in particular from outside Germany. A governing board, re3data.org as an independent organization, and strategies to coordinate with similar efforts such as <a href="https://web.archive.org/web/20171029050202/http://www.databib.org/">Databib</a> are possible strategies.</li>
</ul>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Metrics and attribution: my thoughts for the panel at the ORCID-Dryad symposium on research attribution]]></title>
        <id>2ghmgw6-ghq9tc9-e9xhmv0-9tvaz</id>
        <link href="https://blog.front-matter.io/mfenner/metrics-and-attribution-my-thoughts-for-the-panel-at-the-orcid-dryad-symposium-on-research-attribution"/>
        <updated>2013-05-31T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Oxford. Source: Wikimedia CommonsThis Thursday I take part in a panel discussion at the Joint ORCID – Dryad Symposium on Research Attribution. Together with Trish Groves (BMJ) and Christine Borgman (UCLA) I will discuss several aspects of attribution....]]></summary>
        <content type="html"><![CDATA[<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/A_Bicycle_in_Oxford.jpeg" class="kg-image" width="1280" height="960" alt="Oxford. Source: Wikimedia Commons" /><figcaption aria-hidden="true">Oxford. Source: <a href="https://commons.wikimedia.org/wiki/File:A_Bicycle_in_Oxford.JPG">Wikimedia Commons</a></figcaption>
</figure>
<p>This Thursday I take part in a panel discussion at the <a href="http://orcid.org/orcid-outreach-meeting-symposium-and-codefest-may-2013">Joint ORCID – Dryad Symposium on Research Attribution</a>. Together with <a href="http://www.bmj.com/about-bmj/editorial-staff/trish-groves">Trish Groves</a> (BMJ) and <a href="http://polaris.gseis.ucla.edu/cborgman/Chriss_Site/Welcome.html">Christine Borgman</a> (UCLA) I will discuss several aspects of attribution. Trish will speak about ethics, Christine will highlight problems, and I will add my perspective on metrics. This blog post summarizes the main points I want to make.</p>
<p>Scholarly metrics can be used in discovery tools, as business intelligence for funders, research organizations or publishers, and for research assessment. For all these scenarios – and in particular for research assessment – it is important to not only collect metrics for a particular journal publication, dataset or other research output, but to also link these metrics to the creators of that research output.  That is why unique identifiers for researchers, and ORCID in particular, are so important for scholarly metrics, and this is also reflected in the ORCID membership of organizations such as Thomson Reuters, Elsevier/Scopus, Altmetric or F1000Prime who provide metrics in a variety of ways.</p>
<h2 id="dora">DORA</h2>
<p>A good starting point for any discussion on metrics for research assessment is the San Francisco Declaration on Research Assessment (<a href="https://sfdora.org">DORA</a>) that was published las week, together with a set of editorials in several journals, including the <em><a href="http://jcb.rupress.org/content/early/2013/05/14/jcb.201304162.full">Journal of Cell Biology</a></em>, <em><a href="http://www.molbiolcell.org/content/24/10/1505.full">Molecular Biology of the Cell</a></em>, <em><a href="http://www.nature.com/emboj/journal/vaop/ncurrent/full/emboj2013126a.html">EMBO Journal</a></em>, <em><a href="http://www.sciencemag.org/content/340/6134/787">Science</a></em>, <em><a href="http://jcs.biologists.org/content/early/2013/05/09/jcs.134460.full.pdf+html">Journal of Cell Science</a></em>, and <em><a href="http://elife.elifesciences.org/content/elife/2/e00855.full.pdf">eLife</a></em>. The first three recommendations are a good starting point for the panel discussion:</p>
<ol>
<li><em>Do not use journal-based metrics, such as Journal Impact Factors, as a surrogate measure of the quality of individual research articles, to assess an individual scientist’s contributions, or in hiring, promotion, or funding decisions.</em></li>
<li><em>Be explicit about the criteria used in evaluating the scientific productivity of grant applicants and clearly highlight, especially for early-stage investigators, that the scientific content of a paper is much more important than publication metrics or the identity of the journal in which it was published.</em></li>
<li><em>For the purposes of research assessment, consider the value and impact of all research outputs (including datasets and software) in addition to research publications, and consider a broad range of impact measures including qualitative indicators of research impact, such as influence on policy and practice.</em></li>
</ol>
<h2 id="persistent-identifiers">Persistent Identifiers</h2>
<p>Before we can collect any metrics, we need persistent identifiers for research outputs. Most journal articles now come with a DOI, but we should make it easier for smaller publishers to use DOIs, as cost unfortunately is still an issue.</p>
<p>Persistent identifiers for data are a much more complex issue, as there a number of persistent identifiers out there (including DOIs, handles, ARKs and purls), in addition to all the domain-specific identifiers, e.g. for nucleotide sequence or protein structures. DataCite DOIs are probably the first choice for attribution, as this is their main use case and they have features that make attribution easier (e.g. familiar to researchers, funders and publishers, global resolver). There are many other use cases for identifiers for data (e.g. to identify temporary datasets in an ongoing experiment), and is of course possible to use several identifiers for the same dataset. CrossRef is of course also issuing DOIs for datasets on behalf of their members, and the publisher PLOS is for example using CrossRef <a href="https://front-matter.io/mfenner/direct-links-to-figures-and-tables-using-component-dois/">component DOIs</a> for figures and supplementary information associated with a journal article, and is <a href="http://blogs.plos.org/plos/2013/01/easier-access-to-plos-data/">making them available via figshare</a>.</p>
<p>Particular challenges with persistent identifiers for research data include different versions of a dataset, and aggregation of datasets (e.g. whether we want to cite the aggregate dataset, or a particular subset). Persistent identifiers for other research outputs are an even bigger challenge, e.g. how to uniquely identify scientific software.</p>
<p>In addition to persistent identifiers for research outputs, we also need persistent identifiers for researchers. ORCID is obviously a good candidate, as it focusses on attribution (by allowing researchers to claim their research outputs and by integration in many researcher workflows). But it is clear that ORCID is not the only persistent identifiers for researchers, and that we need to link these identifiers, e.g. <a href="https://orcid.org/blog/2013/04/22/orcid-and-isni-issue-joint-statement-interoperation-april-2013">ORCID and ISNI</a>.</p>
<p>Depending on how we want to aggregate the metrics we are interested in, we might also need persistent identifiers for institutions, for funding agencies and their grant IDs, and for resources such as particle accelerators or research vessels. Unfortunately much more work is needed in these areas.</p>
<h2 id="attribution">Attribution</h2>
<p>Attribution is then the next step, linking persistent identifiers for research outputs to their creators. Attribution is therefore essential for research assessment. The <a href="https://www.force11.org/AmsterdamManifesto">Amsterdam Manifesto on Data Citation Principles</a> that came out of the Beyond the PDF 2 workshop in March are an excellent document, but are unfortunately missing the important step of linking persistent identifiers for data to the persistent identifiers of their creators.</p>
<p>One important issue related to attribution is the provenance of the claims. Has a researcher claimed authorship for a particular paper, is a data center linking creators to research data, or is a funder doing this? The ORCID registry is built around the concept of self-claims by authors, but will allow the other stakeholders to confirm these claims.</p>
<h2 id="metrics">Metrics</h2>
<p>Metrics for scholarly content fall into one of three categories:</p>
<ul>
<li>Citations</li>
<li>Usage stats</li>
<li>Altmetrics</li>
</ul>
<p>Altmetrics is a mixed bag of many different things, from sharing on social media such as Twitter or Facebook to more scholarly activities such as Mendeley bookmarks or F1000Prime reviews. I therefore expect the altmetrics category to over time further evolve into 2-3 sub-categories.</p>
<p>We are all familiar with citation-based metrics for journal articles. We currently see the long-overdue shift from journal-based citation metrics to article-level metrics (see #1 from the DORA statement above for the reasoning), and as the technical lead for the PLOS Article-Level Metrics project I of course welcome this shift in focus. We also see a trend towards opening up reference lists that will make citation-based metrics much more accessible, and the <a href="http://opencitations.net/">JISC Open Citations</a> project by David Shotton and others is an important driver in this, as is the <a href="http://openbiblio.net/">Open Bibliographic Data</a> project by OKFN. Until open bibliographic data become the norm, we have to deal with different citation counts from different sources. PLOS is collecting citations from Web of Science, Scopus, CrossRef and PubMed Central, and the citation counts are highly correlated overall (e.g. R2= 0.87 for CrossRef and Scopus citations for 2009 PLOS Biology papers), but for some papers differ substantially. Similar to persistent identifiers, reference lists of publications should become part of the open e-infrastructure for science and not depend on proprietary systems. This makes citation metrics more transparent and easier to compare, and fosters research and innovation, in particular by smaller organizations.</p>
<p>The data citation community has adopted the journal article citation model, and we are starting to see more citations to datasets. Even though data citations look similar to citations of journal articles, many essential tools and services still don’t properly handle datasets. The <a href="http://wokinfo.com/products_tools/multidisciplinary/dci/">Web of Knowledge Data Citation Index</a> is an important step in the right direction, as is the <a href="http://odin-project.eu/2013/05/13/new-orcid-integrated-data-citation-tool/">new DataCite import tool for ORCID</a>. Something that we should pay closer attention to is the citation counts of the paper(s) associated with a dataset. Maybe the major scientific impact is in the data, but scientific practice still dictates to the cite the corresponding paper and not the dataset itself (one of the reasons we see data journals being launched). The DataCite metadata can contain the persistent identifier of the corresponding journal article, thus making it possible to associate the citation count of the corresponding paper with the dataset. This approach is particularly important for datasets that are always part of a paper, as is the case for Dryad. One important consideration is that contributor lists may differ between journal article and dataset, or between related datasets.</p>
<p>Another problem with data citation is that citation counts might not be the best way to reflect the scientific impact of a dataset. We are increasingly seeing usage stats for datasets, and DataCite for example has <a href="https://web.archive.org/web/20171029102820/http://www.datacite.org/node/76">started in January</a> to publish monthly stats for the most popular datasets by number of DOI resolutions. The #1 dataset in March was the raw data to a figure in a F1000Research article, <a href="https://figshare.com/articles/Figure_7_raw_data_Effect_of_variable_exposure_to_PTHrP_1_36_on_bone_nodules_and_AP_activity_in_high_plating_density_cultures_/154685">hosted on figshare</a>.</p>
<p>Similar to citations we see a strong trend for usage stats to move from aggregate numbers for journals to article-level metrics. COUNTER has <a href="https://web.archive.org/web/20171029102820/http://www.projectcounter.org/pirus.html">released</a> a draft code of practice for their PIRUS (Publisher and Institutional Repository Usage Statistics) standard in February, and increasing numbers of publishers and repository infrastructure providers such as <a href="https://web.archive.org/web/20171029102820/http://www.irus.mimas.ac.uk/">IRUS-UK</a> and <a href="https://web.archive.org/web/20171029102820/http://www.dini.de/projekte/oa-statistik/english">OA-Statistics</a> are providing usage stats for individual articles.</p>
<p>One challenge with usage stats, in particular with Open Access content, is that an article or other research output might be available in more than one place, e.g. publisher (or data center), disciplinary repository and institutional repository. For PLOS articles we don’t know the aggregated usage stats from institutional repositories, but we know that 17% of HTML pageviews and 33% of PDF downloads happen not at the PLOS website, but at PubMed Central.</p>
<p>Altmetrics provide new challenges, but they are also a more recent development compared to usage stats and citations. Similar to usage stats they are easier to game than citations, and for some altmetrics sources (e.g. Twitter) standardization is still difficult. Altmetrics not necessarily measure impact, but sometimes rather reflect attention or self-promotion. We have just started to look into altmetrics beyond the numbers, e.g. who is tweeting, bookmarking or discussing a paper or dataset. Altmetrics provide the opportunity to show the broader social impact (as Mike Taylor from Elsevier explains it) of research, e.g. changing clinical practice or policies.</p>
<h2 id="contributions">Contributions</h2>
<p>One important aspect to attribution is contribution, i.e. what is the specific contribution by a researcher to a paper or other research output. An <a href="https://web.archive.org/web/20171029102820/http://projects.iq.harvard.edu/attribution_workshop">International Workshop on Contributorship and Scholarly Attribution</a> was held together with the May 2012 ORCID Outreach Meeting to discuss this topic. Authorship position (e.g. first author, last author) is used in some metrics, but overall the contributor role is still poorly appreciated in most metrics. David Shotton has proposed a <a href="http://purl.org/spar/scoro/Shotton_SCoRO_and_SCoRF_Contributions-Workshop_Harvard_16May2012.pdf">Scholarly Contributions and Roles Ontology</a> (ScoRO), and is suggesting to split authorship credit in percentage points based on relative contributions, but I haven’t seen these numbers used in the context of metrics.</p>
<h2 id="conclusions">Conclusions</h2>
<p>Persistent identifiers for people, attribution and metrics are closely interrelated and we have seen a lot of exciting developments in this area in the last two years. The widespread adoption of ORCID identifiers by the research community will have a huge impact on scholarly metrics. But with all the excitement we should never forget that a) there will never be a single metric that can be used for research assessment, and b) that scientific content will always be more important than any metric. I look forward to a great panel discussions on Thursday, and welcome any feedback via comments, Twitter or email.</p>
<p><em>May 23, 2013: Post updated with minor corrections and additions.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New DataCite / ORCID Integration Tool]]></title>
        <id>1r4m6m6-m6g9wer-r2r2dxa-mk5d0</id>
        <link href="https://blog.front-matter.io/mfenner/new-datacite-orcid-integration-tool"/>
        <updated>2013-05-18T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[A new service allows researchers to add research datasets – and other content with DataCite DOIs, including all figshare content – to their ORCID profile by integrating with the DataCite Metadata Store. The tool is an adaption (or fork)...]]></summary>
        <content type="html"><![CDATA[<p>A <a href="https://web.archive.org/web/20171029102032/http://datacite.labs.orcid-eu.org/">new service</a> allows researchers to add research datasets – and other content with DataCite DOIs, including all <a href="https://web.archive.org/web/20171029102032/http://figshare.com/">figshare</a> content – to their <a href="https://web.archive.org/web/20171029102032/http://about.orcid.org/">ORCID</a> profile by integrating with the <a href="https://search.datacite.org">DataCite Metadata Store</a>. The tool is an adaption (or fork) of the <a href="https://web.archive.org/web/20171029102032/http://search.crossref.org/">CrossRef Metadata Search</a> developed by <a href="https://web.archive.org/web/20171029102032/https://twitter.com/karlward">Karl Ward</a>, and was developed by <a href="https://web.archive.org/web/20171029102032/https://twitter.com/gthorisson">Gudmundur Thorisson</a> and myself as part of work in the EU-funded <a href="https://web.archive.org/web/20171029102032/http://odin-project.eu/">ODIN project</a>. More details can be found <a href="https://web.archive.org/web/20171029102032/http://odin-project.eu/2013/05/13/new-orcid-integrated-data-citation-tool/">here</a>.</p>
<p>There are many things I like about this new DataCite/ORCID integration tool:</p>
<ul>
<li>it makes it easier for researchers to get credit for their research outputs.</li>
<li>it shows the value of persistent identifiers for data, publications and people, and linking them together</li>
<li>it shows the Creative Commons licenses for DataCite content where this info is available, facilitating reuse of content</li>
<li>it demonstrates the power of open source (thanks CrossRef!), open collaboration, standard REST APIs, and lightweight programming (Sinatra/Ruby) and deployment (Vagrant, Amazon EC2, Rackspace) tools</li>
<li>it shows that we don’t need a single – often closed – system, but open services that build on top of each other using accepted community standards. Tools using the ORCID API can immediately reuse the new DataCite content, altmetrics provided by <a href="https://impactstory.org/">ImpactStory</a> are a good example</li>
</ul>
<p>I want to explore some of these ideas in the panel <strong>Attribution: Managing Provenance, Ethics, and Metrics</strong> at the combined <a href="https://web.archive.org/web/20171029102032/http://orcid.org/orcid-outreach-meeting-symposium-and-codefest-may-2013">ORCID/Dryad Meeting</a> in Oxford next Thursday.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Announcing Markdown for Science Workshop on June 8th]]></title>
        <id>rknfx47-2e9m7rz-bgqkgk3-39g2</id>
        <link href="https://blog.front-matter.io/mfenner/announcing-markdown-for-science-workshop-on-june-8th"/>
        <updated>2013-05-08T17:20:00.000Z</updated>
        <summary type="html"><![CDATA[On Saturday June 8th – exactly a month from today – the PLOS San Francisco offices will host a workshop/hackathon about using markdown for science. A lot of people are experimenting with markdown for authoring scientific articles – see blog posts here,...]]></summary>
        <content type="html"><![CDATA[<p>On Saturday June 8th – exactly a month from today – the PLOS San Francisco offices will host a workshop/hackathon about using markdown for science. A lot of people are experimenting with markdown for authoring scientific articles – see blog posts <a href="http://blog.yoavram.com/markx/">here</a>, <a href="http://inundata.org/2012/06/01/markdown-and-the-future-of-collaborative-manuscript-writing/">here</a> or my post <a href="https://blog.martinfenner.org/posts/a-call-for-scholarly-markdown/">here</a>, and the scientific manuscript <a href="https://github.com/weecology/data-sharing-paper/">here</a>.</p>
<p>Markdown is a simple markup language for text, and is primarily used for HTML content on the web, but can also be converted to PDF, LaTeX and others. One challenge with markdown is that there are a number of slightly different “flavors” out there, from the original markdown to multimarkdown, github-flavored markdown and pandoc. Some of the advanced formatting of scientific documents – tables, citations, math – is still a challenge for markdown.</p>
<p>Will markdown become our next authoring format for scientific content? Will there be yet another flavor, scholarly markdown? How will markdown writing tools be different from LaTeX tools or Microsoft Word? If you care about any of these questions and are in or near San Francisco, join us on for all full day on June 8th. Free registration is open at <a href="http://mdsci13.eventbrite.com/">http://mdsci13.eventbrite.com</a>. We are collecting workshop ideas at <a href="https://github.com/karthikram/markdown_science/wiki/workshop">https://github.com/karthikram/markdown_science/wiki/workshop</a>, the Twitter hashtag is #mdsci13.</p>
<p>This event is organized by <a href="http://twitter.com/houshuang">Stian Håklev</a> and myself, with generous support by a <a href="http://www.force11.org/node/4358">1K Challenge prize from Force11</a>, and hosting provided by PLOS.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Baby steps toward better metrics]]></title>
        <id>b54165d-789tbs6-69xrthh-2frb</id>
        <link href="https://blog.front-matter.io/mfenner/baby-steps-toward-better-metrics"/>
        <updated>2013-04-16T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Article-Level Metrics provide new ways to look at the impact of scholarly research. Two important concepts are a) to track metrics for individual scholarly articles instead of using numbers aggregated by journal, and b)...]]></summary>
        <content type="html"><![CDATA[<p><a href="https://web.archive.org/web/20171029102027/http://article-level-metrics.plos.org/">Article-Level Metrics</a> provide new ways to look at the impact of scholarly research. Two important concepts are a) to track metrics for individual scholarly articles instead of using numbers aggregated by journal, and b) to go beyond citations and also include usage stats and altmetrics.</p>
<p>Article-Level Metrics is also doing something else: instead of tracking impact by year, it looks at usage, altmetrics and citations in real-time. There might have been technical reasons to do so 20 years ago, but there really is no longer any reason why scholarly impact should be tracked on a yearly basis in 2013. Unfortunately there is one big stumbling block:</p>
<blockquote>
<em><em>The publication date of a scholarly article is often difficult or impossible to obtain. Publication year may be the only available information.</em></em>
</blockquote>
<p>A good example is CrossRef. They provide a lot of interesting metadata about an article and make this information available in <a href="https://web.archive.org/web/20171029102027/http://search.crossref.org/">a very nice search interface</a>. But they only require the publisher to provide the publication year, information about the publication month and day is optional. There are many other examples of journals and services that just can’t tell you when exactly an article was published. This might have made sense when periodicals were printed on paper, but doesn’t work for digital content.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You should be able to install my software in less than one hour – or why DevOps is important]]></title>
        <id>45t23e2-fd5828a-8sw84wb-4dqkj</id>
        <link href="https://blog.front-matter.io/mfenner/you-should-be-able-to-install-my-software-in-less-than-one-hour-or-why-devops-is-important"/>
        <updated>2013-04-14T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Cameron Neylon yesterday wrote a great blog post about appropriate business models for shared scholarly communications infrastructure. This is an area I have also been thinking about a lot recently, and in this post I want to add a technical perspective (and an announcement)...]]></summary>
        <content type="html"><![CDATA[<p>Cameron Neylon yesterday wrote a <a href="https://web.archive.org/web/20171029102958/http://cameronneylon.net/blog/whats-the-right-model-for-shared-scholarly-communications-infrastructure/">great blog post</a> about appropriate business models for shared scholarly communications infrastructure. This is an area I have also been <a href="https://web.archive.org/web/20171029102958/http://blogs.plos.org/mfenner/2013/03/20/the-price-of-innovation-my-thoughts-for-beyond-the-pdf/">thinking about a lot recently</a>, and in this post I want to add a technical perspective (and an announcement) to the discussion.</p>
<p><a href="https://web.archive.org/web/20171029102958/http://en.wikipedia.org/wiki/DevOps">DevOps</a> is an important trend that brings software development and administration of IT infrastructure closer together. Agile software development, server virtualization, cloud infrastructure and software automation tools such as <a href="https://web.archive.org/web/20171029102958/http://www.opscode.com/chef/">Chef</a>, <a href="https://web.archive.org/web/20171029102958/https://puppetlabs.com/">Puppet</a> or <a href="https://web.archive.org/web/20171029102958/http://cfengine.com/">CFEngine</a> are an important pars of DevOps, but it is really the collaborative aspect of IT administrators working much closer with software developers what defines DevOps. The end result is often <a href="https://web.archive.org/web/20171029102958/http://readwrite.com/2013/03/27/devops-booms-in-the-enterprise">faster and more stable software releases</a>, and that is what is users and customers care about.</p>
<p>This makes DevOps particularly relevant for all areas where innovation is important, and that of course includes <a href="https://web.archive.org/web/20171029102958/http://science.okfn.org/tools-for-open-science/">tools and services for Open Science</a>. We not only need infrastructure that facilitates software development (with services like Github, among many others), but we also have to streamline IT administration. The question is not whether you do your development in Java, Python, Ruby, PHP or Javascript, but how well you integrate your software development and IT administration. The shift towards web-based tools has centralized software installation and updates, but these web-based services are becoming increasingly complex and difficult to set up and administer. Running an institutional respository, research information system or a journal is a complex task. The software may be freely available as open source (e.g. <a href="https://web.archive.org/web/20171029102958/http://www.dspace.org/">Dspace</a>, <a href="https://web.archive.org/web/20171029102958/http://vivoweb.org/">VIVO</a> or <a href="https://web.archive.org/web/20171029102958/http://pkp.sfu.ca/?q=ojs">Open Journal Systems</a>), but the resources required to run such a service still make this a big investment.</p>
<p>Two solutions to this dilemma are to pay either a vendor for installation and maintenance, or to use the software as a service (SaaS) that is hosted somewhere else. Why these two options are popular, they may not always be the best choices because they mean that you are locked in to a particular vendor or service provider, and that you may give expertise and direct access to your data away. I believe that these are helpful approaches for auxillary services, but that ideally the core services of a library, publisher or other provider of scientific infrastructure should not be outsourced. Developing software for scientific infrastructure that you want organizations to install locally should therefore always include work on integration with IT infrastructure, and just providing manual installation instructions isn’t good enough anymore.</p>
<p><a href="https://web.archive.org/web/20171029102958/http://blogs.plos.org/mfenner/tag/article-level-metrics/">Article-Level Metrics</a> (ALM) and the related altmetrics are becoming increasingly popular. The collection and display of this information is a complex process, as it requires the integration of information from several upstream APIs which may be temporarily unavailable, have changed their data format, or put up restrictions on how you can use the data. In turn this information has to be processed and aggregated, and then reliably be provided to downstream users. This kind of information gathering fits perfectly with a service provider model, and organizations such as <a href="https://web.archive.org/web/20171029102958/http://www.altmetric.com/">Altmetric</a>, <a href="https://web.archive.org/web/20171029102958/http://impactstory.org/">ImpactStory</a> and <a href="https://web.archive.org/web/20171029102958/http://www.plumanalytics.com/">Plum Analytics</a>. PLOS is collecting and displaying this information with <a href="https://web.archive.org/web/20171029102958/https://github.com/articlemetrics/alm">its own tool</a>. The simple reason is that PLOS started doing this several years before the services above became available, and none of them currently provide the same comprehensive set of information about citations, usage stats and altmetrics (although there are of course a lot of things they do better than the PLOS ALM application).</p>
<p>But there is also the question of whether Article-Level Metrics are a core service for every publisher and are best collected in-house. This not only makes it easier to collect information from some sources (e.g. usage stats or CrossRef citations), but also gives unrestricted access to the data in real-time. When I took over as technical lead for the PLOS Article-Level Metrics project last May, I therefore not only worked on improving the ALM application for PLOS, but we are also working hard on making it easier for other publishers to install and use the application. We want to provide an attractive alternative for organizations for which the service provider model is not the best option.</p>
<p>To that end I want to announce the latest feature which allows the automated installation of the PLOS ALM application on an Amazon Web Services (AWS) EC2 instance. This option is great not only for setting up an ALM production service, but because of the EC2 pricing model by hour (about $1 a day for a small EC2 instance) without setup costs is a great way to test-drive the application for a publisher, to analyze a particular set of papers from different publishers for a research project, or to set up a PLOS ALM server for a hackathon or workshop.</p>
<figure>
<img src="https://web.archive.org/web/20171029102958im_/http://blogs.plos.org/mfenner/files/2013/04/vagrant_aws.png" class="kg-image" />
</figure>
<p>There are of course many options to automate software deployment on a production server, including the PaaS (platform as a service) providers <a href="https://web.archive.org/web/20171029102958/https://www.heroku.com/">Heroku</a>, <a href="https://web.archive.org/web/20171029102958/http://www.cloudfoundry.com/">CloudFoundry</a> and <a href="https://web.archive.org/web/20171029102958/https://www.openshift.com/">OpenShift</a>, and the recently announced <a href="https://web.archive.org/web/20171029102958/http://aws.amazon.com/de/opsworks/">Amazon OpsWorks</a>. I am a big fan of the <a href="https://web.archive.org/web/20171029102958/http://www.vagrantup.com/">Vagrant</a> software development tool in combination with Chef for automation, and in March Vagrant added <a href="https://web.archive.org/web/20171029102958/http://www.hashicorp.com/blog/preview-vagrant-aws.html">support for Amazon AWS</a>. This makes deployment of the PLOS ALM application to AWS really simple:</p>
<ol>
<li>Install Vagrant and the <a href="https://web.archive.org/web/20171029102958/https://github.com/mitchellh/vagrant-aws">vagrant-aws plugin</a></li>
<li>Setup an Amazon Web Services Account</li>
<li>Check out the <a href="https://web.archive.org/web/20171029102958/https://github.com/articlemetrics/alm">PLOS ALM source code</a> from Github</li>
<li>run the command <strong>vagrant up –provider aws</strong></li>
</ol>
<p>Step #4 took 898 sec or about 15 min on my computer (see screenshot), and at the end I had a PLOS ALM server where I could access the admin dashboard via the web interface. If you are familiar with Amazon Web Services – you have to think about  the right size for the EC2 instance, an appropriate AMI, security groups, elastic IPs, and DNS service – then the whole process should be done in well under an hour. I will use this instance to load and analyze some articles from a publisher for a presentation next week. When I’m done, another command (<strong>vagrant destroy</strong>) will destroy this server and Amazon will stop billing me. During testing I have created and destroyed many servers, and the <a href="https://web.archive.org/web/20171029102958/http://www.hashicorp.com/blog/preview-vagrant-aws.html">vagrant-aws video</a> shows you how easy this process is.</p>
<p>At this stage the installation process is working (and has been working for a local Virtualbox install for many months), but needs testing and documentation. I therefore invite everyone interested in testing this out to contact me so that we can make this well-documented and working reliably.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mendeley and Elsevier]]></title>
        <id>641g46b-27185y8-jadkeby-mdj58</id>
        <link href="https://blog.front-matter.io/mfenner/mendeley-and-elsevier"/>
        <updated>2013-04-11T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Earlier this week the rumors that started in January became official: Elsevier is buying Mendeley (see also here). A lot has been written about this announcement, in particular about the fear that Mendeley as a product and organization will turn into...]]></summary>
        <content type="html"><![CDATA[<p>Earlier this week the rumors that started in January became official: <a href="https://web.archive.org/web/20170731044726/http://blog.mendeley.com/start-up-life/team-mendeley-is-joining-elsevier/">Elsevier is buying Mendeley</a> (see also <a href="https://web.archive.org/web/20170731044726/http://elsevierconnect.com/elsevier-welcomes-mendeley/">here</a>). A <a href="https://web.archive.org/web/20170731044726/http://enjoythedisruption.com/post/47527556151/my-thoughts-on-mendeley-elsevier-why-i-left-to-start">lot has been written</a> about this announcement, in particular about the fear that Mendeley as a product and organization will turn into something not as open and collaborative as before.</p>
<p>I first met Victor and Jan from Mendeley in 2008 and did an <a href="https://web.archive.org/web/20170731044726/http://blogs.plos.org/mfenner/2008/09/05/interview_with_victor_henning_from_mendeley/">interview with Victor</a> in September 2008. We worked together in the organization of two <a href="https://web.archive.org/web/20170731044726/http://www.scienceonlinelondon.org/">Science Online London conferences</a> (2009 and 2010, together with Nature.com and others), and my current job started with an entry for an <a href="https://web.archive.org/web/20170731044726/http://blog.mendeley.com/design-research-tools/winners-of-the-first-binary-battle-apps-for-science-contest/">API programming contest</a> co-organized by PLOS and Mendeley, with the <a href="https://web.archive.org/web/20170731044726/http://blogs.plos.org/mfenner/2011/09/28/announcing-sciencecard/">first lines of code written</a> in the Mendeley offices during the Science Online London 2011 hackathon. I wish Mendeley all the best with their new parent.</p>
<p>What this acquisition signals to me is that commercial publishers are now moving into the software tools for scientists business at full speed. They have always done this, but with <a href="https://web.archive.org/web/20170731044726/http://www.readcube.com/">ReadCube</a> by Digital Science (a Nature Publishing Group sister company) in 2011, the acquisition of <a href="https://web.archive.org/web/20170731044726/http://www.papersapp.com/">Papers</a> by Springer last year and now Mendeley, reference management now often means using a tool owned by a publisher – this market used to be dominated academic software such as <a href="https://web.archive.org/web/20170731044726/http://www.zotero.org/">Zotero</a> and commercial software vendors such as Thomson Reuters (<a href="https://web.archive.org/web/20170731044726/http://endnote.com/">Endnote</a>) or ProQuest (<a href="https://web.archive.org/web/20170731044726/http://www.refworks.com/">RefWorks</a>).</p>
<p>For me this trend signals that publishers have realized that we are moving into an Open Access publishing model, which in contrast to subscription publishing is not about owning the content, but about providing valuable services around content that is free to read and reuse.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comment: the case for open preprints in biology]]></title>
        <id>7mf15w4-7fj90jr-txe6nky-x47xv</id>
        <link href="https://blog.front-matter.io/mfenner/comment-the-case-for-open-preprints-in-biology"/>
        <updated>2013-03-30T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Last week Philippe Desjardins-Prouly et al. published the article The case for open preprints in biology – naturally as a preprint on figshare. The article sees preprint servers as a great opportunity for open science, and discusses the status of preprints in the biological sciences....]]></summary>
        <content type="html"><![CDATA[<p>Last week Philippe Desjardins-Prouly et al. published the article <a href="https://doi.org/10.6084/M9.FIGSHARE.655710">The case for open preprints in biology</a> – naturally as a preprint on figshare. The article sees preprint servers as a great opportunity for open science, and discusses the status of preprints in the biological sciences. In this blog post I want to add some comments to the text.</p>
<h2 id="e-biomed">E-BIOMED</h2>
<p>What is now PubMed Central <a href="https://web.archive.org/web/20170731034700/http://www.nih.gov/about/director/pubmedcentral/ebiomedarch.htm">started out as E-BIOMED in 1999</a> and initially was envisioned to include a repository for preprints. It is important to look back at what happened then, and why the preprint repository was dropped from what then became PubMed Central. Harold Varmus talks a bit about this in this <a href="https://web.archive.org/web/20170731034700/http://poynder.blogspot.de/2006/06/interview-with-harold-varmus.html">interview</a> from 2006.</p>
<h2 id="nature-precedings">Nature Precedings</h2>
<p>The article talks about why biologists have not developed a culture of sharing preprints. It would be good to mention <a href="https://web.archive.org/web/20170731034700/http://precedings.nature.com/">Nature Precedings</a>, a preprint server for the life sciences started in 2007 that stopped taking new submissions in 2012. <a href="https://web.archive.org/web/20170731034700/http://retractionwatch.wordpress.com/2012/03/30/nature-precedings-to-stop-accepting-submissions-next-week-after-finding-model-unsustainable/">This blog post</a> on RetractionWatch cites the announcement by Nature Publishing Group (which doesn’t explain why the service was shut down), and there are a good number of interesting comments.</p>
<h2 id="ssrn">SSRN</h2>
<p>Preprints in other disciplines are mentioned in the text, in particular ArXiv, but also RePEc. I would also include <strong>SSRN</strong> (<a href="https://web.archive.org/web/20170731034700/http://www.ssrn.com/">Social Science Research Network</a>), which uses a different model, but is as important for the working paper and preprint culture in the social sciences as ArXiV is in physics/mathematics.</p>
<h2 id="google-scholar-metrics">Google Scholar Metrics</h2>
<p>In April 2012 Google launched <a href="https://web.archive.org/web/20170731034700/http://scholar.google.com/citations?view_op=top_venues&amp;hl=en&amp;vq=en">Google Scholar Metrics</a>, listing the top 100 publications (according to their h5-index) in several disciplines. Six out of the top 10 publications in physics/mathematics are ArXiV sections (arXiv Astrophysics (astro-ph) is #2), the <a href="https://web.archive.org/web/20170731034700/http://www.iza.org/en/webcontent/publications/papers">IZA Discussion Papers</a> are #1 in Social Sciences, and the <a href="https://web.archive.org/web/20170731034700/http://www.nber.org/papers.html">NBER Working Papers</a> are #1 in Economics, and arXiv Astrophysics (astro-ph) is #12 on the top 100 list for all disciplines (#1-5 are journals in biology and medicine: <em>Nature</em>, <em>New England Journal of Medicine</em>, <em>Science</em>, <em>Lancet</em>, <em>Cell</em>). All these metrics are a strong indicator that preprints can be highly cited.</p>
<h2 id="citation-advantage-of-preprints">Citation Advantage of Preprints</h2>
<p>Anne Gentil-Beccot et al. have written a nice paper (of course <a href="https://web.archive.org/web/20170731034700/http://arxiv.org/abs/0906.5418">available as preprint</a>) that shows that publication as preprint now only increases the citation rate for the corresponding peer-reviewed article published later, but also leads to much faster citations, with a peak immediately after publication.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/gentil-beccot.png" class="kg-image" width="500" height="319" alt="Average number of citations per article per month as a function of the time of the citation relative to the time of publication. From http://arxiv.org/abs/0906.5418" /><figcaption aria-hidden="true">Average number of citations per article per month as a function of the time of the citation relative to the time of publication. From <a href="https://web.archive.org/web/20170731034700/http://arxiv.org/abs/0906.5418">http://arxiv.org/abs/0906.5418</a></figcaption>
</figure>
<h2 id="scoap3">SCOAP3</h2>
<p>The Sponsoring Consortium for Open Access Publications in High Energy Physics (<a href="https://scoap3.org/">SCOAP3</a>) is working on turning the majority of peer-reviewed publications in high energy physics into gold open access. It is important to understand that the high energy physics community feels that they need peer-reviewed journal articles in addition to ArXiv.</p>
<h2 id="preprint-culture-in-clinical-medicine">Preprint Culture in Clinical Medicine</h2>
<p>It is a little known fact that there is a strong preprint culture in clinical medicine. I have written about this topic in <a href="https://sensiblescience.io/mfenner/in-which-i-suggest-a-preprint-archive-for-clinical-trials/">October 2010</a>. Clinical trials have to be registered before starting the trial, and information about the trial is publicly available in <a href="https://web.archive.org/web/20170731034700/http://clinicaltrials.gov/">clinicaltrials.gov</a> and other registries. Results are presented in conferences (as poster or oral presentation), at which stage it becomes public information. The peer-reviewed paper – with a few exceptions – follows much later, sometimes even after drug approval by the FDA (in the blog post I used the <a href="https://doi.org/10.1016/S0140-6736(10)61389-X">TROPIC</a> trial as example). The problem is of course that information in oral presentations and posters is incomplete and difficult to find. But publication of a clinical trial in a peer-reviewed journal is more about giving credit to the researchers involved (similar to SCOAP3 in high energy physics) than about spreading the knowledge. Peer review is not an appropriate filter for whether or not a new drug or drug combination should be used to treat patients – the approval process by regulatory authorities is much more extensive than any peer review can ever be.</p>
<h2 id="preprint-culture-in-biology">Preprint culture in Biology</h2>
<p>The paper mentions several reasons why the field of biology has essentially no preprint culture. One argument against preprints is that it would be easier to steal ideas. Although I agree with the authors that preprints are a great way to establish precedence, there is a big difference between research based on years of work using expensive equipment (as is often the case in high energy physics but also some other fields), and research that can be reproduced in a few weeks. In the latter case it is possible that someone else is faster in publishing the peer-reviewed paper. Another difference is the community: “stealing” ideas from someone else is probably more difficult in smaller scientific communities, and some scientific communities are more competitive and less collaborative than others.</p>
<p>Another concern about preprints raised in the paper is the Ingelfinger rule, i.e. the uncertainty that a journal would accept a manuscript if already published as a preprint. This concern is fortunately unfounded regarding most publishers, and the paper includes a table listing the preprint policies of important publishers in biology.</p>
<p>I would like to add two other reasons why the preprint culture is probably not established in biology. Preprints are competition for the peer-reviewed journal article and scholarly publishers might not be particularly interested in encouraging a preprint culture. A lot has fortunately changed since E-BIOMED in 1999.</p>
<p>Finally, whereas some disciplines use preprints and working papers to communicate, in biology the preferred way to communicate research findings before publication of a peer-reviewed paper is the oral presentation. What we may need is a service that makes it easy to upload and share scientific presentations. We for example already have Slideshare, Speaker Deck as generic tools, and SciVee, figshare aimed at scientists. Speaker Deck <a href="https://sensiblescience.io/mfenner/speaker-deck-for-sharing-presentations/">is currently my favorite tool</a> and is a Github product (Github has been mentioned in the manuscript as an option for hosting preprints). Maybe what is missing is a killer combination of features in a new or existing service – persistent identifiers, uploading of background material (text, data, software, video) in addition to the slides, non-textual search, cooperation with conference organizers, etc. – for presentation sharing to take off as a way to establish a preprint culture in biology.</p>
<p><em>Update 4/4/13: Yesterday PeerJ launched a new preprint service for life sciences research. Read <a href="https://blogs.scientificamerican.com/guest-blog/2013/04/03/who-killed-the-preprint-and-could-it-make-a-return/">this blog post</a> for details, and this <a href="https://web.archive.org/web/20170731034700/http://blog.mendeley.com/open-access/is-the-time-right-for-a-preprint-server-for-life-science/">post</a> on the Mendeley blog.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using d3.js to visualize Article-Level Metrics over time]]></title>
        <id>5z5tqh1-szj8efv-ph0xnth-r7jn8</id>
        <link href="https://blog.front-matter.io/mfenner/using-d3-js-to-visualize-article-level-metrics-over-time"/>
        <updated>2013-03-26T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[PLOS Article-Level Metrics (ALM) are a great set of data (available via API and as monthly data dump) for some nice data visualizations. I have recently become a big fan of the d3.js javascript library,...]]></summary>
        <content type="html"><![CDATA[<p>PLOS Article-Level Metrics (ALM) are a great set of data (available via API and as <a href="https://web.archive.org/web/20170731170128/http://article-level-metrics.plos.org/plos-alm-data/">monthly data dump</a>) for some nice data visualizations. I have recently become a big fan of the <a href="https://web.archive.org/web/20170731170128/http://d3js.org/">d3.js</a> javascript library, and have now used d3 to look at some ALM data over time.</p>
<p>I like simple visualizations without too many labels or axes, and wanted to do a visualization inspired by <a href="https://web.archive.org/web/20170731170128/http://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0001OR">sparklines</a> ever since we discussed this idea in our <em>altviz</em> breakout group at the <a href="https://web.archive.org/web/20170731170128/http://article-level-metrics.plos.org/alm-workshop-2012/hackathon/#altviz">ALM workshop hackathon</a> in November 2012 (kudos in particular to Juan Alperin, Karthik Ram and Carl Boettiger). In the chart below every column represents the numbers for a given month, with alternating colors for the years (the article was published November 2009).</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/sparklines4-e1364338936408.png" class="kg-image" width="300" height="263" alt="CiteULike bookmarks, usage stats from PLOS website and blog posts for article Article-Level Metrics and the Evolution of Scientific Impact by month, available at http://dx.doi.org/10.1371/journal.pbio.1000242." /><figcaption aria-hidden="true">CiteULike bookmarks, usage stats from PLOS website and blog posts for article <strong>Article-Level Metrics and the Evolution of Scientific Impact</strong> by month, available at <a href="https://web.archive.org/web/20170731170128/http://dx.doi.org/10.1371/journal.pbio.1000242">http://dx.doi.org/10.1371/journal.pbio.1000242</a>.</figcaption>
</figure>
<p>You can see a pattern that is probably typical for many articles independent of the absolute numbers: most pageviews and downloads happen in the weeks after publication, as does academic bookmarking and science blogging.</p>
<p>The second example shows a very different pattern. This is not only the <a href="https://web.archive.org/web/20170731170128/http://alm.plos.org/sources/counter">most-downloaded</a> PLOS article, but  the distribution of downloads over time is very different, with the number of monthly downloads actually higher the last two years (this article was published in August 2005). We also see a few spikes in the usage stats, probably indicating events that triggered usage. Academic bookmarking was most active from 2009 to 2011 and not right after publication, although that might also have to do with the relative popularity of CiteULike over time.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/sparklines5-500x262.png" class="kg-image" width="500" height="262" alt="CiteULike bookmarks, usage stats from PLOS website and blog posts for article Why Most Published Research Findings Are False by month, available at http://dx.doi.org/10.1371/journal.pmed.0020124." /><figcaption aria-hidden="true">CiteULike bookmarks, usage stats from PLOS website and blog posts for article <strong>Why Most Published Research Findings Are False</strong> by month, available at <a href="https://web.archive.org/web/20170731170128/http://dx.doi.org/10.1371/journal.pmed.0020124">http://dx.doi.org/10.1371/journal.pmed.0020124</a>.</figcaption>
</figure>
<p>Citation data are unfortunately more difficult to get with exact publication dates (why is that so difficult?), but we can at least look at CrossRef numbers by year for the same article.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/sparklines31-e1364340861413.png" class="kg-image" width="200" height="169" alt="CiteULike bookmarks, usage stats from PLOS website and blog posts for article Why Most Published Research Findings Are False by year, available at http://dx.doi.org/10.1371/journal.pmed.0020124." /><figcaption aria-hidden="true">CiteULike bookmarks, usage stats from PLOS website and blog posts for article <strong>Why Most Published Research Findings Are False</strong> by year, available at <a href="https://web.archive.org/web/20170731170128/http://dx.doi.org/10.1371/journal.pmed.0020124">http://dx.doi.org/10.1371/journal.pmed.0020124</a>.</figcaption>
</figure>
<p>The citation numbers by year are still increasing (the last bar is for 2013), indicating that this article is still of general interest 8 years after publication. This would probably be unusual for a life sciences research article, but the article is an essay looking at common pitfalls in the statistical analysis of research data.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New version of Article-Level Metrics app released]]></title>
        <id>578gnqx-8nm8v2b-vvdrscx-yhp2p</id>
        <link href="https://blog.front-matter.io/mfenner/new-version-of-article-level-metrics-app-released"/>
        <updated>2013-03-21T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[On Tuesday we released the latest version of the PLOS Article-Level Metrics application. As always, the source code is available at Github. The changes in this version focus on improving API performance, making it easier to install the application,...]]></summary>
        <content type="html"><![CDATA[<p>On Tuesday we released the latest version of the <a href="https://web.archive.org/web/20170731155505/http://article-level-metrics.plos.org/">PLOS Article-Level Metrics application</a>. As always, the source code is available at <a href="https://web.archive.org/web/20170731155505/https://github.com/articlemetrics">Github</a>. The changes in this version focus on improving API performance, making it easier to install the application, and RSS feeds for the most popular articles by source and publication date (e.g. <a href="https://web.archive.org/web/20170731155505/http://alm.plos.org/sources/twitter.rss?days=7">the most tweeted papers published in the last 7 days</a>). See the <a href="https://web.archive.org/web/20170731155505/https://github.com/articlemetrics/alm/wiki/2.6">Github Wiki page</a>for more details, in the Wiki you also find the development <a href="https://web.archive.org/web/20170731155505/https://github.com/articlemetrics/alm/wiki/Roadmap">roadmap</a> and the <a href="https://web.archive.org/web/20170731155505/https://github.com/articlemetrics/alm/issues">issue tracker</a> for feature suggestions.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Price of Innovation – my Thoughts for Beyond the PDF]]></title>
        <id>7pxchgv-6328rh9-49ypk1r-tkcnr</id>
        <link href="https://blog.front-matter.io/mfenner/the-price-of-innovation-my-thoughts-for-beyond-the-pdf"/>
        <updated>2013-03-20T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The Beyond the PDF Conference is currently taking place in Amsterdam. Unfortunately I am unable to attend in person this time (I took part in the first Beyond the PDF in January 2011), but I was watching the livestream of the Business Case panel...]]></summary>
        <content type="html"><![CDATA[<p>The <a href="https://web.archive.org/web/20170731155123/http://www.force11.org/taxonomy/term/27">Beyond the PDF Conference</a> is currently taking place in Amsterdam. Unfortunately I am unable to attend in person this time (I took part in the first Beyond the PDF in January 2011), but I was watching the <a href="https://web.archive.org/web/20170731155123/http://www.force11.org/beyondthepdf2/live">livestream</a> of the <a href="https://web.archive.org/web/20170731155123/http://blogs.plos.org/mfenner/2013/03/20/the-price-of-innovation-my-thoughts-for-beyond-the-pdf/ww.force11.org/Business_Case">Business Case</a> panel discussion yesterday afternoon.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/291798596_dbfcfc26d7_c.jpg" class="kg-image" width="800" height="532" alt="Globe of Science and Innovation at CERN, Flickr photo by nico h." /><figcaption aria-hidden="true">Globe of Science and Innovation at CERN, <a href="https://web.archive.org/web/20170731155123/http://www.flickr.com/photos/nico_h/291798596/">Flickr photo</a> by <strong>nico h</strong>.</figcaption>
</figure>
<p>How to pay for the development of new scientific infrastructure and tools is something that I think a lot about science moving away from academia to become a developer of scientific software last year. I would assume three things:</p>
<ul>
<li>there are a lot of great ideas out there to improve scholarly communication</li>
<li>there is enough money out there to pay for improvements in scholarly communication</li>
<li>we are frustrated because progress is much slower than we anticipate</li>
</ul>
<p>If we have enough great ideas and enough money, but don’t see the results we expect, something must be going wrong. A simple answer would be that it is different people and organizations that have the ideas from those that have the money, but I don’t think that this is the reason. My suspicion is that there is a deeper problem, and that the approach we take to scholarly innovation is broken. Below is how innovation is approached by the major players:</p>
<ul>
<li>individual scientists and/or software developers come up with great ideas, but don’t get past the prototype stage because of limited resources</li>
<li>academic tools and infrastructure are built as part of a funded project (anywhere from 6 months to a few years), but there are no resources to turn this into a service that is persistent beyond the project</li>
<li>publishers and large academic institutions have the resources to build these tools. They are often less innovative because of their size</li>
<li>funders pay for projects (see above), but rarely for infrastructure, and they rarely get involved in innovative projects themselves</li>
<li>commercial organizations can quickly bring great ideas to market (in particular small startups), but it is often unclear how their services are paid for in the long run</li>
</ul>
<p>At the end of the day it seems that we have a lot of great ideas, but many of them never reach critical mass, and an even smaller number has long-term sustainability. I can think of a number of great projects that have never gained traction, and of a number of great tools and services where I have no idea how their development and service is paid for. The idea to get to a large number of users no matter what it costs, and figure out the business plan later is popular with internet startups, but dangerous when we care about tools we want to still use two years from now. Two projects that are not specific to science, but are important for science and have made this work are <strong>Wikipedia</strong> and <strong>Github</strong>. From the long list of tools for scientists I would not pick <strong>Mendeley</strong> or <strong>figshare</strong> (both great services, but still in search of sustainability), but <strong>ArXiV</strong> and <strong>Papers</strong>.It also doesn’t help that most scientists are a conservative bunch when it comes to technology, and that the scientific market is fairly small compared to the overall number of users. Another big challenge is to innovate in an open environment, i.e. to make the innovation available to as many people as possible without barriers of access. Some of my personal conclusions from all this are the following:</p>
<ul>
<li>we should acknowledge that we have an innovation problem, and it is not simply solved by getting more money</li>
<li>we have a collaboration problem, too many people are doing similar things without talking to each other and working together</li>
<li>scientific infrastructure and tools cost money. We need the right people to pay (ideally not the individual researcher), fair prices and intelligent business models</li>
<li>funders should reconsider how they pay for scientific infrastructure, as the project-based approach is broken</li>
<li>large organizations (commercial, non-commercial and academic) should think about their approach to innovation, in particular how they support innovation outside of their organization</li>
</ul>
<p>You can follow the Beyond the PDF <a href="https://web.archive.org/web/20170731155123/http://www.force11.org/beyondthepdf2/live">livestream</a> today or follow the Twitter hashtag <a href="https://web.archive.org/web/20170731155123/https://twitter.com/search?q=%23btpdf2&amp;src=hash">#btpdf2</a>.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Some Thoughts on Beyond the Paper]]></title>
        <id>4darqdg-z8k8nxt-hvxdar7-5adm4</id>
        <link href="https://blog.front-matter.io/mfenner/some-thoughts-on-beyond-the-paper"/>
        <updated>2013-03-20T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Today the journal <em>Nature</em> has released a special on the Future of Publishing. It includes a lot of interesting reading, but I want to focus on the comment Beyond the Paper by Jason Priem. In the comment Jason describes his vision of the future of scholarly communication,...]]></summary>
        <content type="html"><![CDATA[<p>Today the journal <em>Nature</em> has released a special on the <a href="https://web.archive.org/web/20170731155331/http://www.nature.com/news/specials/scipublishing/index.html">Future of Publishing</a>. It includes a lot of interesting reading, but I want to focus on the comment <a href="https://web.archive.org/web/20170731155331/http://dx.doi.org/10.1038/495437a">Beyond the Paper</a> by Jason Priem. In the comment Jason describes his vision of the future of scholarly communication, a future where many of today’s roles for articles and journals will be replaced by the <em>decoupled journal</em> and online tools taking the lead in dissemination and filtering of scholarly content.</p>
<p>Jason makes a strong case for this vision, and takes his time to also discuss the concerns and challenges. He doesn’t have the space to discuss in more detail how we get to that future, and in particular what the role of researchers, publishers, libraries and funders be in that transition.</p>
<p>Jason’s vision will probably be overwhelming for many researchers, and might not directly address what is probably the biggest issue for most researchers: funding for grants and jobs is limited, and the processes we use to select for good science and good scientists are inefficient and often arbitrary. Most students entering graduate school will not be able to have a career in academia, and most academics will say that they spend far too much time with evaluations – of their own work and the work of others. It is unclear to me how we can get from the current system – where one misstep such as denied grant or submission to the wrong journal can mean the end of a career – to the system that Jason envisions. The current climate doesn’t really foster experimentation by researchers and I am interested to understand how researchers can take part in this process of change.</p>
<p>The vision of the decoupled journal is very threatening for some of the stakeholders of the current scholarly communication ecosystem, in particular publishers and libraries. Every journal publisher and library knows that it has to reinvent itself to survive the digital transformation, but a vision that is build around a new ecosystem of service providers needs to be clear how publishers and libraries can be part of the transformation process.</p>
<p>Lastly, I disagree with the notion that <em>today’s publication silos will be replaced by a set of decentralized, interoperable services that are built on a core infrastructure of open data and evolving standards — like the Web itself.</em> I would argue that both scholarly communication and the web in general have a tendency for centralization, and that scientific infrastructure needs to be interoperable first and decentralized second. Without a focus on interoperability the future of scholarly communication will not be open and in the hands of many, but will be a race to become one of the dominant players in this new ecosystem, and we might end up with not 1000s of libraries and publishers but just a handful of technology companies holding the keys to our scientific infrastructure.</p>
<h2 id="references">References</h2>
<p>Priem, J. (2013). Scholarship: Beyond the paper <em>Nature, 495</em> (7442), 437-440 DOI: <a href="https://web.archive.org/web/20170731155331/http://dx.doi.org/10.1038/495437a">10.1038/495437a</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bye-bye Google Reader]]></title>
        <id>1p1cbg5-p9x9gsb-stgbhrv-83f9k</id>
        <link href="https://blog.front-matter.io/mfenner/bye-bye-google-reader"/>
        <updated>2013-03-14T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Yesterday Google announced that they will shut down Google Reader July 1st. In a way this announcement didn’t surprise me, as my own use of RSS readers has gone down in favor of news readers such as Flipboard and using Twitter as a discovery tool....]]></summary>
        <content type="html"><![CDATA[<p>Yesterday Google announced that they will <a href="https://web.archive.org/web/20170731151455/http://googleblog.blogspot.de/2013/03/a-second-spring-of-cleaning.html">shut down Google Reader</a> July 1st. In a way this announcement didn’t surprise me, as my own use of RSS readers has gone down in favor of news readers such as <a href="https://web.archive.org/web/20170731151455/http://blogs.plos.org/mfenner/2010/09/05/flipboard-plos-blogs-on-the-ipad/">Flipboard</a> and using Twitter as a discovery tool. And built-in support for RSS had slowly been depreciated in web browsers such as Firefox (version 4, 2011) and Safari (version 6, 2012).</p>
<p>Although RSS (and the related Atom) may never have caught on with the typical web user, it is an essential tool for scholarly content. It is the best format to subscribe to journal table of contents, much more suitable than email alerts. <a href="https://web.archive.org/web/20170731151455/http://www.journaltocs.ac.uk/">JournalTOCs</a> is a good place to get started, but most publishers prominently display the RSS icon. RSS is also great for searches you want to do regularly and is supported by PLOS, <a href="https://web.archive.org/web/20170731151455/http://www.nlm.nih.gov/bsd/disted/pubmedtutorial/040_060.html">PubMed</a>, and others. Because it is a machine-readable format, it is also used by many websites to automatically read in article information. RSS feeds for journal table of contents differ in format, but CrossRef in 2009 has posted <a href="https://oxford.crossref.org/best_practice/rss/">recommendations</a> for publishers.</p>
<p>Google Reader is of course only one of many RSS readers, so this announcement shouldn’t have any immediate impact. Nevertheless it is probably another sign that the web is moving away from RSS, and that we should start to think about alternatives for distributing tables of content. Or that we should use different strategies for finding interesting articles that have recently been published, e.g. follow the article recommendations in your social network.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Additional Markdown we need in Scholarly Texts]]></title>
        <id>4erjzwd-tvn8j2a-wdpwv3x-yescj</id>
        <link href="https://blog.front-matter.io/mfenner/additional-markdown-we-need-in-scholarly-texts"/>
        <updated>2012-12-18T17:34:00.000Z</updated>
        <summary type="html"><![CDATA[Following up from my post last week, below is a suggested list of features that should be supported in documents written in scholarly markdown. Please provide feedback via the comments, or by editing the Wiki version I have set up here....]]></summary>
        <content type="html"><![CDATA[<p>Following up from <a href="https://sensiblescience.io/mfenner/a-call-for-scholarly-markdown/">my post last week</a>, below is a suggested list of features that should be supported in documents written in scholarly markdown. Please provide feedback via the comments, or by editing the Wiki version I have set up <a href="https://github.com/mfenner/scholarly-markdown/wiki">here</a>. Listed are features that go beyond the <a href="http://daringfireball.net/projects/markdown/syntax">standard markdown syntax</a>.</p>
<p>The goals of scholarly markdown are</p>
<ol>
<li>to support writing of complete scholarly articles,</li>
<li>don’t make the syntax more complicated than it is today, and</li>
<li>don’t rely on HTML as the fallback mechanism.</li>
</ol>
<p>In practice this means that scholarly markdown should support most, but not all scholarly texts – documents that are heavy in math formulas, have complicated tables, etc. may be better written with LaTeX or Microsoft Word. It also means that scholarly markdown will probably contain only limited semantic markup, as this is difficult to do with a lightweight markup language and much easier with XML or a binary file format.</p>
<h2 id="cover-page">Cover Page</h2>
<p>Optional metadata about a document. Typically used for title, authors (including affiliation), and publication date, but should be flexible enough to handle any kind of metadata (keywords, copyright, etc.).</p>
<pre><code>---
layout: post
title: &quot;Additional Markdown we need in Scholarly Texts&quot;
tags: [markdown]
authors:
 - name: Martin Fenner
   orcid: 0000-0003-1419-2405
copyright: http://creativecommons.org/licenses/by/3.0/deed.en
---</code></pre>
<h2 id="typography">Typography</h2>
<p>Scholarly markdown should support superscript and subscript text, and should provide an easy way to enter greek ζ letters.</p>
<h2 id="tables">Tables</h2>
<p>Tables should work as anchors (i.e. you can link to them) and table captions should support styled text. Unless the table is very simple, tables are probably better written as CSV files with another tool, and then imported into the scholarly markdown document similar to figures.</p>
<table style="box-sizing: border-box; border-collapse: collapse; border-spacing: 0px; max-width: 100%; background-color: rgb(255, 255, 255); font-size: 19px; font-family: ff-tisa-sans-web-pro, Arial, sans-serif; width: 750px; margin-bottom: 21px; color: rgb(0, 0, 0); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;">
<caption><strong>This is the table caption</strong>. We can explain the table here.</caption>
<thead style="box-sizing: border-box;">
<tr class="header header" style="box-sizing: border-box;">
<th style="text-align: center;" style="box-sizing: border-box; padding: 8px; vertical-align: bottom; border-top: 0px; font-weight: bold">Centered Header</th>
<th style="text-align: right;" style="box-sizing: border-box; padding: 8px; vertical-align: bottom; border-top: 0px; font-weight: bold">Right Aligned</th>
<th style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: bottom; border-top: 0px; font-weight: bold">Left Aligned</th>
</tr>
</thead>
<tbody style="box-sizing: border-box;">
<tr class="odd odd" style="box-sizing: border-box;">
<td style="text-align: center;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)">First</td>
<td style="text-align: right;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)">12.0</td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)">Example of a row that spans multiple lines.</td>
</tr>
<tr class="even even" style="box-sizing: border-box;">
<td style="text-align: center;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)">Second</td>
<td style="text-align: right;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)">5.0</td>
<td style="text-align: left;" style="box-sizing: border-box; padding: 8px; vertical-align: top; border-top: 1px solid rgb(221, 221, 221)">Here’s another one. Note the blank line between rows.</td>
</tr>
</tbody>
</table>
<h2 id="figures">Figures</h2>
<p>Figures in scholarly works are separated from the text, and have a figure caption (which can contain styled text). Figures should work as anchors (i.e. you can link to them). Figures can be in different file formats, including TIFF and PDF, and those formats have to be converted into web-friendly formats when exporting to HTML (e.g. PNG and SVG).</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/set-operations-illustrated-with-venn-diagrams.png" class="kg-image" width="500" height="500" alt="Set operations illustrated with Venn diagrams. Example taken TeXample.net." /><figcaption aria-hidden="true">Set operations illustrated with Venn diagrams. Example taken <a href="https://texample.net/tikz/examples/set-operations-illustrated-with-venn-diagrams/">TeXample.net</a>.</figcaption>
</figure>
<h2 id="citations-and-links">Citations and Links</h2>
<p>Scholarly articles typically don’t have inline links, but rather citations. The external links (both scholarly identifiers such as DOIs and regular web URLs) are collected in a bibliography at the end of the document, and the citations in the text link to this bibliography. This functionality is similar to footnotes.</p>
<p>Citations should include a citation key in the text, e.g. <code>[@kowalczyk2011]</code>, parsed as (Kowalczyk &amp; Shankar, 2011), and a separate bibliography file in BibTeX (or RIS) format that contains references for all citations. Inserting citations and creating the bibliography can best be done with a reference manager.</p>
<p>Cross-links – i.e. links within a document – are important for scholarly texts. It should be possible to link to section headers (e.g. the beginning of the discussion section), figures and tables.</p>
<h2 id="math">Math</h2>
<p>Complicated math is probably best done in a different authoring environment, but simple formulas, both inline 2‾√x and block elements</p>
<p>ddxarctan(sin(x2))=−2cos(x2)x−2+(cos(x2))2</p>
<p>should be supported by scholarly markdown.</p>
<h2 id="comments">Comments</h2>
<p>Comments are important for multi-author documents and if reviewer feedback should be included. Comments should be linked to a particular part of a document to provide context, or attached at the end of a document for general comments. It would also be helpful to “comment out” parts of a document, e.g. to indicate parts that are incomplete and need more work. Revisions of a markdown document are best handled using a version control system such as git.</p>
<h2 id="references">References</h2>
<p>Kowalczyk, S., &amp; Shankar, K. (2011). Data sharing in the sciences. <em>Annual Review of Information Science and Technology</em>, <em>45</em>(1), 247–294. Retrieved from <a href="http://doi.org/10.1002/aris.2011.1440450113">http://doi.org/10.1002/aris.2011.1440450113</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Call for Scholarly Markdown]]></title>
        <id>7xfd0pm-75x9jg8-jcg3ncq-18h34</id>
        <link href="https://blog.front-matter.io/mfenner/a-call-for-scholarly-markdown"/>
        <updated>2012-12-13T17:37:00.000Z</updated>
        <summary type="html"><![CDATA[Markdown is a lightweight markup language, originally created by John Gruber for writing content for the web. Other popular lightweight markup languages are Textile and Mediawiki. Whereas Mediawiki markup is of course popular thanks to the ubiquitous Wikipedia,...]]></summary>
        <content type="html"><![CDATA[<p>Markdown is a lightweight markup language, originally created by John Gruber for writing content for the web. Other popular lightweight markup languages are Textile and Mediawiki. Whereas Mediawiki markup is of course popular thanks to the ubiquitous Wikipedia, Markdown seems to have gained momentum among scholars. Markdown really focuses on writing content, many of the features of today’s word processors are just a distraction (e.g. fonts, line spacing or style sheets). Adding markup for document structure (e.g. title, authors or abstract) on the other hand is overly complicated with tools such as Microsft Word.</p>
<p>Fortunately or unfortunately there are several versions (or flavors) of Markdown. The original specification by John Gruber hasn’t been updated for years. Github uses Markdown with some minor modifications. Multimarkdown and Pandoc provide features important for scholarly content, e.g. citations, superscript and tables.</p>
<ul>
<li>Markdown</li>
<li>Github-flavored Markdown</li>
<li>Multimarkdown</li>
<li>Pandoc</li>
</ul>
<p>The Pandoc flavor of Markdown probably comes closest to the requirements of a scholar, but still has limitations, e.g. support for metadata and tables isn’t very flexible. I propose that we as a community create a new Scholarly Markdown flavor, which takes into account most of the use cases important for scholarly content.</p>
<p>One of the big advantages of Markdown is that the format can not only be translated to HTML, but also to other formats, and Pandoc is particularly good in translating to and from many different formats. We want to make sure that Scholarly Markdown not only translates into nice Scholarly HTML (with good support for HTML5 tags relevant for scholars), but also into Microsot Word, LaTeX and PDF, as these are the formats typically required by manuscript tracking systems.</p>
<p>Some of the features required for Scholarly Markdown include:</p>
<ul>
<li>Superscript and subscript</li>
<li>Highlighting text (supporting the HTML tag <code>&lt;mark&gt;</code>)</li>
<li>Captions for tables and figures (with support for the HTML tags <code>&lt;caption&gt;</code> and <code>&lt;figcaption&gt;</code>)</li>
<li>Support for document sections (the HTML5 tags <code>&lt;article&gt;</code>, <code>&lt;header&gt;</code>, <code>&lt;footer&gt;</code>, <code>&lt;section&gt;</code>)</li>
<li>Good table support</li>
<li>Math support</li>
<li>Good citation support</li>
<li>Support for comments and annotations</li>
</ul>
<p>Multimarkdown and Pandoc of course already support many of these features. Tables and citations are two examples where it is important to not only support them, but support them in a non-intrusive way that doesn’t get in the way of the flow of writing.</p>
<p>BTW, this wouldn’t be the first community flavor for Markdown. The screenwriting community has done this already with <a href="http://fountain.io/">Fountain</a>.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ORCID has launched. What’s next?]]></title>
        <id>19ny3te-tey940s-fftt201-avthh</id>
        <link href="https://blog.front-matter.io/mfenner/orcid-has-launched-whats-next"/>
        <updated>2012-10-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Last week has been busy. I went to Berlin for the launch of the Open Researcher &amp; Contributor ID (ORCID) service. ORCID allows researchers to obtain a persistent identifier that can be used to claim publications and other scholarly works....]]></summary>
        <content type="html"><![CDATA[<p>Last week has been busy. I went to Berlin for the launch of the <a href="https://web.archive.org/web/20170518120701/http://orcid.org/">Open Researcher &amp; Contributor ID (ORCID)</a> service. ORCID allows researchers to obtain a persistent identifier that can be used to claim publications and other scholarly works. I’m <a href="https://web.archive.org/web/20170518120701/http://orcid.org/0000-0003-1419-2405">0000-0003-1419-2405</a>, and we put the ID (and the QR code linking to the profile on the ORCID website) on the name tags for the ORCID Outreach Meeting last Wednesday (Geek alert: I also have received a T-shirt with my name, ORCID and QR code).</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/12th/nametag-500x283.png" class="kg-image" width="500" height="283" />
</figure>
<p>I was invited to work with ORCID in early 2010 after writing about the initiative that was started in November 2009 on this blog (<a href="https://web.archive.org/web/20170518120701/http://blogs.plos.org/mfenner/2010/01/03/orcid_or_how_to_build_a_unique_identifier_for_scientists_in_10_easy_steps/">ORCID or how to build a unique identifier for scientists in 10 easy steps</a>). And now, after three years and a lot of work by a lot of people, ORCID is real and everyone can use the system. As a researcher, you can go to the ORCID website and register.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/12th/orcid-500x445.png" class="kg-image" width="500" height="445" />
</figure>
<p>Obtaining a number is of course not very interesting in itself, few people get excited about the fact of having a 16-digit unique identifier. What ORCID is really about is claiming your publications and other scholarly works, and <strong>Connecting Research and Researchers</strong> is the slogan of the organization. In the ORCID system you can now claim publications found in the CrossRef database, and other work types will be added over time.</p>
<p>What I’m particularly interested in is the claiming of research datasets. Everyone wants to give researchers better credit for the data they have produced, transformed and annotated, but data citation is still not a widespread practice. I am therefore very excited to be involved in the <a href="https://web.archive.org/web/20170518120701/http://www.odin-project.eu/">ORCID and DataCite Interoperability Network</a> (ODIN), a EU-funded project that had its kickoff meeting last week in Berlin.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/12th/odin-500x319.png" class="kg-image" width="500" height="319" />
</figure>
<p>In the ODIN project we will work closely with <a href="https://web.archive.org/web/20170518120701/http://www.datacite.org/">DataCite</a>, an organization that provides digital object identifiers (DOIs) for research data. One of the many things I like about ODIN is that social sciences is one of the disciplines where we will build a proof of concept (with the British Library, the other discipline is high-energy physics and CERN). We also want to understand how to best link researchers, data and publications. <a href="https://web.archive.org/web/20170518120701/http://datadryad.org/">Dryad</a> is an ODIN project partner and obviously has a lot of experience linking biological datasets to publications, and we will discuss how to integrate ORCID identifiers into the workflow.</p>
<p>Unique identifiers for researchers are of course also an essential part of any work on article-level metrics. I <a href="https://web.archive.org/web/20170518120701/http://blogs.plos.org/mfenner/2012/06/25/random-notes-from-the-altmetrics12-conference/">spoke about this</a> at the altmetrics12 conference in June, and I’m excited that we can now finally start linking things together. <a href="https://web.archive.org/web/20170518120701/http://impactstory.org/">ImpactStory</a> was one of the ORCID launch partners, and I demoed their ORCID integration last week in Berlin.</p>
<p><a href="https://web.archive.org/web/20170518120701/http://sciencecard.org/">ScienceCard</a> is a fork of the open source <a href="https://web.archive.org/web/20170518120701/http://article-level-metrics.plos.org/">PLOS Article-Level Metrics application</a>, and is a project I started <a href="https://web.archive.org/web/20170518120701/http://blogs.plos.org/mfenner/2011/11/20/sciencecard-named-finalist-in-mendeleyplos-api-binary-battle/">about a year ago</a>. ScienceCard allows researchers to list all their publications, and the metrics associated with them. With the launch of ORCID I was able to finally add one important missing piece. Through automatic lookup of the ORCID identifier and retrieval of the publications claimed in the ORCID profile is has become much easier to create and maintain a ScienceCard profile – it shouldn’t take more than 5 min and a few mouse clicks (collecting all metrics takes longer because that happens in the background). I added ORCID integration to ScienceCard over the weekend, using the <a href="https://web.archive.org/web/20170518120701/http://dev.orcid.org/resources">free public ORCID API</a>.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/12th/sciencecard-500x438.png" class="kg-image" width="500" height="438" />
</figure>
<p>ScienceCard is a great tool to explore how research impact can be collected and displayed, and I appreciate feedback in the form of feature requests and bug reports, ideally in the <a href="https://web.archive.org/web/20170518120701/https://github.com/mfenner/alm/issues">GitHub issue tracker</a> of the project. This will also provide very valuable feedback to improve the PLOS Article-Level Metrics application, as they use almost the same code base. The API is for example completely the same, <a href="https://web.archive.org/web/20170518120701/https://github.com/ropensci/rplos">rplos</a> and other tools using the PLOS ALM API can be used with ScienceCard by just changing the URL. Another example is the <a href="https://web.archive.org/web/20170518120701/http://wordpress.org/extend/plugins/plos-alm-widget/">PLOS ALM WordPress Widget</a>, with minor modifications it can be also be used with ScienceCard, allowing a researcher to display the metrics for his publications from PLOS and other sources on his blog. The upcoming <a href="https://web.archive.org/web/20170518120701/https://sites.google.com/site/altmetricsworkshop/">Altmetrics workshop and hackathon</a> (November 1-3 in San Francisco) will be a great opportunity to explore this further.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Announcing the ScienceCard Relaunch]]></title>
        <id>cmjk726-de8jm97-44447kp-pk5f</id>
        <link href="https://blog.front-matter.io/mfenner/announcing-the-sciencecard-relaunch"/>
        <updated>2012-09-19T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Almost exactly a year ago (in the hackathon of the Science Online London 2011 conference) I started the ScienceCard project. ScienceCard is a fork of the Open Source PLOS Article-Level Metrics (ALM) code, personalizing the Article-Level Metrics.A lot has happened in the last 12 months,...]]></summary>
        <content type="html"><![CDATA[<p>Almost exactly a year ago (in the hackathon of the Science Online London 2011 conference) I <a href="https://sensiblescience.io/mfenner/announcing-sciencecard/">started the ScienceCard project</a>. ScienceCard is a fork of the Open Source PLOS Article-Level Metrics (ALM) code, personalizing the Article-Level Metrics.</p>
<p>A lot has happened in the last 12 months, most importantly that I started to work for PLOS as technical lead for the Article-Level Metrics project in May. In July, version 2.0 of the PLOS ALM application was released, and the code made <a href="https://web.archive.org/web/20160402053034/https://github.com/articlemetrics/alm">available on Github</a>. This not only means that everyone can install his own ALM application (assuming some familiarity with the Ruby and Rails web framework), but that we can fork the code and modify it.</p>
<p>Although my focus is now clearly on improving the PLOS application, it didn’t feel right to shut down the <a href="https://web.archive.org/web/20160402053034/http://sciencecard.org/">ScienceCard</a> project. I really like the idea of personalized Article-Level Metrics (something I spoke about at the <a href="https://web.archive.org/web/20160402053034/http://blogs.plos.org/mfenner/2012/06/25/random-notes-from-the-altmetrics12-conference/">altmetrics12</a> conference). So I sat down the last two weekends to upgrade ScienceCard to the PLOS ALM 2.0 code (the source code of my fork can be found <a href="https://github.com/mfenner/alm">here</a>). With the <a href="https://web.archive.org/web/20160402053034/http://about.orcid.org/content/orcid-launch-plan-announced">imminent launch</a> of the Open Researcher &amp; Contributor ID (ORCID) service next month I dropped the functionality to get all articles by a particular person from Microsoft Academic Search. For now you can add your articles by DOI or PubMed ID (currently only articles from PubMed), but you can now also add interesting articles not authored by you. This makes ScienceCard a nice tool to try out the PLOS ALM code without installing the software. Please remember that all metrics are collected in the background, so it can take a few hours until they show up for newly added articles.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/sciencecard_new-500x312.png" title="sciencecard_new" class="kg-image" width="500" height="312" />
</figure>
<p>ScienceCard also shows the power of open source software. Open source doesn’t simply mean free software, it means that you can modify the code if your requirements are different. For ScienceCard I added authentication via Twitter (required to add articles, I don’t want to deal with usernames and passwords), a simple lookup by DOI or PubMed ID (something not needed if you are publisher of the article and have that information), and comments and likes. Article-Level Metrics is not about collecting numbers, it is about capturing the <a href="https://web.archive.org/web/20160402053034/http://blogs.bmj.com/bmj/2011/04/06/richard-smith-what-is-post-publication-peer-review/">activity surrounding an article post-publication</a>.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bye bye Nature Network, welcome SciLogs.com]]></title>
        <id>4dmrx58-fra9kns-xyq2g49-5s46q</id>
        <link href="https://blog.front-matter.io/mfenner/bye-bye-nature-network-welcome-scilogs-com"/>
        <updated>2012-07-26T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The science blogging network Nature Network is moving to a new home. Today SciLogs.com launched as new home for Nature Network bloggers. I have been blogging at Nature Network for three years, starting with my first blog post (Open access may become mandatory for NIH-funded research)...]]></summary>
        <content type="html"><![CDATA[<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/12th/nature-network.jpeg" title="nature-network" class="kg-image" width="287" height="178" />
</figure>
<p>The science blogging network Nature Network is <a href="http://blogs.nature.com/ofschemesandmemes/2012/07/26/a-new-era-for-the-nature-network-blogs">moving to a new home</a>. Today <a href="http://www.scilogs.com/">SciLogs.com</a> launched as new home for Nature Network bloggers. I have been blogging at Nature Network for three years, starting with my first blog post (<a href="https://blog.martinfenner.org/posts/open_access_may_become_mandatory_for_nih_funded_research/">Open access may become mandatory for NIH-funded research</a>) almost exactly 5 years ago to the day. My blog moved to PLOS BLOGS in September 2010 and all my old Nature Network content can be found here at PLOS BLOGS.</p>
<p>Blogging at Nature Network has changed my life in many ways. Thank you Matt and Corie (and later Lou) to make this possible.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Users do with PLOS ONE Papers]]></title>
        <id>4hh2y5j-jdk90zb-x7msqan-tjqw0</id>
        <link href="https://blog.front-matter.io/mfenner/what-users-do-with-plos-one-papers"/>
        <updated>2012-07-24T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Inspired by four recent blog posts and their comments (Comments at journal websites: just turn them off, Open Access and The Dramatic Growth of PLoS ONE, No Comment?, If you email it, they will comment), I created a graphic to show what users do with PLoS ONE papers....]]></summary>
        <content type="html"><![CDATA[<p>Inspired by four recent blog posts and their comments (<a href="https://web.archive.org/web/20170423155530/http://nsaunders.wordpress.com/2012/07/13/comments-at-journals-websites-just-turn-them-off/">Comments at journal websites: just turn them off</a>, <a href="https://web.archive.org/web/20170423155530/http://figshare.com/blog/Open_Access_and_The_Dramatic_Growth_of_PLoS_ONE/41">Open Access and The Dramatic Growth of PLoS ONE</a>, <a href="https://web.archive.org/web/20170423155530/http://blogs.plos.org/everyone/2012/07/23/no-comment/">No Comment?</a>, <a href="https://web.archive.org/web/20170423155530/http://perlsteinlab.com/round-table/if-you-email-it-they-will-comment">If you email it, they will comment</a>), I created a graphic to show what users do with PLoS ONE papers. As always, the data behind the graphic are <a href="https://web.archive.org/web/20170423155530/http://www.plosone.org/static/almInfo.action">openly available</a>. I think that the number of times a paper is informally discussed (comments, Facebook, science blogs, etc.) should be much larger compared to the numer of formal citations. The challenge is of course to have technology that captures all these discussions – this is much more difficult than for bookmarks or citations, and is obviously what <a href="https://web.archive.org/web/20170423155530/http://altmetrics.org/manifesto/">altmetrics</a> is all about. The blog posts I link to above also express another feeling: that there are still too many barriers for scientists to take part in the informal discussion of scholarly research on the web, in particular as comments on journal websites. Hat tip to <a href="https://web.archive.org/web/20170423155530/http://www.davidmccandless.com/">David McCandless</a> for inspiration.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/13th/PLoS-ONE-summary-466x500.png" title="PLoS ONE summary" class="kg-image" width="466" height="500" />
</figure>
<p><em>Update 08/02/12: The publication of the dataset used in this chart was delayed, but the data are now available at the <a href="https://web.archive.org/web/20170423155530/http://www.plosone.org/static/almInfo.action">link</a> provided.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neelie Kroes talks Open Science]]></title>
        <id>2n6mkxr-42y8xj9-14fhpy9-z5jg4</id>
        <link href="https://blog.front-matter.io/mfenner/neelie-kroes-talks-open-science"/>
        <updated>2012-07-21T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Earlier this week the European Commission announced new measures towards open science. As part of the announcement interviews of three scientists with European Commission Vice President Neelie Kroes were posted on YouTube. Here is a summary:...]]></summary>
        <content type="html"><![CDATA[<p>Earlier this week the European Commission <a href="https://blogs.ec.europa.eu/neelie-kroes/open-science/">announced</a> new measures towards open science. As part of the announcement interviews of three scientists with European Commission Vice President Neelie Kroes were posted on YouTube. Here is a summary:</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[More fun with Visualizations]]></title>
        <id>1ry44cb-f9z89e9-jq91q6x-a6mk1</id>
        <link href="https://blog.front-matter.io/mfenner/more-fun-with-visualizations"/>
        <updated>2012-07-20T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[This has been another week working on visualizations. I have summarized some of the results in a blog post over at the PLoS API website. One of my current favorites is the dot chart. PLoS Computational Biology publishes a collection of Ten Simple Rules....]]></summary>
        <content type="html"><![CDATA[<p>This has been another week working on visualizations. I have summarized some of the results in a <a href="https://web.archive.org/web/20160528080207/http://api.plos.org/2012/07/20/example-visualizations-using-the-plos-search-and-alm-apis/">blog post</a> over at the PLoS API website. One of my current favorites is the dot chart. PLoS Computational Biology publishes a <a href="https://web.archive.org/web/20160528080207/http://www.ploscollections.org/article/browseIssue.action?issue=info:doi/10.1371/issue.pcol.v03.i01">collection of Ten Simple Rules</a>. The dot chart below summarizes the HTML pageviews, PDF downloads and Mendeley readers for this collection (click on the image for a larger size).</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/13th/dotchart2-500x386.png" title="dotchart2" class="kg-image" width="500" height="386" />
</figure>
<p>On Wednesday I gave a presentation about Article-Level Metrics, using many of the same visualizations. You can find the slides over at <a href="https://web.archive.org/web/20160528080207/https://speakerdeck.com/u/mfenner/p/article-level-metrics">Speaker Deck</a> (my new favorite to upload presentation slides).</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Europe PubMed Central coming in November]]></title>
        <id>6mengjf-2519w4s-r83r1xd-3hzvk</id>
        <link href="https://blog.front-matter.io/mfenner/europe-pubmed-central-coming-in-november"/>
        <updated>2012-07-16T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The European Research Council on Friday announced that they will participate in the UK PubMed Central (UKPMC) open access repository service. They become the third European funder to join UKPMC, and the existing UKPMC funders have agreed to rebrand...]]></summary>
        <content type="html"><![CDATA[<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/13th/epmc.gif" title="epmc" class="kg-image" />
</figure>
<p>The <a href="https://web.archive.org/web/20160528080726/http://erc.europa.eu/">European Research Council</a> on Friday <a href="https://web.archive.org/web/20160528080726/http://erc.europa.eu/sites/default/files/press_release/files/EuropePMC_press_release_WT_ERC_FINAL.pdf">announced</a> that they will participate in the UK PubMed Central (UKPMC) open access repository service. They become the third European funder to join UKPMC, and the existing UKPMC funders have agreed to rebrand UKPMC as Europe PubMed Central (abbreviated to EPMC?) on November 1st.</p>
<p>More information about these changes can be found on the <a href="https://web.archive.org/web/20160528080726/http://ukpmc.blogspot.de/2012/07/european-research-council-renews-its.html">UKPMC blog</a> and in the <a href="https://web.archive.org/web/20160528080726/http://www.wellcome.ac.uk/News/Media-office/Press-releases/2012/WTVM055890.htm">Wellcome Trust press release</a>.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visualizing tweets linking to a paper]]></title>
        <id>3bz2qjd-n4v8h4v-804jzwn-0ew46</id>
        <link href="https://blog.front-matter.io/mfenner/visualizing-tweets-linking-to-a-paper"/>
        <updated>2012-07-14T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[DNA Barcoding the Native Flowering Plants and Conifers of Wales has been one of the most popular new <em>PLoS ONE</em> papers in June. In the paper Natasha de Vere <em>et al.</em> describe a DNA barcode resource that covers the 1143 native Welsh flowering...]]></summary>
        <content type="html"><![CDATA[<p><a href="https://web.archive.org/web/20170216233248/http://dx.doi.org/10.1371/journal.pone.0037945">DNA Barcoding the Native Flowering Plants and Conifers of Wales</a> has been one of the most popular new <em>PLoS ONE</em> papers in June. In the paper Natasha de Vere <em>et al.</em> describe a DNA barcode resource that covers the 1143 native Welsh flowering plants and conifers.</p>
<p>My new job as technical lead for the <a href="https://web.archive.org/web/20170216233248/http://article-level-metrics.plos.org/">PLoS Article Level Metrics (ALM) project</a> involves thinking about how we can best display the ALM collected for this and other papers. We want these ALM to tell us something important and/or interesting, and it doesn’t hurt if the information is displayed in a visually appealing way. There are many different ways this can be done, but here I want to focus on <strong>Twitter</strong> and <strong>CiteULike</strong>, the only two data sources where PLoS is currently storing every single event (tweet or CiteULike bookmark) with a date. Usage data (HTML and XML views, PDF downloads) are aggregated on a monthly basis, and PLoS doesn’t store the publication dates of citations.</p>
<p>We know from the <a href="https://web.archive.org/web/20170216233248/http://blogs.plos.org/mfenner/2011/12/20/crowdometer-or-trying-to-understand-tweets-about-journal-papers/">work of Gunter Eysenbach</a> and others that most tweets linking to scholarly papers are written in the first few days after publication. It therefore makes sense to display this information on a timeline covering the first 30 days after publication, and the tweets about the de Vere paper follow the same pattern.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/sparklines.png" title="sparklines" class="kg-image" width="500" height="375" />
</figure>
<p>I like the simplicity of sparklines. It would be interesting to also map the 274 Facebook <strong>Likes, Comments</strong> and <strong>Shares</strong>, but we don’t have date information for them. The same is true for the 9 Mendeley readers and groups.</p>
<p>Another way to display the time course of tweets (or bookmarks) is to use a calendar heat map (the paper was published June 6).</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/calendarPlot.png" title="calendarPlot" class="kg-image" width="500" height="320" />
</figure>
<p>The chart looks a little bit empty, a calendar heat map probably works better for information with many daily data points. I would appreciate feedback on how these visualizations can be improved.</p>
<p>The charts were created with data from the <a href="https://web.archive.org/web/20170216233248/http://api.plos.org/">PLoS ALM API</a> and the statistical computing package <a href="https://web.archive.org/web/20170216233248/http://www.r-project.org/">R</a>, the source code is available <a href="https://web.archive.org/web/20170216233248/https://github.com/articlemetrics/plosOpenR/blob/master/sparkLines.R">here</a> and <a href="https://web.archive.org/web/20170216233248/https://github.com/articlemetrics/plosOpenR/blob/master/calendarPlot.R">here</a>.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random notes from the altmetrics12 conference]]></title>
        <id>7nvcg3k-j409nt8-k13kjpc-f65bs</id>
        <link href="https://blog.front-matter.io/mfenner/random-notes-from-the-altmetrics12-conference"/>
        <updated>2012-06-21T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Last week I attended the altmetrics12 workshop in Chicago. You can read all 11 abstracts here, and the conference had good Twitter coverage (using the hashtag #altmetrics12), at least until Twitter had a total blackout around 12 PM our time.All but two...]]></summary>
        <content type="html"><![CDATA[<p>Last week I attended the <a href="https://web.archive.org/web/20160528073512/http://altmetrics.org/altmetrics12/">altmetrics12</a> workshop in Chicago. You can read all 11 abstracts <a href="https://web.archive.org/web/20160528073512/http://altmetrics.org/altmetrics12/program/">here</a>, and the conference had good Twitter coverage (using the hashtag <a href="https://web.archive.org/web/20160528073512/https://twitter.com/search/%23altmetrics12">#altmetrics12</a>), at least until Twitter had a total blackout around 12 PM our time.</p>
<p>All but two presenters used slides – I have uploaded <a href="https://web.archive.org/web/20160528073512/https://speakerdeck.com/u/mfenner/p/altmetrics-will-be-taken-personally-at-plos">my presentation</a> to Speaker Deck. Kelli Barr used the blackboard to explain that</p>
<ul>
<li>filtering (via altmetrics or any other means) by definition always selects out content and therefore runs against the democratization of science</li>
<li>peer review is a black box. altmetrics is also a black box, only much bigger</li>
</ul>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/13th/barr.jpeg" title="barr" class="kg-image" width="500" height="373" alt="Kelli Barr used the blackboard for two important points in her talk." /><figcaption aria-hidden="true">Kelli Barr used the blackboard for two important points in <a href="https://web.archive.org/web/20160528073512/http://altmetrics.org/altmetrics12/barr/">her talk</a>.</figcaption>
</figure>
<p>altmetrics12 was one of the best conferences that I have attended recently. The intensity of the discussions was palpable. My only regrets are that there wasn’t more time for discussions, but many of us convened in the bar afterwards.</p>
<p>We were off to a very strong start with two excellent keynote presentations by Johann Bollen ( Altmetrics: from usage data to social media) and Gregg Gordon (<a href="https://web.archive.org/web/20160528073512/http://ssrnblog.com/2012/05/18/alternative-is-the-new-grey/">Alternative is the new Grey</a>). Johann emphasized why it is both important and fascinating to study how science is actually working. He stressed that science is a gift economy, where the currency is acknowledgement of influence in the form of citations. He sees two major problems with the present citation-based analysis of scientific impact: a) data and b) the metrics. Citation data are very domain specific (with very different citation practices in different disciplines), and delayed (several years after publication of the research they are citing). The problem with current citation-based metrics is that they ignore the network effect in science. Johann then went on to explain the <a href="https://web.archive.org/web/20160528073512/http://mesur.informatics.indiana.edu/">MESUR</a> project, which studies the patterns of scientific activity on a very large scale (1 billion usage events, 500 million citations, 50 million papers).</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/13th/journal.pone.0004803.g005.png" title="Map of Science" class="kg-image" width="320" height="305" alt="Map of science from 2009 PLoS ONE paper." /><figcaption aria-hidden="true">Map of science from <a href="https://web.archive.org/web/20160528073512/http://dx.doi.org/10.1371/journal.pone.0004803">2009 PLoS ONE paper</a>.</figcaption>
</figure>
<p>Johann then briefly explained the results of another <a href="https://web.archive.org/web/20160528073512/http://dx.doi.org/10.1371/journal.pone.0006022">2009 PLoS ONE paper</a> where he analyzed 39 metrics with principal component analysis. He found two major components in the metrics analysis: counting vs. social influence and fast vs. slow.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/13th/journal.pone.0006022.g002.png" title="PCA" class="kg-image" width="320" height="227" alt="Correlation of 37 metrics mapped onto first two principal components in another 2009 PLoS ONE paper." /><figcaption aria-hidden="true">Correlation of 37 metrics mapped onto first two principal components in another <a href="https://web.archive.org/web/20160528073512/http://dx.doi.org/10.1371/journal.pone.0006022">2009 PLoS ONE paper</a>.</figcaption>
</figure>
<p>Johann then told the interesting story behind a <a href="https://web.archive.org/web/20160528073512/http://arxiv.org/abs/1010.3003">2010 paper</a> where he showed that <strong>Twitter mood can predict the stock market</strong>. The paper was rejected by all publishers and was finally published on ArXiV in October 2010. It immediately became very popular in terms of downloads and media attention (and was eventually <a href="https://web.archive.org/web/20160528073512/http://dx.doi.org/10.1016/j.jocs.2010.12.007">published</a> in the <em>Journal of Computational Science</em> in March 2011).</p>
<p>Gregg Gordon has summarized his keynote in a May <a href="https://web.archive.org/web/20160528073512/http://ssrnblog.com/2012/05/18/alternative-is-the-new-grey/">blogpost</a>, so I will focus on a few highlights. Gregg Gordon runs the Social Science Research Network (<a href="https://web.archive.org/web/20160528073512/http://ssrn.com/">SSRN</a>), a leading resource for sharing social sciences research. Gregg thinks that altmetrics can provide the compass to navigate the map of science described earlier. He told us some very interesting anecdotes of users trying to game SSRN by inflating their download counts (e.g. “downloads for donuts”), and mentioned a <a href="https://web.archive.org/web/20160528073512/http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1346397">research paper</a>analyzing gaming at SSRN. Download counts are apparently taken very seriously by SSRN authors and are also used for hiring decisions. SSRN not only has written software to protect against gaming, but also has a person constantly looking over these numbers (Gregg feels that computer algorithms alone are not enough).</p>
<p>The two keynotes were followed by 11 short (10-15 minute) presentations, including two presentations about the PLoS <a href="https://web.archive.org/web/20160528073512/http://article-level-metrics.plos.org/">Article-Level Metrics</a> project. Jennifer Lin spoke about <a href="https://web.archive.org/web/20160528073512/http://altmetrics.org/altmetrics12/lin/">anti-gaming mechanisms</a> and I emphasized the importance of <a href="https://web.archive.org/web/20160528073512/http://altmetrics.org/altmetrics12/fenner/">personalizing altmetrics</a> to fully understand the “network of science”. All presentation abstracts are available <a href="https://web.archive.org/web/20160528073512/http://altmetrics.org/altmetrics12/program/">online</a>.</p>
<p>After the lunch break (and some interesting discussions) we continued with demos of various altmetrics applications and users, including <a href="https://web.archive.org/web/20160528073512/http://total-impact.org/">Total Impact</a>, <a href="https://web.archive.org/web/20160528073512/http://www.plumanalytics.com/">Plum Analytics</a>, <a href="https://web.archive.org/web/20160528073512/http://www.altmetric.com/">altmetric.com</a>, the <a href="https://web.archive.org/web/20160528073512/http://alm.plos.org/">PLoS Article-Level Metrics</a> application, <a href="https://web.archive.org/web/20160528073512/http://knodeinc.com/">Knode</a>, <a href="https://web.archive.org/web/20160528073512/http://academia.edu/">Academia.edu</a>, <a href="https://web.archive.org/web/20160528073512/http://www.biomedcentral.com/">BioMed Central</a> and <a href="https://web.archive.org/web/20160528073512/http://www.ubiquitypress.com/">Ubiquity Press</a> (example <a href="https://web.archive.org/web/20160528073512/http://openarchaeologydata.metajnl.com/article/intensive-survey-data-from-antikythera-greece/">here</a>).</p>
<p>In the last our and a half we split up into several smaller group to discuss issues relevant for altmetrics. I was in the <strong>standards</strong> group and we all agreed that it is too early to sep up rigid standards for this evolving field, but not too early to start the discussion. The two standards experts in our group (Todd Carpenter from <a href="https://web.archive.org/web/20160528073512/http://www.niso.org/home/">NISO</a> and David Baker from <a href="https://web.archive.org/web/20160528073512/http://casrai.org/">CASRAI</a>) were of course very helpful with the discussion in our group. In the closing group discussion the breakout groups reported their major discussion points (which will hopefully be written up by someone). We also learned that the NIH is considering changing the Biosketch format for CVs and is looking for input via a <a href="https://web.archive.org/web/20160528073512/http://grants.nih.gov/grants/guide/notice-files/NOT-OD-12-115.html">Request for Information</a> (RFI) until June 29.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speaker Deck for Sharing Presentations]]></title>
        <id>4h05eae-ng19j3a-n91y67m-5e4h8</id>
        <link href="https://blog.front-matter.io/mfenner/speaker-deck-for-sharing-presentations"/>
        <updated>2012-05-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[It has become common practice to make presentation slides available for those unable to attend in person, or for more in-depth review later. The most popular service to do this is of course Slideshare. Slideshare is a fine service,...]]></summary>
        <content type="html"><![CDATA[<p>It has become common practice to make presentation slides available for those unable to attend in person, or for more in-depth review later. The most popular service to do this is of course <a href="https://web.archive.org/web/20160404225654/http://www.slideshare.net/">Slideshare</a>. Slideshare is a fine service, but the website has become fairly cluttered over the years, and visuals are of course important when it comes to presentations.<br />
<br />
<a href="https://web.archive.org/web/20160404225654/http://speakerdeck.com/">Speaker Deck</a> is an alternative to Slideshare with a focus on “simplicity and beauty”. The service has the features you would expect:</p>
<ul>
<li>upload presentations (currently in PDF only)</li>
<li>view presentations, including fullscreen mode</li>
<li>share presentations, including downloads and embedding</li>
</ul>
<figure>
<img src="https://web.archive.org/web/20160404225654im_/http://blogs.plos.org/mfenner/files/2012/05/speakerdeck.png" title="speakerdeck" class="kg-image" alt="Slide 51 from a presentation I did with Steve Pettifer at UKSG in March." /><figcaption aria-hidden="true"><a href="https://web.archive.org/web/20160404225654/https://speakerdeck.com/u/mfenner/p/future-formats-representing-the-next-generation-of-scholarly-articles?slide=51">Slide 51</a> from a presentation I did with Steve Pettifer at UKSG in March.</figcaption>
</figure>
<p>Speaker Deck was <a href="https://web.archive.org/web/20160404225654/http://orderedlist.com/blog/articles/share-presentations-without-the-mess/">announced last September</a> and is free to use. It would be helpful if Speaker Deck allowed the upload of Powerpoint or Keynote files, making it easier to reuse the slides. But the big item on my wish list is an improvement of the social activities enabled around a presentation. I want to see download counts, comments, number of tweets and Facebook likes, and I want an API for them. Ordered List, the company behind Speakerdeck, was <a href="https://web.archive.org/web/20160404225654/http://orderedlist.com/blog/articles/ordered-list-acquired-by-github/">acquired by Github</a> in December, turning Speaker Deck into a Github product. This makes me confident that we will see these Speaker Deck improvements rather sooner than later.</p>
<p>To see a few Speaker Deck presentations, go to <a href="https://web.archive.org/web/20160404225654/https://speakerdeck.com/u/mfenner">my presentations</a> or the <a href="https://web.archive.org/web/20160404225654/https://speakerdeck.com/c/science">Science category</a>.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PLoS Article-Level Metrics: Interview with Martin Fenner]]></title>
        <id>40g88vt-0c38tja-c6dghh1-sm11b</id>
        <link href="https://blog.front-matter.io/mfenner/plos-article-level-metrics-interview-with-martin-fenner"/>
        <updated>2012-04-23T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[This blog occasionally does interviews with people providing interesting tools for scholars. These interviews have always been among my favorite blog posts. This now is obviously an interview with myself,...]]></summary>
        <content type="html"><![CDATA[<p>This blog occasionally does interviews with people providing interesting tools for scholars. These <a href="https://front-matter.io/interviews">interviews</a> have always been among my favorite blog posts. This now is obviously an interview with myself, but I felt this is the best format to explain some important news.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/429781913_9524791cff_c.jpg" class="kg-image" width="532" height="800" alt="Flickr photo my stevec77." /><figcaption aria-hidden="true"><a href="https://web.archive.org/web/20161226124822/http://www.flickr.com/photos/stevec77/429781913/">Flickr photo</a> my stevec77.</figcaption>
</figure>
<p>Starting May 16 I will be working full-time as technical lead for the PLoS <a href="https://web.archive.org/web/20161226124822/http://article-level-metrics.plos.org/">Article Level Metrics</a> (ALM) project. I will help with development of the <a href="https://web.archive.org/web/20161226124822/http://code.google.com/p/alt-metrics/">PLoS ALM application</a>, and will do community developer outreach for this project.</p>
<p>The PLoS ALM application is written in <a href="https://web.archive.org/web/20161226124822/http://rubyonrails.org/">Ruby on Rails</a>, an open-source web framework I have been working with since 2005. The ALM project was launched in 2009, and I first learned about ALM in a July 2009 presentation by Pete Binfield at <a href="https://web.archive.org/web/20161226124822/http://blogs.plos.org/mfenner/2009/07/10/i_was_at_scibarcamp_palo_alto/">SciBarCamp Palo Alto</a>. A month later I did an <a href="https://web.archive.org/web/20161226124822/http://blogs.plos.org/mfenner/2009/07/10/i_was_at_scibarcamp_palo_alto/">interview with Pete</a> about Article Level Metrics and PLoS ONE.</p>
<h3 id="what-is-article-level-metrics">What is Article Level Metrics?</h3>
<p>Article Level Metrics <em>place transparent and  comprehensive information about the usage and reach of published articles onto the articles themselves, so that the entire academic community can assess their value</em> (from the <a href="https://web.archive.org/web/20161226124822/http://article-level-metrics.plos.org/">PLoS ALM website</a>). A <a href="https://web.archive.org/web/20161226124822/http://dx.doi.org/10.1371/journal.pbio.1000242">November 2009 paper</a> by Cameron Neylon and Shirley Wu gives a more detailed introduction. And a <a href="https://web.archive.org/web/20161226124822/http://www.slideshare.net/kristenratan/metrics-the-new-black">recent presentation</a> by Kristen Ratan, PLoS Director of Product Management, given at the <a href="https://web.archive.org/web/20161226124822/http://www.nfais.org/page/361-program-2012-nfais-annual-conference">2012 NFAIS meeting</a>, provides an update for 2012.</p>
<p>Article Level Metrics is part of the larger <a href="https://web.archive.org/web/20161226124822/http://altmetrics.org/manifesto/">altmetrics</a> movement, which also looks at metrics for other scholarly works besides journal articles.</p>
<h3 id="does-this-mean-that-you-will-be-moving-to-san-francisco">Does this mean that you will be moving to San Francisco?</h3>
<p>For personal reasons I will continue to live in Hannover, Germany and work from home as a contractor with occasional trips to San Francisco.</p>
<h3 id="will-you-miss-treating-cancer-patients-and-doing-cancer-research">Will you miss treating cancer patients and doing cancer research?</h3>
<p>Absolutely. This has not been an easy decision. To make the transition easier, I will continue to spend 10% of my time at Hannover Medical School. I will no longer be seeing patients, but this will allow me to conclude the RADIT <a href="https://web.archive.org/web/20161226124822/http://clinicaltrials.gov/ct2/show/NCT01242631">clinical trial</a> for testicular cancer patients where I am the principal investigator.</p>
<p>I hope to continue doing research in the new position, but with a focus on information science. There are for example still a lot of things we don’t know about altmetrics. A more detailed analysis of our recent <a href="https://web.archive.org/web/20161226124822/http://blogs.plos.org/mfenner/2012/02/19/crowdsourcing-the-analysis-of-scholarly-tweets/">CrowdoMeter</a> project (a crowdsourced analysis of tweets linking to scholarly papers) would be a good start.</p>
<h3 id="what-will-happen-with-sciencecard">What will happen with ScienceCard?</h3>
<p><a href="https://web.archive.org/web/20161226124822/http://sciencecard.org/">ScienceCard</a> is a website that collects author level metrics and was my entry into the <a href="https://web.archive.org/web/20161226124822/http://blogs.plos.org/mfenner/2011/11/20/sciencecard-named-finalist-in-mendeleyplos-api-binary-battle/">Mendeley/PLoS Binary Battle API contest</a> last fall. ScienceCard is based on the PLoS ALM code (which is open source and <a href="https://web.archive.org/web/20161226124822/http://code.google.com/p/alt-metrics/">available via Google Code</a>). I will decide in the coming months what to do with ScienceCard. This depends mainly on how much author level metrics make sense in the PLoS ALM project.</p>
<h3 id="and-what-will-happen-to-your-other-scholarly-communication-activities">And what will happen to your other scholarly communication activities?</h3>
<p>There is no reason not to continue my other activities, including involvement in the Open Researcher &amp; Contributor ID (<a href="https://web.archive.org/web/20161226124822/http://about.orcid.org/">ORCID</a>) initiative, and using WordPress as a <a href="https://web.archive.org/web/20161226124822/http://blogs.plos.org/mfenner/tag/wordpress/">tool to write and publish manuscripts</a>.</p>
<h3 id="what-are-your-future-plans-for-this-blog">What are your future plans for this blog?</h3>
<p>I plan to continue this blog in a very similar format, and I will have more time for more in-depth articles. And of course I will indicate a conflict of interest when I write about Article Level Metrics.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Marketing for Scientists]]></title>
        <id>54c0rpv-bvb861v-7jp5h2f-2438c</id>
        <link href="https://blog.front-matter.io/mfenner/marketing-for-scientists"/>
        <updated>2012-03-27T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The April issue of <em>Nature Materials</em> contains three articles that discuss marketing strategies for scientists. The Editorial (“The scientific marketplace”) introduces the topic and explains why scientists should consider marketing their work....]]></summary>
        <content type="html"><![CDATA[<p>The <a href="https://web.archive.org/web/20160528080410/http://www.nature.com/nmat/journal/v11/n4/index.html">April issue</a> of <em>Nature Materials</em> contains three articles that discuss marketing strategies for scientists. The <a href="https://web.archive.org/web/20160528080410/http://dx.doi.org/10.1038/nmat3300">Editorial</a> (“The scientific marketplace”) introduces the topic and explains why scientists should consider marketing their work. The issue also contains an <a href="https://web.archive.org/web/20160528080410/http://dx.doi.org/10.1038/nmat3276">interview</a> (“The m word”) with astrophysicist Marc Kuchner who published a book titled <em><a href="https://web.archive.org/web/20160528080410/http://marketingforscientists.com/">Marketing for Scientists</a>.</em> Finally, there is a <a href="https://web.archive.org/web/20160528080410/http://dx.doi.org/10.1038/nmat3283">commentary</a> (“One-click science marketing”) by me discussing strategies and tools that scientists can use to promote their work. The commentary also includes links to pages by <a href="https://web.archive.org/web/20160528080410/http://www.scivee.tv/user/4">Phil Bourne</a> (SciVee), <a href="https://web.archive.org/web/20160528080410/http://www.mendeley.com/profiles/jonathan-eisen">Jonathan Eisen</a> (Mendeley), <a href="https://web.archive.org/web/20160528080410/http://rrresearch.fieldofscience.com/">Rosie Redfield</a> (blog), <a href="https://web.archive.org/web/20160528080410/http://johnhawks.net/weblog">John Hawks</a> (blog) and <a href="https://web.archive.org/web/20160528080410/http://cameronneylon.net/">Cameron Neylon</a> (personal webpage).</p>
<p>Scientists may feel uncomfortable about marketing their work, but we all are doing it already. We know that giving a presentation at a key meeting can be a boost for our career, and we know about the importance of maintaining an academic homepage listing our research interests and publications. And people reading this blog will understand that a science blog can be a powerful marketing tool.</p>
<p>Feel free to add your thoughts and suggestions in the comments.</p>
<h2 id="references">References</h2>
<p>Fenner, M. (2012). One-click science marketing. Nature Materials, 11(4), 261–263. <a href="https://doi.org/10.1038/nmat3283">https://doi.org/10.1038/nmat3283</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why I still like FriendFeed, why Twitter is important and other thoughts about Altmetrics]]></title>
        <id>1adqy17-3t8dwrh-g086stk-as00</id>
        <link href="https://blog.front-matter.io/mfenner/why-i-still-like-friendfeed-why-twitter-is-important-and-other-thoughts-about-altmetrics"/>
        <updated>2012-03-05T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Altmetrics – tools to assess the impact of scholarly works based on alternative online measures such as bookmarks, links, blog posts, etc. –have become a regular topic in this blog. The altmetrics manifesto was published in October 2010,...]]></summary>
        <content type="html"><![CDATA[<p><a href="https://web.archive.org/web/20160528083343/http://altmetrics.org/">Altmetrics</a> – tools to assess the impact of scholarly works based on alternative online measures such as bookmarks, links, blog posts, etc. –have become a regular topic in this blog. The <a href="https://web.archive.org/web/20160528083343/http://altmetrics.org/manifesto/">altmetrics manifesto</a> was published in October 2010, and in the last 18 months we have seen a <a href="https://web.archive.org/web/20160528083343/http://altmetrics.org/tools/">number of interesting new altmetrics services</a>, including the <a href="https://web.archive.org/web/20160528083343/http://blogs.plos.org/blog/tag/sciencecard/">ScienceCard</a> service that I started six months ago. ScienceCard has been a very interesting learning experience, because I not only had to write the software, but also think about my perspective on altmetrics. Some of my recent thoughts are listed below.</p>
<p><strong>FriendFeed still is a great model for a scholarly service</strong><br />
I have stopped using FriendFeed a few months ago in favor of Twitter, but I still very much like the design of the service. I think that the concept should also work very well for altmetrics, and I have therefore continued work on an <a href="https://web.archive.org/web/20160528083343/http://en.wikipedia.org/wiki/Activity_stream">activity stream</a> for ScienceCard. At <a href="https://web.archive.org/web/20160528083343/http://sciencecard.org/works">http://sciencecard.org/works</a> you find a listing of all recent scholarly works of your ScienceCard friends, and now you can like/comment/share them. ScienceCard friends are the people you follow on Twitter who also have a ScienceCard account and sharing is possible via Mendeley and CiteULike. Comments are still in the testing stage, and Twitter integration is also in the works. I also want to add more scholarly content including Slideshare presentations and blog posts, although the latter are really hard to do in an automated way.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/13th/activitystream.png" title="activitystream" class="kg-image" width="500" height="287" />
</figure>
<p>Altmetrics tools should not only present scholarly works and their metrics, but they should also allow users to interact with them via sharing, commenting, etc.</p>
<p><strong>Altmetrics is about search</strong><br />
Altmetrics is really about two related concepts: reputation and discovery. ScienceCard tries to summarize the metrics available about a particular researcher, although much more work needs to be done to show that these numbers correlate with reputation. The discovery aspect of altmetrics is at least as important, and this means that these alternative metrics should help provide better search results. Some bibliographic databases let you sort your search results by number of citations – the problem is of course that citations can have a delay of several years. Download counts, social bookmarks, Twitter links, etc. on the other hand can give information about the impact of a paper within days of publication. The search results can be improved further by personalizing them based on what the friends in your social network are publishing, bookmarking or discussing.</p>
<p><strong>Altmetrics is expensive</strong><br />
It is great to see so many altmetrics grassroots projects. Unfortunately it is resource-intensive and therefore costly to collect altmetrics, in particular if it is almost real-time numbers such as Twitter citations. I’m afraid that many people (including myself) will have a hard time providing this service in a sustainable way. There are two possible solutions: providing altmetrics as a commercial service, and providing altmetrics as a collaborative effort. Although I understand the reasoning behind commercial services, I would very much prefer the open and collaborative approach. Collaboration could mean that several people and/or organizations join forces and run an altmetrics service together, but it could also mean that we break altmetrics into smaller services connected via programming interfaces. A typical altmetrics service has at least these functions:</p>
<ul>
<li>an interface to add journal articles and other scholarly objects, whether it is manual input by users or via an API</li>
<li>a searchable database with metadata about scholarly objects</li>
<li>a background service that collects metrics from other sources</li>
<li>a public interface that displays metrics for particular scholarly objects and collections</li>
<li>an interface for visualization and analysis of aggregate numbers</li>
</ul>
<p>I don’t think that all five functions (and possibly more) necessarily have to be provided by the same service. I’m sure that there are enough people that are only interested in providing interesting ways to add scholarly objects, or in visualization and analysis. The background service is probably the most boring and resource-intensive part.</p>
<p><strong>I want second-order metrics</strong><br />
Second-order metrics means the metrics for the scholarly works citing a particular paper or for the person bookmarking or tweeting a scholarly work. This approach would add valuable information and is obviously similar to the PageRank algorithm for web links. Unfortunately this approach also creates a lot of extra work, as this means collecting the metrics (or at least some metrics) for all citing works. Again something that makes altmetrics expensive.</p>
<p><strong>Twitter is important</strong><br />
Many of the <a href="https://web.archive.org/web/20160528083343/http://blogs.lse.ac.uk/impactofsocialsciences/2012/02/09/more-tweets-more-citations/">recent altmetrics discussions</a> have really been about the role of Twitter in scholarly communication. A lot of people are excited about the potential of Twitter to help discover interesting scholarly works, and to allow this within days after publication. It is still too early to know for sure whether <a href="https://web.archive.org/web/20160528083343/http://blogs.plos.org/mfenner/2011/12/20/crowdometer-or-trying-to-understand-tweets-about-journal-papers/">highly tweeted papers will be cited more often later on</a>. Better Twitter integration is high on my to do list for ScienceCard.</p>
<p><strong>Yet another bibliographic database</strong><br />
The core function of altmetrics is to build a database with metadata about scholarly objects, including a variety of metrics. There are of course a large number of bibliographic databases already out there, so maybe existing databases can also be extended to include metrics. Ideally the existing database should not be restricted to particular kinds of scholarly works (e.g. journal articles) or disciplines, should be free to use and should allow reuse of the data with an appropriate license. There are several candidates that fit this description, includind <a href="https://web.archive.org/web/20160528083343/http://bibsoup.net/">BibSoup</a> run by the Open Knowledge Foundation and possibly also the <a href="https://web.archive.org/web/20160528083343/http://www.zotero.org/">Zotero</a> database allowing synchronization with the desktop reference manager. Both services focus on biobliographic collections uploaded by users, whereas my idea of an altmetrics database relies on disambiguated authors and scholarly works.</p>
<p><strong>There are too many altmetrics</strong><br />
It is great that altmetrics increases the variety of available metrics, but too many different metrics can be confusing to users. Although it is interesting that the number of citations for the same work vary widely between PubMed, Web of Science, Scopus, Microsoft Academic Search, Pubmed and Google Scholar, the typical user is probably only interested in one citation count. The same is true for usage metrics and number of social bookmarks. I think it would be helpful to consolidate the metrics about a scholarly work to maybe five numbers, including views/downloads, bookmarks, citations, comments, and tweets. This doesn’t mean that the other information shouldn’t be collected, just that the numbers will be consolidated for display.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Few Questions about Science Spam]]></title>
        <id>2sebqrh-92p91zr-g2nfcdm-6v3s6</id>
        <link href="https://blog.front-matter.io/mfenner/a-few-questions-about-science-spam"/>
        <updated>2012-02-24T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[This was another week with a fair amount of spam in my email inbox. We all receive email spam on a regular basis and most of us have probably also received science spam: invitations to scientific conferences about topics we are not working on,...]]></summary>
        <content type="html"><![CDATA[<p>This was another week with a fair amount of spam in my email inbox. We all receive email <a href="https://web.archive.org/web/20161026234722/http://www.youtube.com/watch?v=anwy2MPT5RE">spam</a> on a regular basis and most of us have probably also received science spam: invitations to scientific conferences about topics we are not working on, invitations to submit articles to journals not covering your field, and information about lab supplies we never had asked for. Although I’m of course aware that spam is now a fact of online life, I don’t quiet understand how this science spam works.</p>
<figure>
<img src="https://web.archive.org/web/20161026234722im_/http://farm1.staticflickr.com/206/519906069_de5953764a_m.jpg" class="kg-image" alt="Flickr photo by AJ Cann." /><figcaption aria-hidden="true"><a href="https://web.archive.org/web/20161026234722/http://www.flickr.com/photos/ajc1/519906069/in/photostream/">Flickr photo</a> by <a href="https://web.archive.org/web/20161026234722/http://scienceoftheinvisible.blogspot.com/">AJ Cann</a>.</figcaption>
</figure>
<p><strong>1. How do science spammers get my email address?</strong><br />
Most science spam is not really targeted towards my research interests. From this I conclude that these spammers automatically harvest email addresses of researchers, or they buy these lists. One potential source to harvest <a href="https://web.archive.org/web/20161026234722/http://blogs.plos.org/mfenner/2011/07/13/did-you-receive-spam-because-you-published-a-paper/">email addresses is PubMed</a> and other bibliographic databases, but I don’t know whether this is actually done.</p>
<p><strong>2. Is even a small percentage of researchers responding to this science spam?</strong><br />
Science spam is as uninteresting to me as any other spam, and I can’t really imagine a colleague submitting a manuscript to a journal marketed this way. But the idea behind spam is that there is a – admittedly very small – conversion rate.</p>
<p><strong>3. When will we start to see more social media science spam?</strong><br />
I think it is only a question of time before we see more science spam on Twitter, Google+ and Facebook – I already receive a small amount of spam through these channels.</p>
<p><strong>4. Is there anything an individual researcher can other than using good spam filters in his email program?</strong><br />
Would it for example help if we hide our email address as much as possible? Should we deny publishers the permission to post our email address in journal articles, and should places like PubMed hide them? Do publisher organizations such as OASPA (Open Access Scholarly Publishers Association) enforce their <strong><a href="https://web.archive.org/web/20161026234722/http://www.oaspa.org/conduct.php">Member Code of Conduct</a>,</strong> which clearly state that a<em>ny direct marketing activities publishers engage in shall be appropriate and unobtrusive.</em></p>
<p><strong>5. Will it get worse?</strong><br />
I know this answer. Yes. This blog has seen more than 60,000 spam comments already, most of the spam in my email is filtered out before I even see it, but I think it will get much worse – via email and other channels.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Crowdsourcing the analysis of scholarly tweets]]></title>
        <id>7he32cr-tpd94va-6j9yt9z-jstwq</id>
        <link href="https://blog.front-matter.io/mfenner/crowdsourcing-the-analysis-of-scholarly-tweets"/>
        <updated>2012-02-19T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[In December Euan Adie and I started the CrowdoMeter project, an analysis of the semantic content of tweets linking to scholarly papers. Because classifying almost 500 tweets is a lot of work, we turned this into a crowdsourcing project. We got help from 36 people,...]]></summary>
        <content type="html"><![CDATA[<p>In December Euan Adie and I <a href="https://web.archive.org/web/20161027000313/http://blogs.plos.org/mfenner/2011/12/20/crowdometer-or-trying-to-understand-tweets-about-journal-papers/">started the CrowdoMeter projec</a>t, an analysis of the semantic content of tweets linking to scholarly papers. Because classifying almost 500 tweets is a lot of work, we turned this into a crowdsourcing project. We got help from 36 people, who did 953 classifications, and we discussed the preliminary results (available <a href="https://web.archive.org/web/20161027000313/http://crowdometer.org/ratings">here</a>) at the <a href="https://web.archive.org/web/20161027000313/http://scienceonline2012.com/">ScienceOnline2012</a> conference.</p>
<p>There is no reason to stop the crowdsourcing here, so we have <a href="https://web.archive.org/web/20161027000313/http://hdl.handle.net/10779/01c28ce592291e7e294ed328208a5869">uploaded the result set</a> to <a href="https://web.archive.org/web/20161027000313/http://blogs.plos.org/mfenner/2012/02/16/figshare-interview-with-mark-hahnel/">figshare</a> and invite everybody to help us with the data analysis. For this purpose I have created a <a href="https://web.archive.org/web/20161027000313/https://github.com/mfenner/crowdometer">public repository</a> on Github which contains not only the source code for the CrowdoMeter website, but also all data – the same dataset made available on figshare. I have written a <a href="https://web.archive.org/web/20161027000313/https://github.com/mfenner/crowdometer/blob/crowdometer/public/assets/shares_author.R">first R script</a> that produces the following pie chart:</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/13th/authors.png" title="authors" class="kg-image" width="360" height="376" />
</figure>
<p>The figure doesn’t look all that exciting, but there is some calculation involved. There are different numbers of classifications per tweet and sometimes there is disagreement: true means at least 50% of classifications were true. It would be great if we find people willing to help with data analysis, preferably using <a href="https://web.archive.org/web/20161027000313/http://rstudio.org/">R</a> and contributing their scripts to the Github repository. Please send me an email or contact me <a href="https://web.archive.org/web/20161027000313/http://twitter.com/#!/mfenner">via Twitter</a> if you need write access to the repository.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reference Manager Papers now available for Windows]]></title>
        <id>4csjbxc-79w84ds-z49pe6m-e3kfy</id>
        <link href="https://blog.front-matter.io/mfenner/reference-manager-papers-now-available-for-windows"/>
        <updated>2012-02-17T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Regular readers of this blog know that I’m a big fan of the reference manager Papers – three years ago we even had a poetry contest when the iPhone version was first released. The strength of Papers has always been the very nice user interface,...]]></summary>
        <content type="html"><![CDATA[<p>Regular readers of this blog know that I’m a big fan of the reference manager <a href="https://web.archive.org/web/20161026235642/http://www.mekentosj.com/papers/">Papers</a> – three years ago we even had a <a href="https://web.archive.org/web/20161026235642/http://blogs.plos.org/mfenner/2009/02/19/papers_for_iphone_released_time_for_more_poetry">poetry contest</a> when the iPhone version was first released. The strength of Papers has always been the very nice user interface, and Papers 2 <a href="https://web.archive.org/web/20161026235642/http://blogs.plos.org/mfenner/2011/03/08/papers-2-the-reference-manager-made-with-love/">released last March</a> was a major update that added many more reference types, collaboration and a word processor plugin.</p>
<p>The first five years Papers has been only available for Mac and iPhone/iPad, but this week mekentosj.com released a pre-release version for Windows. Papers for Windows is a collaboration with <a href="https://web.archive.org/web/20161026235642/http://www.scimatic.com/node/386">Scimatic Software</a>, and a <a href="https://web.archive.org/web/20161026235642/http://pfw.mekentosj.com/kb/roadmap/mac-vs-windows-comparison-compatibility">comparison of the Mac and Windows</a> versions is here. A 30 day trial version is <a href="https://web.archive.org/web/20161026235642/http://www.mekentosj.com/papers/">available for download</a> from mekentosj.com.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Figshare: Interview with Mark Hahnel]]></title>
        <id>4djgy7c-6xw8s9r-beg3d5b-cmws1</id>
        <link href="https://blog.front-matter.io/mfenner/figshare-interview-with-mark-hahnel"/>
        <updated>2012-02-16T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[fig<strong>share</strong> <em>allows researchers to publish all of their research outputs in seconds in an easily citable, sharable and discoverable manner</em>. The service was started by Mark Hahnel last year while still a PhD student....]]></summary>
        <content type="html"><![CDATA[<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/11th/me1.png" title="me" class="kg-image" width="200" height="200" />
</figure>
<p>fig<strong>share</strong> <em>allows researchers to publish all of their research outputs in seconds in an easily citable, sharable and discoverable manner</em>. The service was started by Mark Hahnel last year while still a PhD student. Mark joined <a href="https://web.archive.org/web/20161023171633/http://www.digital-science.com/">Digital Science</a> to work on fig<strong>share</strong> in September and last month relaunched a much improved version of the service. I asked Mark a few questions about fig<strong>share</strong> below. I also uploaded two datasetst to fig<strong>share</strong> and made them publicly available:</p>
<ul>
<li><a href="https://web.archive.org/web/20161023171633/http://hdl.handle.net/10779/1c48a305c08c717fea3f6fe1687b3eff">CrowdoMeter Tweets</a> – all 467 tweets used in the CrowdoMeter project</li>
<li><a href="https://web.archive.org/web/20161023171633/http://hdl.handle.net/10779/01c28ce592291e7e294ed328208a5869">CrowdoMeter Classifications</a> – all 953 classifications from the CrowdoMeter project.</li>
</ul>
<h2 id="1-what-is-figshare">1. What is figshare?</h2>
<p><a href="https://web.archive.org/web/20161023171633/http://figshare.com/">figshare</a> is a repository where users can make all of their research outputs available in a citable, sharable and discoverable manner. figshare allows users to upload any file format so that figures, datasets and media can be disseminated in a way that the current scholarly publishing model does not allow.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/11th/screenshot.png" title="screenshot" class="kg-image" width="500" height="258" />
</figure>
<h2 id="2-what-is-the-right-content-for-figshare-and-what-should-rather-go-somewhere-else">2. What is the right content for figshare? And what should rather go somewhere else?</h2>
<p>Every experiment that is completed without error in the methods is valuable. Researchers investigate things because the question is interesting and supposedly unanswered. This means that other researchers will at some point ask that same question. Just because the hypothesis didn’t turn out to be true, doesn’t mean that this data should be thrown away.</p>
<p>With fig<strong>share</strong>, you can share your negative results or results that you were not planning to publish. You can make raw data available or supplementary material that journals cannot handle linked to from the published article. You can even make your papers and posters available and citable. People have uploaded whole chapters of their PhD thesis to share with the world and make sure that all of their hard work is not wasted.</p>
<p>Currently we are not focusing on handling massive datasets, whilst this is an aim for the future. These edge cases seem to be better handled by journals set up specifically for publishing these huge files, such as the excellent <span>GigaScience</span>.</p>
<h2 id="3-you-relaunched-figshare-in-january-what-has-changed-to-the-previous-version">3. You relaunched figshare in January. What has changed to the previous version?</h2>
<p>The old site was a proof of concept based on Mediawiki software. The new site has been completely built from scratch so that it is rapidly extendable in terms of features and scale. This means that we can adapt quickly and easily to the needs of researchers. As well as being much more intuitive, the biggest new feature for fig<strong>share</strong> is the private space.</p>
<p>Whilst we would like everyone to make all of their research objects available, we appreciate that some researchers would like to keep research private for many reasons. Because of this we set about giving users their own private repository to store their research objects. These objects can be uploaded in seconds and all objects are initially held in the private space, from where they can be made publicly available when the user decides. All research is easily tagged and categorisable, so that researchers can filter through their many files to find the one they were looking for in no time at all.</p>
<h2 id="4-how-important-is-the-user-interface-for-figshare-what-particular-features-do-you-like-the-most">4. How important is the user interface for figshare? What particular features do you like the most?</h2>
<p>The user interface is essential, researchers are busy enough as it it. They haven’t got the time to attend training course on how to use a repository. If this was the case with facebook, no one would use facebook. For this reason fig<strong>share</strong> is stupidly simple and allows users to get their research onto the site in seconds, even the PIs. At this point they can choose to make it publicly available and immediately citable, sharable and discoverable, or keep it private – securely hosted, taggable and accessible when they need it, from anywhere in the world.</p>
<p>This is something we are constantly aiming to improve and we not only welcome feedback, we are actively seeking the thoughts of researchers on how we can make this a seemless part of their research process.</p>
<h2 id="5-do-you-have-plans-for-a-desktop-version-of-figshare-e-g-to-watch-folders-for-new-figshare-content">5. Do you have plans for a desktop version of figshare, e.g. to watch folders for new figshare content?</h2>
<p>We do! By creating a desktop uploader, the process becomes even more intuitive for researchers, allowing them to make backups of their research in the cloud with no effort expenditure. Research data management is something I personally was terrible at. My research was organised into folders based on the month and the year I did that work. I lost days trying to find files that ‘I know I worked on sometime last summer’. Hopefully a desktop uploader will add to the current simple management system so that researchers like me have no excuse when it comes to losing (often expensive) results files.</p>
<h2 id="6-how-is-figshare-different-from-data-repositories">6. How is figshare different from data repositories?</h2>
<p>fig<strong>share</strong> is not limited as many repositories are. It caters for all research domains, no matter where your research is carried out. Institutional repositories are often limited to members of said institution. Also, the majority of institutional repositories are built specifically for papers. The Dryad repository has been doing some great work making the datasets behind published articles available under CC0. fig<strong>share</strong> is not limited by the normal constraints of publishing, data generated in the lab can be shared and made available to the world as a citable object the same day.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/11th/usermetrics.png" title="usermetrics" class="kg-image" width="275" height="115" />
</figure>
<p>fig<strong>share</strong> also gives the researchers the credit for their research. By adding metrics to the public uploads, and putting the cumulative metrics of a researcher’s uploads on their profile, users can see the true impact and reach of the hard work they put in.</p>
<h2 id="7-you-currently-use-handles-for-figshare-content-what-are-your-thoughts-on-persistent-identifiers-are-the-plans-to-use-dois">7. You currently use handles for figshare content. What are your thoughts on persistent identifiers? Are the plans to use DOIs?</h2>
<p>Persistent identifiers are essential for the long term availability of research outputs. One of the reasons I set up figshare was because I wanted to cite a video in my thesis. Research data on <a href="https://web.archive.org/web/20161023171633/http://figshare.com/blog/A%20YouTube%20%20for%20Scientists/11">YouTube</a> is not easily citable, by adding persitent identifiers and an organised citation structure, videos as well as any other file format can be easily cited. All citations can be exported to <a href="https://web.archive.org/web/20161023171633/http://mendeley.com/">Mendeley</a>, <a href="https://web.archive.org/web/20161023171633/http://www.endnote.com/">Endnote</a> and <a href="https://web.archive.org/web/20161023171633/http://www.refman.com/">RefMan</a>with one click. We use handles at the moment, but have noticed that researchers tend to be more familiar with DOI’s and so will be making the move over to them shortly. We’re currently working with <a href="https://web.archive.org/web/20161023171633/http://datacite.org/">DataCite</a> through the British Library to get this set up.</p>
<h2 id="8-how-does-figshare-guarantee-long-term-preservation-of-uploaded-data">8. How does figshare guarantee long-term preservation of uploaded data?</h2>
<p>The long term persistence of this research data is essential. The research is backed up locally as soon as it is uploaded. We are currently in talks with <a href="https://web.archive.org/web/20161023171633/http://www.portico.org/">Portico</a> to back up all data and further guarantee this long term persistence.</p>
<h2 id="9-what-are-your-responsibilities-at-figshare-what-did-you-do-before-figshare">9. What are your responsibilities at figshare? What did you do before Figshare?</h2>
<p>I’m basically responsible for making the platform as useful for researchers as possible. I also love going to talk at Universities and conferences to researchers directly, to hear their thoughts/opinions. As a former life science researcher (I finished my PhD in stem cell biology at Imperial College London in September), I understand that what makes sense in a rational world does not make sense in the scientific world. You just have to look at the established model of scientific research dissemination to understand that. fig<strong>share</strong> is useful whether you want to make all of your research outputs available or none. Science would just be a lot more efficient and dynamic if they did.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Personalized Journal]]></title>
        <id>54pgrdf-74h9sxs-9nq61g8-8pvy7</id>
        <link href="https://blog.front-matter.io/mfenner/the-personalized-journal"/>
        <updated>2012-02-10T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Earlier this week I wrote a guest post for the <strong>Impact of Social Sciences</strong> blog. In the post I talk about a recent paper correlating tweets and citations (also discussed on this blog). But the main argument I try to make is that tweets...]]></summary>
        <content type="html"><![CDATA[<p>Earlier this week I wrote a <a href="https://web.archive.org/web/20161026234447/http://blogs.lse.ac.uk/impactofsocialsciences/2012/02/09/more-tweets-more-citations/">guest post</a> for the <strong>Impact of Social Sciences</strong> blog. In the post I talk about a recent paper correlating tweets and citations (<a href="https://web.archive.org/web/20161026234447/http://blogs.plos.org/mfenner/2011/12/20/crowdometer-or-trying-to-understand-tweets-about-journal-papers/">also discussed on this blog</a>). But the main argument I try to make is that tweets are a powerful filter for personalized scholarly content:</p>
<blockquote>
<em><em>A few years from now the “personalized journal” will have replaced the traditional journal as the primary means to discover new scholarly papers with impact to our work.</em></em>
</blockquote>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zotero 3.0 Released]]></title>
        <id>4kh13c0-scp8g4s-yjz3kbk-dk1ya</id>
        <link href="https://blog.front-matter.io/mfenner/zotero-3-0-released"/>
        <updated>2012-01-31T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Zotero 3.0 was officially released today. The big change in version 3.0 of the reference manager is a standalone version that runs outside the Firefox browser. The first beta was released in August 2011....]]></summary>
        <content type="html"><![CDATA[<p>Zotero 3.0 was <a href="https://www.zotero.org/blog/zotero-3-0-is-here/">officially released</a> today. The big change in version 3.0 of the reference manager is a standalone version that runs outside the Firefox browser. The <a href="https://blogs.plos.org/mfenner/2011/08/23/zotero-3-0-beta-released-works-with-chrome-and-safari/">first beta</a> was released in August 2011.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/zotero.png" title="zotero" class="kg-image" width="301" height="67" />
</figure>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Say Hello to F1000 Research]]></title>
        <id>4pwxvg1-be98ftt-tknv1pk-v8p9n</id>
        <link href="https://blog.front-matter.io/mfenner/say-hello-to-f1000-research"/>
        <updated>2012-01-30T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Faculty 1000 today announced <strong>F1000 Research</strong>, <em>a new fully Open Access publishing program across biology and medicine, that will start publishing later this year</em>. The default open access license is CC-BY,...]]></summary>
        <content type="html"><![CDATA[<p>Faculty 1000 today <a href="https://web.archive.org/web/20161010225753/http://f1000research.com/2012/01/30/f1000-research-join-us-and-shape-the-future-of-scholarly-communication-2/">announced</a> <strong>F1000 Research</strong>, <em>a new fully Open Access publishing program across biology and medicine, that will start publishing later this year</em>. The default open access license is CC-BY, and CC0 for data.</p>
<p>Important features include:</p>
<ol>
<li>immediate publication</li>
<li>open, post-publication peer review</li>
<li>revisioning of work</li>
<li>raw data repository</li>
<li>article format and content is not predefined</li>
</ol>
<p>F1000 Research is a <em>publishing program</em> rather than a journal. The format for F1000 Research hasn’t been finalized, and F1000 welcomes comments at the <a href="https://web.archive.org/web/20161010225753/http://f1000research.com/">blog</a> or via the <a href="https://web.archive.org/web/20161010225753/https://twitter.com/#!/F1000Research">Twitter</a> account.</p>
<p>My main question about F1000 Research: is this a preprint archive similar to <a href="https://web.archive.org/web/20161010225753/http://arxiv.org/">ArXiv</a> and <a href="https://web.archive.org/web/20161010225753/http://precedings.nature.com/">Nature Precedings</a>? I’m a big fan of preprint archives, and I think that – contrary to what most people think – they <a href="https://web.archive.org/web/20161010225753/http://blogs.plos.org/mfenner/2010/10/16/in-which-i-suggest-a-preprint-archive-for-clinical-trials/">should work particularly well</a> for clinical trial data.</p>
<p>Update (1/30/12): <a href="https://web.archive.org/web/20161010225753/http://retractionwatch.wordpress.com/2012/01/30/an-arxiv-for-all-of-science-f1000-launches-new-immediate-publication-journal/">RetractionWatch</a> and <a href="https://web.archive.org/web/20161010225753/http://blogs.nature.com/news/2012/01/f1000-launches-fast-open-science-publishing-for-biology-and-medicine.html">Nature News</a> also cover this story.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Altmetrics – Where Do We Go From Here?]]></title>
        <id>332n0jb-aws8rf9-7jfnfgh-pdpjn</id>
        <link href="https://blog.front-matter.io/mfenner/altmetrics-where-do-we-go-from-here"/>
        <updated>2012-01-24T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The ScienceOnline2012 conference last week again was a wonderful experience. This was my third time in North Carolina, and I had many great conversations in the sessions, hallways – and bars. One of many highlights was a lunch meeting with fellow PLoS...]]></summary>
        <content type="html"><![CDATA[<p>The <a href="https://web.archive.org/web/20161027000038/http://scienceonline2012.com/">ScienceOnline2012</a> conference last week again was a wonderful experience. This was my third time in North Carolina, and I had many great conversations in the sessions, hallways – and bars. One of many highlights was a lunch meeting with fellow PLoS bloggers and staffers:</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/13th/6732330879_726113a81d.jpeg" class="kg-image" width="500" height="333" alt="Flickr photo by briandcrawford." /><figcaption aria-hidden="true">Flickr photo by <a href="https://web.archive.org/web/20161027000038/http://www.flickr.com/photos/brian_and_dawn/">briandcrawford</a>.</figcaption>
</figure>
<p>Together with <a href="https://web.archive.org/web/20161027000038/http://twitter.com/Stew">Euan Adie</a> I moderated a session on Friday:</p>
<h2 id="using-altmetrics-tools-to-track-the-scholarly-impact-of-your-research-">Using altmetrics tools to track the scholarly impact of your research.</h2>
<p>We started the session by asking several people in the audience to demonstrate their altmetrics tools: <a href="https://web.archive.org/web/20161027000038/http://altmetric.com/">altmetric.com</a> (Euan Adie), <a href="https://web.archive.org/web/20161027000038/http://readermeter.org/">ReaderMeter</a> (Dario Taraborelli), <a href="https://web.archive.org/web/20161027000038/http://total-impact.org/">Total Impact</a> (Jason Priem), <a href="https://web.archive.org/web/20161027000038/http://article-level-metrics.plos.org/">PLoS Article-Level Metrics</a> (Jennifer Lin), and <a href="https://web.archive.org/web/20161027000038/http://sciencecard.org/">ScienceCard</a> (me). We briefly showed our <a href="https://web.archive.org/web/20161027000038/http://crowdometer.org/">CrowdoMeter</a> project where we crowdsourced the meaning of tweets about scholarly papers.</p>
<p>The discussion covered many interesting aspects. I would like to focus on three of them.</p>
<h3 id="gaming">Gaming</h3>
<p>Altmetrics are still fairly new, and therefore not many people try to the cheat yet (but almost 1% of tweets in the <a href="https://web.archive.org/web/20161027000038/http://crowdometer.org/ratings">CrowdoMeter dataset</a> were already spam). I’m sure that this will change over time, and some metrics will be more prone to gaming than others. Gaming is a particular problem for usage stats, as it is difficult to impossible to verify them. Metrics provided by the producer of a research object (author or publisher) will be more susceptible to gaming than metrics from an independent source. Anonymous metrics (e.g. Mendeley readers) are more susceptible to gaming than metrics that list the source of every citation (e.g. CiteULike bookmarks).</p>
<h3 id="context">Context</h3>
<p>Altmetrics is currently at a stage where we collect various metrics, but don’t really know what these numbers mean. Does 1,000 downloads, 10 Mendeley bookmarks or 50 tweets mean that the paper has impact? And how do we compare altmetrics from different disciplines? Does it make a difference if a <a href="https://web.archive.org/web/20161027000038/http://www.mathunion.org/general/prizes/fields/details/">Fields Medalist</a> blogs about your paper (an example given in the session)? I think that the most interesting metrics are those that take into account who is citing the work, being it a regular citation, a social bookmark or a social media comment. This is of course how Google <a href="https://web.archive.org/web/20161027000038/http://de.wikipedia.org/wiki/PageRank">PageRank</a> works for webpages, and how <a href="https://web.archive.org/web/20161027000038/http://www.eigenfactor.org/">Eigenfactor</a> ranks scholarly journals. The context can be further improved by including the social networks of the person looking for information, e.g. how many people I follow on Twitter have bookmarked this particular paper.</p>
<h3 id="scope">Scope</h3>
<p>The tools discussed in the ScienceOnline session all have a particular approach for gathering altmetrics: altmetrics over a given time period (<em>altmetric.com</em>), altmetrics for content produced by a particular publisher (<em>PLoS ALM</em>), altmetrics for a given researcher (<em>ReaderMeter</em> and <em>ScienceCard</em>), and altmetrics produced for a given dataset on demand (<em>Total-Impact</em>). One obvious advantage of this approach is that it reduces the number of datasets needed to run the service. Unfortunately this is an arbitrary distinction, and it falls apart when you use a PageRank approach and also look at the metrics of citing sources.</p>
<h3 id="conclusions">Conclusions</h3>
<p>I think that altmetrics has made tremendous progress in 2011, but that there is a lot of work to do in 2012. I’m very interested in altmetrics based on PageRank, but also want to take social networks into consideration. This is of course how finding information on the web works – scholarly communication is just a subset. Unfortunately this approach requires a massive database of scholarly citations, something that is impossible to do for the small part-time altmetrics projects mentioned at the beginning of the post.</p>
<p>I’m less interested in usage metrics because they are so prone to gaming and will probably become problematic in a few years, and I want to focus on a reasonable number of altmetrics. I hope that there will never be a single “altmetric”, but I also don’t think that we need 20 different altmetrics for every scholarly work. A lot of interesting work ahead for my ScienceCard project.</p>
<p>I’m looking forward to the altmetrics session at ScienceOnline2013.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sloan Foundation funds Columbia and Mendeley to develop a Citation-Style Language Editor]]></title>
        <id>2cz0cs2-s7794mb-68qmjwq-61yz4</id>
        <link href="https://blog.front-matter.io/mfenner/sloan-foundation-funds-columbia-and-mendeley-to-develop-a-citation-style-language-editor"/>
        <updated>2012-01-19T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The Sloan Foundation has awarded a $125,000 grant to Columbia University and Mendeley to fund the development of a Citation-Style Language (CSL) editor. CSL is a XML-based language to format citations and bibliographies, and is used by the reference managers Zotero,...]]></summary>
        <content type="html"><![CDATA[<p>The Sloan Foundation has awarded a $125,000 grant to Columbia University and Mendeley to fund the development of a <a href="https://blog.martinfenner.org/posts/citation-style-language-an-interview-with-rintze-zelle-and-ian-mulvany/">Citation-Style Language</a> (CSL) editor. CSL is a XML-based language to format citations and bibliographies, and is used by the reference managers Zotero, Mendeley and Papers, and in many other places. Even though more than 1000 citation styles are available at <a href="https://web.archive.org/web/20161026235632/http://citationstyles.org/">CitationStyles.org</a> (and built into the tools using CSL), it has until now been fairly hard to create new citation styles – editing XML files is not everyones idea of having fun. The CSL editor will be made available as open source software.</p>
<p>Building a dedicated CSL editor will be a tremendous boost to the format. The press release is <a href="https://web.archive.org/web/20161026235632/http://www.prnewswire.com/news-releases/mendeley-teams-up-with-columbia-university-libraries-to-develop-a-citation-style-language-editor-through-125000-sloan-foundation-award-137669218.html">here</a>.</p>
<p>In related news, Mekentosj is <a href="https://web.archive.org/web/20161026235632/http://news.mekentosj.com/2012/01/a-serial-for-a-style/">giving away</a> a Papers 2 serial number for every citation style submitted to CitationStyles.org.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Altmetrics to go – mobile version of ScienceCard available]]></title>
        <id>mnvahwk-mx8w6as-zxtsanr-4mvr</id>
        <link href="https://blog.front-matter.io/mfenner/altmetrics-to-go-mobile-version-of-sciencecard-available"/>
        <updated>2012-01-08T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[ScienceCard is a web service that collects all scientific articles published by an author and displays their aggregate article-level metrics. Yesterday I added a mobile version to ScienceCard, simply browse to ScienceCard with your mobile phone or go to http://mobile.sciencecard.org....]]></summary>
        <content type="html"><![CDATA[<p><a href="https://web.archive.org/web/20161027000751/http://sciencecard.org/">ScienceCard</a> is a web service that collects all scientific articles published by an author and displays their aggregate article-level metrics. Yesterday I added a mobile version to ScienceCard, simply browse to ScienceCard with your mobile phone or go to <a href="https://web.archive.org/web/20161027000751/http://mobile.sciencecard.org/">http://mobile.sciencecard.org</a>. This is a first version based on my work with jQuery Mobile on <a href="https://web.archive.org/web/20161027000751/http://blogs.plos.org/mfenner/2012/01/04/crowdometer-goes-mobile/">CrowdoMeter</a>, but I think ScienceCard works really well on a small screen.My ScienceCard looks like <a href="https://web.archive.org/web/20161027000751/http://mobile.sciencecard.org/mfenner">this</a>. Further down the screen are the articles I have published and the collective metrics of these papers.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/13th/sciencecard2.png" title="sciencecard2" class="kg-image" width="320" height="480" />
</figure>
<p>Other ScienceCard users have much more impressive metrics, both in traditional citations, and in altmetrics such as PDF downloads of papers published with PLoS:</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/13th/sciencecard3.png" title="sciencecard3" class="kg-image" width="320" height="480" />
</figure>
<p>Metrics of an individual paper (<a href="https://web.archive.org/web/20161027000751/http://doi.org/dm9">http://doi.org/dm9</a>), with links to the journal and metrics (e.g. CiteULike bookmarks) look like this:</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/13th/sciencecard.png" title="sciencecard" class="kg-image" width="320" height="480" />
</figure>
<p>One idea behind ScienceCard is to make the collection and display of this kind of information as simple as possible. I hope this is another small step in the right direction.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CrowdoMeter goes Mobile]]></title>
        <id>2js41xk-h7w8ekt-vq7y6zp-1d1ts</id>
        <link href="https://blog.front-matter.io/mfenner/crowdometer-goes-mobile"/>
        <updated>2012-01-04T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Two weeks ago Euan Adie from altmetric.com and myself launched the website CrowdoMeter, a crowdsourcing project that tries to classify tweets about scholarly articles using the Citation Typing Ontology (CiTO)....]]></summary>
        <content type="html"><![CDATA[<p>Two weeks ago Euan Adie from <a href="https://web.archive.org/web/20170107002621/http://altmetric.com/">altmetric.com</a> and myself <a href="https://web.archive.org/web/20170107002621/http://blogs.plos.org/mfenner/2011/12/20/crowdometer-or-trying-to-understand-tweets-about-journal-papers/">launched</a> the website <a href="https://web.archive.org/web/20170107002621/http://crowdometer.org/">CrowdoMeter</a>, a crowdsourcing project that tries to classify tweets about scholarly articles using the Citation Typing Ontology (CiTO). Despite the holidays we have gotten off to a good start with currently 597 classifications by 56 different users, already covering 93% of the tweets we wanted to classify. We will discuss the results of this project at the <a href="https://web.archive.org/web/20170107002621/http://scienceonline2012.com/">ScienceOnline2012</a> conference in two weeks, but the most important findings can also be watched in real-time <a href="https://web.archive.org/web/20170107002621/http://crowdometer.org/ratings">here</a>.</p>
<p>To our knowledge this is the first time that CiTO has been used for the systematic classification of tweets, and the preliminary results seem to confirm what we and others had thought, i.e. that most tweets contain little semantic information and often only retweet the title of a paper. But not only do we now have numbers to confirm this, but we can also make some interesting additional observations. We find for example that only 1% of tweets disagree with the statements made in a paper – most Twitter users don’t seem to care telling others about papers they dislike or disagree with.</p>
<p>This project is far from over, ideally we want 3-5 classifications per tweet or an additional 1,000 classifications. It is a challenge to build a website so that enough people want to help with this project. One idea is to make the classifications as simple as possible, and to help further with this we today launched a mobile version of CrowdoMeter. Simply browse to <a href="https://web.archive.org/web/20170107002621/http://crowdometer.org/">http://crowdometer.org</a> with your iPhone or Android phone, sign in via your Twitter account, and you should see something similar to this:</p>
<figure>
<img src="https://web.archive.org/web/20170107002621im_/http://blogs.plos.org/mfenner/files/2012/01/crowdometer.png" title="crowdometer" class="kg-image" />
</figure>
<p>CrowdoMeter uses <a href="https://web.archive.org/web/20170107002621/http://jquerymobile.com/">jQuery Mobile</a>, a touch-optimized Javascript framework for smartphones and tablets. There are still some minor issues, but in general jQuery Mobile is a great tool to optimize a website for mobile users. Users are presented with 10 random tweets they haven’t classified yet, and see a simple classification screen when clicking (touching) a tweet:</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/13th/crowdometer1.png" title="crowdometer1" class="kg-image" width="320" height="480" />
</figure>
<p>It should not take longer than 15 minutes to classify 15-25 tweets, and this would be a tremendous help for the project.</p>
<p>The CrowdoMeter results page displayed the same information as in the desktop version of the website, the charts are produced by the <a href="https://web.archive.org/web/20170107002621/http://www.highcharts.com/">Highcharts</a> Javascript library (and again jQuery).</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/13th/crowdometer2.png" title="crowdometer2" class="kg-image" width="320" height="480" />
</figure>
<p>I’m interested to see how well the crowdsourcing for CrowdoMeter will work in the coming weeks. We hope to finish the data gathering part in January. If this project generates enough interest I could imagine doing another crowdsourcing project, maybe again using the Citation Typing Ontology, but this time for blog posts about scholarly papers.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing Annotum to WordPress Bloggers]]></title>
        <id>7pbv8yc-5wj8z8r-whqkw7x-sjb9q</id>
        <link href="https://blog.front-matter.io/mfenner/introducing-annotum-to-wordpress-bloggers"/>
        <updated>2011-12-08T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Version 1.0 of Annotum, the free WordPress theme for writing scholarly articles, was announced in late November. Back in June I wrote about the first public version of Annotum, but until now using Annotum was experimental....]]></summary>
        <content type="html"><![CDATA[<p>Version 1.0 of <a href="https://web.archive.org/web/20120421012848/http://annotum.org/">Annotum</a>, the free WordPress theme for writing scholarly articles, was <a href="https://web.archive.org/web/20120421012848/http://googleblog.blogspot.com/2011/11/more-spring-cleaning-out-of-season.html">announced</a> in late November. Back in June I <a href="https://web.archive.org/web/20120421012848/http://blogs.plos.org/mfenner/2011/06/30/annotum-publishing-with-wordpress-soon-coming-to-a-journal-near-you/">wrote about</a> the first public version of Annotum, but until now using Annotum was experimental. Annotum is available in the <a href="https://web.archive.org/web/20120421012848/http://wordpress.org/extend/themes/annotum-base">WordPress Themes Directory</a> at WordPress.org (and has been downloaded more than 9,000 times in the past three weeks), and is also available for users of WordPress.com. I have installed Annotum 1.0 <a href="https://web.archive.org/web/20120421012848/http://blogs.scienceonlinelondon.org/annotum/">here</a>, please drop me a note if you want an account.</p>
<p>But how is an Annotum blog different from a regular WordPress blog?</p>
<h3 id="annotum-is-a-wordpress-theme">Annotum is a WordPress Theme</h3>
<p>WordPress can be extended via Plugins and Themes. Whereas Plugins add functionality, themes usually change the look and feel of a WordPress site. Annotum is a theme that includes a lot of plugin functionality. This strategy makes it easier to get started with Annotum, as there is only one theme to install and not a set of plugins that has to work with a particular theme. Annotum can still be extended via <a href="https://web.archive.org/web/20120421012848/http://codex.wordpress.org/Child_Themes">child themes</a>, e.g. if you want a different look for your blog. And of course you can still use other scholarly plugins.</p>
<h3 id="annotum-uses-articles-and-not-posts">Annotum uses articles and not posts</h3>
<p>Annotum uses the custom post type <em><em>article</em></em> for scholarly content. This can be confusing in the beginning, but makes it easier to separate scholarly content from regular blog posts.</p>
<h3 id="the-annotum-editor-knows-about-document-structure"><strong><strong>The Annotum editor knows about document structure</strong></strong></h3>
<p>Scholarly articles have more structure than blog posts, and you can add this structure with the Annotum editor (see below). This structure is enforced, and the WordPress HTML editor is disabled. This makes it easier to create content that conforms to the NLM-DTD XML format.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/11th/annotum1.png" title="annotum1" class="kg-image" width="487" height="319" />
</figure>
<h3 id="annotum-can-import-and-export-in-the-nlm-dtd-format">Annotum can import and export in the NLM-DTD format</h3>
<p>More specifically, Annotum supports the <a href="https://web.archive.org/web/20120421012848/http://dtd.nlm.nih.gov/ncbi/kipling/">Kipling subset</a> of the NLM Journal Publishing DTD. NLM-DTD is the standard XML format for scholarly articles, and you can for example import published articles (with a license that allows reuse) into Annotum to get started. Unfortunately there aren’t that many NLM-DTD tools for authors (I haven’t tested the<a href="https://web.archive.org/web/20120421012848/http://blogs.nature.com/mfenner/2008/11/07/interview-with-pablo-fernicola">Microsoft Word Article Authoring Add-In</a> with Annotum), but this is a great way to get content written somewhere else into Annotum.</p>
<h3 id="annotum-knows-that-articles-can-have-multiple-authors">Annotum knows that articles can have multiple authors</h3>
<p>This is of course important for scholarly articles, and the <a href="https://web.archive.org/web/20120421012848/http://wordpress.org/extend/plugins/co-authors-plus/">Co-Authors Plus Plugin</a> also adds this feature to any WordPress blog.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/11th/annotum2.png" title="annotum2" class="kg-image" width="416" height="166" />
</figure>
<h3 id="annotum-knows-about-tables-figures-equations-and-references">Annotum knows about tables, figures, equations and references</h3>
<p>Scholarly articles have special formatting requirements for these content types, particularly references. Annotum adds visual editors for all of them. Annotum also has an editor for LaTeX equations.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/11th/annotum4-406x500.png" title="annotum4" class="kg-image" width="406" height="500" />
</figure>
<p>Annotum currently does not integrate with reference managers (Endnote, Mendeley, Zotero, etc.) or other WordPress tools that insert citations into blog posts (e.g <a href="https://web.archive.org/web/20120421012848/http://wordpress.org/extend/plugins/kcite/">kcite</a> or <a href="https://web.archive.org/web/20120421012848/http://wordpress.org/extend/plugins/link-to-link/">Link to Link</a>), but you can look up references via DOI and PubMed ID.</p>
<h3 id="annotum-knows-about-reviewers-and-editors">Annotum knows about reviewers and editors</h3>
<p>Annotum has  a built-in review system that knows about authors, reviewers and editors. Annotum allows comments only visible to authors and reviewers, and sends out notification emails. The <a href="https://web.archive.org/web/20120421012848/http://editflow.org/">Edit Flow</a> plugin provides similar functionality.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/11th/annotum3.png" title="annotum3" class="kg-image" width="298" height="206" />
</figure>
<h3 id="annotum-can-export-to-pdf">Annotum can export to PDF</h3>
<p>Annotum automatically creates a PDF version of your article (using the <a href="https://web.archive.org/web/20120421012848/http://code.google.com/p/dompdf/">dompdf</a> HTML to PDF converter).  Annotum also works with my <a href="https://web.archive.org/web/20120421012848/http://wordpress.org/extend/plugins/epub-export/">ePub Export plugin</a>, using <a href="https://web.archive.org/web/20120421012848/https://gist.github.com/1046450">this hack</a> to display the ePub link next to the PDF link:</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/11th/annotum5-500x148.png" title="annotum5" class="kg-image" width="500" height="148" />
</figure>
<h3 id="summary">Summary</h3>
<p>Annotum is a complete and free solution for starting a scholarly journal using WordPress. It has everything you need to write great scholarly content with WordPress and improves a regular WordPress blog in several important ways. Thanks to the support for the NLM-DTD format, Annotum can also be used as a writing tool for articles intended for submission somewhere else. One of the biggest strengths of WordPress is that it is really a writing <em><em>platform</em></em> that can be extended in many interesting ways. Maybe we will see WordPress plugins that enhance the equation editor or the reference management – or that connect Annotum to traditional journal submission systems.</p>
<p>Science bloggers will also be interested in many of the features of Annotum, but they don’t need the review workflow and might find that the support for NLM-DTD restricts them in how they can write content. Annotum is at version 1.0, and it is therefore not surprising that it still has a few rough edges. My biggest wish for a future version is better support for revisions and inline comments – Google Docs and other collaborative writing tools do a much better job highlighting the changes made in a text.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ScienceCard named Finalist in Mendeley/PLoS API Binary Battle]]></title>
        <id>bch4zcg-719sva9-k8kcz75-b7xq</id>
        <link href="https://blog.front-matter.io/mfenner/sciencecard-named-finalist-in-mendeley-plos-api-binary-battle"/>
        <updated>2011-11-20T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[I’m very proud to report that ScienceCard last week has been named finalist in the Mendeley/PLoS API Binary Battle. Not bad for a project that started only two months ago in a hackathon following the Science Online London conference and is done in my spare time....]]></summary>
        <content type="html"><![CDATA[<p>I’m very proud to report that <a href="https://web.archive.org/web/20120525033358/http://sciencecard.org/">ScienceCard</a> last week has been <a href="https://web.archive.org/web/20120525033358/http://dev.mendeley.com/api-binary-battle">named finalist</a> in the Mendeley/PLoS API Binary Battle. Not bad for a project that started only two months ago in a hackathon following the <a href="https://web.archive.org/web/20120525033358/http://www.scienceonlinelondon.org/">Science Online London</a> conference and is done in my spare time. The winners of the contest will be named on November 30, but I’m more than happy that the project has even gotten this far.</p>
<figure>
<img src="https://web.archive.org/web/20120525033358im_/http://blogs.plos.org/mfenner/files/2011/11/sciencecard.jpg" title="sciencecard" class="kg-image" />
</figure>
<p>The idea of ScienceCard is threefold:</p>
<ul>
<li>make it as easy as possible to create a profile page with your publications</li>
<li>Automatically collect citations and other metrics for these publications</li>
<li>make it as easy as possible to reuse this information, e.g. in your personal blog or reference manager</li>
</ul>
<p>Registration with ScienceCard is easy. You create an account by logging in via Twitter (using the OAuth 2 protocol for the technically inclined), and then add your account name from one or more author identifier services. I added Google Scholar today, the other options are Microsoft Academic Search, AuthorClaim and Mendeley (I’m working on Scopus Author ID). If you add an Microsoft Academic Search or AuthorClaim identifier, ScienceCard will import all your publications from these services. ScienceCard currently only understands publications with a DOI, other scholarly items could be added in the future. With Mendeley and Google Scholar you only link to these services, I’m still having trouble with the Mendeley OAuth 1 authentication and Google Scholar doesn’t have an API.</p>
<p>ScienceCard is importing all the relevant bibliographic information using the CrossRef service. And ScienceCard is creating a <a href="https://web.archive.org/web/20120525033358/http://blogs.plos.org/mfenner/2011/10/17/serving-shortdois/">shortDOI</a> for all papers. But more interestingly, ScienceCard is calculating <a href="https://web.archive.org/web/20120525033358/http://article-level-metrics.plos.org/">article-level metrics</a> for all publications, and composite numbers for authors. For this ScienceCard is using the PLoS Article-Level Metrics API code, but I have added more sources, including Mendeley, Microsoft Academic Search and altmetric.com (but here only the number of blog posts citing a paper). ScienceCard links to most sources so that you can see the actual citations there, only CrossRef and altmetric.com don’t offer that. A future update will show all citations directly in ScienceCard.</p>
<p>ScienceCard tries to display a nice profile page for each researcher, but also makes it easy to reuse the information somewhere else. Each author page is available in six different formats, simply add the extension after the (Twitter) username, e.g. <a href="https://web.archive.org/web/20120525033358/http://sciencecard.org/mfenner.json">http://sciencecard.org/mfenner.json</a></p>
<ul>
<li>HTML – for regular viewing</li>
<li>XML – for reuse by another computer</li>
<li>JSON – for reuse by another computer</li>
<li>CSV – comma-separated values for import into a spreadsheet</li>
<li>RIS – for reuse by a reference manager</li>
<li>BIB – BibTeX, for reuse by a reference manager</li>
</ul>
<p>JSON is the most interesting format, because it makes it very easy to connect two different services. If you for example want to use ScienceCard data on your personal WordPress blog you can simply download my <a href="https://web.archive.org/web/20120525033358/http://wordpress.org/extend/plugins/contact-info-options/">Contact Info Options WordPress</a> plugin, add your ScienceCard username in your WordPress settings and do some hacking of the author template. The process is unfortunately different for every WordPress theme, but you can see an example WordPress profile that uses the <a href="https://web.archive.org/web/20120525033358/http://annotum.wordpress.com/">Annotum</a> theme (and publishing platform) <a href="https://web.archive.org/web/20120525033358/http://blogs.scienceonlinelondon.org/annotum/author/mfenner/">here</a>:</p>
<figure>
<img src="https://web.archive.org/web/20120525033358im_/http://blogs.plos.org/mfenner/files/2011/11/annotum2.jpg" title="annotum2" class="kg-image" />
</figure>
<p>This WordPress author profile is generated on the fly using ScienceCard data, and will therefore automatically update.</p>
<p>The ScienceCard project is still at the beginning, and there are enough ideas to move this project forward in interesting ways. I’m particularly interested in improving the user interface and display of information, in adding datasets and other scholarly content, and in integrating with the ORCID unique author identifier service once the service launches in spring 2012. If you are interested to learn more about ScienceCard, please contact me via email or Twitter, or attend the altmetrics session at <a href="https://web.archive.org/web/20120525033358/http://scienceonline2012.com/">ScienceOnline2012</a>, which I will do together with Euan Adie from altmetric.com and <a href="https://web.archive.org/web/20120525033358/http://altmetric.com/interface/plos.html">PLoS Impact Explorer</a>, and Jason Priem from <a href="https://web.archive.org/web/20120525033358/http://total-impact.org/">Total Impact</a> (two other Mendeley/PLoS API Binary Battle finalists).</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why BibTeX, RIS and Endnote XML will soon be broken]]></title>
        <id>3a0p5mp-w8s80kt-443d0jn-y3w77</id>
        <link href="https://blog.front-matter.io/mfenner/why-bibtex-ris-and-endnote-xml-will-soon-be-broken"/>
        <updated>2011-11-08T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[BibTeX is one of the most popular file formats for bibliographies, and is therefore commonly used to transfer bibliographies from one reference manager to another, or to other applications that handle bibliographic references....]]></summary>
        <content type="html"><![CDATA[<p><a href="https://web.archive.org/web/20120610135415/http://www.bibtex.org/">BibTeX</a> is one of the most popular file formats for bibliographies, and is therefore commonly used to transfer bibliographies from one reference manager to another, or to other applications that handle bibliographic references. <a href="https://web.archive.org/web/20120610135415/http://www.refman.com/support/risformat_intro.asp">RIS</a> and <a href="https://web.archive.org/web/20120610135415/http://www.endnote.com/support/helpdocs/endnote.zip">Endnote XML</a> are probably the other two bibliographic file formats most commonly used. Most reference managers support all three formats, making it easy to move references around.</p>
<p>All three formats have been around for a while, BibTeX for example since 1985. Reference management has of course gone through many changes during this time, and an important change will happen next year: <a href="https://web.archive.org/web/20120610135415/http://blogs.plos.org/mfenner/tag/orcid/">unique identifiers for scholarly authors</a>. In 2012 the Open Researcher &amp; Contributor ID (<a href="https://web.archive.org/web/20120610135415/http://www.orcid.org/">ORCID</a>) initiative will start issuing unique identifiers for researchers, and researchers, universities, funding organizations, publishers and hopefully everyone else will start using them. But ORCID will only be successful if as many bibliographic tools as possible can handle ORCID identifiers, and if these tools can exchange these author identifiers.</p>
<p>None of the three bibliographic file formats mentioned above can handle unique author identifiers. If we take for example <a href="https://web.archive.org/web/20120610135415/http://sciencecard.org/articles/d6n">this paper</a> from ScienceCard, then the authors would look like this in BibTeX:</p>
<pre><code>author = {Kirstein, Janine and Dougan, David and Gerth, Ulf and Hecker, Michael and Turgay, Kürşad}</code></pre>
<p>And like this in RIS:</p>
<pre><code>AU  - Kirstein, Janine 
AU  - Dougan, David 
AU  - Gerth, Ulf 
AU  - Hecker, Michael 
AU  - Turgay, Kürşad</code></pre>
<p>I suggest we extend the BibTeX  format to understand author identifiers like this:</p>
<pre><code>orcid = {1274643, 8474644, 847412, 9183414, 7461414}</code></pre>
<p>And RIS:</p>
<pre><code>AI  - 1274643 
AI  - 8474644 
AI  - 847412 
AI  - 9183414 
AI  - 7461414</code></pre>
<p>This would look similar in Endnote XML. Will we see these changes to BibTeX, RIS and Endnote XML in 2012? I don’t know, but I very much hope so. Imagine what your Zotero, Mendeley or Endnote could do if the application knew about unique author identifiers, e.g. find all papers by a particular author, alert me when a particular author publishes something new, or organize your reference library by author.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wikis, Q&As and Cookbooks]]></title>
        <id>25tjk5m-nfd84ft-v7rykac-am7cm</id>
        <link href="https://blog.front-matter.io/mfenner/wikis-q-as-and-cookbooks"/>
        <updated>2011-10-31T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Last week I attended the Transforming Scholarly Communication workshop in Cambridge, Massachusetts. The main goal of the workshop was to come up with practical recommendations for the topics #resources #review #literature #media #recognition and...]]></summary>
        <content type="html"><![CDATA[<p>Last week I attended the <a href="https://web.archive.org/web/20120610120856/http://research.microsoft.com/en-US/events/escience2011-scholarly-communications/default.aspx">Transforming Scholarly Communication</a> workshop in Cambridge, Massachusetts. The main goal of the workshop was to come up with practical recommendations for the topics <a href="https://web.archive.org/web/20120610120856/http://msrworkshop.tumblr.com/tagged/resources">#resources</a> <a href="https://web.archive.org/web/20120610120856/http://msrworkshop.tumblr.com/tagged/review">#review</a> <a href="https://web.archive.org/web/20120610120856/http://msrworkshop.tumblr.com/tagged/literature">#literature</a> <a href="https://web.archive.org/web/20120610120856/http://msrworkshop.tumblr.com/tagged/media">#media</a> <a href="https://web.archive.org/web/20120610120856/http://msrworkshop.tumblr.com/tagged/recognition">#recognition</a> and <a href="https://web.archive.org/web/20120610120856/http://msrworkshop.tumblr.com/tagged/platforms">#platforms</a>:</p>
<blockquote>
This workshop strives to be different in one important way—rather than focusing on utopian visions for their own sake, we will focus on the existing and newly developed technologies designed to enhance scholarship and scholarly communication in order to determine factors for their success and their potential.
</blockquote>
<figure>
<img src="https://web.archive.org/web/20120610120856im_/http://farm3.static.flickr.com/2543/4027551747_121a2d46b6.jpg" class="kg-image" alt="Ketchup Recipe by Chiot’s Run on Flickr" /><figcaption aria-hidden="true"><a href="https://web.archive.org/web/20120610120856/http://www.flickr.com/photos/chiotsrun/4027551747/">Ketchup Recipe</a> by Chiot’s Run on Flickr</figcaption>
</figure>
<p>The workshop started with product demos (18 demos in a little over three hours!), we then spent two half days working in smaller groups on one of the six topics above. I was assigned to the <strong><strong>#recognition</strong></strong> group, and we started with a very open discussion about recognition, reputation, altmetrics and related topics. The work was much more focussed on the second day, where we produced a <a href="https://web.archive.org/web/20120610120856/http://msrworkshop.tumblr.com/tagged/recognition">draft text</a> with practical recommendations.</p>
<p>We all reported back to the whole group in the plenary discussion the second day, but unfortunately didn’t have much time (and energy) left to discuss the reports of the other topic groups in more detail. It was suggested that we create a Wikipedia page summarizing our reports, so that researchers interested in new scholarly communication tools would have a starting point.</p>
<p>I personally felt that most reports (including our own) had too much detail information – they are great resources of information, but are probably overwhelming for the average researcher. I therefore thought that a good alternative to a wiki page would be a Question &amp; Answer site similar to the hugely popular <a href="https://web.archive.org/web/20120610120856/http://stackoverflow.com/">Stack Overflow</a> and <a href="https://web.archive.org/web/20120610120856/http://www.quora.com/">Quora</a>. Even simpler would be a book in the <a href="https://web.archive.org/web/20120610120856/http://shop.oreilly.com/category/series/cookbooks.do">Cookbook</a> or <a href="https://web.archive.org/web/20120610120856/http://pragprog.com/book/fr_arr/advanced-rails-recipes">Recipes</a> format so popular with programmers. The book would be a collection of solutions for many of the small problems we face in scholarly communication today. We probably need hundreds of recipes, to start this off I today wrote a <a href="https://web.archive.org/web/20120610120856/http://blogs.plos.org/mfenner/scholarly-communication-cookbook-create-a-researcher-profile-page/">recipe for creating a researcher profile page</a> (very much based on our topic report).</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Book Review: Reinventing Discovery by Michael Nielsen]]></title>
        <id>2ptn56g-7af95x9-yrwjnq0-fjpmw</id>
        <link href="https://blog.front-matter.io/mfenner/book-review-reinventing-discovery-by-michael-nielsen"/>
        <updated>2011-10-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Reinventing Discovery, the book by Michael Nielsen we all have been waiting for, has finally been published on Friday. Today I flew to Boston for the Microsoft Research eScience Workshop: Transforming Scholarly Communication,...]]></summary>
        <content type="html"><![CDATA[<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/Reinventing_Discovery.jpeg" class="kg-image" width="257" height="388" />
</figure>
<p>Reinventing Discovery, the book by Michael Nielsen we all have been waiting for, has finally been published on Friday. Today I flew to Boston for the Microsoft Research eScience Workshop: Transforming Scholarly Communication, and reading the book on the plane was the perfect preparation for the workshop.</p>
<p>Michael says in the book:</p>
<blockquote>
<em><em>I wrote this book with the goal of lightning an almighty fire under the scientific community. … We have an opportunity to change the way knowledge is constructed.</em></em>
</blockquote>
<p>You can <a href="https://press.princeton.edu/chapters/s9517.pdf">download the chapter 1 as PDF</a>, and you can also watch the videos of two of his recent presentations: <a href="https://www.youtube.com/watch?v=DnWocYKqvhw">TEDx Waterloo</a> in March and <a href="https://web.archive.org/web/20120525040623/http://river-valley.tv/keynote-solo2011/">Science Online London</a> in September.</p>
<p>The book uses examples from science and related disciplines – e.g. programming or chess – to examine both the opportunities and challenges of doing Open Science. The book is good reading, because Michael is aware of the many challenges that we face before science can be done differently. One example for this is the peer-reviewed journal article as the main currency to evaluate researchers. Until scientists are also rewarded for producing or curating data, programming scientific software, etc., we will not be able to start a new era of networked science.</p>
<p>Highly recommended reading.</p>
<p>Timo Hannay has also written a <a href="https://doi.org/10.1038/nphys2109">review of the book</a> for Nature Physics, and I’m sure we will soon see many more.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data citation how-to guide released by Digital Curation Centre]]></title>
        <id>2s5q5e3-q54908s-r48mpwq-bw7vw</id>
        <link href="https://blog.front-matter.io/mfenner/data-citation-how-to-guide-released-by-digital-curation-centre"/>
        <updated>2011-10-19T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Earlier this week <strong><strong>Alex Ball</strong></strong> and <strong><strong>Monica Duke</strong></strong> from the Digital Curation Centre released the how-to guide Cite Datasets and Link to Publications. The guide is highly recommended reading for everyone interested in data citation....]]></summary>
        <content type="html"><![CDATA[<p>Earlier this week <strong><strong>Alex Ball</strong></strong> and <strong><strong>Monica Duke</strong></strong> from the Digital Curation Centre released the how-to guide <a href="https://web.archive.org/web/20120525040549/http://www.dcc.ac.uk/resources/how-guides/cite-datasets">Cite Datasets and Link to Publications</a>. The guide is highly recommended reading for everyone interested in data citation. The guide was released under a <a href="https://web.archive.org/web/20120525040549/http://creativecommons.org/licenses/by/2.5/scotland/">Creative Commons Attribution license</a>, allowing me to post the summary for researchers below:</p>
<h2 id="summary-for-researchers">Summary for researchers</h2>
<p><em>If you have generated/collected data to be used as evidence in an academic publication, you should deposit them with a suitable data archive or repository as soon as you are able. If they do not provide you with a persistent identifier or URL for your data, encourage them to do so.</em></p>
<p><em><em>When citing a dataset in a paper, use the citation style required by the editor/publisher. If no form is suggested for datasets, take a standard data citation style (e.g. DataCite’s) and adapt it to match the style for textual publications.</em></em></p>
<p><em><em>Give dataset identifiers in the form of a URL wherever possible, unless otherwise directed.</em></em></p>
<p><em><em>Include data citations alongside those for textual publications. Some reference management packages now include support for datasets, which should make this easier.</em></em></p>
<p><em><em>Cite datasets at the finest-grained level available that meets your need. If that is not fine enough, provide details of the subset of data you are using at the point in the text where you make the citation.</em></em></p>
<p><em><em>If a dataset exists in several versions, be sure to cite the exact version you used.</em></em></p>
<p><em><em>When you publish a paper that cites a dataset, notify the repository that holds the dataset, so it can add a link from that dataset to your paper.</em></em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Serving shortDOIs]]></title>
        <id>e3ytamp-qt8wt9r-dk4kk1j-rrkh</id>
        <link href="https://blog.front-matter.io/mfenner/serving-shortdois"/>
        <updated>2011-10-17T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The shortDOI service was launched by the International DOI Foundation (IDF) in May 2010. The service creates short versions of the often long DOIs, e.g. <strong><strong>10/dvq</strong></strong> instead of <strong><strong>10.1093/hmg/ddp202</strong></strong>...]]></summary>
        <content type="html"><![CDATA[<p>The shortDOI service was launched by the International DOI Foundation (IDF) in May 2010. The service creates short versions of the often long DOIs, e.g. <strong><strong>10/dvq</strong></strong> instead of <strong><strong>10.1093/hmg/ddp202</strong></strong> – written as URL this would be <a href="https://web.archive.org/web/20120531234808/http://doi.org/dvq">http://doi.org/dvq</a> instead of <a href="https://doi.org/10.1093/hmg/ddp202">http://doi.org/10.1093/hmg/ddp202</a>. shortDOIs started as a <a href="https://web.archive.org/web/20120531234808/http://labs.crossref.org/site/toi_dois.html">CrossRef Labs project</a> in 2009 and were originally named TOI DOI – TOI stands for tiny object identifier. For a good introduction I recommend <a href="https://web.archive.org/web/20120531234808/http://go-to-hellman.blogspot.com/2010/05/long-handle-on-shortened-digital-object.html">Eric Hellman’s blog post</a> written in May 2010 when the shortDOI service was launched.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/13th/2093290327_0803f40005.jpeg" class="kg-image" width="320" height="480" />
</figure>
<p>shortDOIs provide exactly the same service as normal DOIs, i.e. they redirect the user to the digital resource. They are convenient, particularly for email and Twitter where space is limited. Links to journal articles created by URL shorteners such as bit.ly or goo.gl look similar, but require two redirects (first to dx.doi.org, then to the journal article). But URL shorteners provide additional services to users, e.g. customized links and usage statistics.</p>
<p>As far as I can tell shortDOIs have not become popular since the service started more than a year ago. One important reason is certainly that publishers are not really using them for their journal articles. I don’t think many users will go through the <a href="https://web.archive.org/web/20120531234808/http://shortdoi.org/">extra steps creating a shortDOI</a> just to use a DOI with Twitter – the Twitter URL shortener t.co will do this automatically for them. Michael Kuhn has created a <a href="https://web.archive.org/web/20120531234808/http://blog.mckuhn.de/2011/02/bookmarklet-for-shortdoiorg.html">bookmarklet</a> that makes it a little bit easier to create shortDOIs. If we want shortDOIs to ever become popular, then we should ask journal publishers, bibliographic databases, reference managers and other places that currently use DOIs to enable them.</p>
<p>Starting yesterday <a href="https://web.archive.org/web/20120531234808/http://sciencecard.org/">ScienceCard</a> is using shortDOIs instead of DOIs, and is also using the shortDOI to link to individual articles on ScienceCard, e.g. <a href="https://web.archive.org/web/20120531234808/http://sciencecard.org/articles/dvq">http://sciencecard.org/articles/dvq</a>. Yesterday I’ve created about 750 shortDOIs for ScienceCard, and  this probably already makes ScienceCard one of the larger shortDOI users. I will decide in the coming months whether shortDOIs have improved the ScienceCard service. A nice feature would for example be the announcement of newly published papers via Twitter.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two reviews of new reference manager ReadCube]]></title>
        <id>3ck3a6m-dy29ms9-1hjgejm-5sqsr</id>
        <link href="https://blog.front-matter.io/mfenner/two-reviews-of-new-reference-manager-readcube"/>
        <updated>2011-10-11T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Today, Digital Science announced an investment in startup Labtiva. And Labtiva released a “community preview” of their reference manager ReadCube. The community preview is a free download for Windows and Mac,...]]></summary>
        <content type="html"><![CDATA[<figure>
<img src="https://web.archive.org/web/20120610121628im_/http://blogs.plos.org/mfenner/files/2011/10/readcube3.jpeg" title="readcube3" class="kg-image" />
</figure>
<p>Today, <a href="https://web.archive.org/web/20120610121628/http://www.digital-science.com/">Digital Science</a> announced an investment in startup <a href="https://web.archive.org/web/20120610121628/http://www.labtiva.com/">Labtiva</a>. And Labtiva released a “community preview” of their reference manager <a href="https://web.archive.org/web/20120610121628/http://www.readcube.com/">ReadCube</a>. The community preview is a free download for Windows and Mac, and this is the summary of my first impressions.</p>
<p>You could write two different reviews about ReadCube. The first version would mention the really slick interface, and the fun you have using the program. ReadCube is doing a good job importing the PDFs on your hard drive and adding bibliographic information to them. In addition to PDF import you can also search PubMed and Google Scholar. ReadCube helps you find related papers by listing the references and citing papers of the paper you imported. ReaderCube also gives recommendations based on the papers in your library.</p>
<figure>
<img src="https://web.archive.org/web/20120610121628im_/http://blogs.plos.org/mfenner/files/2011/10/readcube1-500x276.jpg" title="readcube" class="kg-image" />
</figure>
<p>ReadCube includes an integrated PDF viewer that also allows text highlighting and note taking.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/11th/readcube-500x275.jpeg" title="readcube" class="kg-image" width="500" height="275" />
</figure>
<p>The community preview is free, so please download ReadCube to find out whether you like it.</p>
<p>The second review would take a different approach. It would ask what problem ReadCube tries to solve, and why researchers should start using ReadCube rather than the tools they already use, maybe for many years. ReadCube is a reference manager with a particular focus on organizing the PDFs of scholarly papers. There a number of programs out there that can do the same. <a href="https://web.archive.org/web/20120610121628/http://www.mekentosj.com/papers/">Papers</a> for Macintosh for example is a very similar program, but the first version of it has been released four years ago. Even more traditional reference managers now include inline PDF readers with annotation support, including the <a href="https://web.archive.org/web/20120610121628/http://www.endnote.com/enx5info.asp">latest version of Endnote</a>. Mendeley and Zotero are two other alternatives, and they are both free and available for Windows, Mac and Linux. It’s difficult to see what is unique in ReadCube, and on the other hand some important features are missing (e.g. no bookmarklet to import from other sources, no reference type other than journal articles, no group sharing feature, no integration with Microsoft Word). And ReadCube is based on Adobe Air, a cross-platform development environment that you either love or hate.</p>
<p>It will be interesting to watch what direction ReadCube development will take. They started a few years later than their competitors, and it will be a lot of work to catch up. I wish the Labtiva team good luck.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The trouble with DOIs]]></title>
        <id>b6vfg9y-879v2bb-6vj5q3y-wk88</id>
        <link href="https://blog.front-matter.io/mfenner/the-trouble-with-dois"/>
        <updated>2011-10-09T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[ScienceCard is a new service that I started last month with the simple idea to automatically track all journal articles of a given author, and to collect the article-level metrics (citations, bookmarks, etc.) for these papers....]]></summary>
        <content type="html"><![CDATA[<p><a href="https://web.archive.org/web/20120610120821/http://sciencecard.org/">ScienceCard</a> is a new service that I started last month with the simple idea to automatically track all journal articles of a given author, and to collect the article-level metrics (citations, bookmarks, etc.) for these papers. ScienceCard requires unique identifiers for articles and authors to work. Unique identifiers for authors is a difficult topic <a href="https://web.archive.org/web/20120610120821/http://blogs.plos.org/mfenner/tag/orcid/">regularly discussed</a> in this blog. But I thought that using digital object identifiers (DOI) for journal articles would be easy. The system managed by CrossRef was started 10 years ago, and almost all journal publishers now use DOIs – there were 49,350,542 registered CrossRef DOI links as of today.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/13th/4632436148_7795a0127c.jpeg" class="kg-image" width="500" height="375" alt="Number buttons by fragmented on Flickr" /><figcaption aria-hidden="true"><a href="https://web.archive.org/web/20120610120821/http://www.flickr.com/photos/fragmented/4632436148/">Number buttons</a> by fragmented on Flickr</figcaption>
</figure>
<p>The first problem I encountered is that many bibliographic databases don’t fully support DOIs. Most of them store DOIs, but not all of them allow queries using DOIs, and very few services allow linking to them using DOIs. In the end I had to store various other article identifiers in ScienceCard (currently PubMed ID, PubMed Central ID, Microsoft Academic Search ID, Mendeley UUID, Scopus ID). One side effect of this proliferation of identifiers is that (in very rare cases) DOIs are not unique in these bibliographic services. And it makes it more complicated than necessary to build tools based on DOIs. The members of CrossRef are publishers, the other service providers (whether public or private) seem to be reluctant to fully support a service where they have no direct influence.</p>
<p>The second problem with DOIs is that they are often not web-friendly. DOIs are really permanent URLs, and CrossRef has recently changed the <a href="https://web.archive.org/web/20120610120821/http://www.crossref.org/help/Content/02_Getting_started/Displaying_DOIs_in_print_and_online.htm">display guidelines for DOIs</a> to reflect this. Instead of <strong><strong><a href="https://web.archive.org/web/20120610120821/http://dx.doi.org/10.1371/journal.pcbi.0010057">doi: 10.1371/journal.pcbi.0010057</a></strong></strong> we are supposed to show DOIs as <strong><strong><a href="https://web.archive.org/web/20120610120821/http://dx.doi.org/10.1371/journal.pcbi.0010057">http://doi.org/10.1371/journal.pcbi.0010057</a></strong></strong>. The problem is that DOIs can contain characters such as “+”, “(“, “.” or “/” that need to be escaped when used as URLs. Some ScienceCard examples include the following:</p>
<ol>
<li><a href="https://web.archive.org/web/20120610120821/http://dx.doi.org/10.1016/S0959-8049(05)80357-0">http://doi.org/10.1016/S0959-8049(05)80357-0</a></li>
<li><a href="https://web.archive.org/web/20120610120821/http://dx.doi.org/10.1093/bioinformatics/12.4.357">http://doi.org/10.1093/bioinformatics/12.4.357</a></li>
<li><a href="https://web.archive.org/web/20120610120821/http://dx.doi.org/10.1021/bi980175+">http://doi.org/10.1021/bi980175+</a></li>
<li><a href="https://web.archive.org/web/20120610120821/http://dx.doi.org/10.1642/0004-8038(2002)119%5B0088:SSCPEO%5D2.0.CO;2">http://doi.org/10.1642/0004-8038(2002)119[0088:SSCPEO]2.0.CO;2</a></li>
</ol>
<p>These special characters can create problems when DOIs are used in software programs. ScienceCard for example wants to create links to articles in the format <strong><strong>http://sciencecard.org/10.1642/0004-8038(2002)119[0088:SSCPEO]2.0.CO;2.xml</strong></strong>, but this function is currently broken.</p>
<p>One possible solution are <a href="https://web.archive.org/web/20120610120821/http://shortdoi.org/">shortDOIs</a>. Article (3) would for example become <a href="https://web.archive.org/web/20120610120821/http://doi.org/dcp">http://doi.org/dcp</a>, whereas article (4) is rejected as invalid DOI. I would love to use shortDOIs in ScienceCard and other places (e.g. Twitter), but haven’t found an API yet that automatically returns shortDOIs for DOIs.</p>
<p><a href="https://web.archive.org/web/20120610120821/http://blogs.plos.org/mfenner/2011/03/26/direct-links-to-figures-and-tables-using-component-dois/">Component DOIs</a> directly link to a figure or table of a paper. This is an underused, but very useful feature, and is for example provided by the <em><em>PLoS</em></em> journals. Unfortunately component DOIs can confuse bibliographic databases and make it more difficult to track all the links to a given article. I had to write a little routine to detect component DOIs imported into ScienceCard.</p>
<p>Articles are sometimes updated or corrected, and many publishers will use a different DOI for the updated article. This is a problem when you want to track all references to this particular article. <a href="https://doi.org/10.1371/journal.pcbi.0020121">http://doi.org/10.1371/journal.pcbi.0020121</a> and <a href="https://web.archive.org/web/20120610120821/http://dx.doi.org/10.1371/journal.pcbi.0020181">http://doi.org/10.1371/journal.pcbi.0020181</a> are for example DOIs for the same <em><em>PLoS Computational Biology</em></em> article (the latter is the corrected version). <em><em>Nature Precedings</em></em> uses a format that is easier to understand for computers - <a href="https://web.archive.org/web/20120610120821/http://dx.doi.org/10.1038/npre.2011.4479.3">http://doi.org/10.1038/npre.2011.4479.3</a> is for example a link to the third version of this particular manuscript. <a href="https://web.archive.org/web/20120610120821/http://www.crossref.org/crossmark/index.html">CrossMark</a> is a new CrossRef service that will make it easier to track the different versions of a manuscript, including retractions.</p>
<p>ScienceCard should of course not be limited to journal articles. I’m also interested in other scholarly content, e.g. preprints from <strong><strong>ArXiV</strong></strong> or research datasets from <strong><strong>DataCite</strong></strong>. But I want to first solve the problems with DOIs for journal articles, before I tackle the much bigger problems with uniquely identifying and tracking other scholarly contributions. Science blog posts are a good example. It would be wonderful to track them in ScienceCard, but I don’t see how we can do that before we have a system in place that assigns unique and persistent identifiers to blog posts. For this and other reasons I really want unique identifiers for science blog posts, and we should also think about using DOIs for this purpose.</p>
<p><strong><strong>Update October 9</strong></strong>: A ScienceCard <a href="https://web.archive.org/web/20120610120821/http://sciencecard.org/articles/804">example</a> of multiple identifiers for the same paper:</p>
<ul>
<li>DOI: 10.1007/s10654-011-9572-7</li>
<li>PubMed ID: 21461943</li>
<li>PubMed Central ID: 3115050</li>
<li>Microsoft Academic Search: 48849734</li>
<li>Mendeley: 5b0023f0-609e-11e0-8f54-0024e8453de6</li>
<li>Mendeley URL: http://www.mendeley.com/research/informativeness-indices-blood-pressure-obesity-serum-lipids-relation-ischaemic-heart-disease-mortality-huntii-study/</li>
<li>Scopus: 79959714408</li>
</ul>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Announcing ScienceCard]]></title>
        <id>51gj3gp-x6m9vda-pp35jah-bdrv6</id>
        <link href="https://blog.front-matter.io/mfenner/announcing-sciencecard"/>
        <updated>2011-09-28T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Metrics for scholarly works are used for evaluation and discovery. The Journal Impact Factor is widely used, but is not the best tool to look at the metrics of an individual article. In the past few years we finally started to have the technology to do article-level metrics (citations,...]]></summary>
        <content type="html"><![CDATA[<p>Metrics for scholarly works are used for evaluation and discovery. The Journal Impact Factor is widely used, but is not the best tool to look at the metrics of an individual article. In the past few years we finally started to have the technology to do article-level metrics (citations, downloads, etc.) and PLoS has <a href="https://web.archive.org/web/20120610140041/http://article-level-metrics.plos.org/">pushed this concept</a> since at least 2009. I first heard about the PLoS Article-Level Metrics project in a presentation given by Pete Binfield in July 2009, and a month later I <a href="https://web.archive.org/web/20120610140041/http://blogs.plos.org/mfenner/2009/08/15/plos_one_interview_with_peter_binfield/">interviewed him</a> about this project.</p>
<p>Article-level metrics are a major step forward from journal-level metrics, but I always wanted to extend this concept to authors. Three events in the past few weeks worked nicely together and made me start an author-level metrics project myself: <a href="https://web.archive.org/web/20120610140041/http://dev.mendeley.com/api-binary-battle">programming contest by Mendeley and PLoS</a> with a September 30 deadline, a hackfest following the <a href="https://web.archive.org/web/20120610140041/http://www.scienceonlinelondon.org/">Science Online London Conference</a> in early September, and two conferences (in <a href="https://web.archive.org/web/20120610140041/http://irisc-workshop.org/irisc2011-helsinki/">Helsinki</a> and <a href="https://web.archive.org/web/20120610140041/http://www.orcid.org/civicrm/event/info?id=2&amp;reset=1">Geneva</a>) in mid-September discussing the value of unique author identifiers for researchers.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/13th/sciencecard-500x345.jpeg" title="sciencecard" class="kg-image" width="500" height="345" />
</figure>
<p>I’ve taken the <a href="https://web.archive.org/web/20120610140041/http://code.google.com/p/alt-metrics/">Open Source Article-Level Metrics API Server</a> from PLoS and added a few important features: metrics by author, additional metrics from Mendeley and Microsoft Academic Search, and a web-based interface that less authors register via their Twitter account. The result is <a href="https://web.archive.org/web/20120610140041/http://sciencecard.org/">ScienceCard</a>, a website I launched this weekend and today entered for the Mendeley/PLoS Binary Battle contest. ScienceCard currently uses <a href="https://web.archive.org/web/20120610140041/http://code.google.com/p/alt-metrics/">Microsoft Academic Search</a> to find all papers of a particular author, the next version will allow users to retrieve info about their personal papers from Mendeley.</p>
<p>Working on ScienceCard has already taught me a lot about the problems we face when doing metrics for scholarly works. Most problems are social and not technical. Digital object identifiers (DOIs) for example have been the standard identifier for journal articles for ten years, but many places (including PubMed, Microsoft Academic Search and Mendeley) want users to use their own identifiers for journal articles. This makes it unnecessarily difficult to find articles and collect metrics. Identifiers for authors are even more complicated. ScienceCard will of course use <a href="https://web.archive.org/web/20120610140041/http://orcid.org/">ORCID</a> identifiers when they become available in 2012, and I hope that I can make the transition to ORCID easier for ScienceCard users.</p>
<p>Let me know what you think.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hacking the PLoS Article-Level Metrics API Server]]></title>
        <id>5c3xbkf-y459c0s-qsad6na-6ab6w</id>
        <link href="https://blog.front-matter.io/mfenner/hacking-the-plos-article-level-metrics-api-server"/>
        <updated>2011-09-05T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The Science Online London 2011 Conference was a great event that took place last Friday and Saturday. I was able to celebrate the first <em><em>PLoS Blogs</em></em> anniversary together with community manager <strong><strong>Brian Mossop</strong></strong>,...]]></summary>
        <content type="html"><![CDATA[<p>The <a href="https://web.archive.org/web/20120610134729/http://www.scienceonlinelondon.org/">Science Online London 2011 Conference</a> was a great event that took place last Friday and Saturday. I was able to celebrate the first <em><em>PLoS Blogs</em></em> anniversary together with community manager <strong><strong>Brian Mossop</strong></strong>, but a detailed conference post will follow later. The blog posts covering the event are <a href="https://web.archive.org/web/20120610134729/http://scienceonlinelondon.wikidot.com/coverage">here</a>, and the list is growing by the hour.</p>
<p>On Sunday a few brave souls met at the <a href="https://web.archive.org/web/20120610134729/http://www.mendeley.com/">Mendeley</a> offices for the Science Online London hackathon to spend some time on a cool programming project. We were greeted by <strong><strong>Victor Henning</strong></strong> and <strong><strong>Jason Hoyt</strong></strong> and quickly came up with a few good ideas. Jason finished work on <a href="https://web.archive.org/web/20120610134729/http://twendeley.ologeez.org/">Twendeley</a>, a cool Twitter/Mendeley mashup that looks for papers mentioned in your Twitter stream and finds relevant articles in Mendeley. I wanted to do some work on the PLoS <a href="https://web.archive.org/web/20120610134729/http://code.google.com/p/alt-metrics/">Article-Level Metrics API</a>.</p>
<p><a href="https://web.archive.org/web/20120610134729/http://article-level-metrics.plos.org/">Article-Level Metrics</a> looks at the usage and reach of an individual article instead of using the <strong><strong>Journal Impact Factor</strong></strong> as a proxy for the impact of a paper. The PLoS Article-Level Metrics API provides access to some interesting numbers: not only citations, HTML pageviews and PDF downloads, but also social metrics such as number of bookmarks in CiteULike or number of readers in Mendeley.</p>
<p>But for the hackathon I was not interested in building a tool that talks to the Article-Metrics API. On Sunday morning I had discovered that the PLoS Article-Level Metrics server software is not only available as Open Source software, but is built with <strong><strong>Ruby on Rails</strong></strong>, a programming framework that I’m familiar with. So I thought it would be fun to start improving the software by adding metrics on the author level. And by building your own Article-Level Metrics server, you are not limited to papers published in PLoS journals, or to the kinds of metrics provided by PLoS. Wouldn’t it for example be nice to also include download counts for <a href="https://web.archive.org/web/20120610134729/http://datadryad.org/">Dryad</a> datasets?</p>
<figure>
<img src="https://web.archive.org/web/20120610134729im_/http://blogs.plos.org/mfenner/files/2011/09/alm2-500x355.png" title="alm2" class="kg-image" />
</figure>
<p>With the help of <strong><strong>Kristi Holmes</strong></strong> and <strong><strong>Cameron Neylon</strong></strong> we quickly got the API server up and running, added a few papers and retrieved citation counts from PubMed Central and the number of bookmarks from CiteULike (see above). A few hours later we could add authors, and today I was able to automatically import the first papers by author.</p>
<figure>
<img src="https://web.archive.org/web/20120610134729im_/http://blogs.plos.org/mfenner/files/2011/09/alm-500x274.png" title="alm" class="kg-image" />
</figure>
<p>For author-level metrics we of course need a widely used unique author identifier, and an API we can talk to. Otherwise author disambiguation quickly becomes frustrating. Until <a href="https://web.archive.org/web/20120610134729/http://www.orcid.org/">ORCID</a> comes along next year, the <a href="https://web.archive.org/web/20120610134729/http://social.microsoft.com/Forums/en-US/mas/thread/9a23b2d6-6599-4853-acf5-c1692a64365e">Microsoft Academic Search API</a> looks like one of the best ways to retrieve DOIs for papers published by a particular author.</p>
<p>The code for this project is available at <a href="https://web.archive.org/web/20120610134729/http://github.com/mfenner/plos-alt-metrics">Github</a>. This obviously needs a lot more work, but it shouldn’t take more than a few months to have the author part working properly and to find a host for a server. And because this is an API server based on the PLoS code, all tools that interact with the PLoS Article-Metrics server can use this system immediately.</p>
<p>This service should make it a little bit easier to build a <a href="https://web.archive.org/web/20120610134729/http://www.phdcomics.com/comics.php?f=1417">professional trading card for scientists</a>, or a dashboard of the <a href="https://web.archive.org/web/20120610134729/http://total-impact.org/">total impact</a> of your research.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Microattribution]]></title>
        <id>75cftck-kfx8kr8-jpyqw83-q2ppd</id>
        <link href="https://blog.front-matter.io/mfenner/on-microattribution"/>
        <updated>2011-08-28T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[At the Science Online London Conference later this week I will moderate a session on <strong><strong>microattribution</strong></strong>, together with Mike Peel, Bora Zivkovic and Scott Edmunds. I thought that microattribution would be an established concept,...]]></summary>
        <content type="html"><![CDATA[<p>At the <a href="https://web.archive.org/web/20120610133410/http://www.scienceonlinelondon.org/">Science Online London Conference</a> later this week I will moderate a session on <strong><strong>microattribution</strong></strong>, together with <a href="https://web.archive.org/web/20120610133410/http://www.mikepeel.net/blog/">Mike Peel</a>, <a href="https://web.archive.org/web/20120610133410/http://blogs.scientificamerican.com/a-blog-around-the-clock/">Bora Zivkovic</a> and <a href="https://web.archive.org/web/20120610133410/http://blogs.openaccesscentral.com/blogs/gigablog/">Scott Edmunds</a>. I thought that microattribution would be an established concept, so I was surprised to find so little information about it. Wikipedia <a href="https://web.archive.org/web/20120610133410/http://en.wikipedia.org/w/index.php?title=Microattribution&amp;action=edit&amp;redlink=1">doesn’t know</a> about microattribution. A search in PubMed retrieves only two hits (the 2011 <em><em>Nature Genetics</em></em> paper mentioned below and an editorial in the same issue). One of the first mentions of the term appears to be an August 2007 Editorial in <em><em>Nature Genetics</em></em> (<strong><strong><a href="https://web.archive.org/web/20120610133410/http://dx.doi.org/10.1038/ng0807-931">Compete, collaborate, compel</a></strong></strong>), but at the time the term also included what we today would call data citation. It therefore might be a good idea to try a definition:</p>
<blockquote>
Microattribution ascribes a small scholarly contribution to a particular author.
</blockquote>
<p>Important for this definition is the <strong><strong>small scholarly contribution</strong></strong>, which until now we have been unable to appropriately associate with its contributor. This scholarly contribution is too small to merit a scholarly paper or publication as dataset. And in most cases we have not bothered to provide unique identifiers for the scholarly contribution and/or the author.</p>
<p>A very good example for where microattribution can be valuable is the description of genetic variation. A March 2011 <em><em>Nature Genetics</em></em> paper (<strong><strong><a href="https://web.archive.org/web/20120610133410/http://dx.doi.org/10.1038/ng.785">Systematic documentation and analysis of human genetic variation in hemoglobinopathies using the microattribution approach</a>)</strong></strong> concluded that microattribution <em><em>demonstrably increased the reporting of human variants, leading to a comprehensive online resource for systematically describing human genetic variation</em></em>. One example described in the paper is the genetic variation in the promoter of the KLF1 erythroid transcription factor, explaining differences in the level of fetal hemoglobin (HbF). Most of these variants were never published in a paper (the blue squares in the figure).</p>
<figure>
<img src="https://web.archive.org/web/20120610133410im_/http://blogs.plos.org/mfenner/files/2011/08/ng.785-F3-500x305.jpg" title="ng.785-F3" class="kg-image" alt="Figure 3 from Giardine B et al. Nature Genetics 2011. https://doi.org/10.1038/ng.785." /><figcaption aria-hidden="true"><strong><strong>Figure 3</strong></strong> <em><em>from Giardine B et al. Nature Genetics 2011.</em> https://<em><a href="https://doi.org/10.1038/ng.785">doi.org/10.1038/ng.785</a>.</em></em></figcaption>
</figure>
<p>For the first time we now seem to have both the technology and willingness to enable microattributions on a large scale. There will be ample time for discussion in the microattribution session on Friday, but I’m personally most interested in the next practical steps to move microattribution forward. My background is unique author identifiers (ORCID) and my co-moderators bring in their experience with Wikipedia (Mike Peel), science blog aggregation (Bora Zivkovic) and crowdsourcing of sequencing efforts (Scott Edmunds). Some of the questions that I would like to address in the session are:</p>
<ol>
<li>Should all microattribution information be collected in one, several or many places?</li>
<li>Do we need one, several or many identifier schemes for contributions and authors?</li>
<li>What level of detail should we allow for microattributions?</li>
<li>Should microattributions use persistent identifiers?</li>
<li>Can and should we keep our scholarly contributions separate from other contributions?</li>
</ol>
<figure>
<img src="https://web.archive.org/web/20120610133410im_/http://blogs.plos.org/mfenner/files/2011/08/Scientific-Attribution-v.2.jpg" title="Scientific Attribution v.2" class="kg-image" />
</figure>
<p><em><em>My simplified view of the scholarly record, updated</em></em></p>
<p>I <a href="https://web.archive.org/web/20120610133410/http://blogs.plos.org/mfenner/scientific-attribution-principles/">have said before</a> that I think that attribution should be separated from evaluation, and I think is also true for microattribution. The main reason is that evaluation is something we still know very little about, and a scholarly record available to everybody will make it much easier to make progress here. I don’t think anybody knows yet what distinguishes a good from a bad microattribution, or whether it is possible to compare the scientific impact of 10 or 100 microattributions to one scholarly paper.</p>
<p><em><em>Disclaimer: I sit on the Board of Directors of the Open Researcher &amp; Contributor ID (ORCID) initiative which aims to help solve this and related problems.</em></em></p>
<h2 id="references">References</h2>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zotero 3.0 Beta released, works with Chrome and Safari]]></title>
        <id>34tc576-wqj80d8-01mj39a-tzd3z</id>
        <link href="https://blog.front-matter.io/mfenner/zotero-3-0-beta-released-works-with-chrome-and-safari"/>
        <updated>2011-08-23T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The first beta of the reference manager Zotero 3.0 was released yesterday. The big news is that Zotero 3.0 no longer only runs within the Firefox browser, but is now also available as a standalone version similar to other reference managers.Zotero...]]></summary>
        <content type="html"><![CDATA[<p>The first beta of the reference manager Zotero 3.0 was <a href="https://web.archive.org/web/20120525045252/http://www.zotero.org/blog/announcing-zotero-3-0-beta-release/">released</a> yesterday. The big news is that Zotero 3.0 no longer only runs within the Firefox browser, but is now also available as a standalone version similar to other reference managers.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/zotero-500x328.png" title="zotero" class="kg-image" width="500" height="328" />
</figure>
<p>Zotero Connectors integrate with the Chrome and Safari browser. They also allow saving directly to your Zotero library at zotero.org.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/zotero-500x328%20(1).png" title="zotero_safari" class="kg-image" width="500" height="328" />
</figure>
<p>Zotero does not support Internet Explorer. Users of this blog are not representative of all internet users, and Zotero 3.0 would offer browser support for more than 80% of them.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/YNgML-web-browsers-used-by-gobbledygook-readers-august-2011.png" title="analytics" class="kg-image" width="1240" height="1096" />
</figure>
<p>Zotero 3.0 is beta software and you should expect bugs. One small problem I discovered is that the German translation is incomplete, e.g. in the menus. But overall Zotero looks and feels very similar to the familiar Zotero 2.1 – just as a standalone version. One new feature I like is duplicate detection, an experimental feature of earlier versions.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personal names around the world]]></title>
        <id>648eccq-wne9dqt-tqh7vpq-9emtk</id>
        <link href="https://blog.front-matter.io/mfenner/personal-names-around-the-world"/>
        <updated>2011-08-14T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Yesterday I discovered (via a tweet by Owen Stephens) a very interesting document Personal names around the world that discusses the following question:How do people’s names differ around the world, and what are the implications of those differences on the design of forms,...]]></summary>
        <content type="html"><![CDATA[<p>Yesterday I discovered (via a tweet by <a href="https://web.archive.org/web/20120610133741/https://twitter.com/#!/ostephens">Owen Stephens</a>) a very interesting document <a href="https://web.archive.org/web/20120610133741/http://www.w3.org/International/questions/qa-personal-names">Personal names around the world</a> that discusses the following question:</p>
<blockquote>
How do people’s names differ around the world, and what are the implications of those differences on the design of forms, databases, ontologies, etc. for the Web?
</blockquote>
<p>The document was written by <a href="https://web.archive.org/web/20120610133741/http://rishida.net/">Richard Ishida</a>, Internationalization Activity Lead at the <a href="https://web.archive.org/web/20120610133741/http://www.w3.org/International/">W3C</a> (World Wide Web Consortium). The document was published on July 26, and Richard was seeking comments until August 7 before finalizing the document.</p>
<p>The document is a good summary how names for people differ around the world, e.g. multiple family names (Spain, Latin America), no family name (Iceland), different ordering of names (China, Korea, Japan), non-latin characters in names (many countries), and other issues.</p>
<p>The second part of the document makes a few suggestions of how these variations in names could be handled on the web. The text should be required reading for anybody who is designing databases that handle international names – and there are a lot of them. A form that asks for <strong><strong>first name</strong></strong>, <strong><strong>middle initial</strong></strong> and <strong><strong>last name</strong></strong> will just not be appropriate for a lot of people.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/12th/noname.jpeg" title="noname" class="kg-image" width="500" height="500" alt="Flickr photo by Giant Gingko." /><figcaption aria-hidden="true"><em><em>Flickr photo by <a href="https://web.archive.org/web/20120610133741/http://www.flickr.com/photos/giantginkgo/37740313/">Giant Gingko</a>.</em></em></figcaption>
</figure>
<p>I am lucky that my German name doesn’t contain any German umlauts (ä, ö, ü) or the letter ß, so I haven’t had any bad (or funny) experiences with my name. But I know that it can be a big problem for a lot of people. The result is that people end up having several spellings for their name, or write their name in ways that were not intended, e.g. a hyphen between the two family names in Spain. The name is something very personal, and I think the least we can do is to allow people to use their name appropriately.</p>
<p>Science is probably no better or worse in this regard than other domains. If we are lucky, we find a journal that prints our name correctly, or have a database that understands that ü should be sorted as ue. But for the most part, this seems to be an unresolved issue. And I don’t want everybody to start using a first name and last name in that order and only with ASCII or latin characters. That would be boring. So please start thinking about this issue when you design systems that use personal names, and use the W3C document by Richard Ishida as a starting point.</p>
<p><em><em>Disclaimer: I sit on the Board of Directors of the Open Researcher &amp; Contributor ID (ORCID) initiative which aims to help solve this and related problems.</em></em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Book Review: Visualize This by Nathan Yau]]></title>
        <id>6wbf7nr-7t290kr-h6r00qt-74k03</id>
        <link href="https://blog.front-matter.io/mfenner/book-review-visualize-this-by-nathan-yau"/>
        <updated>2011-08-13T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[In July Wiley published the book <strong><strong>Visualize This – The Flowing Data Guide to Design, Visualization and Statistics</strong></strong>. The book is written by <em><em>Nathan Yau</em></em>, and he is of course also behind the popular FlowingData blog about the same topic....]]></summary>
        <content type="html"><![CDATA[<p>In July Wiley published the book <strong><strong><a href="https://web.archive.org/web/20120611111645/http://book.flowingdata.com/">Visualize This – The Flowing Data Guide to Design, Visualization and Statistics</a></strong></strong>. The book is written by <em><em>Nathan Yau</em></em>, and he is of course also behind the popular <a href="https://web.archive.org/web/20120611111645/http://flowingdata.com/">FlowingData</a> blog about the same topic. This is a short review of the book.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/visualize-this-drop.png" title="Visualize This" class="kg-image" width="304" height="383" />
</figure>
<p>Please keep in mind that I’m no expert in data visualization. The book is written for people like me, an expert in the topic will probably look at the book differently.</p>
<p>And the book is intended as an introduction to important concepts. It is probably the wrong book if you are interested in using a particular tool – e.g. Adobe Illustrator, R or Flash – for Data Visualization.</p>
<p>The examples in the book are about topics similar to those used in the FlowingData blog, most of them are about data journalism. This is not a book about visualization of scientific experiments.</p>
<p>The first chapter of the book talks about how to tell stories with data. This is a very important chapter, as learning how to tell a good story is more important than knowing all the details on how to use a tool for visualization.</p>
<p>Nathan continues with a chapter on how to find and format data for data visualization. We are then introduced to a number of important visualization tools, and their particular strengths. Nathan uses a large number of tools in the book, but seems to particularly like Python for formatting data, R for for calculation and visualization, and Adobe Illustrator for perfecting the result for publication. This is one of the take-home messages of the book for me: instead of perfecting the use of one particular tool, learn to decide what works best for a particular problem. R and Illustrator are a good start for most problems.</p>
<p>The next few chapters look at visualizations in different contexts: patterns over time, proportions, relationships, differences and spatial relationships. All chapters are written with examples that you can follow along, and with enough basic information that you can start solving similar problems on your own. Below is a chart I tried myself after reading the book. It looks at the number of member organizations of the non-profit ORCID over time (more info <a href="https://web.archive.org/web/20120611111645/http://www.orcid.org/news/250-participating-organizations-have-joined-orcid">here</a>).</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/orcid-500x400.png" title="orcid" class="kg-image" width="500" height="400" />
</figure>
<p>I can highly recommend <strong><strong>Visualize This</strong></strong> to anybody interested in learning about data visualization. I found only two things I didn’t like. I wish Nathan had spent more time explaining how a data graph can be improved visually. In many examples he shows how small changes in color or font size can make a big difference in presentation, but I would prefer to see a more systematic approach to this important topic. I wouldn’t mind if he would drop the chapter on <a href="https://en.wikipedia.org/wiki/Chernoff_face">Chernoff faces</a> instead – fun stuff, but not really important in an introductory text.</p>
<p>My other problem is with the publisher. I read the book as ePub on my iPad. It is nice to have web links you can click in the text, but it would have been even nicer if the ePub had ben prepared with a little bit more care. Most images in the text (there are many) are too small – in an ePub you expect figures to enlarge and show more detail when you click on them. And figure headings were regularly orphaned (on a different page than the figure itself). ePub is an evolving standard, but a little bit of consideration for typography and layout would go a long way.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mendeley 1.0 released Today]]></title>
        <id>4gmbn8n-4re9m69-22vkhdv-dz9ac</id>
        <link href="https://blog.front-matter.io/mfenner/mendeley-1-0-released-today"/>
        <updated>2011-07-27T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Version 1.0 of the reference manager Mendeley was released today. In good Web 2.0 tradition it took three years from the first Beta release to the first “finished” product. I interviewed co-founder Victor Henning back in September 2008,...]]></summary>
        <content type="html"><![CDATA[<p>Version 1.0 of the reference manager Mendeley was <a href="https://web.archive.org/web/20120525041606/http://www.prnewswire.com/news-releases/mendeley-rips-beta-label-off-award-winning-research-collaboration-software-with-v10-release-126244103.html">released</a> today. In good Web 2.0 tradition it took three years from the first Beta release to the first “finished” product. I <a href="https://web.archive.org/web/20120525041606/http://blogs.plos.org/mfenner/2008/09/05/interview_with_victor_henning_from_mendeley/">interviewed</a> co-founder Victor Henning back in September 2008, and both the software and the company have gone a long way since then. Congratulations.</p>
<figure>
<img src="https://web.archive.org/web/20120525041606im_/http://farm4.static.flickr.com/3212/2822759480_c330822084_o.jpg" class="kg-image" alt="Mendeley in 2008. Picture taken from my interview with Victor." /><figcaption aria-hidden="true"><em><em>Mendeley in 2008. Picture taken from <a href="https://web.archive.org/web/20120525041606/http://blogs.plos.org/mfenner/2008/09/05/interview_with_victor_henning_from_mendeley/">my interview</a> with Victor.</em></em></figcaption>
</figure>
<p>Mendeley has changed reference management in many ways. Most importantly it has added another choice for users, and their constant push for new features has benefitted everybody, including the competition.</p>
<p>The Mendeley software has been downloaded one million times and 100 million papers have been uploaded to the service. Mendeley is no longer the new kid on the blog. With their popularity and size also comes an increased responsibility for the community. On top of my wish list: a decent <a href="https://web.archive.org/web/20120525041606/http://blogs.plos.org/mfenner/2010/09/24/citation-style-language-an-interview-with-rintze-zelle-and-ian-mulvany/">Citation Style</a> Editor that would also benefit <strong><strong>Zotero</strong></strong> and <strong><strong>Papers 2</strong></strong> users (<a href="https://web.archive.org/web/20120525041606/http://www.mendeley.com/blog/research-tutorials/howto-edit-citation-styles-for-use-in-mendeley/">hacking the XML files</a> is not an option for most people).</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Google Scholar Citations, Researcher Profiles, and why we need an Open Bibliography]]></title>
        <id>bpy6pdq-my947sb-6vd72k9-6ac1</id>
        <link href="https://blog.front-matter.io/mfenner/google-scholar-citations-researcher-profiles-and-why-we-need-an-open-bibliography"/>
        <updated>2011-07-27T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Last week Google Scholar announced a new feature on the Google Scholar Blog: <strong><strong>Google Scholar Citations</strong></strong>. The stated purpose of this tool is to allow researchers to calculate their citation metrics, e.g....]]></summary>
        <content type="html"><![CDATA[<p>Last week Google Scholar <a href="https://web.archive.org/web/20120610140256/http://googlescholar.blogspot.com/2011/07/google-scholar-citations.html">announced a new feature</a> on the Google Scholar Blog: <strong><strong>Google Scholar Citations</strong></strong>. The stated purpose of this tool is to allow researchers to calculate their citation metrics, e.g. their <a href="https://web.archive.org/web/20120610140256/http://blogs.plos.org/mfenner/2007/08/17/do_you_know_your_hirsch_number/">Hirsch index</a> (H-index).</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/12th/SafariSnapshot001-500x306.jpeg" title="SafariSnapshot001" class="kg-image" width="500" height="306" />
</figure>
<p>This is an interesting new service, that not only helps with calculating citation metrics, but also shows you who is citing your papers – a great discovery tool. Signup to Google Scholar Citations is currently limited, but I was able to create a profile <a href="https://web.archive.org/web/20120610140256/http://scholar.google.com/citations?user=N05QljgAAAAJ&amp;hl=en">here</a>.</p>
<p>The problem? We have this service already. <strong><strong>Scopus</strong></strong>, <strong><strong>Researcher ID</strong></strong> and others have provided this information for some time, and Google Scholar Citations looks very much like a response to the recently launched Microsoft Academic Search:</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/12th/SafariSnapshot002-500x349.jpeg" title="SafariSnapshot002" class="kg-image" width="500" height="349" />
</figure>
<p>Will we soon see a similar offering from <strong><strong>Mendeley</strong></strong> or <strong><strong>ResearchGate</strong></strong>? There is of course nothing wrong with competition, and there is no reason why we can’t have more than one place that provides researcher profiles. But as I have <a href="https://web.archive.org/web/20120610140256/http://blogs.plos.org/mfenner/scientific-attribution-principles/">argued before</a>,</p>
<p><strong><strong>Systems that measure and evaluate scientific contributions can and should be separate from the databases that hold the scholarly record.</strong></strong></p>
<p>It is not only a waste of resources (both Google Scholar’s and the individual researchers’ who maintain their profiles) to many many different bibliographic databases, but it also makes it impossible to compare citation metrics. In the examples above Alonzo Church has a H-index of 19 at Google Scholar, but only 11 at Microsoft Academic Search (and probably again a different one somewhere else). This means that we can only use an H-index when we mention where (and when) it was calculated.</p>
<p>The better solution is a common <a href="https://web.archive.org/web/20120610140256/http://3lib.org/">open bibliography</a>, and the difference between the various service would be how they calculate the citation metric or present the bibliographic data – you can see the different approaches taken by Google Scholar and Microsoft Academic Search in the screenshots above. This is a difficult task, but not impossible to do. The first step would be to realize that having a common open bibliography would create tremendous value for everybody as we can start building tools on top this bibliography without requiring to collect all the bibliographic data ourselves. We see something like this happening in smaller domains, and the <a href="https://web.archive.org/web/20120610140256/http://blogs.plos.org/mfenner/2011/04/29/web-tools-for-searching-the-biomedical-literature-part-i/">tools using the PubMed database</a> are a good example.</p>
<p>From a researcher perspective it makes little sense to have many different places where you can maintain your publications. It makes much more sense to do this once and then see the information reused in different services. This is the approach the Open Researcher &amp; Contributor ID initiative is <a href="https://web.archive.org/web/20120610140256/http://www.orcid.org/principles">taking</a>:</p>
<p><strong><strong>All profile data contributed to ORCID by researchers or claimed by them will be available in standard formats for free download (subject to the researchers’ own privacy settings) that is updated once a year and released under the CC0 waiver.</strong></strong></p>
<p><em><em>Disclaimer: I sit on the Board of Directors of the Open Researcher &amp; Contributor ID (ORCID) initiative which aims to help solve this and related problems.</em></em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to formally cite a blog post]]></title>
        <id>hnbrg5s-bp86qt3-f5msdzv-ktnd</id>
        <link href="https://blog.front-matter.io/mfenner/how-to-formally-cite-a-blog-post"/>
        <updated>2011-07-21T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Other blog posts often provide important background material for your own posts, and they are typically cited by inline links in the text. But sometimes we need more formal citations, e.g. when citing blog posts in a journal article or when providing a bibliography....]]></summary>
        <content type="html"><![CDATA[<p>Other blog posts often provide important background material for your own posts, and they are typically cited by <a href="https://sensiblescience.io/mfenner/beyond-the-pdf-%E2%80%A6-is-epub/">inline links</a> in the text. But sometimes we need more formal citations, e.g. when citing blog posts in a journal article or when providing a bibliography. But how do you properly cite a blog post?</p>
<figure>
<img src="https://web.archive.org/web/20170913072431im_/http://farm5.static.flickr.com/4049/4534468843_176c790812.jpg" class="kg-image" />
</figure>
<p><em><a href="https://web.archive.org/web/20170913072431/http://www.flickr.com/photos/ccacnorthlib/4534468843/">Flickr picture</a> by CCAC North Library.</em></p>
<p>The <a href="https://web.archive.org/web/20170913072431/http://www.apastyle.org/">APA Style</a> – from the Publication Manual of the American Psychological Association – suggests the following format:</p>
<p><strong>Fenner, M.H.</strong> (2011, January 23). Beyond the PDF … is ePub [Web log post]. Retrieved from <a href="https://sensiblescience.io/mfenner/beyond-the-pdf-is-epub">https://sensiblescience.io/mfenner/beyond-the-pdf-is-epub</a></p>
<p>This is a good start, but I think the citation should include the name of the blog. The <a href="https://web.archive.org/web/20170913072431/http://www.chicagomanualofstyle.org/tools_citationguide.html">Chicago Manual of Style</a> does this:</p>
<p><strong>Fenner, M.H.,</strong> “Beyond the PDF … is ePub,” Gobbledygook, January 23, 2011, <a href="https://sensiblescience.io/mfenner/beyond-the-pdf-is-epub">https://sensiblescience.io/mfenner/beyond-the-pdf-is-epub</a></p>
<p>The <a href="https://web.archive.org/web/20170913072431/http://www.library.illinois.edu/learn/tutorials/mla.html">MLA Style</a> also mentions the Publisher, and the date the blog post was accessed (in addition to the publication date):</p>
<p><strong>Fenner, M.H.</strong> “Beyond the PDF … is ePub”. <em>Gobbledygook</em>. PLoS Blogs. January 23, 2011. Web. July 21, 2011. <a href="https://sensiblescience.io/mfenner/beyond-the-pdf-is-epub">https://sensiblescience.io/mfenner/beyond-the-pdf-is-epub</a></p>
<p>This is a little bit too much for me. There is an argument to use the accession date for web content, but having two dates can be confusing and the publication date is more important. I personally prefer a citation format that looks very similar to a journal article citation (and comes closest to the Chicago Manual of Style):</p>
<p><strong>Fenner, M.H.</strong> Beyond the PDF … is ePub. <em>Gobbledygook</em>, January 23, 2011. <a href="https://sensiblescience.io/mfenner/beyond-the-pdf-is-epub">https://sensiblescience.io/mfenner/beyond-the-pdf-is-epub</a></p>
<p>If we want to include blog posts in a reference list, we also have to think about the formatting. In <strong>BibTeX</strong>a blog entry would look like this:</p>
<p>@misc{ author = {Fenner, Martin}, title = {Beyond the PDF … is ePub}, journal = {Gobbledygook}, type = {Blog}, number = {January 23}, year = {2011}, howpublished = {\url{https://sensiblescience.io/mfenner/beyond-the-pdf-is-epub/}}</p>
<p><strong>BibTeX</strong> doesn’t have an entry type for blog posts, so we have to use @misc and use the type field. @unpublished is an alternative, but that is a strange name. We have to put the URL into the howpublished field, which is also awkward.</p>
<p><a href="https://web.archive.org/web/20170913072431/http://www.refman.com/support/risformat_intro.asp">RIS</a> is another popular format, and knows about blogs and URLs:</p>
<pre><code>TY  - BLOG 
AU  - Fenner, Martin 
TI  - Beyond the PDF … is ePub 
JF  - Gobbledygook 
PY  - 2011/01/23 
UR  - https://sensiblescience.io/mfenner/beyond-the-pdf-is-epub/ 
ER  -</code></pre>
<p><strong>BibTeX</strong> and <strong>RIS</strong> both predate the web. A modern reference format should use HTML or XML so that it can be embedded directly in the blog post. The recently announced <a href="https://web.archive.org/web/20170913072431/http://blogs.plos.org/mfenner/2011/06/07/schema-org-for-scholarly-html/">Schema.org</a> micro data format is a strong candidate for this <a href="https://web.archive.org/web/20170913072431/http://scholarlyhtml.org/">Scholarly HTML</a>, and it has a <a href="https://web.archive.org/web/20170913072431/http://schema.org/BlogPosting">BlogPosting</a> format:</p>
<p>&lt;div itemscope itemtype="http://schema.org/BlogPosting"&gt; &lt;span itemprop="author" itemscope itemtype="http://schema.org/Person"&gt; &lt;span itemprop="name"&gt;Fenner, Martin&lt;/span&gt; &lt;/span&gt; &lt;span itemprop="name"&gt;Beyond the PDF … is ePub&lt;/span&gt; &lt;span itemprop="publisher"&gt;Gobbledygook&lt;/span&gt; &lt;time datetime="2011-01-23"&gt;01/23/11&lt;/time&gt; &lt;a href="<a href="https://sensiblescience.io/mfenner/beyond-the-pdf-is-epub/">https://sensiblescience.io/mfenner/beyond-the-pdf-is-epub/</a>" itemprop="url"&gt;<a href="https://sensiblescience.io/mfenner/beyond-the-pdf-is-epub/">https://sensiblescience.io/mfenner/beyond-the-pdf-is-epub/</a>&lt;/a&gt; &lt;/div&gt;</p>
<p>There are other ways to describe this blog post with <strong>Schema.org</strong>, but the best practices are still evolving. How this BlogPosting above looks in a blog obviously depends on the CSS styling used, without any styling it looks like this:</p>
<p>Fenner, Martin<br />
Beyond the PDF … is ePub<br />
Gobbledygook<br />
01/23/11<br />
<a href="https://sensiblescience.io/mfenner/beyond-the-pdf-is-epub/">https://sensiblescience.io/mfenner/beyond-the-pdf-is-epub/</a></p>
<p>With a little work on the CSS for this blog, the citation could look exactly as shown above, but with added semantic information that is understood by search engines and other tools. It is clear to me that HTML-based standards such as Schema.org are the future for embedding citation information in blog posts. We need better tools to make this process as painless as possible.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Did you receive spam because you published a paper?]]></title>
        <id>40tfzsh-v609hxt-g3p2vg7-ne8cy</id>
        <link href="https://blog.front-matter.io/mfenner/did-you-receive-spam-because-you-published-a-paper"/>
        <updated>2011-07-13T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Brendan Thomas has published an interesting paper that looks at author email addresses in the PubMed database of biomedical literature. Email addresses of first authors have been added to PubMed since 1996, and they can be retrieved via the standard web interface or automated software....]]></summary>
        <content type="html"><![CDATA[<p>Brendan Thomas has published an interesting paper that looks at author email addresses in the PubMed database of biomedical literature. Email addresses of first authors have been added to PubMed since 1996, and they can be retrieved via the standard web interface or automated software. This makes PubMed an excellent place to find the email address of an academic author, but also shows that PubMed is very vulnerable to email address harvesting.</p>
<p>The problem is not limited to PubMed; this is also an issue with many scholarly journals. Email addresses are used as contact information in scholarly papers, and they are commonly displayed on journal webpages. You can harvest email addresses from <em><em>NEJM</em></em>, <em><em>Science</em></em> or <em><em>PLoS</em></em> webpages, and you don’t even need a subscription. <em><em>JAMA</em></em> is hiding the email address, and <em><em>Nature</em></em> is providing an author contact form. Both journals provide the email address in the PDF.</p>
<p>Most journals provide the postal address of corresponding authors. Journals added email addresses in the 1990s, and this of course has become the preferred method of communication among scientists – when was the last time you received a postcard for a reprint request? Unfortunately we have long learned that it is no longer safe to make email addresses publicly available – more than 90% of email traffic is <a href="https://web.archive.org/web/20120611043917/http://www.ftc.gov/bcp/edu/microsites/spam/">spam</a>.</p>
<p>Journal publishers have to rethink their policies regarding contact information of their authors. And authors should demand from journals that their email addresses are treated with more respect for privacy. There are better ways to provide contact information for corresponding authors, and I don’t mean a link to their Twitter account or LinkedIn profile. Journal publishers should create author profile pages that not only list all publications of a particular author, but also the relevant contact information. There should be contact forms instead of plain email addresses, and authors should be able to control what information is displayed in their author profile. Author profiles of course can be further extended in many ways, from links to publications with other publishers to author-level metrics.</p>
<p><em><em>Disclaimer: I sit on the Board of Directors of the Open Researcher &amp; Contributor ID (ORCID) initiative which aims to help solve this and related problems.</em></em></p>
<h2 id="references">References</h2>
<p><strong><strong>Thomas, B.</strong></strong> (2011). E-mail Address Harvesting on PubMed–A Call for Responsible Handling of E-mail Addresses. <em><em>Mayo Clinic Proceedings Mayo Clinic</em></em>, 86(4), 362. https://doi.org/<a href="https://doi.org/10.4065/mcp.2010.0817">10.4065/mcp.2010.0817</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conference microblogging: FriendFeed, Twitter and now Google+ ?]]></title>
        <id>4aebvhd-5hp9wj8-rjmypxd-kr8es</id>
        <link href="https://blog.front-matter.io/mfenner/conference-microblogging-friendfeed-twitter-and-now-google"/>
        <updated>2011-07-04T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Blogging is a great format to report from conferences. The regular blog format works best for posts written at the end of the day – unless you are typing really fast. Microblogging, i.e. a number of short or very short posts by a group of people,...]]></summary>
        <content type="html"><![CDATA[<p>Blogging is a <a href="https://blog.martinfenner.org/posts/conference_blogging_interview_with_alex_knoll/">great format</a> to report from conferences. The regular blog format works best for posts written at the end of the day – unless you are typing really fast. Microblogging, i.e. a number of short or very short posts by a group of people, works better for live blogging of an event and has become very popular.</p>
<p><a href="https://web.archive.org/web/20120611100409/http://www.friendfeed.com/">FriendFeed</a> was probably the first widely used microblogging tool for scientific conferences starting in 2008, and a <a href="https://doi.org/10.1371/journal.pcbi.1000263">January 2009 paper</a> describes the experience at 2008 ISMB conference in Toronto. By late 2009 Twitter had surpassed FriendFeed in popularity, and at some point in 2010 most people seemed to have switched from FriendFeed to Twitter for conference microblogging.</p>
<p>Twitter is a great tool in many ways, but is far from perfect for conferences. The biggest problem is that the tweets about a particular topic aren’t really connected. Hashtags help to find tweets about a particular conference, but hashtags for a particular session have never cught on. This makes it very difficult to have a real discussion, or to find related messages later on.</p>
<p>Facebook is not only extremely popular, but has also taken many of the features of FriendFeed. But for some reason it has not become a popular conference microblogging tool for scientists. I think this is because many people use Facebook primarily for their social interactions, and keep those separate from their professional discussions.</p>
<p><a href="https://plus.google.com/">Google+</a> was released last week, and the services has many of the features of FriendFeed and Facebook (who bought FriendFeed in August 2009, one of the reasons the service has never become that popular). Google+ allows discussions around a particular message (post, tweet?). It doesn’t have groups about a particular topic (e.g. conference), and doesn’t yet understand a concept similar to hashtags. The service looks very nice from a desktop computer, but the mobile version for iPhone and iPad isn’t very polished (I haven’t tested the Android app).</p>
<p>The experience from Twitter tells me that the most important feature for a great conference microblogging tool is popularity, and this is only partly related to the technology behind the service. There are already a good number of science bloggers and other Science 2.0 folks on Google+ and we have seen some good discussions (e.g. a <a href="https://plus.google.com/106537123721037364937/posts/buxDWrvq8vU#106537123721037364937/posts/buxDWrvq8vU">discussion about conference microblogging</a>), so that is a very good sign. But the experience with Google Wave (<a href="https://blog.martinfenner.org/posts/what_comes_after_google_wave/">discontinued</a>) and Google Buzz (not that many people seem to use it) makes me a bit cautious, and I will wait how Google+ evolves in the coming months before I get too exited.</p>
<p>Update: Added a paragraph about Facebook.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Annotum: Publishing with WordPress soon coming to a journal near you]]></title>
        <id>5kw01b9-2cg8ays-tbfj867-3y29y</id>
        <link href="https://blog.front-matter.io/mfenner/annotum-publishing-with-wordpress-soon-coming-to-a-journal-near-you"/>
        <updated>2011-06-30T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Last week the first Alpha version of <strong><strong>Annotum</strong></strong> was released. Annotum is “a scholarly authoring and publishing platform based on WordPress”. I first learned about Annotum at the Beyond the PDF workshop in January....]]></summary>
        <content type="html"><![CDATA[<p>Last week the first <a href="http://annotum.wordpress.com/2011/06/23/alpha-release/">Alpha</a> version of <strong><strong>Annotum</strong></strong> was released. <a href="http://annotum.wordpress.com/about/">Annotum</a> is “a scholarly authoring and publishing platform based on WordPress”. I <a href="http://river-valley.tv/annotem-an-open-source-journal-authoring-and-publishing-platform-based-on-wordpress/">first learned</a> about Annotum at the <a href="https://sites.google.com/site/beyondthepdf/">Beyond the PDF</a> workshop in January. One of the themes of the workshop was that we need better tools for authoring, reviewing and publishing of scholarly articles. For many at the workshop it was obvious that these tools should be web-based, use HTML as the file format, and should use readily available tools whenever possible.</p>
<p>WordPress is a perfect candidate for such a tool, but needs to be extended with functions required for scholarly articles: better handling of citations, figures and tables, support of standard document formats such as <a href="http://dtd.nlm.nih.gov/">NLM-DTD</a>, and support of  a review workflow with co-authors, editors and reviewers. WordPress can easily be extended with <a href="https://web.archive.org/web/20120611092155/http://wordpress.org/extend/plugins/">plugins</a>, and a good number of plugins have been released for scholars. A good starting point are plugins tagged <a href="http://wordpress.org/extend/plugins/tags/res-comms">res-comms</a> and the <a href="https://groups.google.com/forum/#!forum/wordpress-for-scientists">WordPress for Scientists</a> Google Group.</p>
<p>There is one problem with these scholarly plugins: none of them offers a complete solution, and it is sometimes tricky to make them work with other required plugins or with each other (a particular problem are plugins that use Javascript). This means that there is currently no out-of-the-box solution to create, review and publish scholarly content with WordPress.</p>
<p><strong><strong>Annotum</strong></strong> is an ambitious project that wants to change this. It helps that Annotum seems to have sufficient funding and commitment to launch a production version later this year – rather than being the part-time effort of a single developer. Technically it is a WordPress theme, and not a plugin. Themes normally are responsible for the look of a WordPress site, but they can also contain functions normally found in plugins. Installing a single theme is much easier for the average WordPress user than installing a number of plugins plus a theme that works with all plugins. BTW, themes can have child themes, so this doesn’t mean that all Annotum sites will look the same.</p>
<p>Annotum of course can’t do everything by itself and has to work with existing plugins (it took me only one hour to <a href="https://web.archive.org/web/20120611092155/http://blogs.xartrials.org/annotum/article/the-mycobacterium-tuberculosis-drugome-and-its-polypharmacological-implications/">make it work</a> with my <a href="https://web.archive.org/web/20120611092155/http://wordpress.org/extend/plugins/epub-export/">ePub Export</a> plugin). Annotum’s Carl Leubsdorf is part of the WordPress for Scientists community and I think he will make sure that Annotum will work well with the plugins already out there.</p>
<p>I hope that Annotum will not only become a nice out-of-the-box solution to create a WordPress-based journal or science blog, but will also be a big stimulus for the budding community of scholarly WordPress users and developers.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Three funding organizations will launch new open access journal]]></title>
        <id>20r7nmv-3bq9xta-tpc8wj2-8t6cv</id>
        <link href="https://blog.front-matter.io/mfenner/three-funding-organizations-will-launch-new-open-access-journal"/>
        <updated>2011-06-23T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Three leading funding organizations today made this important announcement:The Howard Hughes Medical Institute, the Max Planck Society and the Wellcome Trust announced today that they are to support a new, top-tier,...]]></summary>
        <content type="html"><![CDATA[<p>Three leading funding organizations today made this important <a href="https://web.archive.org/web/20120611031434/http://www.wellcome.ac.uk/News/Media-office/Press-releases/2011/WTVM051897.htm">announcement</a>:</p>
<blockquote>
The Howard Hughes Medical Institute, the Max Planck Society and the Wellcome Trust announced today that they are to support a new, top-tier, open access journal for biomedical and life sciences research.
</blockquote>
<p>Some features of the new journal:</p>
<ul>
<li>open access</li>
<li>name of journal to be decided</li>
<li>aims to attract and define the very best research publications</li>
<li>papers will be accepted or rejected as rapidly as possible</li>
<li>first issue expected for summer of 2012</li>
<li>editor-in-chief and editorial team will be active scientists</li>
<li>journal will enable improved data presentation</li>
<li>long-term business model to be developed, the three organisations made a commitment to cover initial costs</li>
</ul>
<p>This is an important announcement, as the journal has the potential to have a major impact on publishing biomedical research. Two aspects of the announcement surprise me: many details about the new journal have not been decided, I would have expected at least a decision about the journal name and Editor-in-Chief. And from the few details in the announcement the new journal appears to take a fairly conservative approach to publishing. You could for example compare today’s press release with the editorial explaining the launch of PLoS Biology from October 2003 (<a href="https://web.archive.org/web/20120611031434/http://dx.doi.org/doi:10.1371/journal.pbio.0000036">Why PLoS became a Publisher</a>). This approach is understandable as most scientists are conservative and don’t care much about new publishing technologies or even open access; they only care about having their research accepted in a journal with high impact. Publishing this kind of journal – which usually means high rejection rates and time-consuming editorial work – is difficult with with an author-pays business model. The deep pockets of the three funding organizations change this equation.</p>
<p>Earlier this year we saw the the announcements of <a href="https://web.archive.org/web/20120611031434/http://blogs.plos.org/mfenner/2011/01/06/new-journal-nature-one-launched-today/">Scientific Reports</a>, <a href="https://web.archive.org/web/20120611031434/http://dx.doi.org/10.1136/bmj.d1190">BMJ Open</a> and similar high volume open access journals that follow the success of PLoS ONE (<a href="https://web.archive.org/web/20120611031434/http://www.slideshare.net/PBinfield/ssp-presentation4">PLoS ONE and the Rise of the Open Access Mega Journal</a>). The new journal announced today takes a very different approach to open access publishing.</p>
<p>This announcement has of course also been discussed elsewhere, including:</p>
<ul>
<li><a href="https://web.archive.org/web/20120611031434/http://phylogenomics.blogspot.com/2011/06/new-openaccess-journals-welcome.html">The Tree of Life</a></li>
<li><a href="https://web.archive.org/web/20120611031434/http://cameronneylon.net/default/a-new-sustainability-model-major-funders-to-support-oa-journal/">Science in the Open</a></li>
<li><a href="https://web.archive.org/web/20120611031434/http://bytesizebio.net/index.php/2011/06/27/suggest-a-name-for-the-next-big-journal/">Byte Size Biology</a></li>
<li><a href="https://web.archive.org/web/20120611031434/http://scholarlykitchen.sspnet.org/2011/06/27/top-tiered-open-access-journal-arrives-with-fanfare-few-details/">Scholarly Kitchen</a></li>
</ul>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is Schema.org about a technical standard or about something else?]]></title>
        <id>2qq6esm-bca9699-qkdzd2e-pjkkm</id>
        <link href="https://blog.front-matter.io/mfenner/is-schema-org-about-a-technical-standard-or-about-something-else"/>
        <updated>2011-06-19T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[At the beginning of the month Google, Bing and Yahoo announced schema.org, a new initiative for structured markup on the web. Richard MacManus responded with a critical piece at ReadWriteWeb (Is Schema.org really a Google Land Grab?),...]]></summary>
        <content type="html"><![CDATA[<p>At the beginning of the month Google, Bing and Yahoo <a href="https://web.archive.org/web/20120525041721/http://blogs.plos.org/mfenner/2011/06/07/schema-org-for-scholarly-html/">announced</a> schema.org, a new initiative for structured markup on the web. Richard MacManus responded with a critical piece at ReadWriteWeb (<a href="https://web.archive.org/web/20120525041721/http://www.readwriteweb.com/archives/is_schemaorg_really_a_google_land_grab.php">Is Schema.org really a Google Land Grab?</a>), mainly criticizing that the initiative didn’t use RDFa and didn’t seem to have consulted with the web standards body W3C. The best discussion about schema.org that I found so far (via Eric Hellman’s <a href="https://web.archive.org/web/20120525041721/http://go-to-hellman.blogspot.com/2011/06/our-metadata-overlords-and-that.html">post</a>) is by Henri Sivonen (<a href="https://web.archive.org/web/20120525041721/http://hsivonen.iki.fi/schema-org-and-communities/">Schema.org and Pre-Existing Communities</a>). He explains why he thinks it makes sense that schema.org picked microdata over RDFa or microformats as markup standard. And he discusses how the web community has developed standards in the past and what characterizes the successful efforts. I personally believe that schema.org will bring us closer to the semantic web, and the discussion reminds me a little bit of the discussion around HTML 5 vs. XHTML 2 (where the pragmatic solution supported by the major browser vendors won over the ideal but dogmatic solution discussed for many years in standards bodies). Schema.org is will certainly be discussed at the <a href="https://web.archive.org/web/20120525041721/http://scienceonlinelondon.wikidot.com/start">Science Online London Conference</a> in September.</p>
<p>Whether schema.org becomes a success depends in large part on the adoption by the community. Schema.org is backed by the three largest search engines (Google, Bing, Yahoo), so everybody who wants to be found by them will take a close look at the standard. Scholarly content is for the most part collected and curated in specialized databases. This makes it not only a technical decision whether or not to adopt schema.org for markup (we need more and better defined scholarly data types in the standard), but also a business decision. If scholarly publishers start marking up their journal articles, they will be easier to find via Google, Bing, etc. But will this mean that institutions would start re-evaluating their subscriptions to commercial services like Scopus or Web of Science? And will schema.org help or hinder the very large market of discovery tools for scholarly content?</p>
<p>At this point this is very much a theoretical discussion. We first need better scholarly data types in the schema.org standard. The Association of Educational Publishers and Creative Commons <a href="https://web.archive.org/web/20120525041721/http://www.aepweb.org/mediacenter/AEP-CC-Schema_6-7-11.htm">has announced</a> an initiative to create a metadata framework for educational resources based on schema.org. And we hopefully see a similar initiative from scholarly publishers. I hope that the group behind schema.org is open to this input.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Please join us for the Science Online London Conference in September]]></title>
        <id>36zg0sj-kmm9z7b-t8t48c9-rzpad</id>
        <link href="https://blog.front-matter.io/mfenner/please-join-us-for-the-science-online-london-conference-in-september"/>
        <updated>2011-06-13T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The Science Online London 2011 Conference takes place September 2-3 at the British Library. I am again one of the organizers (together with <em><em>Lou Woodley</em></em> from Nature.com and <em><em>Kaitlin Thaney</em></em> from Digital Science),...]]></summary>
        <content type="html"><![CDATA[<p>The <a href="https://web.archive.org/web/20120611105557/http://www.scienceonlinelondon.org/">Science Online London 2011 Conference</a> takes place September 2-3 at the British Library. I am again one of the organizers (together with <em><em>Lou Woodley</em></em> from Nature.com and <em><em>Kaitlin Thaney</em></em> from Digital Science), and I’m getting both excited and nervous the closer we get to September.</p>
<figure>
<img src="https://web.archive.org/web/20120611105557im_/http://www.scienceonlinelondon.org/images/solo11.jpg" title="Science Online London 2011" class="kg-image" alt="Photo by Mendeley.com (if you look really hard, you can find me in this picture from the 2010 conference)." /><figcaption aria-hidden="true">Photo by Mendeley.com (if you look really hard, you can find me in this picture from the 2010 conference).</figcaption>
</figure>
<p>We have recently posted a first draft of the <a href="https://web.archive.org/web/20120611105557/http://www.scienceonlinelondon.org/programme.html">conference program</a>, and you can see that we are trying something new. In addition to the keynotes, panel discussions and breakout sessions we had the last few years we have also scheduled four workshops for the second day of the conference. I’m doing a workshop called <strong><strong>Beyond scholarly publication</strong></strong> together with <em><em>Eva Amsen</em></em> (The Node) and <em><em>Mark Hahnel</em></em> (FigShare). The idea is to really have a hands-on experience, and in this workshop we will focus on blogs and related tools. We will try to do something interesting both for someone who wants to start a science blog, but also for more experienced writers (e.g. <a href="https://web.archive.org/web/20120611105557/http://scholarlyhtml.org/">Scholarly HTML</a> or <a href="https://web.archive.org/web/20120611105557/http://blogs.plos.org/mfenner/2011/01/23/beyond-the-pdf-%E2%80%A6-is-epub/">ePub</a>). We haven’t work out the details, so please drop us a note if there is a particular topic that you want to see covered. All workshops will focus on <strong><strong>Spinal Muscular Atrophy</strong></strong> as a theme, and we hope that everybody has learned a few tricks by the end of the day.</p>
<p>We are still looking for breakout session suggestions for the first day of the conference. If you have a good idea, please add it to the <a href="https://web.archive.org/web/20120611105557/http://scienceonlinelondon.wikidot.com/">wiki</a>, as we hope to have a program that is (close to) final by July.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Schema.org for Scholarly HTML?]]></title>
        <id>a8x8qze-6v9t2sa-rmdvzqm-4enr</id>
        <link href="https://blog.front-matter.io/mfenner/schema-org-for-scholarly-html"/>
        <updated>2011-06-07T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Last Thursday the search engines Google, Bing and Yahoo announced schema.org, a new initiative for structured data markup on the web. Websites that use this schema to markup their data (more than 100 data types are supported)...]]></summary>
        <content type="html"><![CDATA[<p>Last Thursday the search engines Google, Bing and Yahoo <a href="https://web.archive.org/web/20120525041638/http://googleblog.blogspot.com/2011/06/introducing-schemaorg-search-engines.html">announced</a> <a href="https://web.archive.org/web/20120525041638/http://schema.org/">schema.org</a>, a new initiative for structured data markup on the web. Websites that use this schema to markup their data (more than 100 data types are supported) will make it easier for the three largest web search engines to find their content. Schema.org uses microdata, not microformats or RDFa, according to the <a href="https://web.archive.org/web/20120525041638/http://schema.org/docs/faq.html#15">FAQ</a> this was a pragmatic decision.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/12th/schema-500x456.jpeg" title="schema" class="kg-image" width="500" height="456" />
</figure>
<p>Schema.org also defines the <a href="https://web.archive.org/web/20120525041638/http://schema.org/ScholarlyArticle">ScholarlyArticle</a>, but unfortunately this data type doesn’t add any properties beyond those of the parent <a href="https://web.archive.org/web/20120525041638/http://schema.org/Article">Article</a> (e.g. the Digital Object Identifier). The <a href="https://web.archive.org/web/20120525041638/http://www.schema.org/Person">Person</a> data type is also interesting (and uses a professor as an example), but for example doesn’t include an <strong><strong>isAuthor</strong></strong> relationship.</p>
<p>It will be interesting to see whether future versions of the schema.org standard will have better support for scholarly content, and how well this microformat can be integrated into our efforts to create <a href="https://web.archive.org/web/20120525041638/http://scholarlyhtml.org/">Scholarly HTML</a>. And how quickly scholarly publishers and other providers of scholarly content will adopt this new standard., which in its current form already is a big improvement over plain HTML.</p>
<p><em>Update (6/7/11): Google today <a href="https://web.archive.org/web/20120525041638/http://insidesearch.blogspot.com/2011/06/authorship-markup-and-web-search.html">announced</a> support for authorship markup, either via the schema.org format or using the <strong><strong>rel</strong></strong> attribute.</em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Web Tools for Searching the Biomedical Literature – part II]]></title>
        <id>558f84j-r1t9dz9-dfk36fv-dnmwj</id>
        <link href="https://blog.front-matter.io/mfenner/web-tools-for-searching-the-biomedical-literature-part-ii"/>
        <updated>2011-05-09T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Ten days ago I mentioned a paper by Zhiyong Lu that gives an overview over the available web tools to search the biomedical literature. Most of these tools enhance the PubMed service, and Zhiyong Lu in fact works for the NCBI, the developer of PubMed....]]></summary>
        <content type="html"><![CDATA[<p>Ten days ago I <a href="https://web.archive.org/web/20120610114233/http://blogs.plos.org/mfenner/2011/04/29/web-tools-for-searching-the-biomedical-literature-part-i/">mentioned</a> a paper by Zhiyong Lu that gives an overview over the available web tools to search the biomedical literature. Most of these tools enhance the PubMed service, and Zhiyong Lu in fact works for the NCBI, the developer of PubMed. In this post I want to take a more detailed look at the available tools.</p>
<figure>
<img src="https://web.archive.org/web/20120610114233im_/http://blogs.plos.org/mfenner/files/2011/05/pubmed-500x177.png" title="pubmed" class="kg-image" />
</figure>
<p>A good starting point is the <a href="https://web.archive.org/web/20120610114233/http://www.ncbi.nlm.nih.gov/CBBresearch/Lu/search/">companion webpage</a> to the paper, listing 28 services. One of the most painful shortcomings of PubMed is the sorting of results by date. Zhiyong lists 8 tools that can present search results sorted by relevance. Semantic search, seeking relevant concepts, and visualization of search results are all offered by several tools. Three tools will identify similar publications, and three tools will find potential experts, reviewers or collaborators. It is a good sign that there are so many tools that integrate with PubMed – in addition to the reference managers (e.g. Endnote, Papers) that do the same.</p>
<p>The analysis has one important shortcoming: only tools that specifically cover the biomedical literature are discussed. General purpose search tools such as <strong><strong>Web of Science</strong></strong>, <strong><strong>Scopus</strong></strong>, <strong><strong>Google Scholar</strong></strong> or <strong><strong>Microsoft Academic Search</strong></strong> are therefore not covered, neither are online bibliographic tools such as <strong><strong>Mendeley</strong></strong> and <strong><strong>CiteULike</strong></strong>. All of them also include biomedical literature – although the coverage at the fairly new Microsoft Academic Search is still limited. They all offer unique features not found in Pubmed or the 28 tools discussed in the paper.</p>
<p>When I think about how I find interesting articles, then it is increasingly through my social networks - including this paper by Zhiyong Lu. His paper unfortunatel fails to discuss this important search strategy. Twitter, FriendFeed, science blogs, etc. are strange places to find interesting literature, but they work for me. <a href="https://web.archive.org/web/20120610114233/http://www.massgenomics.org/2011/04/whole-genome-sequencing-for-cancer-patients.html">This April 22 post</a> on the wonderful <strong><strong>Massgenomics</strong></strong> blog alerted me to two interesting papers published in JAMA on April 20 that describe the use of whole-genome sequencing in the care of two patients with acute leukemia. I presented the paper by Welch et al. in a journal club last Friday and we had a very interesting discussion.</p>
<p>The paper has also been discussed in the following places: <a href="https://web.archive.org/web/20120610114233/http://nnlm.gov/pnr/dragonfly/2011/02/15/beyondpubmed/">Dragonfly</a>, <a href="https://web.archive.org/web/20120610114233/http://cmch.typepad.com/cmch/2011/03/res.html">Center on Media and Child Health</a>, <a href="https://web.archive.org/web/20120610114233/http://beckerinfo.net/scp/2011/04/29/pubmed-and-beyond-a-survey-of-web-tools-for-searching-biomedical-literature/">Bernard Becker Medical Library</a>, <a href="https://web.archive.org/web/20120610114233/http://laikaspoetnik.wordpress.com/2011/05/08/3rd-call-for-submissions-for-medical-information-matters-tools-for-searching-the-biomedical-literature/">Medical Information Matters</a>, <a href="https://web.archive.org/web/20120610114233/http://autoimmunbuch.de/?p=447">Friendly Fire</a> (German), <a href="https://web.archive.org/web/20120610114233/http://usalbiomedica.wordpress.com/2011/05/05/pubmed-and-beyond-a-survey-of-web-tools-for-searching-biomedical-literature/">Usalbiomedica</a> (Spanish). This post is my much-delayed contribution to the <a href="https://web.archive.org/web/20120610114233/http://blogcarnival.com/bc/cprof_6092.html">medlib’s round</a> blog carnival. Unfortunately I haven’t received any submissions since my call on April 29, but I probably didn’t do enough advertising.</p>
<h2 id="references">References</h2>
<p><strong><strong>Lu Z.</strong></strong> PubMed and beyond: a survey of web tools for searching biomedical literature. <em><em>Database</em></em>. 2011 Jan;2011. doi: <a href="https://doi.org/10.1093/database/baq036">http://doi.org/10.1093/database/baq036</a>.</p>
<p><strong><strong>Welch JS, Westervelt P, Ding L, Larson DE, Klco JM, Kulkarni S, et al</strong></strong>. Use of whole-genome sequencing to diagnose a cryptic fusion oncogene. <em><em>JAMA : the journal of the American Medical Association</em></em>. 2011 Apr;305(15):1577-1584. doi:  <a href="https://doi.org/10.1001/jama.2011.497">http://doi.org/10.1001/jama.2011.497</a>.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Web Tools for Searching the Biomedical Literature – part I]]></title>
        <id>25mw2wk-65f8p0t-aztv7tm-bezh7</id>
        <link href="https://blog.front-matter.io/mfenner/web-tools-for-searching-the-biomedical-literature-part-i"/>
        <updated>2011-04-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Zhiyong Lu recently published an excellent overview of the web tools that are currently available to search the biomedical literature. The article has also a companion web page that allows user to filter for the features they are interested in,...]]></summary>
        <content type="html"><![CDATA[<p>Zhiyong Lu recently published an excellent overview of the web tools that are currently available to search the biomedical literature. The article has also a <a href="https://web.archive.org/web/20120610131235/http://www.ncbi.nlm.nih.gov/CBBresearch/Lu/search/">companion web page</a> that allows user to filter for the features they are interested in, and to report new tools.</p>
<p>The author describes 28 tools developed specifically for the biomedical domain. The tools are grouped based on their most important features into</p>
<ol>
<li>Ranking search results</li>
<li>Clustering results into topics</li>
<li>Extracting and displaying semantics and relations</li>
<li>Improving search interface and retrieval experience</li>
</ol>
<p>Zhiyong Lu works for the National Center for Biotechnology Information (NCBI). NCBI developed and maintains PubMed, and this is obviously the database to which the other tools are compared. Or looked at this differently, the proliferation of tools in the four areas listed above is an indication of the areas where PubMed is not very good at (e.g. it returns search results not by relevance, but in chronological order).</p>
<p>Instead of reviewing the article, I decided to do this in two parts. I will write about my thoughts on the various tools next week to give everybody time to look at this themselves. This is my delayed contribution to the <a href="https://web.archive.org/web/20120610131235/http://blogcarnival.com/bc/cprof_6092.html">medlib’s round</a> blog carnival. Feel free to write a comment, or submit directly to the blog carnival using <a href="https://web.archive.org/web/20120610131235/http://blogcarnival.com/bc/cprof_6092.html">this link</a>. The post with my own thoughts and the feedback I have received will go up Saturday next week.</p>
<p><strong><strong>Lu Z.</strong></strong> PubMed and beyond: a survey of web tools for searching biomedical literature. <em><em>Database</em></em>. 2011 Jan;2011. <a href="https://doi.org/10.1093/database/baq036">https://doi.org/10.1093/database/baq036</a>.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explaining the ORCID Principles]]></title>
        <id>3qp7m3c-aap9649-8b5fcyb-jrwe9</id>
        <link href="https://blog.front-matter.io/mfenner/explaining-the-orcid-principles"/>
        <updated>2011-04-13T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[On Monday I gave a presentation about ORCID, based on the ORCID Principles. The slides are hopefully a good introduction to ORCID and the current status of the initiative.A good in-person update of the ORCID initiative is the next ORCID Participant Meeting that takes place May 18 in Boston....]]></summary>
        <content type="html"><![CDATA[<p>On Monday I gave a presentation about ORCID, based on the <a href="https://web.archive.org/web/20120610125808/http://www.orcid.org/principles">ORCID Principles</a>. The slides are hopefully a good introduction to ORCID and the current status of the initiative.</p>
<div class="iframe">
<div id="svPlayer" height="100%">

</div>
</div>
<p>A good in-person update of the ORCID initiative is the next ORCID Participant Meeting that takes place May 18 in Boston. Registration is free and everybody interested in unique identifiers for scholarly authors is invited to attend. More information <a href="https://web.archive.org/web/20120610125808/http://www.orcid.org/civicrm/event/info?reset=1&amp;id=1">at the ORCID website</a>.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Please help the Nippon Science Support Network]]></title>
        <id>21gwfgb-0q19tt9-9vq9rak-c7y4d</id>
        <link href="https://blog.front-matter.io/mfenner/please-help-the-nippon-science-support-network"/>
        <updated>2011-04-06T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The recent tragic events in Japan have made it difficult or impossible for many Japanese scientists to continue their work. The newly launched Nippon Science Support Network has therefore established a database of positions and stipends in other countries for Japanese students,...]]></summary>
        <content type="html"><![CDATA[<p>The recent tragic events in Japan have made it difficult or impossible for many Japanese scientists to continue their work. The newly launched <a href="https://web.archive.org/web/20120610130607/http://nipponsciencesupport.net/">Nippon Science Support Network</a> has therefore established a database of positions and stipends in other countries for Japanese students, research fellows and scientific personnel. The network has started as a cooperation between German and Japanese researchers, but because of overwhelming demand today turned into a global initiative, allowing researchers from all countries help to support Japanese scientists.</p>
<p>Behind this initiative are a lot of people, but most importantly Phil Selenko – a group leader from the Leibniz Institute of Molecular Pharmacology in Berlin. Longtime readers of this blog will remember that Phil helped organize a <a href="https://web.archive.org/web/20120610130607/http://blogs.nature.com/mfenner/2008/05/24/why-local-hubs-in-nature-network-are-important">great effort in 2008</a> to bring Nobel Laureates and other senior scientists together with graduate students, and <em><em>to give them the same kind of networking opportunities that are usually restricted to seasoned faculty (and often happens behind closed doors).</em></em> I wish him the best of luck with this initiative.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Direct links to figures and tables using component DOIs]]></title>
        <id>3j5wtan-3g18gyr-56xvmhw-3aest</id>
        <link href="https://blog.front-matter.io/mfenner/direct-links-to-figures-and-tables-using-component-dois"/>
        <updated>2011-03-26T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[We are all familiar with digital object identifiers (DOIs) provided by CrossRef to identify (and link to) journal articles. Some of us are familiar with the DOIs issued by DataCite to link to datasets. But most of us don’t know that CrossRef is also...]]></summary>
        <content type="html"><![CDATA[<p>We are all familiar with digital object identifiers (DOIs) provided by <a href="https://web.archive.org/web/20120610122411/http://www.crossref.org/">CrossRef</a> to identify (and link to) journal articles. Some of us are familiar with the DOIs issued by <a href="https://web.archive.org/web/20120610122411/http://datacite.org/whatisdoi.html">DataCite</a> to link to datasets. But most of us don’t know that CrossRef is also providing <a href="https://web.archive.org/web/20120610122411/http://www.crossref.org/help/Content/Components.htm">component DOIs</a> that can provide persistent links to a particular table or figure in a paper. The <em><em>PLoS</em></em> journals use component DOIs, for example for this figure:</p>
<figure>
<img src="https://web.archive.org/web/20120610122411im_/http://blogs.plos.org/mfenner/files/2011/03/journal.pone_.0006022.g002.jpg" title="Image generated by AFPL Ghostscript (device=pnmraw)" class="kg-image" alt="Correlations between 37 measures mapped onto first two principal components (cumulative variance = 83.4%) of PCA. doi:10.1371/journal.pone.0006022.g002. From Bollen J, Van de Sompel H, Hagberg A, Chute R (2009) A Principal Component Analysis of 39 Scientific Impact Measures. PLoS ONE 4(6): e6022. doi:10.1371/journal.pone.0006022" /><figcaption aria-hidden="true"><em><em>Correlations between 37 measures mapped onto first two principal components (cumulative variance = 83.4%) of PCA.</em></em> doi:<a href="https://web.archive.org/web/20120610122411/http://dx.doi.org/10.1371/journal.pone.0006022.g002">10.1371/journal.pone.0006022.g002</a>. From <strong><strong>Bollen J, Van de Sompel H, Hagberg A, Chute R</strong></strong> (2009) A Principal Component Analysis of 39 Scientific Impact Measures. <em><em>PLoS ONE</em></em> 4(6): e6022. doi:<a href="https://web.archive.org/web/20120610122411/http://dx.doi.org/10.1371/journal.pone.0006022">10.1371/journal.pone.0006022</a></figcaption>
</figure>
<p>Linking to component DOIs is straightforward, table 1 of the paper above is <a href="https://web.archive.org/web/20120610122411/http://dx.doi.org/10.1371/journal.pone.0006022.t001">10.1371/journal.pone.0006022.t001</a>. Unfortunately <a href="https://web.archive.org/web/20120610122411/http://www.niso.org/publications/isq/free/Feeney_DOIs_for_Journals_ISQ_v22no3.pdf">only about 300,000</a> of the more than 40 million CrossRef DOIs are compontent DOIs.  There are many good reasons to use a direct (and persistent) link to a specific part of a scholarly journal article. And this becomes an even more important issue once DOIs for datasets are more commonly cited.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zotero 2.1 released with support for CSL 1.0]]></title>
        <id>4ntwhcb-gpf8ztt-8pekms5-beks9</id>
        <link href="https://blog.front-matter.io/mfenner/zotero-2-1-released-with-support-for-csl-1-0"/>
        <updated>2011-03-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Version 2.1 of the reference manager Zotero was released last Friday. The biggest change for me is the support of Citation Style Language (CSL) 1.0. CSL is an open XML-based standard for citations and bibliographies. Older versions of Zotero used CSL 0.8.1,...]]></summary>
        <content type="html"><![CDATA[<p>Version 2.1 of the reference manager Zotero was <a href="https://web.archive.org/web/20120610122458/http://www.zotero.org/blog/new-release-zotero-2-1/">released last Friday</a>. The biggest change for me is the support of Citation Style Language (CSL) 1.0. <a href="https://web.archive.org/web/20120610122458/http://citationstyles.org/">CSL</a> is an open XML-based standard for citations and bibliographies. Older versions of Zotero used CSL 0.8.1, but the improved CSL 1.0 was <a href="https://web.archive.org/web/20120610122458/http://citationstyles.org/2011/03/18/csl-1-0-first-anniversary/">released</a> a year ago.</p>
<p>With Zotero 2.1, Papers 2 (both released in March) and Mendeley we now have three major reference managers supporting the current version of CSL. Which reference manager will be the next one? And why aren’t more journals providing downloadable citation styles in CSL format in their <a href="https://web.archive.org/web/20120610122458/http://blogs.plos.org/mfenner/2010/09/24/citation-style-language-an-interview-with-rintze-zelle-and-ian-mulvany/">author guidelines</a>?</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A very brief history of Scholarly HTML]]></title>
        <id>74rkzy2-wew8meb-sd3j8ct-5bhdd</id>
        <link href="https://blog.front-matter.io/mfenner/a-very-brief-history-of-scholarly-html"/>
        <updated>2011-03-19T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The history of HTML begins 1989 at CERN, the European Laboratory for Particle Physics in Geneva. <strong><strong>Tim Berners-Lee</strong></strong>, <strong><strong>Robert Cailliau</strong></strong> and colleagues invented HTML (as well as the transport protocol HTTP and the web browser)...]]></summary>
        <content type="html"><![CDATA[<p>The history of HTML begins 1989 at <a href="https://web.archive.org/web/20120610122136/http://info.cern.ch/">CERN</a>, the European Laboratory for Particle Physics in Geneva. <strong><strong>Tim Berners-Lee</strong></strong>, <strong><strong>Robert Cailliau</strong></strong> and colleagues invented HTML (as well as the transport protocol HTTP and the web browser) to facilitate collaboration between CERN physicists.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/12th/4268841912_c2b34ca43c.jpeg" class="kg-image" width="500" height="281" alt="HTML by nidhug at Flickr." /><figcaption aria-hidden="true"><em><em>HTML by nidhug at Flickr.</em></em></figcaption>
</figure>
<p>HTML was originally invented for scholarly communication, but of course by the mid-1990s was also used by everybody else. But when electronic distribution of scholarly journal articles became possible, most publishers switched to PDF instead of HTML as the electronic document format of choice.</p>
<p>In April 2003 Inera, Mulberry Technologies and the NCBI published the <a href="https://web.archive.org/web/20120610122136/http://www.inera.com/nlmresources.shtml">NLM Journal Archiving and Interchange DTD Suite</a> (NLM-DTD, read <a href="https://web.archive.org/web/20120610122136/http://old.diglib.org/preserve/hadtdfs.pdf">here</a> about the history of this DTD Suite). The NLM-DTD has become the de facto XML standard for scholarly publishing and archiving. Although some tools for authors can write articles in this format (including Microsoft Word with the <a href="https://web.archive.org/web/20120610122136/http://blogs.plos.org/mfenner/2008/11/07/interview_with_pablo_fernicola/">Article Authoring Add-In</a>), the NLM-DTD has never caught on as an authoring format for scholars and I’m not aware of any publisher accepting manuscripts written in this format.</p>
<p>In April 2009  <em><em>Learned Publishing</em></em> published a paper by <strong><strong>David Shotton</strong></strong> titled <a href="https://web.archive.org/web/20120610122136/http://dx.doi.org/10.1087/2009202">Semantic publishing: the coming revolution in scientific journal publishing</a>. David listed six rules for semantic publishers:</p>
<ol>
<li>Start simply and improve functionality incrementally.</li>
<li>Expect greater things of your authors.</li>
<li>Exploit your existing in-house skills fully.</li>
<li>Use established standards wherever possible.</li>
<li>Publish raw datasets to the Web.</li>
<li>Release article metadata, particularly reference lists, in machine-readable form.</li>
</ol>
<p>Although the paper focusses on scholarly publishers, these rules also very much apply to what we could call Scholarly HTML today.</p>
<p>At about the same time (March 31, 2009) <strong><strong>Peter Sefton</strong></strong> for the first time used the term <em><em>Scholarly HTML</em></em> in a <a href="https://web.archive.org/web/20120610122136/http://ptsefton.com/2009/03/31/scholarly-html.htm">blog post</a>. He thinks that Scholarly HTML should allow the following:</p>
<ol>
<li>Documents should definitely have headings.</li>
<li>Protocols for representing things like examples.</li>
<li>Metadata, using a linked data approach.</li>
<li>Links from terms mentioned in the text to ontologies that describe them.</li>
<li>Linkable paragraphs.</li>
<li>Dead simple reference management via links to trusted sources.</li>
</ol>
<p>In January 2011 <strong><strong>Phil Bourne</strong></strong> organized the <a href="https://web.archive.org/web/20120610122136/http://blogs.plos.org/mfenner/2010/11/06/beyond-the-pdf-it-is-time-for-a-workshop/">Beyond the PDF</a> workshop, and I was lucky to attend. We had a number of <a href="https://web.archive.org/web/20120610122136/http://river-valley.tv/conferences/beyondthepdf-2011">interesting sessions</a> about how to improve the current scholarly paper published as PDF. I was involved in the working group thinking about better authoring tools, and one of my personal conclusions was that <a href="https://web.archive.org/web/20120610122136/http://blogs.plos.org/mfenner/2011/01/23/beyond-the-pdf-%E2%80%A6-is-epub/">ePub</a> is a very interesting alternative to PDF if we need a packing format for HTML, e.g. for journal submission or archiving.</p>
<p><strong><strong>Peter Murray-Rust</strong></strong> was able to capture the ideas of the authoring working group with a <a href="https://web.archive.org/web/20120610122136/http://blogs.ch.cam.ac.uk/pmr/2011/02/09/scholarly-html-hackfest-cambridge-uk-march/">drawing</a>, and he picked the term <em><em>Scholarly HTML</em></em> as the best description of what we want to achieve. He subsequently invited Peter Sefton, Brian McMahon and me (and a number of other people interested in Scholarly HTML) to a workshop that took place last weekend in Cambridge. Some of the thoughts of Peter Murray-Rust and Peter Sefton are summarized <a href="https://web.archive.org/web/20120610122136/http://blogs.ch.cam.ac.uk/pmr/2011/03/14/scholarly-html-%e2%80%93-major-progress/">here</a> and <a href="https://web.archive.org/web/20120610122136/http://ptsefton.com/2011/03/18/scholarly-html-fraglets-of-progress.htm">here</a>. Two outcomes of the workshop are Scholarly HTML <a href="https://web.archive.org/web/20120610122136/http://okfnpad.org/schtml-principles">Principles</a> and a <a href="https://web.archive.org/web/20120610122136/http://okfnpad.org/schtml-faqs">FAQ</a>.</p>
<p>My approach to Scholarly HTML is through developing WordPress plugins. The future will hopefully bring of a number of Scholarly HTML authoring tools, and WordPress will be just one of them. But Wordpress is a great platform to test ideas and improve them over time, or to quote David Shotton: <em><em>start simply and improve functionality incrementally</em></em>. One starting point has been citations in Scholarly HTML and we have already made <a href="https://web.archive.org/web/20120610122136/http://okfnpad.org/schtml-citations">good progress</a>.</p>
<p>The idea behind Scholarly HTML is not to build alternatives for Microsoft Word or LaTeX for authoring, but instead to build tools that will allow us to do something new and exciting with scholarly content. The trick here is to improve the scholarly document – and the author should see the immediate benefit - without putting too much extra burden on the author. And this might in fact be the big challenge for Scholarly HTML.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discussing science with microformats]]></title>
        <id>zbkmdxk-6h92h9n-g6enaam-kktk</id>
        <link href="https://blog.front-matter.io/mfenner/discussing-science-with-microformats"/>
        <updated>2011-03-15T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The best and quickest discussions of a scientific paper now sometimes happen in science blogs rather than in the peer-reviewed literature. Whereas we have a number of scholarly databases that track citations between papers, we don’t have the same tools for science blogs....]]></summary>
        <content type="html"><![CDATA[<p>The best and quickest discussions of a scientific paper now sometimes happen in science blogs rather than in the peer-reviewed literature. Whereas we have a number of scholarly databases that track citations between papers, we don’t have the same tools for science blogs. Following all science blogs manually has simply become impossible (unless your first name is <a href="https://web.archive.org/web/20120525235200/http://coturnix.org/">Bora</a>). This makes it difficult to find all blog posts about a particular paper  - either for <a href="https://web.archive.org/web/20120525235200/http://blogs.plos.org/mfenner/2010/08/27/supplementary_information_should_i_stay_or_should_i_go/">proper discussion</a> of an article or for doing automated <a href="https://web.archive.org/web/20120525235200/http://blogs.nature.com/mfenner/2009/08/15/plos-one-interview-with-peter-binfield">article-level metrics</a>.</p>
<h3 id="aggregation">Aggregation</h3>
<p>Aggregation can help solve this problem. <a href="https://web.archive.org/web/20120525235200/http://www.researchblogging.org/">ResearchBlogging</a> aggregates blog posts about peer-reviewed research. <a href="https://web.archive.org/web/20120525235200/http://scienceseeker.org/">ScienceSeeker</a> aggregates all science blog posts (currently aggregating over 400 blogs) and was <a href="https://web.archive.org/web/20120525235200/http://scienceblogging.org/2011/01/15/introducing-scienceseeker/">announced</a> in February. <a href="https://web.archive.org/web/20120525235200/http://blogs.nature.com/">Nature Blogs</a> also aggregates science blogs, but doesn’t seem to be up-to-date.</p>
<h3 id="microformats">Microformats</h3>
<p>Microformats are an alternative – but of course complimentary – strategy. Microformats are small snippets of HTML that represent commonly published things. A good example is <a href="https://web.archive.org/web/20120525235200/http://microformats.org/wiki/rel-license">Rel-License</a>, a microformat indicating licensed content:</p>
<pre><code>&lt;a href=&quot;http://creativecommons.org/licenses/by/2.0/&quot; rel=&quot;license&quot;&gt;cc by 2.0&lt;/a&gt;</code></pre>
<p>In February Google <a href="https://web.archive.org/web/20120525235200/http://microformats.org/2011/02/24/google-launches-microformat-powered-recipe-search">launched</a> a new Recipe view feature based on the <a href="https://web.archive.org/web/20120525235200/http://microformats.org/wiki/hrecipe">hRecipe</a> microformat, demonstrating how microformats can help discovering content. There is currently no <a href="https://web.archive.org/web/20120525235200/http://blogs.ch.cam.ac.uk/pmr/2011/03/14/scholarly-html-%E2%80%93-major-progress/">standard microformat for scholarly citations</a>. The simplest format would again use the <strong><strong>rel</strong></strong> tag – together with the Citation Typing Ontology (<a href="https://web.archive.org/web/20120525235200/http://blogs.plos.org/mfenner/2011/02/14/how-to-use-citation-typing-ontology-cito-in-your-blog-posts/">CiTO</a>):</p>
<pre><code>&lt;a href=&quot;http://dx.doi.org/10.1126/science.1197258&quot; rel=&quot;cito:discusses&quot;&gt;this paper&lt;/a&gt;</code></pre>
<p>There are <a href="https://web.archive.org/web/20120525235200/http://www.jbiomedsem.com/content/1/S1/S6/table/T1">more than 20</a> CiTO tags for describing what we think about a particular paper or science blog post – many science bloggers would probably have used <em><em>cito:critiques</em></em> for the above paper. I suggest <em><em>cito:discusses</em></em> as the standard CiTO relationship for most papers and blog posts. You can add this tag manually, or use a tool such as the <a href="https://web.archive.org/web/20120525235200/http://wordpress.org/extend/plugins/link-to-link/">Link to Link</a> WordPress plugin (I added <em><em>cito:discusses</em></em> to version 1.1.2).</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Papers 2: the reference manager made with love]]></title>
        <id>rfe2s3f-178jj92-a15zbxw-8sg8</id>
        <link href="https://blog.front-matter.io/mfenner/papers-2-the-reference-manager-made-with-love"/>
        <updated>2011-03-08T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The reference management software Papers has been a regular topic on this blog. I wrote about Papers in one of my first blog posts in May 2008, interviewed main developer Alex Griekspoor in October 2008,...]]></summary>
        <content type="html"><![CDATA[<p>The reference management software <a href="https://web.archive.org/web/20120525044214/http://www.mekentosj.com/papers/">Papers</a> has been a regular topic on this blog. I wrote about Papers in one of my first blog posts in May 2008, <a href="https://web.archive.org/web/20120525044214/http://blogs.plos.org/mfenner/2008/10/03/interview_with_alexander_griekspoor/">interviewed</a> main developer Alex Griekspoor in October 2008, and held a <a href="https://web.archive.org/web/20120525044214/http://blogs.plos.org/mfenner/2009/02/19/papers_for_iphone_released_time_for_more_poetry/">poetry contest</a> when Papers for iPhone was released in February 2009 (Stephen Curry won the first price for <a href="https://web.archive.org/web/20120525044214/http://blogs.plos.org/mfenner/2009/02/19/papers_for_iphone_released_time_for_more_poetry/#comment-898">this poem</a>).</p>
<figure>
<img src="https://web.archive.org/web/20120525044214im_/http://blogs.plos.org/mfenner/files/2011/03/Papers-2c.jpg" title="Papers 2c" class="kg-image" />
</figure>
<p>Today Papers 2 <a href="https://web.archive.org/web/20120525044214/http://news.mekentosj.com/">was released</a>, and with this major update Papers has grown into a full-fledged reference manager for the Macintosh (there is no Windows or Linux version). Papers 1 was released in 2007 as a tool to manage the PDF files of scholarly papers on your hard drive. Papers 1 did this job very well, but it was not a reference manager in the strict sense, because you couldn’t use Papers to insert citations into the manuscripts you were writing.</p>
<p>In order to add this feature, Papers first had to be completely rewritten to not only handle journal articles and the associated PDF files, but also all the other common reference formats – conference proceeding, book chapter, website, etc. A nice side effect that I haven’t fully explored yet is that Papers can now also store all your Powerpoint or Keynote presentations (but fulltext search in these presentations doesn’t seem to work yet).</p>
<p>Inserting references into manuscripts is done using a floating window activated from the menu bar or with a shortcut:</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/11th/papers2-500x475.jpeg" title="papers2" class="kg-image" width="500" height="475" />
</figure>
<p>This feature works similar to the plugins used by Endnote, Mendeley, Zotero, etc., but in typical Papers fashion the implementation is much nicer. The plugin is available to all applications, e.g. other word processors besides Microsoft Word, your blogging software or your email program.</p>
<p>The other big limitation of Papers 1 wasn’t so obvious when Papers originally launched in 2007. But today most major reference managers allow users to <a href="https://web.archive.org/web/20120525044214/http://blogs.plos.org/mfenner/reference-manager-overview/">share their references</a> in private and/or public groups, as most research is done in groups and most manuscripts are written by multiple authors.</p>
<p>The sharing feature of Papers 2 is called <strong><strong>Livfe</strong></strong>. It is not as complete as the sharing features of some other reference managers (e.g. CiteULike or Mendeley). It is for example not yet possible to upload a complete Papers library to Livfe. And Livfe doesn’t have a web interface, but only works through Papers.</p>
<figure>
<img src="https://web.archive.org/web/20120525044214im_/http://blogs.plos.org/mfenner/files/2011/03/Papers-2a-500x243.jpg" title="Papers 2a" class="kg-image" />
</figure>
<p>The difference between Papers 2 and other reference managers is not so much the feature set. What sets Papers really apart is the love that went into the design of the program. Papers doesn’t just get the job done, it allows you to have fun searching for references or reading – and now also writing – a paper. This makes Papers 2 a strong competitor against some of the other established programs that for example have more than 25 years of experience with reference management (Endnote), think that reference management should be done as Open Source software (Zotero), do everything in the browser (Refworks), or see reference management primarily as a social application (Mendeley).</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Trouble with Bibliographies]]></title>
        <id>5wn0ckd-pxz85vv-swe7315-9zg4j</id>
        <link href="https://blog.front-matter.io/mfenner/the-trouble-with-bibliographies"/>
        <updated>2011-03-07T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The bibliography of a scholarly paper is interesting and important reading material. You can see whether the authors have cited the relevant literature, and you often find references to interesting papers you didn’t know about....]]></summary>
        <content type="html"><![CDATA[<p>The bibliography of a scholarly paper is interesting and important reading material. You can see whether the authors have cited the relevant literature, and you often find references to interesting papers you didn’t know about. Bibliographies are obviously also needed to count citations, and then do all kinds of useful and not so useful things with them.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/13th/5124103273_968a3c50cc.jpeg" class="kg-image" width="319" height="500" alt="Bibliography by quinn.anya at Flickr." /><figcaption aria-hidden="true"><em><em>Bibliography by quinn.anya at Flickr.</em></em></figcaption>
</figure>
<p>Unfortunately almost all bibliographies are in the wrong format. What you want is at least a direct link to the cited work using the DOI (if available), and a lot of journals do that. You don’t want to have a link to PubMed using the PubMed ID as the only option (as in PubMed Central), as this requires a few more mouse clicks to get to the full-text article. And you don’t want to go to an extra page, then use a link to search the PubMed database, and then use a few more mouse clicks to get to the full-text article (something that could happen to you with a PLoS journal).</p>
<p>A bibliography should really be made available in a downloadable format such as BibTeX. Unfortunately journal publishers – including Open Access publishers – in most cases don’t see that they can provide a lot of value here without too much extra work. One of the few publishers offering this service is BioMed Central – feel free to mention other journals that do the same in the comments.</p>
<p>This weekend <a href="https://web.archive.org/web/20120610124417/http://blogs.ch.cam.ac.uk/pmr/2011/03/04/scholarly-html-hackfest-and-visit-of-peter-sefton-and-martin-fenner/">Peter Murray-Rust invited Peter Sefton and me to Cambridge</a> (UK) for a very interesting workshop about <strong><strong>Scholarly HTML</strong></strong>. Our goal is to discuss how we can define standards and build tools to make HTML the best platform for scholars and scholarly works. The event is in fact a hackfest, and we hope to have something to show by Sunday evening.</p>
<p>My idea for the hackfest is a tool that extracts all links (references and weblinks) out of a HTML document (or URL) and creates a bibliography. The generated bibliography should be both in HTML (using the <a href="https://web.archive.org/web/20120610124417/http://blogs.plos.org/mfenner/2010/09/24/citation-style-language-an-interview-with-rintze-zelle-and-ian-mulvany/">Citation Style Language</a> ) and BibTex formats, and should ideally also support the Citation Typing Ontology (<a href="https://web.archive.org/web/20120610124417/http://blogs.plos.org/mfenner/2011/02/14/how-to-use-citation-typing-ontology-cito-in-your-blog-posts/">CiTO</a>) and <a href="https://web.archive.org/web/20120610124417/http://ocoins.info/">COinS</a> -  a standard to embed bibliographic metadata in HTML. I will use PHP as a programming language and will try to build both a generic tool and something that can work as a WordPress plugin. Obviously I will not start from scratch, but will reuse several already existing libraries. Any feedback or help for this project is much appreciated.</p>
<p>If I had a tool with which I could create my own bibliographies (and in the formats I want), I would no longer care so much about journals not offering this service. One big problem would still persist, and that is that most subscription journals wouldn’t allow the redistribution of the bibliographies to their papers. A single citation can’t have a copyright, but a compilation of citations can. I’m sure we will also discuss this topic at the workshop, as Peter Murray-Rust is one of the biggest proponents of <a href="https://web.archive.org/web/20120610124417/http://openbiblio.net/principles/">Open Bibliographic Data</a>.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to use Citation Typing Ontology (CiTO) in your blog posts]]></title>
        <id>2n46cam-p8c9ffa-yw4md3q-y0re4</id>
        <link href="https://blog.front-matter.io/mfenner/how-to-use-citation-typing-ontology-cito-in-your-blog-posts"/>
        <updated>2011-02-14T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[One of the annoyances with bibliographies as we use them for scholarly papers is that is usually unclear why a particular paper was cited. It is often possible for readers to gather this information by looking at the citation in the context of the surrounding text,...]]></summary>
        <content type="html"><![CDATA[<p>One of the annoyances with bibliographies as we use them for scholarly papers is that is usually unclear why a particular paper was cited. It is often possible for readers to gather this information by looking at the citation in the context of the surrounding text, but this is very difficult to automate. A highly cited paper might contain a method that everybody uses, might be a review, or it might contain information that everybody disagrees with. David Shotton has thought a lot about this problem and has come up with <a href="https://web.archive.org/web/20120611093455/http://dx.doi.org/10.1186/2041-1480-1-S1-S6">CiTO</a>, the Citation Typing Ontology:</p>
<blockquote>
CiTO, the Citation Typing Ontology, is an ontology for describing the nature of reference citations in scientific research articles and other scholarly works, both to other such publications and also to Web information resources, and for publishing these descriptions on the Semantic Web.
</blockquote>
<p>Using CiTO obviously means extra work for the author, so for widespread use it is very important that CiTO is as easy to use as possible. The first step would be to reduce the number of possible relationships to a manageable number, e.g. not more than ten (CiTO defines more than 20 relationships). Following a dinner discussion at the <a href="https://web.archive.org/web/20120611093455/http://blogs.plos.org/mfenner/2010/11/06/beyond-the-pdf-it-is-time-for-a-workshop/">Beyond the PDF</a> workshop, David Shotton kindly provided 10 popular CiTO relationships to Alex Wade from Microsoft Research and me. I made three little changes to the list: added “cites” as the default generic relationship, dropped “shares authors with”, as this can be done better with <a href="https://web.archive.org/web/20120611093455/http://blogs.plos.org/mfenner/2010/09/07/orcid-as-unique-author-identifier-what-is-it-good-for-and-should-we-worry-or-be-happy/">unique author identifiers</a>, and added “disagrees with” to have at least one relationship that expresses disagreement.</p>
<p>In the next step I added these relationships to my <a href="https://web.archive.org/web/20120611093455/http://blogs.plos.org/mfenner/2011/01/11/having-fun-with-citations-at-scienceonline2011/">Link to Link</a> WordPress plugin, and I released the updated version (1.1) today. Using CiTO is an option that can be turned off, but the plugin makes it very easy to use CiTO relationships when inserting references into a blog post:</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/12th/cito-500x433.jpeg" title="cito" class="kg-image" width="500" height="433" />
</figure>
<p>The CiTO relationship is stored in the <strong><strong>rel</strong></strong> attribute of the link that is created – currently as free-form text, but this can be changed to the <strong><strong>cito:DisagreesWith</strong></strong> format. This information can easily be extracted by computers, or made available in the bibliography to readers. The Reference Manager CiteULike is <a href="https://web.archive.org/web/20120611093455/http://opencitations.wordpress.com/2010/10/21/use-of-cito-in-citeulike/">also supporting CiTO</a>, but we need many more CiTO tools for authors.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OPDS: RSS for ePub or how to distribute ePub files]]></title>
        <id>5jz442n-0rz9rjs-9x7nxnd-cg62d</id>
        <link href="https://blog.front-matter.io/mfenner/opds-rss-for-epub-or-how-to-distribute-epub-files"/>
        <updated>2011-02-06T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[ePub is a great format for scholarly content, and there are a number of tools to create ePub files. But creating content is only half the story, at least as important is an easy mechanism for distribution. This is particularly true if your ePub files are not books,...]]></summary>
        <content type="html"><![CDATA[<p>ePub is a great <a href="https://web.archive.org/web/20120611043846/http://blogs.plos.org/mfenner/2011/01/23/beyond-the-pdf-%E2%80%A6-is-epub/">format for scholarly content</a>, and there are a <a href="https://web.archive.org/web/20120611043846/http://blogs.plos.org/mfenner/2011/02/01/epub-wordpress-plugin-released-today/">number of tools</a> to create ePub files. But creating content is only half the story, at least as important is an easy mechanism for distribution. This is particularly true if your ePub files are not books, but shorter pieces of content: journal articles, blog posts or even output from your ongoing research. <a href="https://web.archive.org/web/20120611043846/http://en.wikipedia.org/wiki/RSS">RSS</a> or the related Atom protocol has of course become the standard mechanism to publish frequently updated works.</p>
<p>Open Publication Syndication System (<a href="https://web.archive.org/web/20120611043846/http://code.google.com/p/openpub/wiki/CatalogSpecDraft#Introduction">OPDS</a>) is a syndication format for electronic publications based on Atom. OPDS is a relatively new format, but is supported by a growing number of (book) publishers, ePub tools and readers. If you want to try OPDS for yourself, pick one the feeds listed <a href="https://web.archive.org/web/20120611043846/http://code.google.com/p/openpub/wiki/AvailableFeeds">here</a> and add the feed as a Book source in the Stanza reader (in the <strong><strong>Shared</strong></strong> section). Alternatively, install the <a href="https://web.archive.org/web/20120611043846/http://www.epubread.com/de/manual.php">EPUBReader plugin</a> for Firefox. As you can see, OPDS does more than just list titles with a summary and image, you can also categorize the content by author (or tag), and provide a search interface.</p>
<figure>
<img src="https://web.archive.org/web/20120611043846im_/http://blogs.plos.org/mfenner/files/2011/02/OPDS-500x371.jpg" title="OPDS" class="kg-image" />
</figure>
<p>I’m looking forward to the first scholarly publisher that not only provides ePub files of his journal articles, but also makes them available via OPDS. Although OPDS is currently used mostly for electronic books, I think that this is a very interesting protocol for scholarly publishers. Did I mention that OPDS also provides facilities for buying or lending content? And that you probably shouldn’t expect OPDS support in the Apple iBooks application for iPhone and iPad anytime soon as they don’t seem to like distributed content delivery? I would like to see scholarly publishers providing their journal content via ePub and OPDS that can be consumed with one of the many available readers, rather than everybody creating his own little app that only works with a single publisher on a single platform.</p>
<p>OPDS is a relatively simple protocol and similar to RSS should make it easy for everybody to provide his content in catalog form. It should be straightforward to add OPDS support to the WordPress <a href="https://web.archive.org/web/20120611043846/http://wordpress.org/extend/plugins/epub-export/">ePub plugin</a> that I released last week, thus making a blog (or blog network) available directly in the ePub reader. While I expect that we will continue to read most blogs via the Web or RSS reader, ePub might be the better format for longer posts and the posts you want to store on your computer.</p>
<p>ePub is a very interesting format for packaging research objects, e.g. the description and data files from an experiment. OPDS would be very helpful as a distribution mechanism for these ePubs, and also works on the small scale of a research group. You can for example use the Calibre eBook management tool – which can create not only ePub files but also OPDS catalogs – together with Dropbox to <a href="https://web.archive.org/web/20120611043846/http://dearauthor.com/wordpress/2010/02/14/create-your-own-cloud-of-ebooks-with-calibre-calibre-opds-dropbox/">share your content</a>. The <a href="https://web.archive.org/web/20120611043846/http://wiki.mobileread.com/wiki/Calibre2opds">calibre2opds</a> tool also creates an OPDS catalog from the Calibre metadata database but doesn’t require Calibre to run in order to use the OPDS catalog. And the <a href="https://web.archive.org/web/20120611043846/http://www.pincette.biz/index.xhtml">Pincette</a> document management system automatically creates an OPDS catalog for the ePub files dropped into a Pincette folder. I hope that more tools for researchers start to support not only ePub but also OPDS. An OPDS interface to the data produced by your favorite piece of lab equipment (gel documentation system, microarray reader, etc.) would be an interesting sight.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discussing WordPress for Scientists]]></title>
        <id>10cca3m-fc99e0a-18a1dw1-77k4n</id>
        <link href="https://blog.front-matter.io/mfenner/discussing-wordpress-for-scientists"/>
        <updated>2011-02-04T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Regular readers of this blog know about my current interest in WordPress as a tool to create scholarly content. In the last few weeks I have released several WordPress plugins for reference management and to create ePub files....]]></summary>
        <content type="html"><![CDATA[<p>Regular readers of this blog know about <a href="https://web.archive.org/web/20120628134839/http://blogs.plos.org/mfenner/tag/wordpress/">my current interest in WordPress</a> as a tool to create scholarly content. In the last few weeks I have released several WordPress plugins for <a href="https://web.archive.org/web/20120628134839/http://wordpress.org/extend/plugins/bibtex-importer/">reference management</a> and to <a href="https://web.archive.org/web/20120628134839/http://wordpress.org/extend/plugins/epub-export/">create ePub</a> files. Obviously I’m not the only person having this idea. Some interesting projects are:</p>
<ul>
<li><a href="https://web.archive.org/web/20120628134839/http://knowledgeblog.org/">Knowledge Blog</a> – a JISC-funded project for light-weight scientific publishing</li>
<li><a href="https://web.archive.org/web/20120628134839/http://wiki.code4lib.org/index.php/Code4Lib_Journal_WordPress_Input_Guidelines">Code4Lib</a> – a journal that provides practical solutions for technologists working in libraries, using WordPress as publishing platform</li>
<li><a href="https://web.archive.org/web/20120628134839/http://ptsefton.com/2010/03/30/my-obsession-with-wordpress.htm">My obsession with WordPress</a> – Peter Sefton’s thoughts on WordPress as a scholarly publishing platform</li>
<li><a href="https://web.archive.org/web/20120628134839/http://wordpress.org/extend/plugins/mendeley-related-research/">Mendeley Related Research</a> – a plugin that finds academic research related to your blog posts</li>
<li><a href="https://web.archive.org/web/20120628134839/http://www.youtube.com/watch?v=twxSOGGQV84&amp;feature=player_embedded">Annotem</a> – an open-source journal authoring and publishing platform based on WordPress</li>
</ul>
<p>WordPress is by far the most popular blogging platform, and many science bloggers (including us here at PLoS Blogs) use WordPress. And science bloggers have some specific requirements, e.g. easy to use tools for linking to scholarly papers or aggregators of blog posts about a particular paper (<a href="https://web.archive.org/web/20120628134839/http://www.researchblogging.org/">ResearchBlogging</a>) or science blogging in general (<a href="https://web.archive.org/web/20120628134839/http://scienceblogging.org/2011/01/15/introducing-scienceseeker/">ScienceSeeker</a>). A number of people use WordPress as a lab notebook (e.g. <a href="https://web.archive.org/web/20120628134839/http://www.carlboettiger.info/">Carl Boettinger</a>). There is no clear difference between WordPress as a scholarly writing tool and WordPress as a blogging tool, and I expect that the amount of scholarly writing done with WordPress will only increase.</p>
<p>Two days ago Ed Yong <a href="https://web.archive.org/web/20120628134839/http://blogs.discovermagazine.com/notrocketscience/2011/02/02/research-into-reprogrammed-stem-cells-an-interactive-timeline/">published an interactive timeline</a> of research into reprogrammed stem cells. John Rennie yesterday cited Ed Yong and this post as a wonderful example of the <a href="https://web.archive.org/web/20120628134839/http://blogs.plos.org/retort/2011/02/03/why-ed-yong-is-the-future-of-science-news-and-you-could-be-too/">future of science news</a>. The future of science news depends on many things (not least brilliant writers such as Ed and John), but I think we also need better tools to make science writing fun and exciting.</p>
<p>In the hope that this will improve WordPress as a science writing platform, Mark Hahnel from the <a href="https://web.archive.org/web/20120628134839/http://www.science3point0.com/">Science 3.0</a> blogging network and me today created the <a href="https://web.archive.org/web/20120628134839/https://groups.google.com/forum/?hl=en#!forum/wordpress-for-scientists">WordPress for Scientists</a> Google Group. We invite developers who are working on scholarly plugins and themes for WordPress to join this group, but also researchers that need specific WordPress tools (e.g. to hook up their lab equiment to WordPress), and science bloggers that have cool ideas on how to improve WordPress for their needs.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ePub WordPress plugin released today]]></title>
        <id>4dn3cpk-kp48cca-atg6rc0-dxjkg</id>
        <link href="https://blog.front-matter.io/mfenner/epub-wordpress-plugin-released-today"/>
        <updated>2011-02-01T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The Beyond the PDF workshop took place a little over a week ago. One take-home message for me was that ePub is a very interesting document format for scholarly publishing and has several advantages over PDF. The workshop had a wonderful spirit <em><em>to do something</em></em>,...]]></summary>
        <content type="html"><![CDATA[<p>The <a href="https://web.archive.org/web/20120628132920/https://sites.google.com/site/beyondthepdf/">Beyond the PDF</a> workshop took place a little over a week ago. One take-home message for me was that <a href="https://web.archive.org/web/20120628132920/http://blogs.plos.org/mfenner/2011/01/23/beyond-the-pdf-%E2%80%A6-is-epub">ePub is a very interesting</a> document format for scholarly publishing and has several advantages over PDF. The workshop had a wonderful spirit <em><em>to do something</em></em>, and in this spirit I wrote a WordPress plugin that automatically creates ePub files from blog posts. The plugin was <a href="https://web.archive.org/web/20120628132920/http://wordpress.org/extend/plugins/epub-export">released today</a>, and can be installed directly from your WordPress installation. A sample ePub can be downloaded from <a href="https://web.archive.org/web/20120628132920/http://blogs.xartrials.org/2011/01/09/embedding-adobe-illustrator-charts-in-wordpress-using-html5/">this blog post</a>, using the link at the bottom.</p>
<figure>
<img src="https://web.archive.org/web/20120628132920im_/http://farm6.static.flickr.com/5095/5406202716_43d1b632e0.jpg" class="kg-image" alt="The workshop example paper from PLoS Comp Biol, as seen on the iPad." /><figcaption aria-hidden="true"><em><em>The workshop example paper from PLoS Comp Biol, as seen on the iPad.</em></em></figcaption>
</figure>
<p>This is version 1.0 of the plugin, and there a still number of small bugs, mainly because ePub is a complex format. A big problem is page breaks, and <a href="https://web.archive.org/web/20120628132920/http://en.wikipedia.org/wiki/Widows_and_orphans">widows and orphans</a> can currently only be avoided by workarounds. You can also see in the screenshot that the shortcode wasn’t parsed for the ePub.</p>
<p>But these are minor issues that can be solved in the coming weeks. More interesting for version 1.1 is the inclusion of attachments (other than images) in the ePub. I have to do some more thinking on how to do this, especially how to handle all the possible mime types.</p>
<p>I like reading science blogs in ePub format, using either Adobe Digital Editions on the Mac or iBooks on iPad and iPhone. This works particularly well for longer posts, e.g. those lovely posts from my science writer colleagues here on PLoS Blogs. If you have access to WordPress, then this plugin is one of the easiest ways to produce content in ePub format.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nature.com iPad app released today]]></title>
        <id>7sqhfqt-3m80xrc-yz3qkgx-rz13</id>
        <link href="https://blog.front-matter.io/mfenner/nature-com-ipad-app-released-today"/>
        <updated>2011-01-24T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Today nature.com released their iPad reader application. The app gives users access to<em><em> Nature News</em></em> and abstracts from <em><em>Nature</em></em>, <em><em>Nature Genetics</em></em>, <em><em>Nature Medicine</em></em>, <em><em>Nature Biotechnology</em></em>,...]]></summary>
        <content type="html"><![CDATA[<p>Today nature.com <a href="https://web.archive.org/web/20120628132319/http://www.nature.com/press_releases/ipad.html">released</a> their iPad reader application. The app gives users access to <em><em>Nature News</em></em> and abstracts from <em><em>Nature</em></em>, <em><em>Nature Genetics</em></em>, <em><em>Nature Medicine</em></em>, <em><em>Nature Biotechnology</em></em>, <em><em>Nature Physics</em></em>, <em><em>Nature Reviews Microbiology, Nature Reviews Genetics</em></em> and <em><em>Nature Communications</em></em>. Full text access to these journals can be purchased for $69.99 per year (<em><em>Nature</em></em> costs $79.99 per year with free access to full-text articles until February 28).</p>
<p>The iPad app is in many ways similar to the nature.com iPhone app <a href="https://blog.martinfenner.org/posts/nature_com_iphone_app_in_pictures/">released last February</a>, but takes advantage of the larger screen. A <em><em>Nature News</em></em> article looks like this on the website:</p>
<figure>
<img src="https://web.archive.org/web/20120628132319im_/http://blogs.plos.org/mfenner/files/2011/01/IMG_0038-500x375.jpg" title="IMG_0038" class="kg-image" />
</figure>
<p>… and like this in the nature.com reader:</p>
<figure>
<img src="https://web.archive.org/web/20120628132319im_/http://blogs.plos.org/mfenner/files/2011/01/IMG_0037-500x375.jpg" title="IMG_0037" class="kg-image" />
</figure>
<p>Much nicer. The <strong><strong>Add to Connotea</strong></strong> button is interesting. Connotea is a social bookmarking service by nature.com, but less popular than CiteULike and Mendeley. So why is there no choice?</p>
<p>The home screen lists all sources and articles. Users can search nature.com, PubMed and arXiv, but the latter two sources are hidden behind the <strong><strong>Advanced</strong></strong> button (in the upper right corner).</p>
<figure>
<img src="https://web.archive.org/web/20120628132319im_/http://blogs.plos.org/mfenner/files/2011/01/IMG_0039-500x375.jpg" title="IMG_0039" class="kg-image" />
</figure>
<p>The saved searches and bookmarks are stored on nature.com and can be shared with the website and iPhone app. The home screen doesn’t follow iPad conventions – the sources don’t appear in a floating window when holding the iPad vertical. Journals can be added here:</p>
<figure>
<img src="https://web.archive.org/web/20120628132319im_/http://blogs.plos.org/mfenner/files/2011/01/IMG_0040-500x375.jpg" title="IMG_0040" class="kg-image" />
</figure>
<p>Journal articles look similar to <em><em>Nature News</em></em>. References open in a popup window:</p>
<figure>
<img src="https://web.archive.org/web/20120628132319im_/http://blogs.plos.org/mfenner/files/2011/01/IMG_0042-500x375.jpg" title="IMG_0042" class="kg-image" />
</figure>
<p>Figures open not in a popup, but in a separate window, and they can be resized. It is unfortunately not possible to scroll through the figures of a paper.</p>
<figure>
<img src="https://web.archive.org/web/20120628132319im_/http://blogs.plos.org/mfenner/files/2011/01/IMG_0041-500x375.jpg" title="IMG_0041" class="kg-image" />
</figure>
<p>The nature.com reader doesn’t reflow content when you hold the iPad vertically, just shows less white margin on both sides. Author names are not links, and for some content the app switches to web browser mode.</p>
<figure>
<img src="https://web.archive.org/web/20120628132319im_/http://blogs.plos.org/mfenner/files/2011/01/IMG_0043-375x500.jpg" title="IMG_0043" class="kg-image" />
</figure>
<p>Reading papers on the iPad is much more fun than on the iPhone. And it was a smart move <a href="https://web.archive.org/web/20120628132319/http://blogs.nature.com/wp/nascent/2010/02/new_naturecom_iphone_app.html">to use ePub</a> for these mobile apps instead of PDF, I think that ePub <a href="https://web.archive.org/web/20120628132319/http://blogs.plos.org/mfenner/2011/01/23/beyond-the-pdf-%E2%80%A6-is-epub/">has a bright future</a> for scholarly content. This is version 1.0, I’m sure future versions will do further improve the reading experience thanks to HTML5. The prices for a personal subscription sounds reasonable, especially for the weekly <em><em>Nature</em></em>.</p>
<p>I have one problem with the application: the papers I read are not all published by Nature Publishing Group. I would very much prefer an ePub reader for all journal content, similar to the iPad PDF readers <strong><strong>Sente</strong></strong>, <strong><strong>Papers</strong></strong> and <strong><strong>Mendeley</strong></strong>. We already have several ePub readers for the iPad (e.g. Stanza and iBooks), but we need more journal publishers that offer their content in this format. The <a href="https://web.archive.org/web/20120628132319/http://www.nature.com/press_releases/iphone.html">press release last February</a> hinted that Nature Publishing Group will provide content in ePub format for other e-readers. Subscription journals such as the <em><em>Nature</em></em> journals have another problem, they want to charge the iPad user for reading content. This makes it more difficult for them than for open access publishers to distribute content in innovative ways, and that is probably one reason the nature.com iPad app is a closed system that can’t import, export or print articles.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond the PDF … is ePub]]></title>
        <id>15j06td-45c8jy8-v64hk98-drese</id>
        <link href="https://blog.front-matter.io/mfenner/beyond-the-pdf-is-epub"/>
        <updated>2011-01-23T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Having breakfast at the end of a conference is a good way to recap what was discussed and can help to generate new ideas. Two years ago at ScienceOnline09 a conversation with Cameron Neylon that followed up on the session Reputation, authority and incentives....]]></summary>
        <content type="html"><![CDATA[<p>Having breakfast at the end of a conference is a good way to recap what was discussed and can help to generate new ideas. Two years ago at <a href="https://web.archive.org/web/20120628140912/http://scienceonline09.com/">ScienceOnline09</a> a conversation with Cameron Neylon that followed up on the session <a href="https://web.archive.org/web/20120628140912/http://precedings.nature.com/documents/2801/version/1/html">Reputation, authority and incentives. Or: How to get rid of the Impact Factor</a> (moderated by Björn Brembs and Pete Binfield) was started my interest in unique author identifiers for researchers. An <a href="https://blog.martinfenner.org/posts/nterview_with_geoffrey_bilder/">interview with Geoff Bilder</a> about author identifiers and the CrossRef Contributor ID project followed a month later – still one of my favorite blog posts. I have since become deeply involved with author identifiers and have joined the Board of the Open Researcher &amp; Contributor ID (<a href="https://web.archive.org/web/20120628140912/http://www.orcid.org/">ORCID</a>) initiative last September.</p>
<figure>
<img src="https://web.archive.org/web/20120628140912im_/http://farm6.static.flickr.com/5010/5376544624_9914bebffa.jpg" class="kg-image" alt="Beyond the PDF group photo my lesliekwchan, on Flickr" /><figcaption aria-hidden="true"><em><em>Beyond the PDF group photo my lesliekwchan, on Flickr</em></em></figcaption>
</figure>
<p>Wednesday to Friday I attended the <a href="https://web.archive.org/web/20120628140912/https://sites.google.com/site/beyondthepdf/">Beyond the PDF</a> workshop in San Diego to discuss how we can do better in scholarly publishing. The limitations of the PDF format were just one topic, the main themes were annotation, data, provenance, new models, writing and reviewing and impact. This is my presentation:</p>
<p>We had two very productive breakout sessions about writing and reading tools, and we agreed that we should <a href="https://web.archive.org/web/20120628140912/https://sites.google.com/site/beyondthepdf/home/program-draft/notes-from-the-breakout">build something that makes it much easier to describe and distribute our research data</a>. Most people in the group took a very pragmatic approach and want to build simple tools appropriate for small research groups in the next few months. We thought that graduate students would be good early adopters, and we already have three principal investigators willing to test these tools in their labs.</p>
<figure>
<img src="https://web.archive.org/web/20120628140912im_/http://blogs.plos.org/mfenner/files/2011/01/BTPDFdiagram-500x373.jpg" title="BTPDFdiagram" class="kg-image" alt="Our first prototype, as drawn by Peter Murray Rust" /><figcaption aria-hidden="true"><em><em>Our first prototype, as drawn by Peter Murray Rust</em></em></figcaption>
</figure>
<p>Peter Sefton demonstrated his <a href="https://web.archive.org/web/20120628140912/http://fascinator-demo.com/portal/default/search">Fascinator</a> tool that already has a lot of the required functionality. But it was the breakfast discussion before departing – again with Cameron Neylon, but this time also including Peter Murray Rust, Peter Sefton and Ana Nelson – that helped me to put all my thoughts into place.</p>
<blockquote>
ePub should become the standard document format for authoring, distributing and reading scholarly content.
</blockquote>
<p>The <a href="https://web.archive.org/web/20120628140912/http://www.idpf.org/">ePub format</a> uses a collection of files held together in a zip archive. Content is displayed using a combination of XHTML and CSS – not different from web pages – and the ePub can also contain other files. Journal publishers use XML internally, and it is therefore easy to distribute journal articles in ePub format – <a href="https://web.archive.org/web/20120628140912/http://www.hindawi.com/epub.html">some of them</a> are already doing this routinely. ePub has several advantages over PDF, including:</p>
<ul>
<li>ePub can be used for all steps in the creation of a scholarly document, including data collection, authoring, annotating and peer review. There is no need for time-consuming and expensive format conversions. Currently most manuscripts are submitted in Microsoft Word or LateX formats, and then converted first to XML and then to HTML and PDF. Metadata such as author identifiers, digital object identifiers and semantic information can be added early on and don’t get lost in a format conversion.</li>
<li>ePub makes it easy to include supplementary material, e.g. video and other multimedia content, the datasets used in the publication (particularly the data used for tables and figures), all cited references in BibTeX format, etc.</li>
<li>ePub is much better suited for reading on mobile devices, as the format allows reflowing of content. Most articles today are printed from the PDF and then read, but this behavior is <a href="https://web.archive.org/web/20120628140912/http://blogs.plos.org/mfenner/2010/01/10/how_do_you_read_papers_2010_will_be_different/">rapidly changing</a>.</li>
</ul>
<p>ePub is relatively new, and not many applications for scientists already support this format. We want lab equipment that stores its data in ePub, lab notebooks that write all files from an experiment into an ePub file, reference managers that store and display papers in ePub format, authoring tools that import all these ePub files and thus make it much easier to write, annotate and submit a manuscript, and journal submission systems that take ePub files. I have <a href="https://web.archive.org/web/20120628140912/http://blogs.plos.org/mfenner/2010/12/28/wordpress-for-reference-management/">written a lot about WordPress</a> recently and this is of course a platform that would play nicely with ePub. At least two WordPress plugins <a href="https://web.archive.org/web/20120628140912/http://wordpress.org/extend/plugins/tags/epub">support ePub</a>, and it should be possible to modify them to the requirements of the scholarly paper.</p>
<figure>
<img src="https://web.archive.org/web/20120628140912im_/http://blogs.plos.org/mfenner/files/2011/01/epub-500x199.jpg" title="epub" class="kg-image" />
</figure>
<p>Almost as important as the document format is the distribution mechanism of these ePub files. We need a system that makes it easy to collaborate on a document, and that includes version control. The simplest solution would of course be centralized and web-based, but I’m not sure that this is a realistic scenario. We talked a lot about <a href="https://web.archive.org/web/20120628140912/http://www.dropbox.com/">Dropbox</a> during the meeting, but a solution using git (and <a href="https://web.archive.org/web/20120628140912/https://github.com/">github</a>), Amazon <a href="https://web.archive.org/web/20120628140912/http://aws.amazon.com/s3/">Simple Storage Service</a>, <a href="https://web.archive.org/web/20120628140912/http://explore.live.com/windows-live-skydrive">Windows Live SkyDrive</a> or the repository software ePrints or DSpace is also possible. As an ePub document can contain all required documents, the submission of a manuscript to a journal or institutional repository could become as simple as uploading a single file, and all the peer review (including reviewer comments and revisions) could be done with that file. Submissions of datasets to databases such as <a href="https://web.archive.org/web/20120628140912/http://datadryad.org/about">Dryad</a> could of course also be done using ePub files. A versioned distribution system should also make it easier to automatically get information about corrections or retractions (e.g. using the <a href="https://web.archive.org/web/20120628140912/http://www.crossref.org/crossmark.html">CrossMark</a> system that will launch in 2011), and to receive regular updates of article-level metrics, including new citations of the article or dataset.</p>
<p>Several of us attending the meeting will continue the discussion in the coming weeks, and I hope I can convince them of the advantages of ePub. It shouldn’t take us more than a month or two to produce a nice ePub of the <a href="https://web.archive.org/web/20120628140912/http://dx.doi.org/10.1371/journal.pcbi.1000976">sample PLoS Computational Biology article</a> provided for the Beyond the PDF workshop. The next <strong><strong>Science Online London Conference</strong></strong> will be September 2-3 at the British Library. This is a good opportunity to discuss the progress of this project, ideally including reports about new ePub tools for scientists, more journals using ePub for their articles, and practical feedback from the first users.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DataCite: visit their new blog]]></title>
        <id>443wgyd-rk98zzr-azejdsc-tmrqr</id>
        <link href="https://blog.front-matter.io/mfenner/datacite-visit-their-new-blog"/>
        <updated>2011-01-14T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[DataCite is an international consortium for data citation. DataCite originally started as a project at the German National Library of Science and Technology (TIB).Since 2005 the TIB was providing digital object identifiers (DOIs) to research datasets....]]></summary>
        <content type="html"><![CDATA[<p><a href="https://web.archive.org/web/20120611041541/http://datacite.org/whatisdc.html">DataCite</a> is an international consortium for data citation. DataCite originally started as a <a href="https://web.archive.org/web/20120611041541/http://www.std-doi.de/front_content.php">project</a> at the <a href="https://web.archive.org/web/20120611041541/http://www.tib-hannover.de/">German National Library of Science and Technology (TIB)</a>.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/datacite-screen-225.jpeg" title="datacite" class="kg-image" width="225" height="258" />
</figure>
<p>Since 2005 the TIB was providing digital object identifiers (DOIs) to research datasets. In December 2009 research libraries and technical information centres from 6 countries founded the DataCite initiative. In December 2010 DataCite reach an important milestone by registering the 1,000,000th DOI name.</p>
<p>Many people are familiar with <a href="https://web.archive.org/web/20120611041541/http://www.crossref.org/">CrossRef</a> which is providing DOIs for research papers. CrossRef is operated by Publishers International Linking Association (PILA), a non-profit organization formed by scholarly publishers in 2000. Think of DataCite as the counterpart that is providing DOIs for research data. And as research data are typically stored in data centers associated with technical information centers  - e.g. the British Library or the  Canada Institute for Scientific and Technical Information (CISTI) - and large research libraries (e.g. California Digital Library), DataCite members come from the academic community.</p>
<p>The TIB is located in Hannover, just a few miles away from Hannover Medical School where I work. In December I sat down with Jan Brase from the TIB, one of the driving forces behind DataCite and on its Board of Directors. We talked not only about DataCite, but also how DataCite could interoperate with <a href="https://web.archive.org/web/20120611041541/http://www.orcid.org/">ORCID</a>, the unique author identifier initiative (where I am a member of the Board). One of the <a href="https://web.archive.org/web/20120611041541/http://www.orcid.org/principles">aims</a> of ORCID is to create a <em><em>permanent, clear and unambiguous record of scholarly communication,</em></em> and this of course includes not only publications, but also the contribution of research datasets. Gudmundur Thorisson gave a <a href="https://web.archive.org/web/20120611041541/http://blogs.plos.org/mfenner/2010/09/07/orcid-as-unique-author-identifier-what-is-it-good-for-and-should-we-worry-or-be-happy/">nice presentation</a> about the possible integration of DataCite and ORCID in September.</p>
<p>Earlier this week, Jan presented about DataCite at the <a href="https://web.archive.org/web/20120611041541/http://www.ape2011.eu/">Academic Publishing in Europe</a> conference in Berlin. His slides are available on SlideShare, the video of his talk (like all the other presentations) will soon be available <a href="https://web.archive.org/web/20120611041541/http://river-valley.tv/">here</a>.</p>
<p>Also this week Jan started the <a href="https://web.archive.org/web/20120611041541/http://datacite.wordpress.com/">DataCite blog</a>. In his <a href="https://web.archive.org/web/20120611041541/http://datacite.wordpress.com/2011/01/13/some-more-details-on-2011/">first post</a> (after the obligatory welcome post) Jan talks about the plans DataCite has for 2011. I am particularly excited about the central metadata repository that should be up and running by June. DataCite has allowed Thomson Reuters to crawl the repository so that the metadata will also appear in the <a href="https://web.archive.org/web/20120611041541/http://thomsonreuters.com/products_services/science/science_products/a-z/web_of_science/">Web of Science</a>. A lot of interesting stuff is going to happen in 2011 around unique identifiers for researchers and their scholarly works.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Having fun with citations at ScienceOnline2011]]></title>
        <id>4t97q2s-tek82ev-wv8dmsk-xahpy</id>
        <link href="https://blog.front-matter.io/mfenner/having-fun-with-citations-at-scienceonline2011"/>
        <updated>2011-01-11T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[ScienceOnline2011, the fifth annual international meeting on Science and the Web, is only two days away. I am very excited for many reasons, most importantly because it gives me the chance to meet many online friends in person – again or for the first...]]></summary>
        <content type="html"><![CDATA[<p><a href="https://web.archive.org/web/20120611072611/http://scienceonline2011.com/">ScienceOnline2011</a>, the fifth annual international meeting on Science and the Web, is only two days away. I am very excited for many reasons, most importantly because it gives me the chance to meet many online friends in person – again or for the first time.</p>
<p>On Saturday I will help moderate two (related) sessions: <strong><strong>How is the Web changing the way we identify scientific impact</strong></strong> and <strong><strong>Having fun with citations</strong></strong>. The first session will be about alternative ways to measure scientific impact, and about the alternative types of publications we can measure. The second session is all about citations. We use them all the time in scholarly communications, but we don’t really think about them. To give a couple of examples: why are there so many different citation styles, why is it so difficult to find open bibliographic data, how can we describe why we decided to cite something, or how can we cite things that are not publications?</p>
<p>Since I suggested this session a few months ago, my thinking about citations has changed. Citations should be easy to use, wherever we need them. Even though he have a number of clever <a href="https://web.archive.org/web/20120611072611/http://blogs.plos.org/mfenner/reference-manager-overview/">reference management tools</a>, the process is still too complicated. What would really help is an easier format for references, and the best format I can think of is the ubiquitous internet link. I have <a href="https://web.archive.org/web/20120611072611/http://blogs.plos.org/mfenner/2010/12/11/citations-are-links-so-where-is-the-problem/">written about links</a> before, and I have spent the last few weeks to work on WordPress plugins to better handle links for scholarly works. Today I released the <a href="https://web.archive.org/web/20120611072611/http://wordpress.org/extend/plugins/link-to-link/">Link to Link</a> plugin:</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/screenshot-1-500x432.png" title="screenshot-1" class="kg-image" width="500" height="432" alt="Some science-related references in my WordPress Links Manager." /><figcaption aria-hidden="true"><em><em>Some science-related references in my WordPress Links Manager.</em></em></figcaption>
</figure>
<p>The Link to Link window in the WordPress editor should look familiar to users of traditional reference managers. You search for references and then insert them into the text. The references are stored in the WordPress Links Manager, and you can get them in there using the <a href="https://web.archive.org/web/20120611072611/http://wordpress.org/extend/plugins/bibtex-importer/">BibTeX Importer</a> plugin I wrote two weeks ago. Both plugins are available from the WordPress plugins directory and can be installed directly from WordPress.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New journal “Nature ONE” launched today]]></title>
        <id>1rdn800-2w188nb-005xzqt-phr4a</id>
        <link href="https://blog.front-matter.io/mfenner/new-journal-nature-one-launched-today"/>
        <updated>2011-01-06T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[It’s very unusual for me to post two blog posts in a single day, and even more so when both posts are based on press releases by the same organization. But this is important news for everybody interested in scholarly publishing.In a press release earlier today,...]]></summary>
        <content type="html"><![CDATA[<p>It’s very unusual for me to post two blog posts in a single day, and even more so when both posts are based on press releases by the same organization. But this is important news for everybody interested in scholarly publishing.</p>
<p>In a <a href="https://web.archive.org/web/20120628140907/http://www.nature.com/press_releases/scientificreports.html">press release</a> earlier today, the Nature Publishing Group announced a new journal that</p>
<ul>
<li>is covering biology, chemistry, earth sciences and physics,</li>
<li>is an open access journal, giving the authors the choice of two Creative Commons non-commercial licenses,</li>
<li>will publish all papers that are judged to be technically valid and original, and</li>
<li>uses article-level metrics to put the emphasis on the individual article rather than the journal as a whole.</li>
</ul>
<p>The new journal is called <a href="https://web.archive.org/web/20120628140907/http://www.nature.com/srep/marketing/index.html"><em><em>Scientific Reports</em></em></a>, and obviously resembles <a href="https://web.archive.org/web/20120628140907/http://www.plosone.org/"><em><em>PLoS ONE</em></em></a> in many ways, down to the article-processing charges which are $1350 for both journals (but will go up to $1700 for <em><em>Scientific Reports</em></em> in 2012). The journal is open for submissions and will publish the first papers this summer. With <em><em>Nature</em></em>, <em><em>Nature Communications</em></em> (also see my <a href="https://web.archive.org/web/20120628140907/http://blogs.plos.org/mfenner/2009/11/26/nature_communications_interview_with_lesley_anson/">interview</a> with Chief Editor Lesley Anson) and <em><em>Scientific Reports</em></em> the Nature Publishing Group now publishes three journals covering all areas of the natural sciences.</p>
<p>The launch of <em><em>Scientific Reports</em></em> is a good sign that <em><em>PLoS ONE</em></em> is doing something right. It will be interesting to watch whether the two journals will only be competitive – they are aiming for the same manuscript submissions – or will also collaborate on projects such as article-level metrics.</p>
<blockquote>
“Scholarly communication has always, will always, and should always be served by a mix of models.”
</blockquote>
<p>In another <a href="https://web.archive.org/web/20120628140907/http://www.nature.com/press_releases/statement.html">press release</a> today, David Hoole explains the position of the Nature Publishing Group on open access publishing. He argues that journals such as <em><em>Nature</em></em> with a 90% rejection rates are better served by a subscription model, and that he feels that journal submission fees (as discussed in a <a href="https://web.archive.org/web/20120628140907/http://blogs.plos.org/mfenner/2010/12/12/submission-fees-for-open-access-journals/">recent report</a>) are not an attractive option. The press release also mentions that 40% of the content of the hybrid journal <a href="https://web.archive.org/web/20120628140907/http://blogs.plos.org/mfenner/2010/12/12/submission-fees-for-open-access-journals/">Nature Communications</a> is currently open access.</p>
<p>Although I can follow the arguments made in the press release, I disagree that we need different “tiers” of journals. The <em><em>Scientific Reports</em></em> FAQ makes it clear that the journal doesn’t expect authors to submit their best works, but rather papers that require “speed of publication”, promise “no conceptual advance in the field” and show “negative results”. Let’s see how authors will respond to this and whether it will be possible 10 years from now to distinguish papers published in <em><em>Nature,</em></em> <em><em>Nature Communications</em></em> and <em><em>Scientific Reports</em></em>.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Now in your DeepDyve store: Nature papers to rent]]></title>
        <id>4bs94mt-64r9yar-dv566wf-8wfbz</id>
        <link href="https://blog.front-matter.io/mfenner/now-in-your-deepdyve-store-nature-papers-to-rent"/>
        <updated>2011-01-06T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Articles published in <em><em>Nature</em></em>, <em><em>Nature Biotechnology</em></em>, <em><em>Nature Cell Biology</em></em>, <em><em>Nature Medicine</em></em> or <em><em>Nature Chemical Biology</em></em> are now available for renting from DeepDyve....]]></summary>
        <content type="html"><![CDATA[<p>Articles published in <em><em>Nature</em></em>, <em><em>Nature Biotechnology</em></em>, <em><em>Nature Cell Biology</em></em>, <em><em>Nature Medicine</em></em> or <em><em>Nature Chemical Biology</em></em> are now available for renting from <a href="https://web.archive.org/web/20120611024706/http://www.deepdyve.com/nature">DeepDyve</a>. Downloading or printing is not possible, and the $3.99 rental is for 24 hours.</p>
<p>Later this month, Nature plans to release the nature.com reader for the iPad. Monthy access to Nature (again read-only) will cost $9.99. For more information see <a href="https://web.archive.org/web/20120611024706/http://www.nature.com/press_releases/rental.html">yesterday’s press release</a>.</p>
<p>In the discussion of subscriptions vs. author-pays for scholarly papers we sometimes forget that it’s really a question of how much we are willing to pay. I’m looking forward to the nature.com app for the iPad, and $9.99 per month seems reasonable. I would not rent a single article for $3.99 – this should either be $0.99 or give me the PDF that I can download and print, something that subscription journals typically charge $10-$30.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HTML5 or messages from beyond the PDF]]></title>
        <id>600sfvt-80b9w48-pkkbh6s-2npkn</id>
        <link href="https://blog.front-matter.io/mfenner/html5-or-messages-from-beyond-the-pdf"/>
        <updated>2011-01-05T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[In 1990 Tim Berners-Lee and others started HTML and the world wide web to facilitate scientific communications at CERN, the world’s largest particle physics laboratory.<em><em>xmos_basser by .M., on Flickr,...]]></summary>
        <content type="html"><![CDATA[<p>In 1990 Tim Berners-Lee and others started HTML and the world wide web to facilitate scientific communications at CERN, the world’s largest particle physics laboratory.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/2429771653_68e45ff431.jpeg" class="kg-image" width="452" height="500" alt="xmos_basser by .M., on Flickr, November 1993" /><figcaption aria-hidden="true"><em><em>xmos_basser by .M., on Flickr, November 1993</em></em></figcaption>
</figure>
<p>Although the world wide web profoundly changed scholarly publishing (and of course many other things), HTML did not become the standard document format for scientific papers. In fact, there is no standard document format. We have document formats for authors, for the internal workflow of publishers, and for the distribution and reading of papers.</p>
<p>There are of course many good reasons to use LaTeX for writing, XML for workflows, PDF to print papers or ePub for mobile devices. But reformatting a manuscript into different formats several times is both expensive (in terms of time and costs) and means that the formatting options used will be a compromise of what is available in all formats.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/5327592909_f00585d28e.jpeg" class="kg-image" width="500" height="350" />
</figure>
<p>All this would be much easier if we just used HTML. With HTML, authors, publishers and readers can all use the same document format. And they will have an endless number of tools at their hands, including of course <a href="https://web.archive.org/web/20120611084546/http://blogs.plos.org/mfenner/2010/12/05/blogging-beyond-the-pdf/">WordPress</a> for writing and the web browser of choice for reading. HTML in 2010 is very different from HTML in 1990. HTML5 supports new semantic elements such as &lt;article&gt;, microdata, embedding of video without plugins, geolocation, and offline web applications.</p>
<p>An HTML-based scholarly publishing workflow will make it</p>
<ul>
<li>faster and cheaper to publish a paper,</li>
<li>easier to create rich interactive documents,</li>
<li>easier to add additional steps, such as integration of data from <a href="https://web.archive.org/web/20120611084546/http://www.axiope.com/">lab notebooks</a>, <a href="https://web.archive.org/web/20120611084546/http://www.arxiv.org/">publishing of pre-prints</a>, etc.</li>
<li>easier to integrate additional services, from <a href="https://web.archive.org/web/20120611084546/http://cran.r-project.org/web/packages/googleVis/vignettes/googleVis.pdf">data visualization</a> to <a href="https://web.archive.org/web/20120611084546/http://languageediting.nature.com/editing-services">language editing</a>.</li>
</ul>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Author Identifier Overview]]></title>
        <id>42skpvv-j809bnr-pfapa2e-3przc</id>
        <link href="https://blog.front-matter.io/mfenner/author-identifier-overview"/>
        <updated>2011-01-02T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Today I posted the pre-print of a paper titled <strong><strong>Author Identifier Overview</strong></strong> that I submitted to the journal Libreas. This is the abstract:Unique identifiers for scholarly authors are still not commonly used,...]]></summary>
        <content type="html"><![CDATA[<p>Today I <a href="https://web.archive.org/web/20120611042244/http://blogs.plos.org/mfenner/author-identifier-overview/">posted the pre-print</a> of a paper titled <strong><strong>Author Identifier Overview</strong></strong> that I submitted to the journal <a href="https://web.archive.org/web/20120611042244/http://edoc.hu-berlin.de/browsing/libreas/index.php">Libreas</a>. This is the abstract:</p>
<blockquote>
Unique identifiers for scholarly authors are still not commonly used, but provide a number of benefits to authors, institutions, publishers, funding organizations and scholarly societies. This report gives an overview about some of the popular author identifier systems, and their characteristics. The report also discusses several important issues that need to be addressed by author identifier systems, namely identity, reputation and trust.
</blockquote>
<p>The paper was of course written with WordPress. I wish you all a Happy New Year.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WordPress for Reference Management]]></title>
        <id>1vhwx08-jkh8jv9-chbcxxq-czk79</id>
        <link href="https://blog.front-matter.io/mfenner/wordpress-for-reference-management"/>
        <updated>2010-12-28T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[One of the more complicated aspects of scientific writing is reference management – an important limitation of online collaborative tools such as <em><em>Google Docs</em></em>. I have argued before that WordPress has the potential to become a great scientific writing tool....]]></summary>
        <content type="html"><![CDATA[<p>One of the more complicated aspects of scientific writing is <a href="https://web.archive.org/web/20120611082245/http://blogs.plos.org/mfenner/tag/reference-management/">reference management</a> – an important limitation of online collaborative tools such as <em><em>Google Docs</em></em>. I have <a href="https://web.archive.org/web/20120611082245/http://blogs.plos.org/mfenner/2010/12/05/blogging-beyond-the-pdf/">argued before</a> that WordPress has the potential to become a great scientific writing tool. Wordpress can’t do reference management out of the box, and the <a href="https://web.archive.org/web/20120611082245/http://wordpress.org/extend/plugins/search.php?q=bibtex">available plugins</a> are somewhat limited. But WordPress is a good platform to add reference management functions: it is not only extremely popular (meaning a lot of people have expertise and many tools are available), but also already knows a lot about links, and has a wonderful plugin architecture.</p>
<h3 id="reference-database">Reference database</h3>
<p>The first question with reference management in WordPress is where to store the references. Should they not be stored at all and be directly imported from the source (journal website or bibliographic database such as PubMed)? This is what most of us are currently doing when writing blog posts, but this approach has obvious limitations on more ambitious writing projects. Or should references be inserted from an online reference manager such as CiteULike, Mendeley, Refworks or Endnote Web? This is how we use these reference managers with Microsoft Word and similar word processors.</p>
<p>Instead I prefer a third approach: store the references in the built-in Links Manager in WordPress. References are nothing more than <a href="https://web.archive.org/web/20120611082245/http://blogs.plos.org/mfenner/2010/12/11/citations-are-links-so-where-is-the-problem/">specialized links</a> after all. We loose functionality compared to reference manager databases, but we get some very interesting features for free, including <a href="https://web.archive.org/web/20120611082245/http://wordpress.org/extend/plugins/broken-link-checker/">automatic checking of broken links</a>, <a href="https://web.archive.org/web/20120611082245/http://wordpress.org/extend/plugins/blogrollsync/">automatic link synchronization</a> with other WordPress installations, and all the other cool things for links that people have come up with. And this makes it much easier for authors to collaborate if they use different reference managers, as all required references are stored in a common database.</p>
<h3 id="bibtex-importer">BibTeX Importer</h3>
<p>There is currently no good solution to important references into the Links Manager, so I wrote a WordPress plugin to do just that. It took me two days, which says less about my skills, but more about the WordPress plugin architecture. Like all plugins hosted at WordPress.org, my <a href="https://web.archive.org/web/20120611082245/http://wordpress.org/extend/plugins/bibtex-importer/">BibTeX Importer</a> plugin can be installed directly from within your WordPress installation in less than 5 minutes.</p>
<p>The plugin takes any BibTeX file (BibTeX is one of the more common file formats for references, all good reference managers can export into that format) and imports it into the WordPress Links Manager. The plugin can also import a BibTex file via URL (e.g. <a href="https://web.archive.org/web/20120611082245/http://www.citeulike.org/bibtex/user/mfenner">http://www.citeulike.org/bibtex/user/mfenner</a>).</p>
<figure>
<img src="https://web.archive.org/web/20120611082245im_/http://blogs.plos.org/mfenner/files/2010/12/screenshot-2-500x198.jpg" title="BibTeX Importer" class="kg-image" />
</figure>
<p>The plugin creates WordPress links where the link name is in the format <strong><strong>first author – year – title</strong></strong>. The original BibTeX entry is stored in the link notes. The plugin does some extra work, e.g. checks for duplicates before importing and picks the DOI if several URLs are available for the link. The plugin also checks the BibTex entry type (article, PhD thesis, book chapter, etc.) and automatically assigns a link category with that name. The next step would be a solution that automatically synchronizes your WordPress Links Manager with a reference manager.</p>
<h3 id="inserting-references">Inserting references</h3>
<p>Once the references are in the Links Manager, we can use them in the articles we write. Unfortunately WordPress doesn’t provide an easy way to do that. I personally like the WordPress TinyMCE editor, so I made some changes to the wonderful <a href="https://web.archive.org/web/20120611082245/http://wordpress.org/extend/plugins/link-to-post/">Link to Post</a> plugin. The plugin provides a searchable interface to link to the posts, pages, categories and tags of your own WordPress blog. It took me half a day to add a link section, so now I can search for links and insert them into a post in a way not that different from how we use reference managers with Microsoft Word. The plugin also searches for co-authors, journal names and words in abstracts, as all this information is stored in the Link notes as BibTeX entry. I will make the updated Link to Post plugin available for download (I have contacted the plugin author), for the time being please contact me if you are interested.</p>
<figure>
<img src="https://web.archive.org/web/20120611082245im_/http://blogs.plos.org/mfenner/files/2010/12/link2post-500x377.jpg" title="Link to Post" class="kg-image" />
</figure>
<h3 id="providing-a-bibliography">Providing a bibliography</h3>
<p>This is the last feature that I need for reference management with WordPress, and that should also be feasible to do. Ideally the bibliography should be created automatically from the links in the text, using the BibTeX info stored in the links database. In fact, this is something that a publisher could also do after manuscript submission, as the required information is all there. The bibliography should use the <a href="https://web.archive.org/web/20120611082245/http://blogs.plos.org/mfenner/2010/09/24/citation-style-language-an-interview-with-rintze-zelle-and-ian-mulvany/">Citation Style Language,</a> and <a href="https://web.archive.org/web/20120611082245/http://ocoins.info/">COinS</a> (a standard for publishing machine-readable reference information in HTML), and should check for duplicate references, broken links, etc.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Paperpile: an open source reference manager for Mac and Linux]]></title>
        <id>42dm3s2-gc784g9-45jp1gb-g44sq</id>
        <link href="https://blog.front-matter.io/mfenner/paperpile-an-open-source-reference-manager-for-mac-and-linux"/>
        <updated>2010-12-23T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Last week the first public beta (version 0.5) of Paperpile was released (available for Mac and Linux). Paperpile is a desktop reference manager with typical features: search in PubMed, Google Scholar or ArXiv, import PDF files,...]]></summary>
        <content type="html"><![CDATA[<p>Last week the first public beta (version 0.5) of <a href="https://web.archive.org/web/20120612092613/http://paperpile.com/">Paperpile</a> was released (available for Mac and Linux). Paperpile is a desktop reference manager with typical features: search in PubMed, Google Scholar or ArXiv, import PDF files, support for BibTex and other standard file formates, etc. Paperpile currently doesn’t sync with a web-based version, and Paperpile doesn’t insert citations into manuscripts.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/paperpile.jpeg" title="paperpile" class="kg-image" width="500" height="415" />
</figure>
<p>What is interesting about Paperpile is that the source code is available at <a href="https://web.archive.org/web/20120612092613/https://github.com/wash/paperpile">github</a> (using the <a href="https://web.archive.org/web/20120612092613/http://www.gnu.org/licenses/">GNU Affero General Public License</a>). Other reference managers that make their source code available include <a href="https://web.archive.org/web/20120612092613/https://www.zotero.org/trac">Zotero</a>, <a href="https://web.archive.org/web/20120612092613/http://sourceforge.net/apps/mediawiki/jabref/index.php?title=Developing_and_extending_JabRef">JabRef</a> and <a href="https://web.archive.org/web/20120612092613/http://www.connotea.org/code">Connotea</a>.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New and emerging technologies for reference software]]></title>
        <id>1nbez55-r3q85ta-jycpp0a-qaqcb</id>
        <link href="https://blog.front-matter.io/mfenner/new-and-emerging-technologies-for-reference-software"/>
        <updated>2010-12-23T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Jason Rollins gave a presentation with that topic at the recent STM Innovations Seminar in London. The video of his presentation has now been made available by River Valley TV, and his slides are here.Jason talks about some recent trends in reference management software....]]></summary>
        <content type="html"><![CDATA[<p>Jason Rollins gave a presentation with that topic at the recent <a href="https://web.archive.org/web/20120612114551/http://www.stm-assoc.org/event.php?event_id=58">STM Innovations Seminar</a> in London. The video of his presentation <a href="https://web.archive.org/web/20120612114551/http://river-valley.tv/new-and-emerging-technologies-for-reference-software/">has now been made available</a> by River Valley TV, and his slides are <a href="https://web.archive.org/web/20120612114551/http://www.stm-assoc.org/2010_12_03_Innovations_Rollins_Bibliographic_Management_Software.pdf">here</a>.</p>
<p>Jason talks about some recent trends in reference management software. For him one important new trend is APIs/extensions. I fully agree with him, but we haven’t really seen many tools based on these APIs. Reference managers that provide APIs include <a href="https://web.archive.org/web/20120612114551/http://dev.mendeley.com/">Mendeley</a>, <a href="https://web.archive.org/web/20120612114551/http://www.endnote.com/api/">Endnote</a>, Refworks, <a href="https://web.archive.org/web/20120612114551/http://developer.mekentosj.com/">Papers</a>, <a href="https://web.archive.org/web/20120612114551/http://www.zotero.org/blog/zoteros-next-big-step/">Zotero</a> and <a href="https://web.archive.org/web/20120612114551/http://www.connotea.org/wiki/WebAPI">Connotea</a>. The availability of an API has done a lot for the success of Web 2.0 tools such as Twitter and Flickr.</p>
<p>Another interesting topic was brought up in the discussion: should reference managers track difference versions of an article (e.g. pre-print, post-print, publisher PDF)? According to Jason many Endnote users request this feature. Jason is leading the <a href="https://web.archive.org/web/20120612114551/http://www.endnote.com/">Endnote</a> development team, but also mentions Mendeley, CiteULike and Papers in his presentation. I <a href="https://blog.martinfenner.org/posts/endnote_interview_with_jason_rollins/">interviewed</a> Jason about Endnote back in August.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Digital Science launched: closing the gap between science and technology?]]></title>
        <id>7yqf2h7-fy8952a-7ya86vy-7dmek</id>
        <link href="https://blog.front-matter.io/mfenner/digital-science-launched-closing-the-gap-between-science-and-technology"/>
        <updated>2010-12-18T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Two weeks ago <strong><strong>Eva Amsen</strong></strong> wrote in a thoughtful blog post:There’s a gap between science and technology, and it’s growing.Eva argues that – contrary to popular belief – there is actually a divide between science and technology....]]></summary>
        <content type="html"><![CDATA[<p>Two weeks ago <strong><strong>Eva Amsen</strong></strong> wrote in a thoughtful <a href="https://web.archive.org/web/20120612094223/http://blogs.nature.com/eva/2010/12/05/a-metaphor-for-science-and-technology">blog post</a>:</p>
<blockquote>
There’s a gap between science and technology, and it’s growing.
</blockquote>
<p>Eva argues that – contrary to popular belief – there is actually a divide between science and technology. Scientists are on average not really comfortable using technology, and many computing tools aimed for scientists really miss the point of what scientists really care about.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/3877930229_a84a550d0e.jpeg" class="kg-image" width="500" height="120" alt="Canada Science and Technology Museum logo by cstmweb, on Flickr." /><figcaption aria-hidden="true"><em><em>Canada Science and Technology Museum logo by <a href="https://web.archive.org/web/20120612094223/http://www.flickr.com/photos/cstmweb">cstmweb</a>, on Flickr.</em></em></figcaption>
</figure>
<p>Two days later, on December 7, <a href="https://web.archive.org/web/20120612094223/http://www.digital-science.com/">Digital Science</a>, a new division of Macmillan Publishing launched. From the <a href="https://web.archive.org/web/20120612094223/http://international.macmillan.com/MediaArticle.aspx?id=2598">press release</a>:</p>
<blockquote>
Digital Science will focus on providing world-class software tools and services to scientists, managers and funders with the ultimate aim of making research more productive through the use of technology.
</blockquote>
<p>Initial products include the chemical text-mining tool <a href="https://web.archive.org/web/20120612094223/http://www.surechem.org/">SureChem Portal</a>, the laboratory research management system <a href="https://web.archive.org/web/20120612094223/http://www.biodata.com/">BioData</a> and the research information system <a href="https://web.archive.org/web/20120612094223/http://www.symplectic.co.uk/products/publications.html">Symplectic Elements</a>. You might see some familiar faces when you look at the Digital Science <a href="https://web.archive.org/web/20120612094223/http://www.digital-science.com/meet-the-team/">team pictures</a>.</p>
<p>As a big fan of using technology for science I am looking forward to what Digital Science will do in the coming months. I am particularly interested in their answer to the questions asked by Eva. Have they found a better way to understand what technologies scientists really want? Or are tools for digital science something that the majority of scientists don’t really care about?</p>
<p>Related posts:</p>
<ul>
<li>Cameron Neylon: <a href="https://web.archive.org/web/20120612094223/http://cameronneylon.net/blog/macmillan-do-interesting-stuff/">Macmillan do interesting stuff</a></li>
<li>Benjamin Good: <a href="https://web.archive.org/web/20120612094223/http://i9606.blogspot.com/2010/12/digital-science-launches.html">Digital Science launches</a></li>
<li>John Dupuis: <a href="https://web.archive.org/web/20120612094223/http://scienceblogs.com/confessions/2010/12/open_digital_research_science.php">Open Science Digital Computation Research</a></li>
</ul>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Occam’s Typewriter: please welcome a new science blogging network]]></title>
        <id>2ht6re1-red9y7s-prp92cf-fqeg8</id>
        <link href="https://blog.front-matter.io/mfenner/occams-typewriter-please-welcome-a-new-science-blogging-network"/>
        <updated>2010-12-13T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Last Friday the latest science blogging network officially launched: Occam’s Typewriter. The independent blogging network started out with eight bloggers and one guest blog, all of them well characterized by Bob O’Hara. Most of the bloggers have moved their blog from Nature Network,...]]></summary>
        <content type="html"><![CDATA[<p>Last Friday the latest science blogging network officially launched: <a href="https://web.archive.org/web/20120612092936/http://occamstypewriter.org/">Occam’s Typewriter</a>. The independent blogging network started out with eight bloggers and one guest blog, <a href="https://web.archive.org/web/20120612092936/http://scientopia.org/blogs/thisscientificlife/2010/12/12/occams-typewriter-here-at-last/">all of them well characterized</a> by Bob O’Hara. Most of the bloggers have moved their blog from <a href="https://web.archive.org/web/20120612092936/http://network.nature.com/">Nature Network</a>, where I wrote next to them from 2007 until September this year.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/occam.png" title="occam" class="kg-image" width="500" height="105" />
</figure>
<p>Richard, Jenny, Stephen, Austin, Frank, Erika, Cath and Henry, I wish you all good luck with the new blogging network.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Citations are links, so where is the problem?]]></title>
        <id>6xdc7pe-kty8vwa-jyv16yz-br8m7</id>
        <link href="https://blog.front-matter.io/mfenner/citations-are-links-so-where-is-the-problem"/>
        <updated>2010-12-11T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Citations are a fundamental concept of scholarly works. Unfortunately they are also difficult to do. Traditional writing tools such as Microsoft Word can’t really handle them in a way that is appropriate for a scientific manuscript,...]]></summary>
        <content type="html"><![CDATA[<p>Citations are a fundamental concept of scholarly works. Unfortunately they are also difficult to do. Traditional writing tools such as Microsoft Word can’t really handle them in a way that is appropriate for a scientific manuscript, and that is why we have reference managers such as Endnote, Zotero or Mendeley. And the lack of this functionality is a major reason that Google Docs and other online collaborative writing tools haven’t become popular for writing scholarly works.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/11th/4363633528_5026a5ce8b.jpeg" class="kg-image" width="500" height="333" alt="1958 Edsel Citation by Roadsidepictures, on Flickr." /><figcaption aria-hidden="true"><em><em>1958 Edsel Citation by <a href="https://web.archive.org/web/20120611074314/http://www.flickr.com/photos/roadsidepictures/">Roadsidepictures</a>, on Flickr.</em></em></figcaption>
</figure>
<p>Using citations is hard for paper authors. The process is still complicated when using a reference manager, and it remains one of the more time consuming aspects of writing a manuscript. The main reason is that something always seems to go wrong with the formatting of the bibliography, but there are also issues of wrong or duplicate citations (including <a href="https://web.archive.org/web/20120611074314/http://www.the-scientist.com/news/display/57698/">citation mutations</a>), correct citation styles, etc. I can’t comment on how well BibTeX integrates citation management into LaTeX, but the main issue seems to be that citations usually are not one of the core functions of the writing tool.</p>
<h3 id="wordpress-and-reference-managers">WordPress and reference managers</h3>
<p>The blogging platform <strong><strong>WordPress</strong></strong> could <a href="https://web.archive.org/web/20120611074314/http://blogs.plos.org/mfenner/2010/12/05/blogging-beyond-the-pdf/">become an excellent authoring platform</a> for scientific papers. But to become successful, WordPress has to handle scholarly citations, and not just with copy and paste. Carl Boettinger <a href="https://web.archive.org/web/20120611074314/http://www.carlboettiger.info/archives/570">has written about doing citations in WordPress</a> ealier this week and there is also an <a href="https://web.archive.org/web/20120611074314/http://friendfeed.com/science-2-0/deef8494/how-do-you-manage-citations-when-writing-on-web">ongoing FriendFeed discussion</a>. I have also looked at the available plugins, in particular <a href="https://web.archive.org/web/20120611074314/http://wordpress.org/extend/plugins/papercite/">papercite</a> (based on <a href="https://web.archive.org/web/20120611074314/http://wordpress.org/extend/plugins/bib2html/">bib2html</a>) which uses the BibTex format and is giving me some problems. I can’t get the <a href="https://web.archive.org/web/20120611074314/http://labs.crossref.org/site/blog_plugins.html">CrossRef Citation plugin</a> to work (SyntaxError: Parse error) and the <a href="https://web.archive.org/web/20120611074314/http://wordpress.org/extend/plugins/mendeleyplugin/">Mendeley Plugin</a> is displaying bibliographies, rather than inserting citations. There is currently probably no easy solution to cite scholarly works in WordPress and I don’t think that creating a WordPress Plugin for one of the reference managers is the right approach.</p>
<h3 id="citations-are-links">Citations are links</h3>
<p>If we think about it, citations are nothing more than specialized links that contain additional information and formatting. And the references section is a list of footnotes. Links are a genuine part of WordPress, and this system should therefore also be used when writing scholarly works with WordPress. A Citation Plugin should extend this system, and solve these issues:</p>
<ul>
<li>WordPress isn’t very smart about footnotes. I use the <a href="https://web.archive.org/web/20120611074314/http://www.elvery.net/drzax/wordpress-footnotes-plugin">WP-Footnotes</a> Plugin, but we need additional functionality: avoid duplicates, formatting options of in-text citations (e.g. range of citations or author-year) and sorting of footnotes by occurence or name.</li>
<li>The tool to create links in articles is not really integrated with the WordPress Links system (in contrast to images, where you have access to the media gallery when inserting an image).</li>
</ul>
<p>Both of  these issues can be solved, especially since they are not specific to scholarly works and could be tackled by thousands of WordPress developers out there.</p>
<p>We don’t want to use WordPress as a reference manager, as there are already many tools out there that can do this job much better. We rather want reference manager integration with WordPress, and the easiest way to do this would be an automatic synchronization with the WordPress Links database. We can already do this with the social bookmarking tool <strong><strong>delicious</strong></strong> (I use <a href="https://web.archive.org/web/20120611074314/http://wordpress.org/extend/plugins/wp-deliciouslinks/">DeliciousLinkSync</a>), so it shouldn’t be difficult to do this with the social bookmarking tools for scientists such as <strong><strong>CiteULike</strong></strong> (you can <a href="https://web.archive.org/web/20120611074314/http://blog.citeulike.org/?p=174">sync CiteULike with delicious</a>, a workaround I currently use), <strong><strong>Connotea</strong></strong> or <strong><strong>BibSonomy</strong></strong>.</p>
<h3 id="links-are-powerful">Links are powerful</h3>
<p>Using the WordPress Links system makes it very easy to extend the core functionality, and many interesting tools are already out there. A good example is the <a href="https://web.archive.org/web/20120611074314/http://wordpress.org/extend/plugins/broken-link-checker/">Broken Link Checker</a>. The Plugin can regularly check the links in your blog posts, but could also be used to check DOIs for references in a semi-automated way. The Broken Link Checker found 30 broken links in the <a href="https://web.archive.org/web/20120611074314/http://blogs.xartrials.org/2010/12/05/the-mycobacterium-tuberculosis-drugome-and-its-polypharmacological-implications-2/">Blogging Beyond the PDF sample article</a> (all my fault), and automatically changed the display style for them.</p>
<p>And there is so much more that can be done with links. I am particularly interested in adding meaning to links using the Citation Typing Ontology (<a href="https://web.archive.org/web/20120611074314/http://dx.doi.org/10.1186/2041-1480-1-S1-S6">CiTO</a>). And I want to be able to cite specific parts of an article. Dave Winer has <a href="https://web.archive.org/web/20120611074314/http://scripting.com/stories/2010/10/21/newBloggingTechniques.html">introduced</a> paragraph-level permalinks to blogs, and I can do this on WordPress using the <a href="https://web.archive.org/web/20120611074314/http://danielbachhuber.com/2010/10/27/winerlinks-v0-2-released/">WinerLinks Plugin</a>. The broken links in my Blogging Beyond the PDF sample article are all internal links, and I can now use WinerLinks to fix them. An example where I have already done this is the reference to Table S8 in <a href="https://web.archive.org/web/20120611074314/http://blogs.xartrials.org/2010/12/05/the-mycobacterium-tuberculosis-drugome-and-its-polypharmacological-implications-2/#p16">this paragraph</a>.</p>
<p><em><em>Update on 12/11/10: I’ve installed the <a href="https://web.archive.org/web/20120611074314/http://ptsefton.com/2010/12/09/beyond-the-pdf-proposed-session-bring-the-web-to-the-researcher-mainly-on-authoring-tools.htm">Anotar Plugin</a> by Peter Sefton that adds paragraph-level commenting.</em></em></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Submission Fees for Open Access Journals?]]></title>
        <id>32541wj-vpk8kdr-t3sdtcb-4v4v4</id>
        <link href="https://blog.front-matter.io/mfenner/submission-fees-for-open-access-journals"/>
        <updated>2010-12-10T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[In early December Knowledge Exchange, a partnership of JISC (United Kingdom), SURF (Netherlands), DEFF (Denmark) and DFG (Germany) released a report on submission fees that they had commissioned to Mark Ware Consulting....]]></summary>
        <content type="html"><![CDATA[<p>In early December <a href="https://web.archive.org/web/20120611025454/http://www.knowledge-exchange.info/">Knowledge Exchange</a>, a partnership of JISC (United Kingdom), SURF (Netherlands), DEFF (Denmark) and DFG (Germany) <a href="https://web.archive.org/web/20120611025454/http://www.knowledge-exchange.info/Default.aspx?ID=413">released a report</a> on submission fees that they had commissioned to <a href="https://web.archive.org/web/20120611025454/http://mrkwr.wordpress.com/mark-ware-consulting/">Mark Ware Consulting</a>. The report was also discussed by Robert Kiley on the <a href="https://web.archive.org/web/20120611025454/http://ukpmc.blogspot.com/2010/12/submission-fees-viable-business-model.html">UK PubMed Central blog</a> and by Phil Davis on the <a href="https://web.archive.org/web/20120611025454/http://scholarlykitchen.sspnet.org/2010/12/09/open-access-submission-fees/">Scholarly Kitchen blog</a>.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/2800029836_f351e8c826.jpeg" class="kg-image" width="500" height="375" alt="Buck O’ Five (Mission 24: Independent) by darabidduckie, on Flickr" /><figcaption aria-hidden="true"><em><em>Buck O’ Five (Mission 24: Independent) by <a href="https://web.archive.org/web/20120611025454/http://www.flickr.com/photos/darabidduckie/">darabidduckie</a>, on Flickr</em></em></figcaption>
</figure>
<p>Submission fees are more common than I thought, particularly in economics and the life sciences. The American Physiological Society journals, <em><em>Cancer Research</em></em>, <em><em>FASEB Journal</em></em>, <em><em>Journal of Clinical Investigation</em></em>, <em><em>Journal of Immunology</em></em> and <em><em>Journal of Neuroscience</em></em> all charge submission fees between $50 and $100, the <em><em>Journal of Biological Chemistry</em></em> dropped the $60 submission fee in 2010. Most journals that charge submission fees are society journals.</p>
<p>The report finds that submission fees would be particularly interesting for Open Access journals with high rejection rates (70% and more), as this would greatly reduce the article-procesing charges for accepted manuscripts. A journal that accepts 10% of submitted papers could use a $150 submission fee to reduce the fee for accepted manuscripts from $2500 to $1150. There wouldn’t really be a price reduction for journals that accept 50% of submitted papers.</p>
<p>Mark Ware reports that the Open Access publishers he talked to weren’t really interested in starting submission fees for their journals, mainly because of the risks involved in changing their business model. I personally believe that submission fees may be the only option for journals with high rejection rates to become Open Access journals (unless you want to cross-subsidize those journals). I like submission fees because they help to cover the actual costs involved, instead of the costs of handling manuscripts that are ultimately rejected being paid either by journal subscribers or the authors of accepted manuscripts.</p>
<p>The full 13-page report can be downloaded <a href="https://web.archive.org/web/20120611025454/http://www.knowledge-exchange.info/Files/Filer/downloads/Open%20Access/KE_Submission_fees_Short_Report_2010-11-25.pdf">here</a>.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blogging Beyond the PDF]]></title>
        <id>7nfde5q-zrx8e49-bpgxp8g-bn1d0</id>
        <link href="https://blog.front-matter.io/mfenner/blogging-beyond-the-pdf"/>
        <updated>2010-12-05T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Four weeks ago I wrote about the Beyond the PDF workshop that is planned for January in San Diego. The goal of the workshop is to identify a set of requirements, and a group of willing participants to develop open source code to accelerate scientific knowledge sharing....]]></summary>
        <content type="html"><![CDATA[<p>Four weeks ago I <a href="https://web.archive.org/web/20120611081800/http://blogs.plos.org/mfenner/2010/11/06/beyond-the-pdf-it-is-time-for-a-workshop/">wrote about</a> the <a href="https://web.archive.org/web/20120611081800/https://sites.google.com/site/beyondthepdf/">Beyond the PDF</a> workshop that is planned for January in San Diego. The goal of the workshop is to identify a set of requirements, and a group of willing participants to develop open source code to accelerate scientific knowledge sharing. The <a href="https://web.archive.org/web/20120611081800/http://groups.google.com/group/beyond-the-pdf?pli=1">Google Group</a> for the workshop has already seen a lot of interesting discussions. I have since had more time to think about my contribution and decided to propose the following:</p>
<p><em><em>Evaluate the blogging software</em></em> <strong><strong><em><em>WordPress</em></em></strong></strong> <em><em>as a platform to author, review and publish scientific manuscripts. Extend the <strong><strong>WordPress</strong></strong> functionality for authors and citations.</em></em></p>
<h3 id="blogging-beyond-the-pdf">Blogging Beyond the PDF</h3>
<p>Wordpress is a very interesting candidate for this because of the following features:</p>
<ol>
<li>WordPress is Open Source and can be easily modified and extended by Themes, Plugins, etc.</li>
<li>WordPress is used by millions of users, a market much larger than the scientific software market. Many requirements for a scientific writing platform are not specific for scientific software.</li>
<li>WordPress treats article writing as a workflow that includes collaborative writing, version control, and a document status.</li>
</ol>
<p>Of course I’m not the first to think about WordPress in this context, the <a href="https://web.archive.org/web/20120611081800/http://journal.code4lib.org/">Code4Lib journal</a> and <a href="https://web.archive.org/web/20120611081800/http://knowledgeblog.org/">Knowledge Blog</a> are just two examples. To get started, I installed WordPress and a number of interesting Plugins at a new <a href="https://web.archive.org/web/20120611081800/http://blogs.xartrials.org/">Blogging Beyond the PDF</a> website. Phil Bourne has kindly provided material from a recent <em><em>PLoS Computational Biology</em></em> <a href="https://web.archive.org/web/20120611081800/http://dx.doi.org/10.1371/journal.pcbi.1000976">paper</a> for the workshop. I formatted a first version of this paper using WordPress and the result can be seen <a href="https://web.archive.org/web/20120611081800/http://blogs.xartrials.org/2010/12/05/the-mycobacterium-tuberculosis-drugome-and-its-polypharmacological-implications-2/">here</a>. There are a lot of rough edges (several tables and most references are still missing), but to me this looks good enough considering this is the result of maybe 10 hours of the work. In the next six weeks I will continue to work on this example manuscript to get a better idea of the strengths and weaknesses of the WordPress platform. I also hope to learn more from the experience of others. My first impressions are below.</p>
<h3 id="authors">Authors</h3>
<p>The <a href="https://web.archive.org/web/20120611081800/http://wordpress.org/extend/plugins/co-authors-plus/">Co-Authors Plus</a> Plugin enables multiple authors per article. Each author can be linked to an author page for displaying biographical info. WordPress could be extended to include additional info such as institution or past publications. Linking the WordPress user account to the unique author identifier <a href="https://web.archive.org/web/20120611081800/http://www.orcid.org/">ORCID</a>, and describing the role of the author in the paper (e.g. <em><em>conceived and designed the experiments</em></em> or <em><em>analyzed the data</em></em>) would be particularly interesting. Plugins such as <a href="https://web.archive.org/web/20120611081800/http://www.editflow.org/">Edit Flow</a> can extend the workflow by adding custom status messages (e.g. <em><em>resubmission</em></em>), reviewer comments, and email notifications.</p>
<h3 id="figures">Figures</h3>
<p>Wordpress has good functionality to add figures to articles, including the option to add a caption or resize the figure. In addition, there is a good number of Plugins that help with the resizing, other manipulations and display of images.</p>
<h3 id="tables">Tables</h3>
<p>Wordpress doesn’t do tables, but several Plugins add that functionality, including <a href="https://web.archive.org/web/20120611081800/http://tobias.baethge.com/wordpress-plugins/wp-table-reloaded-english/">WP-Table Reloaded</a>. This Plugin adds some very interesting functions, including the option to export table data as CSV or XLS files. This makes it much easier to reuse these data, something that I <a href="https://web.archive.org/web/20120611081800/http://blogs.plos.org/mfenner/2010/09/30/why-cant-i-reuse-these-tables-and-figures/">find very useful</a>.</p>
<h3 id="citations">Citations</h3>
<p>The <a href="https://web.archive.org/web/20120611081800/http://www.elvery.net/drzax/wordpress-footnotes-plugin">WP-Footnotes</a> Plugin is one of several plugins that adds footnotes to articles. Several Plugins integrate with reference managers such as CiteULike, but the functionality is still very limited compared to how most reference managers are integrated with Microsoft Word or LaTeX.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medical Information Matters]]></title>
        <id>6m71d05-gfv8azb-b7ndhmm-7vfhx</id>
        <link href="https://blog.front-matter.io/mfenner/medical-information-matters"/>
        <updated>2010-12-03T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[In December I am hosting the blog carnival Medical Information Matters, a blog carnival about – <strong><strong>medical information</strong></strong>. The deadline for submissions is next Tuesday, and I have already received a number of interesting posts....]]></summary>
        <content type="html"><![CDATA[<p>In December I am hosting the blog carnival <a href="https://web.archive.org/web/20120612112930/http://laikaspoetnik.wordpress.com/2010/09/30/may-i-introduce-to-you-a-new-name-for-the-medlibs-round/">Medical Information Matters</a>, a blog carnival about – <strong><strong>medical information</strong></strong>. The deadline for submissions is next Tuesday, and I have already received a number of interesting posts. As Christmas is right around the corner, I thought that a good theme for the carnival would be a <strong><strong>wish list for better medical information</strong></strong>. This could mean many different things, e.g. a database that covers a specific area, better access to full-text papers or clinical trial results, etc. Please submit your posts <a href="https://web.archive.org/web/20120612112930/http://blogcarnival.com/bc/submit_6092.html">here</a>.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Repositories: Researcher Perspective]]></title>
        <id>2m8wqm5-maw8aba-qw61yac-5ahfz</id>
        <link href="https://blog.front-matter.io/mfenner/repositories-researcher-perspective"/>
        <updated>2010-11-30T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Earlier today I gave this presentation at the DINI/Helmholtz Repositories Workshop in Berlin. I’m looking forward to the second day tomorrow....]]></summary>
        <content type="html"><![CDATA[<p>Earlier today I gave this presentation at the <a href="https://web.archive.org/web/20120611031918/http://www.dini.de/veranstaltungen/workshops/dinihelmholtz-workshop-repositorien-praxis-und-vision/">DINI/Helmholtz Repositories Workshop</a> in Berlin. I’m looking forward to the second day tomorrow.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Researchers’ reasons for publishing their work]]></title>
        <id>7s3nw3x-j169yt9-07ajsnb-d6brj</id>
        <link href="https://blog.front-matter.io/mfenner/researchers-reasons-for-publishing-their-work"/>
        <updated>2010-11-28T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[When we want to change something, we have to look at the incentives for those involved.ReferencesSwan, A. (2006) The culture of Open Access: researchers’ views and responses. In: <em><em>Open Access: Key Strategic, Technical and Economic Aspects</em></em>,...]]></summary>
        <content type="html"><![CDATA[<figure>
<img src="https://web.archive.org/web/20120611024948im_/http://blogs.plos.org/mfenner/files/2010/11/swan.jpg" title="swan" class="kg-image" />
</figure>
<p>When we want to change something, we have to look at the incentives for those involved.</p>
<h2 id="references">References</h2>
<p>Swan, A. (2006) The culture of Open Access: researchers’ views and responses. In: <em><em>Open Access: Key Strategic, Technical and Economic Aspects</em></em>, Chandos. <a href="https://web.archive.org/web/20120611024948/http://eprints.ecs.soton.ac.uk/12428">http://eprints.ecs.soton.ac.uk/12428</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making your study public before you start can be fun]]></title>
        <id>6pt21qs-wjr9zra-x14tbdx-9ag0g</id>
        <link href="https://blog.front-matter.io/mfenner/making-your-study-public-before-you-start-can-be-fun"/>
        <updated>2010-11-27T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[On Monday I was finally able to start the clinical trial <strong><strong>Everolimus for patients with relapsed/refractory germ cell cancer</strong></strong> (RADIT), and I’m now looking forward to recruit the first patient....]]></summary>
        <content type="html"><![CDATA[<p>On Monday I was finally able to start the clinical trial <strong><strong>Everolimus for patients with relapsed/refractory germ cell cancer</strong></strong> (<a href="https://web.archive.org/web/20101124185406/http://www.mh-hannover.de/studien/en/protocols/191.html">RADIT</a>), and I’m now looking forward to recruit the first patient. We aim to treat 25 patients with cancer of the testis with the mTOR inhibitor everolimus in this phase II trial, and eight German hospitals are participating.</p>
<p>The International Committee of Medical Journal Editors (ICMJE) <a href="https://web.archive.org/web/20101124185406/http://www.icmje.org/publishing_10register.html">requires</a>, as a condition of consideration for publication in their journals, registration in a public trials registry. I registered my trial with Clinicaltrials.gov as <a href="https://web.archive.org/web/20101124185406/http://clinicaltrials.gov/ct2/show/NCT01242631">NCT01242631</a>. The registration process was straightforward, especially compared to all the other paperwork required for this trial.</p>
<p>The requirement for registration of clinical trials helps to prevent publication bias – meaning that negative results are likely to never be  published. Clinical trial registries make it unlikely that you will be “scooped”, as you know about the other studies addressing the same questions years before they finish.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evolving English at the British Library]]></title>
        <id>75ax2ab-qsn8fet-7qv2tgx-30hpr</id>
        <link href="https://blog.front-matter.io/mfenner/evolving-english-at-the-british-library"/>
        <updated>2010-11-24T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[On Saturday I went to see the exhibition Evolving English at the British Library in London. The exhibition explores the English language in all its diversity and was recommended to me by Matt Brown, expert on all things London (his review for the Londonist is here)....]]></summary>
        <content type="html"><![CDATA[<p>On Saturday I went to see the exhibition <a href="https://web.archive.org/web/20120611032112/http://www.bl.uk/evolvingenglish/">Evolving English</a> at the British Library in London. The exhibition explores the English language in all its diversity and was recommended to me by Matt Brown, expert on all things London (his review for the Londonist is <a href="https://web.archive.org/web/20120611032112/http://londonist.com/2010/11/review_evolving_english_british_lib.php">here</a>). <strong><strong>Evolving English</strong></strong> is interesting in many different ways, but here I want to focus on English as the language of science – used in two examples in the exhibition.</p>
<p>The book <em><em>Micrographia</em></em> from 1665 by Robert Hooke is the first major scientific work devoted to exploration using the microscope. It is also an important step in the development of scientific English. The book was praised for the clarity of the language at the time, but from today’s science writing perspective contains sentences that are too long and convoluted, uses conversational English, and frequently the active voice. The National Library of Medicine provides a beautiful <a href="https://web.archive.org/web/20120611032112/http://archive.nlm.nih.gov/proj/ttp/flash/hooke/hooke.html">online version</a> of the book, where observation XXV starts with:</p>
<blockquote>
<em><em>A nettle is a plant so well known to every one, as to what the appearance of it is to the naked eye, that it needs no description; and there are very few that have not felt as well as seen it; and therefore it will be no news to tell that a gentle and slight touch of the skin by a nettle, does oftentime, not onely create very sensible and acute pain, much like that of a burn or scald, but often also very angry and hard swellings and inflammations of the parts, such as will presently rise and continue swoln divers hours.</em></em>
</blockquote>
<p>As an example of current scientific English, the exhibitors picked the paper describing the cloning of Dolly (Wilmut 1997). The text is described in the exhibition as:</p>
<blockquote>
<em><em>The language is punctual throughout and technical terms are mostly used without explanation. The main text is supported by illustrations (here called figures), tables of data and – over the page – bibliographic references to other scientific research. The purpose is to to maximize accuracy, remove any chance of ambiguity and provide writers with every opportunity to give detailed evidence.</em></em>
</blockquote>
<p>The exhibition makes it very clear that the English language is constantly evolving, and scientific English is no exception. Papers written in 10, 20 or 100 years might use a very different language. One important influence today is of course science writing in blogs and other social media. And with electronic publishing, open access, citizen science and other trends, scientific papers have become available to a much larger audience.</p>
<h2 id="references">References</h2>
<p>Wilmut I, Schnieke AE, McWhir J, Kind AJ, Campbell KH. Viable offspring derived from fetal and adult mammalian cells. <em><em>Nature</em></em>. 1997;385:810-813. DOI: <a href="https://web.archive.org/web/20120611032112/http://dx.doi.org/10.1038/385810a0">http://doi.org/10.1038/385810a0</a>.<br />
</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UK PubMed Central explained in Nucleic Acids Research Paper]]></title>
        <id>52fr465-ms0936s-1sfj68c-p86er</id>
        <link href="https://blog.front-matter.io/mfenner/uk-pubmed-central-explained-in-nucleic-acids-research-paper"/>
        <updated>2010-11-14T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Last Tuesday <em><em>Nucleic Acids Research</em></em> published a nice paper describing the UK PubMed Central (UKPMC) database (McEntyre 2010). UKPMC was started in 2007, the enhanced version described in the paper was launched January 2010....]]></summary>
        <content type="html"><![CDATA[<p>Last Tuesday <em><em>Nucleic Acids Research</em></em> published a nice paper describing the <a href="https://web.archive.org/web/20101124185412/http://ukpmc.ac.uk/">UK PubMed Central</a> (UKPMC) database (McEntyre 2010). UKPMC was started in 2007, the enhanced version described in the paper was <a href="https://web.archive.org/web/20101124185412/http://blogit.realwire.com/?ReleaseID=14754">launched January 2010</a>. In November 2009 I published an <a href="https://blog.martinfenner.org//uk_pubmed_central_interview_with_phil_vaughan/">interview with Phil Vaughan</a>, the senior author of the paper. The paper talks about the specific enhancements done to PubMed Central, including an integrated search of PubMed and PubMed Central, “Cited by information” and semantically enriched content generated by text mining.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/13th/F3.medium.gif" title="figure 3" class="kg-image" width="440" height="252" alt="Content profile for PMC and UKPMC. Figure 3 from McEntyre JR et al." /><figcaption aria-hidden="true"><em><em>Content profile for PMC and UKPMC. Figure 3 from McEntyre JR et al.</em></em></figcaption>
</figure>
<p>Thanks to Duncan Hull we know that PubMed currently contains information about 20 million papers (<a href="https://web.archive.org/web/20101124185412/http://duncan.hull.name/2010/07/27/pubmed-20-million/">Twenty million papers in PubMed: a triumph or a tragedy?</a>). About 10% of these papers are available as fulltext from PubMed Central. What wasn’t clear to me and what I learned from the paper is that only 194,000 papers, or 1% of PubMed content, are from the <a href="https://web.archive.org/web/20101124185412/http://www.ncbi.nlm.nih.gov/pmc/about/openftlist.html">PMC Open Access Subset</a> (and that includes papers with a non-commercial OA license). All these papers are available as <a href="https://web.archive.org/web/20101124185412/http://www.ncbi.nlm.nih.gov/pmc/about/ftp.html">download</a> or via <a href="https://web.archive.org/web/20101124185412/http://www.ncbi.nlm.nih.gov/pmc/about/oai.html">PMC-OAI service</a>. Although 1.8 million papers (the majority digitized back issues) can be freely downloaded as full-text from PubMed Central, they carry a publisher copyright and can’t be reused for research purposes (e.g. full-text mining) without an explicit permission from the publisher.</p>
<h2 id="references">References</h2>
<p>McEntyre JR, Ananiadou S, Andrews S, Black WJ, Boulderstone R, Buttery P, et al. UKPMC: a full text article resource for the life sciences. <em><em>Nucleic Acids Research</em></em>. 2010 November; DOI: <a href="https://doi.org/10.1093/nar/gkq1063">http://doi.org/10.1093/nar/gkq1063</a>.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scientific Attribution Presentation]]></title>
        <id>7vm9tem-mcx8zsa-4en86p8-b9vhb</id>
        <link href="https://blog.front-matter.io/mfenner/scientific-attribution-presentation"/>
        <updated>2010-11-12T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Earlier today I gave a short presentation about the Scientific Attribution Principles I posted here two weeks ago....]]></summary>
        <content type="html"><![CDATA[<p>Earlier today I gave a short presentation about the <a href="https://blog.martinfenner.org/posts/scientific-attribution-principles/">Scientific Attribution Principles</a> I posted here two weeks ago.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[More Gobbledygook at PLoS Blogs]]></title>
        <id>5wdexw0-gm38fbr-9basj8a-tdee7</id>
        <link href="https://blog.front-matter.io/mfenner/more-gobbledygook-at-plos-blogs"/>
        <updated>2010-11-03T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Thanks to a lot of hard work both at <strong><strong>Nature Network</strong></strong> and here at <strong><strong>PLoS Blogs</strong></strong>, all blog posts and comments from my former blog at Nature Network (August 2007 to August 2010)...]]></summary>
        <content type="html"><![CDATA[<p>Thanks to a lot of hard work both at <strong><strong>Nature Network</strong></strong> and here at <strong><strong>PLoS Blogs</strong></strong>, all blog posts and comments from my <a href="https://web.archive.org/web/20101124185441/http://blogs.nature.com/mfenner/">former blog at Nature Network</a> (August 2007 to August 2010) are now also available here. Special thanks go to Lou Woudley and Brian Mossop who made all this happen. There are of course a few formatting glitches here and there, but I will try to fix them over time.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An ORCID discussion group for researchers]]></title>
        <id>2e2srfh-73e8qat-vrkpv90-f5yqb</id>
        <link href="https://blog.front-matter.io/mfenner/an-orcid-discussion-group-for-researchers"/>
        <updated>2010-11-02T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The Open Researcher &amp; Contributor ID (ORCID) initiative is working on a unique researcher identifier for the creation of a clear and unambiguous scholarly record. The initiative is supported by more than 140 universities, research institutes,...]]></summary>
        <content type="html"><![CDATA[<p>The Open Researcher &amp; Contributor ID (<a href="https://web.archive.org/web/20101124191013/http://www.orcid.org/">ORCID</a>) initiative is working on a unique researcher identifier for the creation of a clear and unambiguous scholarly record. The initiative is supported by <a href="https://web.archive.org/web/20101124191013/http://www.orcid.org/directory">more than 140</a> universities, research institutes, funding organizations, publishers and other organizations interested in scholarly communication. The ORCID system will become publicly available in the first half of 2011.</p>
<p>Individual researchers will benefit from a unique researcher identifier because this identifier facilitates the manuscript submission process, and the creation of CVs and publication lists for institutional websites. Unique researcher identifiers will also help discover scholarly works and will make it easier to attribute scientific contributions other than papers –  e.g. research datasets – to individual researchers.</p>
<p>Researchers have a lot to gain from unique researcher identifiers, and they might also have genuine concerns. Because of my interest in scholarly communication, and <a href="https://web.archive.org/web/20101124191013/http://blogs.nature.com/mfenner/2010/01/03/orcid-or-how-to-build-a-unique-identifier-for-scientists-in-10-easy-steps">unique researcher identifiers in particular</a>, I became involved with the ORCID initiative in early 2010. When ORCID became a non-profit organization in August, I was asked to sit on the <a href="https://web.archive.org/web/20101124191013/http://www.orcid.org/board-directors">Board of Directors</a> – one of many interesting things that have happened to me as the result of my science blogging. But although ORCID is certainly open to all individuals and organizations interested in author disambiguation, most researchers simply want to be informed about the progress of the initiative, ask specific questions and start using their identifier at some point.</p>
<p>The <a href="https://web.archive.org/web/20101124191013/http://www.orcid.org/">ORCID website</a> is a good starting for this information, but to get individual answers to specific researcher questions about ORCID, <a href="https://web.archive.org/web/20101124191013/http://twitter.com/kristiholmes">Kristi Holmes</a>, <a href="https://web.archive.org/web/20101124191013/http://twitter.com/gthorisson">Gudmundur Thorisson</a>, <a href="https://web.archive.org/web/20101124191013/http://twitter.com/CameronNeylon">Cameron Neylon</a> and <a href="https://web.archive.org/web/20101124191013/http://twitter.com/mfenner">myself</a> today started the <a href="https://web.archive.org/web/20101124191013/http://groups.google.com/group/orcid-researchers">ORCID Researchers</a> Google Group. Feel free to ask your ORCID-related researcher questions there, or contact us via email or Twitter.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NCBI adds searchable images database]]></title>
        <id>6a7cahj-7ba8qrs-n6v8tf6-qq2fw</id>
        <link href="https://blog.front-matter.io/mfenner/ncbi-adds-searchable-images-database"/>
        <updated>2010-10-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[On Wednesday the NCBI released an Images database, compiled from full text resources at the NCBI – initially PubMed Central articles. The images can be searched by several parameters, e.g. figure caption or author.Using this database,...]]></summary>
        <content type="html"><![CDATA[<p>On Wednesday the NCBI released an <a href="https://web.archive.org/web/20101124191025/http://www.ncbi.nlm.nih.gov/images">Images database</a>, compiled from full text resources at the NCBI – initially PubMed Central articles. The images can be searched by several parameters, e.g. figure caption or author.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/images.jpeg" title="images" class="kg-image" width="500" height="355" />
</figure>
<p>Using this database, images are now displayed together with the PubMed abstract for PubMed Central articles. More info about these changes can be found in the <a href="https://web.archive.org/web/20101124191025/http://www.nlm.nih.gov/pubs/techbull/so10/so10_pm_display_ncbi_images.html">NLM Bulletin</a>.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Some thoughts on principles for scientific attribution]]></title>
        <id>4mwgk0t-bsm9q4s-njmcf3s-q9yzg</id>
        <link href="https://blog.front-matter.io/mfenner/some-thoughts-on-principles-for-scientific-attribution"/>
        <updated>2010-10-28T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Today I posted a document that should help define a set of principles for scientific attribution. These principles will be presented and discussed at the National Science Foundation workshop <strong><strong>Changing the Conduct of Science in the Information Age</strong></strong> on November 12....]]></summary>
        <content type="html"><![CDATA[<p>Today <a href="https://web.archive.org/web/20101124191031/http://blogs.plos.org/mfenner/scientific-attribution-principles/">I posted a document</a> that should help define a set of principles for scientific attribution. These principles will be presented and discussed at the National Science Foundation workshop <strong><strong>Changing the Conduct of Science in the Information Age</strong></strong> on November 12. Many people helped me with this document (<a href="https://web.archive.org/web/20101124191031/http://cameronneylon.net/">Cameron Neylon</a> in particular), and I welcome comments and suggestions.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-motivated vs. mandated archiving]]></title>
        <id>2gp8fg4-28095ga-7wpyr5v-x11b1</id>
        <link href="https://blog.front-matter.io/mfenner/self-motivated-vs-mandated-archiving"/>
        <updated>2010-10-26T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[My post last week about citation rates of mandated vs. self-selected Open Access resulted in an interesting discussion thanks to some good arguments made by Stevan Harnad. One personal conclusion for me: mandates for self-archiving are not a good idea....]]></summary>
        <content type="html"><![CDATA[<p>My post last week about <a href="https://web.archive.org/web/20101124191039/http://blogs.plos.org/mfenner/2010/10/18/new-in-plos-one-citation-rates-of-self-selected-vs-mandated-open-access/">citation rates of mandated vs. self-selected Open Access</a> resulted in an interesting discussion thanks to some good arguments made by Stevan Harnad. One personal conclusion for me: mandates for self-archiving are not a good idea. I would very much prefer researchers to be highly motivated to self-archive thanks to a repository that is both fulfilling important functions and is fun to use.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/17th/4088667379_880b0a796a.jpeg" class="kg-image" width="452" height="500" alt="Flickr image by diff.jisc (more info)." /><figcaption aria-hidden="true"><em><em>Flickr image by diff.jisc (<a href="https://web.archive.org/web/20101124191039/http://www.flickr.com/photos/dff-jisc/4088667379/">more info</a>).</em></em></figcaption>
</figure>
<p>What follows is a short list of ideas – many of them obvious and some of them already implemented – that for me would make repositories more attractive to use.</p>
<h3 id="hosting-of-research-datasets">Hosting of research datasets</h3>
<p>Disciplinary and institutional repositories are good places to make primary research data publicly available. Particularly if no standard database exists, such as <a href="https://web.archive.org/web/20101124191039/http://www.ncbi.nlm.nih.gov/genbank/">GenBank</a> for genetic sequence data. Whereas everybody nowadays talks about <a href="https://web.archive.org/web/20101124191039/http://www.jisc.ac.uk/whatwedo/programmes/~/link.aspx?_id=28A2778937C74EB285F05E38BFBD5DEE&amp;_z=z">making research data available</a>, the threshold to do so is often too high for more specialized datasets. An institutional repository is a good place for simple grassroots solution.</p>
<h3 id="a-disciplinary-repository-for-biomedical-research">A disciplinary repository for biomedical research</h3>
<p><a href="https://web.archive.org/web/20101124191039/http://www.ncbi.nlm.nih.gov/pmc/">PubMed Central</a> and <a href="https://web.archive.org/web/20101124191039/http://ukpmc.ac.uk/">UK PubMed Central</a> are very popular disciplinary repositories for the biomedical and life sciences. Unfortunately I can only submit manuscripts to them if the research was funded by one of a few funders (including the NIH, but none of the major German funders).</p>
<h3 id="a-preprint-archive-for-clinical-trials">A preprint archive for clinical trials</h3>
<p>Deposition of manuscripts prior to peer review <a href="https://web.archive.org/web/20101124191039/http://blogs.plos.org/mfenner/2010/10/09/new-on-arxiv-first-report-of-soap-open-access-publishing-project/">has a long tradition</a> in high-energy physics and related disciplines. Preprint archives probably don’t work in all disciplines, but I have <a href="https://web.archive.org/web/20101124191039/http://blogs.plos.org/mfenner/2010/10/16/in-which-i-suggest-a-preprint-archive-for-clinical-trials/">argued before</a> that they could be a very good idea for clinical trials.</p>
<h3 id="journal-publishers-that-allow-self-archiving">Journal publishers that allow self-archiving</h3>
<p>While most publishers allow some form of self-archiving by authors (pre-print, post-print and/or publisher’s PDF version), there are unfortunately still exceptions. The American Chemical Society (ACS) has a <a href="https://web.archive.org/web/20101124191039/http://scientopia.org/blogs/bookoftrogool/2010/10/05/acs-the-perfect-storm/">particularly unwelcoming policy</a>.</p>
<h3 id="integration-with-the-journal-submission-process">Integration with the journal submission process</h3>
<p>An institutional repository can host pre-prints of submitted papers. But the repository could also enable a tighter integration with the journal submission process. The German <a href="https://web.archive.org/web/20101124191039/https://www.escidoc.org/JSPWiki/en/Overview">eSciDoc</a> project uses that approach. Most researchers would probably welcome technical and financial assistance in the submission process.</p>
<h3 id="integration-with-institutional-bibliography">Integration with institutional bibliography</h3>
<p>The institutional bibliography showcases the scholarly work done by a particular researcher, research group, department or institution. Many researchers like to see up-to-date profiles (including publication lists) on their institutional webpages. Bibliographies are also collected for evaluation purposes. We have started to use <a href="https://web.archive.org/web/20101124191039/http://www.bibapp.org/">BibApp</a> at our institution. Repositories can facilitate full-text access to PhD theses and other publications in the bibliography.</p>
<h3 id="repositories-that-use-unique-author-identifiers">Repositories that use unique author identifiers</h3>
<p>Regular readers of this blog know about <a href="https://web.archive.org/web/20101124191039/http://blogs.plos.org/mfenner/2010/09/07/orcid-as-unique-author-identifier-what-is-it-good-for-and-should-we-worry-or-be-happy/">my involvement</a> in the Open Researcher &amp; Contributor ID (ORCID) initiative. All scholarly contributions, including repository content, should be unambiguously connected to their creators as this will greatly facilitate the discovery process. I look forward to ORCID support in repository software such as <a href="https://web.archive.org/web/20101124191039/http://www.dspace.org/">DSpace</a> and <a href="https://web.archive.org/web/20101124191039/http://www.eprints.org/">EPrints</a>.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Having an impact (factor) and other stories from Gregory Petsko]]></title>
        <id>6xq7ha8-0wa9fbb-wwd2th6-6etwv</id>
        <link href="https://blog.front-matter.io/mfenner/having-an-impact-factor-and-other-stories-from-gregory-petsko"/>
        <updated>2010-10-24T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Biologist Gregory Petsko has written a monthly column for the journal <em><em>Genome Biology</em></em> since the journal launched in 2000. To mark the 10th anniversary of the column and the journal, <em><em>Genome Biology</em></em> created an eBook (iPad/iPhone: free,...]]></summary>
        <content type="html"><![CDATA[<p>Biologist <a href="https://web.archive.org/web/20101124191225/http://www.bio.brandeis.edu/faculty/petsko.html">Gregory Petsko</a> has written a <a href="https://web.archive.org/web/20101124191225/http://genomebiology.com/search/results?drpField1=&amp;txtSearch1=&amp;drpPhrase1=and&amp;drpField2=%5BTI%5D&amp;txtSearch2=&amp;drpPhrase2=and&amp;drpField3=%5BAU%5D&amp;txtSearch3=petsko_g&amp;drpPhrase3=and&amp;drpField4=&amp;txtSearch4=&amp;drpPhrase4=and&amp;excludeField1=&amp;excludeSearchText1=&amp;excludePhrase1=and&amp;articleType=Comment&amp;drpOrderBy=by+date&amp;itemsPerPage=25&amp;search-button=Search">monthly column</a> for the journal <em><em>Genome Biology</em></em> since the journal launched in 2000. To mark the 10th anniversary of the column and the journal, <em><em>Genome Biology</em></em> <a href="https://web.archive.org/web/20101124191225/http://blogs.openaccesscentral.com/blogs/bmcblog/entry/gregory_petsko_in_genome_biology">created an eBook</a> (<a href="https://web.archive.org/web/20101124191225/http://ax.itunes.apple.com/gb/book/gregory-petsko-in-genome-biology/id397541470">iPad/iPhone</a>: free, <a href="https://web.archive.org/web/20101124191225/http://www.amazon.com/gp/product/B0046W6TD0?ie=UTF8&amp;tag=wwwbiomedcent-20&amp;linkCode=as2&amp;camp=1789&amp;creative=390957&amp;creativeASIN=B0046W6TD0">Kindle</a>: 99 c) containing all columns up to August 2010.</p>
<p>I downloaded the iBooks version for iPad/iPhone. The columns read nicely on the iPad display (almost all of them are text-only, a few contain images and/or links), and the texts are of course thoughtful and entertaining. Here are a few teasers:</p>
<ul>
<li>Petsko G (2009a). Nation of twits. https://doi.org/<a href="https://web.archive.org/web/20101124191225/http://dx.doi.org/10.1186/gb-2009-10-8-110">10.1186/gb-2009-10-8-110</a>.</li>
<li>Petsko G (2009b). What my genome told me – and what it didn’t. https://doi.org/<a href="https://web.archive.org/web/20101124191225/http://dx.doi.org/10.1186/gb-2009-10-6-108">10.1186/gb-2009-10-6-108</a>.</li>
<li>Petsko G (2008). Having an impact (factor).<br />
https://doi.org/<a href="https://web.archive.org/web/20101124191225/http://dx.doi.org/10.1186/gb-2008-9-7-107">10.1186/gb-2008-9-7-107</a>.</li>
</ul>
<p>Suggestion for <em><em>Genome Biology</em></em>: if you want more people to read the column, create an RSS feed for it. Or even better, turn it into a blog.</p>
<p>Suggestion for <em><em>EMBO Reports</em></em>: pick up the idea and create an eBook with the editorials by <strong><strong>Frank Gannon</strong></strong>, who wrote a similarly thoughtful column for them <a href="https://web.archive.org/web/20101124191225/http://blogs.nature.com/nautilus/2009/04/frank_gannon_says_farewell_to.html">until April 2009</a>.</p>
<h2 id="references">References</h2>
<p><strong><strong>Petsko G</strong></strong>. Nation of twits. <em><em>Genome Biology</em></em>. 2009;10(8):110+. Available from: https://doi.org/<a href="https://web.archive.org/web/20101124191225/http://dx.doi.org/10.1186/gb-2009-10-8-110">10.1186/gb-2009-10-8-110</a>.</p>
<p><strong><strong>Petsko G</strong></strong>. What my genome told me – and what it didn’t. <em><em>Genome Biology</em></em>. 2009;10(6):108+. https://doi.org/<a href="https://web.archive.org/web/20101124191225/http://dx.doi.org/10.1186/gb-2009-10-6-108">10.1186/gb-2009-10-6-108</a>.</p>
<p><strong><strong>Petsko G</strong></strong>. Having an impact (factor). <em><em>Genome Biology</em></em>. 2008;9(7):107+.<br />
https://doi.org/<a href="https://web.archive.org/web/20101124191225/http://dx.doi.org/10.1186/gb-2008-9-7-107">10.1186/gb-2008-9-7-107</a>.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Helping researchers create scholarly content]]></title>
        <id>19bb84v-jbe8sfa-ttddvng-h1fjk</id>
        <link href="https://blog.front-matter.io/mfenner/helping-researchers-create-scholarly-content"/>
        <updated>2010-10-20T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Two weeks ago I gave a presentation at the STM Annual Conference 2010 in Frankfurt (STM is the International Association of Scientific, Technical and Medical Publishers). I told the audience that publishers could do a much better job helping researchers...]]></summary>
        <content type="html"><![CDATA[<p>Two weeks ago I gave a presentation at the <a href="https://web.archive.org/web/20101114043831/http://www.stm-assoc.org/event.php?event_id=56">STM Annual Conference 2010</a> in Frankfurt (STM is the International Association of Scientific, Technical and Medical Publishers). I told the audience that publishers could do a much better job helping researchers create digital content – both by providing better tools and by reconsidering licenses for content reuse.</p>
<p>A video of my presentation is available <a href="https://web.archive.org/web/20101114043831/http://river-valley.tv/helping-researchers-create-scholarly-content/">here</a>. The other presentations of the event can be viewed <a href="https://web.archive.org/web/20101114043831/http://www.stm-assoc.org/event_presentations.php?event_id=56">here</a> – unfortunately not (yet) the keynote by <a href="https://web.archive.org/web/20101114043831/http://www.guardian.co.uk/profile/stephendunn">Stephen Dunn</a> from the Guardian (Beyond Publishing: how we open up to our users).</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New in PLoS ONE: Citation rates of self-selected vs. mandated Open Access]]></title>
        <id>5t04bhk-xd9k6at-7fbe3p0-jzwq</id>
        <link href="https://blog.front-matter.io/mfenner/new-in-plos-one-citation-rates-of-self-selected-vs-mandated-open-access"/>
        <updated>2010-10-18T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[<em><em>PLoS ONE</em></em> today published a paper very relevant to Open Access Week (which started today):<strong><strong>Gargouri Y, Hajjem C, Larivière V, Gingras Y, Carr L, Brody T, Harnad S</strong></strong>. Self-Selected or Mandated,...]]></summary>
        <content type="html"><![CDATA[<p><em><em>PLoS ONE</em></em> today published a paper very relevant to <a href="https://web.archive.org/web/20101114042817/http://www.openaccessweek.org/">Open Access Week</a> (which started today):</p>
<p><strong><strong>Gargouri Y, Hajjem C, Larivière V, Gingras Y, Carr L, Brody T, Harnad S</strong></strong>. Self-Selected or Mandated, Open Access Increases Citation Impact for Higher Quality Research. <em><em>PLoS ONE</em></em>. 2010;5(10):e13636+. doi:<a href="https://web.archive.org/web/20101114042817/http://dx.doi.org/10.1371/journal.pone.0013636">10.1371/journal.pone.0013636</a>.</p>
<p>The paper studied the citation rates of papers from four institutions with the longest-standing self-archiving mandate: <a href="https://web.archive.org/web/20101114042817/http://www.soton.ac.uk/">Southampton University</a>, <a href="https://web.archive.org/web/20101114042817/http://public.web.cern.ch/public/">CERN</a>, <a href="https://web.archive.org/web/20101114042817/http://www.qut.edu.au/">Queensland University of Technology</a> and <a href="https://web.archive.org/web/20101114042817/http://www.uminho.pt/Default.aspx?lang=en-US">Minho University</a>. The mandates (instituted between 2002 and 2004) increased the rate of self-archiving in the respective institutional repositories from around 15% to 60%:</p>
<figure>
<img src="https://web.archive.org/web/20101114042817im_/http://blogs.plos.org/mfenner/files/2010/10/journal.pone_.0013636.g001.png" title="journal.pone.0013636.g001" class="kg-image" alt="Fig 1. Open Access Self-archiving percentages for institutions with self-archiving mandates compared to non-mandated, self-selected controls. doi:10.1371/journal.pone.00136363.g001" /><figcaption aria-hidden="true"><em><em>Fig 1. Open Access Self-archiving percentages for institutions with self-archiving mandates compared to non-mandated, self-selected controls. doi:<a href="https://web.archive.org/web/20101114042817/http://blogs.plos.org/mfenner/2010/10/18/new-in-plos-one-citation-rates-of-self-selected-vs-mandated-open-access/10.1371/journal.pone.00136363.g001">10.1371/journal.pone.00136363.g001</a></em></em></figcaption>
</figure>
<p>The authors then compared the citation rates of self-archived papers from the four institutions published in non-Open Access (OA) journals (<a href="https://web.archive.org/web/20101114042817/http://www.sherpa.ac.uk/romeoinfo.html">mandated OA</a>) to matched papers from the same non-OA journal, volume and year (non-OA). 15% of the latter papers were also available from repositories (self-selected OA). Overall the study examined the citations of 27,197 papers from 1,984 non-OA journals. The study also looked at the influence of other factors that affect citation rates, e.g. article age, journal impact factor or number of references.</p>
<p>The main results of the study are that OA articles are cited more frequently than non-OA articles and that there is no difference in citation rates between mandated and self-archived articles. The authors conclude that OA articles are cited more often not because of self-selection (articles with higher impact more likely become OA), but because of readers cite OA articles more often.</p>
<p>The study is an effort to better understand if and why OA papers are cited more often, the so-called OA Advantage. Unfortunately I feel that the paper comes a little short. Yes, they did a very detailed analysis of the citation behavior, and take into account important cofactors. But the reader is left with the impression that mandatory self-archiving of post-prints in institutional repositories is the only reasonable Open Access strategy, and the introduction and discussion accordingly leave out some important arguments.</p>
<p>The authors fail to mention that not all studies show a citation advantage for OA articles (e.g. the randomized controlled trial by <a href="https://web.archive.org/web/20101114042817/http://dx.doi.org/10.1136/bmj.a568">Davis PM et al. 2008</a>), and they only look at self-archiving in institutional repositories (green OA). Not only are there important disclipline-specific repositories such as PubMed Central and ArXiv, but of course also OA journals such as <em><em>PLoS ONE</em></em> that published the study (gold OA). And the study only discussed archiving of articles after peer review, even though archiving of preprints is a popular strategy in high-energy physics and related disciplines (and here the self-archiving rate is close to 100%). I also don’t see a discussion of why the self-archiving rate of post-prints in institutions with a mandate is 60% and not close to 100%. And what is wrong if archiving rates are only around 15% if it is left to the researchers to decide? Finally, there are many reasons other than citation rates that make OA worthwhile, including access to the literature for those not working in academic institutions (and maybe never citing a paper), and work that uses the OA literature in new and exciting ways that go beyond citations. For me personally, the OA advantage is much more in these two aspects than in the citation rates of papers.</p>
<p>For me Open Access week is an opportunity to celebrate what has been achieved over the years. But it is equally important to look at the road ahead. There is no “one size fits all” solution for Open Access, and we shouldn’t look away from the things that aren’t working, and the solutions that haven’t been found yet.</p>
<h2 id="references">References</h2>
<p>Gargouri, Y., Hajjem, C., Larivière, V., Gingras, Y., Carr, L., Brody, T., &amp; Harnad, S. (2010). Self-Selected or Mandated, Open Access Increases Citation Impact for Higher Quality Research <em>PLoS ONE, 5</em> (10) https://doi.org/<a href="https://web.archive.org/web/20101114042817/http://dx.doi.org/10.1371/journal.pone.0013636">10.1371/journal.pone.0013636</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Start Open Access Week with Kick-Off Video]]></title>
        <id>7k1de47-hc2841a-3w4xyr4-jqphg</id>
        <link href="https://blog.front-matter.io/mfenner/start-open-access-week-with-kick-off-video"/>
        <updated>2010-10-18T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Open Access Week 2010 from SPARC on Vimeo.Find out more at the Open Access Week website. And watch this blog for another Open Access story later today....]]></summary>
        <content type="html"><![CDATA[<div class="iframe">
<a href="https://vimeo.com/15881200"></a>
<div id="crawler_player">
Play
<img src="https://f.vimeocdn.com/p/images/crawler_logo.png" class="logo" alt="Vimeo" />
</div>
</div>
<p><a href="https://vimeo.com/15881200">Open Access Week 2010</a> from <a href="https://web.archive.org/web/20101114043744/http://vimeo.com/sparcdc">SPARC</a> on <a href="https://vimeo.com/">Vimeo</a>.</p>
<p>Find out more at the <a href="https://web.archive.org/web/20101114043744/http://www.openaccessweek.org/">Open Access Week website</a>. And watch this blog for another Open Access story later today.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[In which I suggest a preprint archive for clinical trials]]></title>
        <id>36s9qmm-srp8ret-7ph8v81-q4x4q</id>
        <link href="https://blog.front-matter.io/mfenner/in-which-i-suggest-a-preprint-archive-for-clinical-trials"/>
        <updated>2010-10-16T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The ArXiv preprint archive for research articles in physics, mathematics, computer science and related disciplines was initiated by Paul Ginsparg in 1991. ArXiv enables the rapid dissemination of research articles prior to peer review,...]]></summary>
        <content type="html"><![CDATA[<p>The <a href="https://web.archive.org/web/20101105203901/http://arxiv.org/">ArXiv</a> preprint archive for research articles in physics, mathematics, computer science and related disciplines <a href="https://web.archive.org/web/20101105203901/http://people.ccmr.cornell.edu/~ginsparg/blurb/pg02pr.html">was initiated</a> by Paul Ginsparg in 1991. ArXiv enables the rapid dissemination of research articles prior to peer review, and it quickly became very successful in this. ArXiv has not made the peer-reviewed journal obsolete, but rather provides a service that traditional journals – and that also includes Open Access journals – can’t provide. By 1998, more than 90% of all peer-reviewed papers published in high-energy physics <a href="https://web.archive.org/web/20101105203901/http://arxiv.org/abs/0906.5418">first appeared on ArXiv</a>. <a href="https://web.archive.org/web/20101105203901/http://precedings.nature.com/">Nature Precedings</a> was started in 2007 to provide similar services for research in biology, medicine (except clinical trials), chemistry and earth sciences. In addition, many preprints are also hosted in institutional repositories.</p>
<p>The research that I am doing is clinical research, trying to improve the treatment of patients with cancer. Most clinical trials are drug trials, but there are also surgical interventions, trials with medical devices, etc. Clinical trials are a special kind of research, and I have talked about some aspects <a href="https://web.archive.org/web/20101105203901/http://blogs.nature.com/mfenner/2010/03/15/chances-and-problems-of-doing-science-online">in a previous post</a>. Registration of clinical trials before the first patient is treated – providing <a href="https://web.archive.org/web/20101105203901/http://www.who.int/ictrp/network/trds/en/index.html">key information</a> from the trial protocol – is now required in many countries, including the United States and the European Union. The U.S. database of registered clinical trials (<a href="https://web.archive.org/web/20101105203901/http://clinicaltrials.gov/">Clinicaltrials.gov</a>) is publicly available, and the EU is working on doing the same with their <a href="https://web.archive.org/web/20101105203901/https://eudract.ema.europa.eu/">EudraCT</a> database. The International Committee of Medical Journal Editors (ICMJE), and in consequence many medical journals, also <a href="https://web.archive.org/web/20101105203901/http://www.icmje.org/publishing_10register.html">requires clinical trial registration</a>. Since September 2008 the U.S. Food and Drug Administration (FDA) also <a href="https://web.archive.org/web/20101105203901/http://blogs.nature.com/mfenner/2008/08/02/fdaaa-push-to-open-data-in-clinical-medicine">requires</a> all clinical trials registered at Clinicaltrials.gov to provide key results within 12 months after the last patient has finished treatment.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/11th/4861245046_54d92ddfa9.jpeg" class="kg-image" width="346" height="500" alt="Flickr image by myelinrepairfoundation." /><figcaption aria-hidden="true"><em><em>Flickr image by myelinrepairfoundation.</em></em></figcaption>
</figure>
<p>The results of clinical trials are rarely first reported in a peer-reviewed journal, but rather are usually first presented at a conference – in the case of important practice-changing clinical trials often before an audience of thousands of people. The conference abstracts are also an important source of information, even though they don’t provide the space for more detailed information, including tables and figures. Many societies publish conference abstract in their society journals, but bibliographic databases such as PubMed probably don’t cover all conference abstracts reporting clinical trial results.</p>
<p>The peer-reviewed paper is usually published months or even years after the conference presentation. This not only allows for more mature data – e.g. survival information – but also critical review and discussion of the data. The publication of the peer-reviewed paper is <strong><strong>not</strong></strong> the first time the medical community learns about the results of a clinical trial, or draws conclusions for their own research or clinical practice. To take a recent example from a clinical trial in cancer: the results of the TROPIC trial of cabazitaxel chemotherapy in hormone-refractory metastatic prostate cancer (Clinicaltrials.gov <a href="https://web.archive.org/web/20101105203901/http://clinicaltrials.gov/ct2/show/NCT00417079">NCT00417079</a>) were <a href="https://web.archive.org/web/20101105203901/http://www.asco.org/ASCOv2/Meetings/Abstracts?&amp;vmview=abst_detail_view&amp;confID=73&amp;abstractID=30560">first reported</a> at the ASCO Genitourinary Cancers Symposium March 5 (and there was a <a href="https://web.archive.org/web/20101105203901/http://en.sanofi-aventis.com/binaries/20100304_Asco_GU_en_tcm28-27547.pdf">press release</a> a day earlier). Capazitaxel <a href="https://web.archive.org/web/20101105203901/http://www.fda.gov/AboutFDA/CentersOffices/CDER/ucm216214.htm">was approved</a> by the FDA on June 17, and I treated the first patient with the drug in September. The peer-reviewed paper <a href="https://web.archive.org/web/20101105203901/http://dx.doi.org/10.1016/S0140-6736(10)61389-X">was published</a> in <em><em>The Lancet</em></em> October 2.</p>
<p>I would like to suggest that we need a preprint archive for clinical trial research papers. The preprints should be uploaded at the time of journal submission, or shortly after the conference presentation. The preprint server should add some structure to the preprints, e.g. linking to both the clinical trials registry (Clinicaltrials.gov, EudraCT, and others) and the published paper. The preprint should fulfill the requirement for reporting results set by the FDA. Articles in the preprint archive should be freely available, and should ideally provide the main results as downloadable datasets. The preprint server will only work if medical journals – and ideally the <a href="https://web.archive.org/web/20101105203901/http://www.icmje.org/">ICMJE</a> – have a clear policy allowing prior publication as preprint.</p>
<p><em><em>This post was inspired by discussions with Salvatore Mele and Ivan Oransky. And it is my first contribution to the</em></em> <a href="https://web.archive.org/web/20101105203901/http://www.openaccessweek.org/"><em><em>Open Access Week</em></em></a> <em><em>that starts on Monday.</em></em></p>
<figure>
<img src="https://web.archive.org/web/20101105203901im_/http://www.linkwithin.com/pixel.png" class="kg-image" />
</figure>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Today I started a new blogging network]]></title>
        <id>4mdbvsx-3s082p9-2s5qyyp-w3hwv</id>
        <link href="https://blog.front-matter.io/mfenner/today-i-started-a-new-blogging-network"/>
        <updated>2010-10-15T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[In the last few weeks I haven’t seen any announcement for a new science blogging network (the last one was probably Scientific American in September). So I thought today would be a good day to start a new one. Earlier today I wrote the first post on the first blog hosted by my institution....]]></summary>
        <content type="html"><![CDATA[<p>In the last few weeks I haven’t seen any announcement for a new science blogging network (the last one was probably <a href="https://web.archive.org/web/20101105204155/http://blog.coturnix.org/2010/09/15/alert-some-big-and-important-and-exciting-news/">Scientific American</a> in September). So I thought today would be a good day to start a new one. Earlier today I wrote <a href="https://web.archive.org/web/20101105204155/http://blogs.mh-hannover.de/tumorzentrum/2010/10/15/herzlich-willkommen/">the first post</a> on the first blog hosted by <a href="https://web.archive.org/web/20101105204155/http://www.mh-hannover.de/index.php?id=2&amp;L=1">my institution</a>. The blog runs on a WordPress 3.0 installation that I hope will see more blogs in the future – our library will probably be next (they are currently <a href="https://web.archive.org/web/20101105204155/http://mhhbibliothek.wordpress.com/">hosted on WordPress.com</a>).</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/12th/5001818922_f30d953f07.jpeg" class="kg-image" width="500" height="333" alt="Flickr image by hanspoldoja." /><figcaption aria-hidden="true"><em><em>Flickr image by hanspoldoja.</em></em></figcaption>
</figure>
<p>The new blog is the official blog of our Comprehensive Cancer Center (my day job). I will write about various aspects of the work there – something I wanted to do for a while. This is also the first time that I write in German, which will be much easier, but at the same time will reach a smaller audience. Writing for my institution will also be an interesting experience. Although I don’t think that this will start something big at our institution, institutions are in fact a good place for blogging networks. Special thanks go to Stefan Zorn in our PR department for encouraging words, and to <a href="https://web.archive.org/web/20101105204155/http://scienceblog.cancerresearchuk.org/about-the-authors/">Henry Scowcroft</a> from Cancer Research UK for many good tips about blogging for an institution.</p>
<p>The new blog of course doesn’t mean that I stop writing here. I’m too deep into this.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nature.com iPhone app updated]]></title>
        <id>3ayefj7-7v69bkv-q16rbg9-ymk7s</id>
        <link href="https://blog.front-matter.io/mfenner/nature-com-iphone-app-updated"/>
        <updated>2010-10-13T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Earlier this year I wrote about the Nature.com iPhone application that was released in February. Two weeks ago the app was updated with the following changes:Access to abstracts from additional journals: <em><em>Nature Genetics</em></em>,...]]></summary>
        <content type="html"><![CDATA[<p>Earlier this year I <a href="https://web.archive.org/web/20101105204928/http://blogs.nature.com/mfenner/2010/02/08/nature-com-iphone-app-in-pictures">wrote about</a> the Nature.com iPhone application that was released in February. Two weeks ago the app was updated with the following changes:</p>
<ul>
<li>Access to abstracts from additional journals: <em><em>Nature Genetics</em></em>, <em><em>Nature Medicine</em></em>, <em><em>Nature Biotechnology</em></em>, <em><em>Nature Reviews Microbiology</em></em>, <em><em>Nature Reviews Genetics, Nature Physics</em></em> and <em><em>Nature Communications.</em></em></li>
<li><em><em>Access to fulltext articles for a monthly subscription fee ($9.99 for Nature, $8.99 for the other journals). Access to Nature News and Nature Communications is free.</em></em></li>
</ul>
<p>The updated app doesn’t work with the iPad, and the Android version that was announced in February has not been released yet. Users that don’t update to the newest version of the app don’t get access to additional journals, but can still read full-text articles from Nature.</p>
<p>Reading papers on the iPad for me is a lot of fun. The reading experience on the iPhone/iPod touch is very different, and not something I would spend $9.99 a month for. But it is obvious where this is heading: many users (including myself) would probably spend $9.99/month for a nicely done iPad version of Nature. Especially if content is free/cheaper with an institutional subscription. And we can already read PLoS content for free with the <a href="https://web.archive.org/web/20101105204928/http://blogs.plos.org/everyone/2010/09/16/plos-reader-2-0/">PLoS Reader</a>.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Survey asks for feedback about ORCID unique researcher identifier]]></title>
        <id>77jqzv9-qa8zd9m-q1shw49-dcap</id>
        <link href="https://blog.front-matter.io/mfenner/survey-asks-for-feedback-about-orcid-unique-researcher-identifier"/>
        <updated>2010-10-12T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The ORCID initiative for unique researcher identifiers yesterday started a survey that everybody interested in ORCID should fill out.<em><em>Flickr image</em></em><em><em> from Wessex Arachaeology.</em></em>The survey asks questions about the main services that users expect from ORCID,...]]></summary>
        <content type="html"><![CDATA[<p>The <a href="https://web.archive.org/web/20101013051207/http://www.orcid.org/">ORCID</a> initiative for unique researcher identifiers yesterday started a <a href="https://web.archive.org/web/20101013051207/http://survey.euro.confirmit.com/wix/p501806648.aspx">survey</a> that everybody interested in ORCID should fill out.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/12th/215910967_7ede5cc61a.jpeg" title="survey" class="kg-image" width="347" height="500" alt="Flickr image from Wessex Arachaeology." /><figcaption aria-hidden="true"><a href="https://web.archive.org/web/20101013051207/http://www.flickr.com/photos/wessexarchaeology/215910967/"><em><em>Flickr image</em></em></a> <em><em>from Wessex Arachaeology.</em></em></figcaption>
</figure>
<p>The survey asks questions about the main services that users expect from ORCID, and how the ORCID service should be paid for (e.g. membership fees or fee-for service).</p>
<p>In quick response to the announcement of the survey on Twitter, an <a href="https://web.archive.org/web/20101013051207/http://friendfeed.com/researchremix/d5611278/rt-orcid_org-please-help-us-move-orcid-forward">interesting discussion</a> started on <strong><strong>FriendFeed</strong></strong>. Several commenters were worried that ORCID tries to be too much. Their thoughts were nicely summarized in <a href="https://web.archive.org/web/20101013051207/http://twitter.com/#!/CameronNeylon/status/27158241887">this tweet</a> by <strong><strong>Cameron Neylon</strong></strong>:</p>
<blockquote>
<em><em>Agree with @ReaderMeter + @mrgunn: @ORCID_Org will be a success when it does amazing things for me and I barely notice it’s there.</em></em>
</blockquote>
<p>The survey is open until October 28, the results will be reported in November.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Registration for ScienceOnline2011 starts today]]></title>
        <id>6v4kz25-rym8dfv-bzfjm3v-dd3jv</id>
        <link href="https://blog.front-matter.io/mfenner/registration-for-scienceonline2011-starts-today"/>
        <updated>2010-10-10T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Registration for the ScienceOnline2011 conference starts today at 12 noon EST. This is the fifth annual conference on science and the web, and will be held January 13-15, 2011 in Research Triangle Park,...]]></summary>
        <content type="html"><![CDATA[<p>Registration for the <a href="https://web.archive.org/web/20101124185427/http://scienceonline2011.com/">ScienceOnline2011</a> conference starts today at 12 noon EST. This is the fifth annual conference on science and the web, and will be held January 13-15, 2011 in Research Triangle Park, North Carolina.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/Scionline2011-100.png" title="scienceonline2011" class="kg-image" width="325" height="100" />
</figure>
<p>This is the second time I will go to the conference – I had a wonderful time in 2009. The discussion around the 2009 session <a href="https://web.archive.org/web/20101124185427/http://jdupuis.blogspot.com/2009/01/scienceonline-09-sunday-summary-and.html">Reputation, Authority and Incentives</a> (moderated by Björn Brembs and Pete Binfield) was really the starting point for my interest in unique researcher identifiers. The conference was also the first time that I met PLoS Blogs partner <a href="https://web.archive.org/web/20101124185427/http://blogs.plos.org/takeasdirected/">David Kroll</a> and many other great people.</p>
<p>You will find my name on the <a href="https://web.archive.org/web/20101124185427/http://scio11.wikispaces.com/Program+Finalization">preliminary program</a> in a session <strong><strong>Having Fun with Citations</strong></strong> (together with Melody Dye)<strong><strong>:</strong></strong></p>
<blockquote>
<em><em>Citations play a central role in science communication, but their role in the traditional scientific publication is often rather boring. We love to count citations for measures of scientific impact, but we spend little time thinking about the context and meaning of citations. In this session I would like to talk about topics ranging from semantic meaning of citations (using CiTO, the Citation Typing Ontology by David Shotton), citations of retracted papers, citations of datasets (using Datacite), the importance of an Open Bibliography, formatting of citations using CSL (Citation Style Language), citations in Twitter and other unusual places, citation mutations and the integration of unique researcher identifiers (using ORCID). There is some interesting work on how citation rates follow Zipfian-like distributions; it would be interested to discuss the background and implications.</em></em>
</blockquote>
<p>The exact session format and the other session moderators have not been finalized. There are also at least two related session proposals: <strong>How is the Web changing the way we identify scientific impact?</strong> (Jason Priem, Paul Groth) <strong><strong></strong></strong> and <strong><strong>The Digital Toolbox: What’s Needed?</strong></strong> (Kaitlin Thaney) – we might combine some of our ideas. And of course I am also looking forward to again meet all these wonderful people that are interested in science and the web.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New on Arxiv: first report of SOAP open access publishing project]]></title>
        <id>6f3kmrg-xn78met-nsk7wpn-scxxw</id>
        <link href="https://blog.front-matter.io/mfenner/new-on-arxiv-first-report-of-soap-open-access-publishing-project"/>
        <updated>2010-10-09T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Practically all papers in high-energy physics (&gt; 90% since the late 1990s) are first published on the ArXiv preprint server. Several related disciplines also have a long ArXiv tradition. But ArXiv is also an excellent source of interesting papers about scientific publishing,...]]></summary>
        <content type="html"><![CDATA[<p>Practically all papers in high-energy physics (&gt; 90% since the late 1990s) are first published on the <a href="https://web.archive.org/web/20101013051257/http://arxiv.org/">ArXiv</a> preprint server. Several related disciplines also have a long ArXiv tradition. But ArXiv is also an excellent source of interesting papers about scientific publishing, including this one from last Monday:</p>
<p><strong><strong>Dallmeier-Tiessen S, Darby R, Goerner B, Hyppoelae J, Igo-Kemenes P, Kahn D, et al.</strong></strong> First results of the SOAP project. Open access publishing in 2010. 2010 Oct; Available from: <a href="https://web.archive.org/web/20101013051257/http://arxiv.org/abs/1010.0506">http://arxiv.org/abs/1010.0506</a>.</p>
<p>The paper is a comprehensive report on the current state of open access publishing – and deserves a separate blog post. <strong><strong>Salvatore Mele</strong></strong> (one of the coauthors) pointed me to this paper in a very interesting dinner conversation last night. Follow the <a href="https://web.archive.org/web/20101013051257/http://www.citeulike.org/group/13987/library">Gobbledygook reading list</a> for more interesting ArXiv papers, including a <a href="https://web.archive.org/web/20101013051257/http://arxiv.org/abs/0906.5418">gem</a> from 2009 on the citation behavior in high-energy physics.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Paper published: Reference Management meets Web 2.0]]></title>
        <id>7zzzdng-jzw88s8-7rnbacm-834a4</id>
        <link href="https://blog.front-matter.io/mfenner/paper-published-reference-management-meets-web-2-0"/>
        <updated>2010-10-08T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[<em><em>Cellular Therapy and Transplantation</em></em> today published my paper Reference Management meets Web 2.0. Regular readers of this blog probably know about most of the things I talk about in the paper,...]]></summary>
        <content type="html"><![CDATA[<p><em><em>Cellular Therapy and Transplantation</em></em> today published my paper <a href="https://web.archive.org/web/20101016090740/http://dx.doi.org/10.3205/ctt-2010-en-000087.01">Reference Management meets Web 2.0</a>. Regular readers of this blog probably know about most of the things I talk about in the paper, but I hope it is a good introduction for those new to reference management.</p>
<p>The paper wouldn’t be possible without FriendFeed, because Claudia Koltzenburg – the Managing Editor of the journal and regular contributor to FriendFeed – contacted me after one of our discussions there.</p>
<h2 id="references">References</h2>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reference Management with the iPad]]></title>
        <id>5krqzep-95p8mhr-31cda62-t29sy</id>
        <link href="https://blog.front-matter.io/mfenner/reference-management-with-the-ipad"/>
        <updated>2010-10-06T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The iPad was released six months ago, and we already have several reference managers available for the platform. Reading the PDF of a scientific paper on an iPad is a positive experience, and to me very different from reading the PDF on a regular computer....]]></summary>
        <content type="html"><![CDATA[<p>The iPad was released six months ago, and we already have several reference managers available for the platform. Reading the PDF of a scientific paper on an iPad is a positive experience, and to me very different from reading the PDF on a regular computer. While this could also be done with generic iPad PDF readers such as <strong><strong>iBooks</strong></strong> or <strong><strong>Goodreader</strong></strong>, reference managers make it much easier to organize a large collection of PDF files.</p>
<h3 id="citeulike">CiteULike</h3>
<p>CiteULike is a browser-based reference manager and works on the iPad as expected – and this includes the bookmarklet. CiteULike is a good tool to collect references when browsing with the iPad. CiteULike doesn’t allow offline reading of PDF files. While you can read the HTML versions of papers with the iPad web browser, this requires not only online access, but often also access through a paywall.<br />
<a href="https://web.archive.org/web/20101016090748/http://www.citeulike.org/">More Info</a></p>
<h3 id="papers-for-ipad">Papers for iPad</h3>
<p>This was the first reference manager specifically designed for the iPad. Papers for Mac owners can synchronize all their references and fulltext PDF files. The functionality is similar to the Mac version and also includes a good PDF reader and searching for references in PubMed and other online databases. This makes Papers a good stand-alone solution that doesn’t depend on syncing to another applications.<br />
<a href="https://web.archive.org/web/20101016090748/http://mekentosj.com/papers/ipad/">More Info</a> – <a href="https://web.archive.org/web/20101016090748/http://itunes.apple.com/app/papers/id304655618?mt=8">iTunes</a></p>
<h3 id="mendeley-lite">Mendeley Lite</h3>
<p>Mendeley Lite syncs with all references you store in your Mendeley web account. I personally prefer this approach as I don’t particularly like syncing between iPad and desktop computer. Just like Papers, this application works both on iPad and iPhone. You can’t directly add references to Mendeley Lite. Mendeley is currently the only application that has both a web-based and an iPad version. This increases the options, but probably also adds to user confusion. Add reference with the web-based version, sync, then read PDF in Mendeley Lite?<br />
<a href="https://web.archive.org/web/20101016090748/http://itunes.apple.com/de/app/mendeley-reference-manager/id380669300?mt=8">iTunes</a></p>
<h3 id="sente-viewer-for-ipad">Sente Viewer for iPad</h3>
<p>Last Friday <strong><strong>Sente Viewer for iPad</strong></strong> was released. Similar to Papers and Mendeley, Sente Viewer for iPad synchronizes with the desktop version of the application. The application allows browsing of all synchronized references and reading of the associated fulltext PDF papers. The <strong><strong>Sente for iPad</strong></strong> application will be released later and will also allow adding and editing of references. I’m looking forward to “targeted browsing” which will capture both reference information and fulltext PDF files using a built-in web browser – another variation of reference manager integration with the web browser.<br />
<a href="https://web.archive.org/web/20101016090748/http://www.thirdstreetsoftware.com/site/iPadSente.html">More Info</a> – <a href="https://web.archive.org/web/20101016090748/http://itunes.apple.com/us/app/sente-viewer/id392355663?mt=8">iTunes</a></p>
<h3 id="further-reading">Further Reading</h3>
<p>Roderic Page has done <a href="https://web.archive.org/web/20101016090748/http://iphylo.blogspot.com/2010/09/viewing-scientific-articles-on-ipad.html">a lot of writing</a> about reading scientific papers on the iPad, including reviews for <a href="https://web.archive.org/web/20101016090748/http://iphylo.blogspot.com/2010/08/viewing-scientific-articles-on-ipad_31.html">Papers for iPad</a> and <a href="https://web.archive.org/web/20101016090748/http://iphylo.blogspot.com/2010/08/viewing-scientific-articles-on-ipad_3052.html">Mendeley Lite</a>. Wizfolio is another web-based reference manager that <a href="https://web.archive.org/web/20101016090748/http://blog.wizfolio.com/index.php/2010/07/2-thumb-power-surfing-on-the-ipad/">works on the iPad</a>.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond the PDF – it is time for a workshop]]></title>
        <id>39e9r62-44e8pma-3hy99bg-jsxwd</id>
        <link href="https://blog.front-matter.io/mfenner/beyond-the-pdf-it-is-time-for-a-workshop"/>
        <updated>2010-10-06T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[PDF has become the standard way we consume scientific papers, but in fact is not a good format for this purpose at all. Or as Martin Robbins said at the recent Science Online London conference: <em><em>PDF is an insult to science</em></em>....]]></summary>
        <content type="html"><![CDATA[<p>PDF has become the standard way we consume scientific papers, but in fact is not a good format for this purpose at all. Or as <a href="https://web.archive.org/web/20101124185433/http://www.mjrobbins.net/">Martin Robbins</a> <a href="https://web.archive.org/web/20101124185433/http://blogs.nature.com/franknorman/2010/09/10/my-science-online-london-2010-impressions">said at the recent Science Online London conference</a>: <em><em>PDF is an insult to science</em></em>. The problem was nicely illustrated by <a href="https://web.archive.org/web/20101124185433/http://wwmm.ch.cam.ac.uk/blogs/murrayrust/">Duncan Hull</a>, using a quote from <a href="https://web.archive.org/web/20101124185433/http://wwmm.ch.cam.ac.uk/blogs/murrayrust/">Peter Murray-Rust</a> from a <a href="https://web.archive.org/web/20101124185433/http://wwmm.ch.cam.ac.uk/blogs/murrayrust/?p=1069">May 2008</a> article:</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/18th/3367901850_5b137da197.jpeg" class="kg-image" width="500" height="375" alt="Flickr image by dullhunk. From his Defrosting the Digital Library slideshow." /><figcaption aria-hidden="true"><em><em>Flickr image by dullhunk. From his <a href="https://www.slideshare.net/dullhunk/defrosting-the-digital-library-a-survey-of-bibliographic-tools-for-the-next-generation-web">Defrosting the Digital Library</a> slideshow.</em></em></figcaption>
</figure>
<p>There are of course ways journal publishers can add metadata to PDF files, e.g. using <a href="https://web.archive.org/web/20101124185433/http://blogs.plos.org/mfenner/2008/12/22/just_doi_it/">XMP</a> markup. But that is only a workaround – what we really want is journal papers in a format that is all about document structure and not about looking good in print. XML is obviously that format with the <a href="https://web.archive.org/web/20101124185433/http://www.inera.com/nlmresources.shtml">NLM-DTD</a> as the de facto standard for scholarly publications. Many journal publishers use that format internally, as does <a href="https://web.archive.org/web/20101124185433/http://www.ncbi.nlm.nih.gov/pmc/">PubMed Central</a> for displaying fulltext papers, as well as archiving projects such as <a href="https://web.archive.org/web/20101124185433/http://www.portico.org/digital-preservation/">Portico</a>.</p>
<p>But there are two problems with how NLM-DTD is used now. The first problem is that most scientific papers still are ultimately read as printouts of PDF files, even though most journals now are online only. I wrote in January that this reading behavior is starting to change (<a href="https://sensiblescience.io/mfenner/how_do_you_read_papers_2010_will_be_different/">How do you read papers? 2010 will be different</a>), and new article formats – such as the <a href="https://web.archive.org/web/20101124185433/http://blogs.plos.org/mfenner/2009/07/26/how_does_the_article_of_the_future_look_like/">Article of the Future</a> by Cell Press – and the success of the iPad are two major reasons for that.</p>
<p>The second problem is that there are no good NLM-DTD tools for authors. <a href="https://sensiblescience.io/mfenner/lemon8_xml_interview_with_mj_suhonos/">Lemon8-XML</a> and the <a href="https://sensiblescience.io/mfenner/interview_with_pablo_fernicola/">Microsoft Word Article Authoring Add-In</a> are two examples that I have written about before. Publishers spend a significant amount of time and money (and using tools such as <a href="https://sensiblescience.io/mfenner/extyles_interview_with_elizabeth_blake_and_bruce_rosenblum/">eXtyles</a>) to convert submitted manuscripts from Microsoft Word, Open Office or LaTeX formats into NLM-DTD. I wrote about my <a href="https://sensiblescience.io/mfenner/my_paper_writing_dream_machine_1_0/">paper writing dream machine</a> that would use the NLM-DTD more than two years ago. I still believe that the ideal authoring tool should be web-based, but in 2010 this should be done in HTML5 and not Adobe Flash or Microsoft Silverlight.</p>
<p>The two problems are obviously related. When more people are reading papers in other formats than PDF and see the advantages, it changes the idea of a scientific paper that most people have from a static document do a document that is enriched with primary research data, visualizations that use 3D and video, links to related resources, the possibility to comment, and has more than one document version. In other words, what HTML and the WWW <a href="https://web.archive.org/web/20101124185433/http://www.w3.org/History/19921103-hypertext/hypertext/WWW/TheProject.html">were invented for</a> by Tim Berners-Lee when working at CERN. At this point the incentives to create better authoring tools will be much greater, and the second problem should solve itself. And HTML and related technologies have now evolved to the point where they allow the building of some very attractive authoring tools.</p>
<p><a href="https://web.archive.org/web/20101124185433/http://www.sdsc.edu/~bourne/">Phil Bourne</a> has written and talked a lot about this problem, including a recent editorial in <em><em>PLoS Computational Biology</em></em> (<a href="https://web.archive.org/web/20101124185433/http://dx.doi.org/10.1371/journal.pcbi.1000787">What do I want from the publisher of the future?</a>) and a related <a href="https://web.archive.org/web/20101124185433/http://www.scivee.tv/node/18299">SciVee video</a>. He has also been working very hard to organize the <strong><strong>Beyond the PDF</strong></strong> workshop, to be held in San Diego January 19-21. The workshop <a href="https://web.archive.org/web/20101124185433/https://sites.google.com/site/beyondthepdf/">website</a> went live last week and is a very good source of information. The goal of the workshop is to identify a set of requirements and to start developing open source code that accelerates moving beyond PDF for scientific papers.</p>
<p>I will of course do everything possible to attend this workshop, but haven’t yet secured funding for the trip. What I hope to contribute to the <strong><strong>Beyond the PDF</strong></strong> project is a more intelligent use of citations (the following was also posted to the Beyond the PDF <a href="https://web.archive.org/web/20101124185433/http://groups.google.com/group/beyond-the-pdf">discussion group</a>):</p>
<h3 id="unique-author-identifiers">Unique Author Identifiers</h3>
<p>Unique author identifiers such as <a href="https://www.orcid.org/">ORCID</a> not only are a big help in knowledge discovery (finding other interesting papers, datasets, etc. by the same person), but they can also help to better define the contributions of a researcher to a particular paper. This could mean a general description of the contribution (had idea for project, collected data, helped write manuscript, etc.), or could also describe a very specific contribution (researcher X did experiment Y).</p>
<h3 id="citation-styles">Citation styles</h3>
<p>We already have far too many citation styles, but the styles we use<br />
have several important shortcomings. I suggest the following changes:</p>
<ul>
<li>Use the <a href="https://www.doi.org/">DOI</a> instead of volume and page numbers whenever possible</li>
<li>Use <strong><strong>ORCID</strong></strong> to link to the authors of the citations</li>
<li>Use the Citation Typing Ontology (<a href="https://web.archive.org/web/20101124185433/http://www.crossref.org/CrossTech/2009/03/citation_typing_ontology.html">CiTO</a>) to describe the meaning of the citation</li>
<li>Use a note field to explain why the citation was used</li>
</ul>
<p>Any authoring tool should obviously use the Citation Style Language (<a href="https://web.archive.org/web/20101124185433/http://blogs.plos.org/mfenner/2010/09/24/citation-style-language-an-interview-with-rintze-zelle-and-ian-mulvany/">CSL</a>) to format citations. And the citations should be provided with a <a href="https://creativecommons.org/publicdomain/zero/1.0/">CC Zero</a> or similar license to allow reuse.</p>
<h3 id="cited-by">Cited by</h3>
<p>At least as important as the citations by a particular paper are the incoming citations, the works that cite a paper. We need much better mechanisms to automate these “trackbacks”, and this should go beyond other papers and also include datasets, blog posts, etc. The incoming citations are of course very helpful for discovery, and the basis for <a href="https://altmetrics.org/manifesto/">alternative metrics</a>.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReaderMeter: researcher-level metrics based on readership]]></title>
        <id>1t7k21d-qs48ffr-8rs83kb-jkne4</id>
        <link href="https://blog.front-matter.io/mfenner/readermeter-researcher-level-metrics-based-on-readership"/>
        <updated>2010-10-03T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[In a blog post last week, Dario Taraborelli officially announced ReaderMeter. ReaderMeter takes the usage data from reference managers (starting with Mendeley) to analyze the impact of publications by a particular author.ReaderMeter is a welcome addition to other metrics of researcher impact,...]]></summary>
        <content type="html"><![CDATA[<p>In a <a href="https://web.archive.org/web/20101016090754/http://www.academicproductivity.com/2010/readermeter-crowdsourcing-research-impact/">blog post last week</a>, Dario Taraborelli officially announced <a href="https://web.archive.org/web/20101016090754/http://readermeter.org/">ReaderMeter</a>. ReaderMeter takes the usage data from reference managers (starting with <a href="https://www.mendeley.com/">Mendeley</a>) to analyze the impact of publications by a particular author.</p>
<p>ReaderMeter is a welcome addition to other metrics of researcher impact, most of which are citation-based. And ReaderMeter was hacked together in a few nights, so the service should improve over time. Dario mentions some of the current limitations in his blog post:</p>
<ul>
<li><strong><strong>Usage data only from Mendeley</strong></strong>. We don’t know whether the reading patterns of users from other reference managers (e.g. CiteULike, Zotero, Endnote Web and Refworks) differ. Dario is working on adding more services.</li>
<li><strong><strong>Author disambiguation</strong></strong>. This is a critical component, and the <a href="https://sensiblescience.io/mfenner/orcid-as-unique-author-identifier-what-is-it-good-for-and-should-we-worry-or-be-happy/">ORCID</a> initiative is probably the best tool to overcome this limitation, especially when data from several reference managers have to be integrated.</li>
<li><strong><strong>Article disambiguation</strong></strong>. Multiple copies of the same reference are common in reference manager databases. The <a href="https://doi.org/">DOI</a> can solve this problem for those publications that have a DOI, but consequent use of the DOI by journals, citation databases and reference managers has yet to become standard practice.</li>
</ul>
<p>ReaderMeter is visually pleasing and fun to use. And the data are also provided in JSON format, which allows integration into other web pages, e.g. the researcher’s home page.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advice for scientists who want to become Wikipedia editors]]></title>
        <id>6e31e3n-x289z38-4trn257-zdcfz</id>
        <link href="https://blog.front-matter.io/mfenner/advice-for-scientists-who-want-to-become-wikipedia-editors"/>
        <updated>2010-10-02T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[This week <em><em>PLoS Computational Biology</em></em> published another helpful editorial in their 10 Simple Rules collection (https://doi.org/10.1371/journal.pcbi.1000941): <em>Ten Simple Rules for Editing Wikipedia. </em>I have very rarely edited Wikipedia articles myself,...]]></summary>
        <content type="html"><![CDATA[<p>This week <em><em>PLoS Computational Biology</em></em> published another helpful editorial in their <a href="https://web.archive.org/web/20101016090759/http://www.ploscollections.org/article/browseIssue.action?issue=info%3Adoi%2F10.1371%2Fissue.pcol.v03.i01">10 Simple Rules</a> collection (https://doi.org/<a href="https://doi.org/10.1371/journal.pcbi.1000941">10.1371/journal.pcbi.1000941</a>): <em>Ten Simple Rules for Editing Wikipedia.</em></p>
<p>I have very rarely edited Wikipedia articles myself, and I suspect this behavior is the rule and not the exception among science bloggers. And this always puzzled me, as editing Wikipedia articles should be a natural thing to do for those familiar with blogs and other online tools.</p>
<p>The editorial not only gives good advice for the beginning Wikipedia editor, there are many other places where you can also find this information. But the editorial is written with the scientist as Wikipedia editor in mind and clarifies some of the issues that can arise.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why can’t I reuse these tables and figures?]]></title>
        <id>1d9bk59-87j9psr-wx0wq0j-q09f3</id>
        <link href="https://blog.front-matter.io/mfenner/why-cant-i-reuse-these-tables-and-figures"/>
        <updated>2010-09-30T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Tables and figures contain the data of a scientific paper in condensed (and often visually appealing) form. This is why they are among the first thing we look at, and why they are often reused when we discuss the paper in a presentation or blog...]]></summary>
        <content type="html"><![CDATA[<p>Tables and figures contain the data of a scientific paper in condensed (and often visually appealing) form. This is why they are among the first thing we look at, and why they are often reused when we discuss the paper in a presentation or blog post.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/ppat.v06.i08.jpg" title="PLoS Pathogens" class="kg-image" width="251" height="251" alt="From https://doi.org/10.1371/journal.ppat.1001042. Image credit: Thomas J. Hannan, Washington University." /><figcaption aria-hidden="true"><em><em>From</em> https://doi.org/<em><a href="https://web.archive.org/web/20120525130956/http://dx.doi.org/10.1371/journal.ppat.1001042">10.1371/journal.ppat.1001042</a>. Image credit: Thomas J. Hannan, Washington University.</em></em></figcaption>
</figure>
<p>Electronic publication has dramatically simplified the reuse of tables and figures, and therefore reuse has become very common – you probably find reused material in most presentations given in academic institutions or at conferences.</p>
<p>Most authors will probably be happy that their results are disseminated, and reuse is likely to lead to more people reading the full paper and citing the work.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/bmccancer-e1285879074865.jpeg" title="bmccancer" class="kg-image" width="500" height="450" alt="From https://doi.org/10.1186/1471-2407-10-286." /><figcaption aria-hidden="true">From https://doi.org/<a href="https://web.archive.org/web/20120525130956/http://dx.doi.org/10.1186/1471-2407-10-286">10.1186/1471-2407-10-286</a>.</figcaption>
</figure>
<p>But this reuse has two problems. The first problem is copyright. Many journals own the copyright of the papers they publish and don’t allow reuse without prior permission. Unfortunately copyright is a complicated issue, and also differs between countries. Most researchers assume “<a href="https://web.archive.org/web/20120525130956/http://www.copyright.gov/fls/fl102.html">Fair use</a>“ when they reuse material, but this might not apply to all situations, e.g. presentations at conferences. And many researchers don’t understand that they have often given away the copyright to their own works, so that they can’t show a figure from one of their papers without permission.</p>
<p>Many publishers have automated the process of obtaining permissions for copyrighted work, e.g. using the <a href="http://www.copyright.com/viewPage.do?pageCode=pu4-n">Rightslink</a> system of the Copyright Clearance Center. But it still requires a considerable investment in time (and often money) to obtain all permissions, especially since these are usually one-time permissions only. This combination of unawareness of the details of copyright law and the required extra work means that many researchers probably don’t obtain permissions prior to reuse.</p>
<p>The solution to the copyright problems is obviously to use material with a <a href="https://creativecommons.org/">Creative Commons</a> license whenever possible, as I have done in this blog post. And most Open Access papers are published under this license, so there is plenty of material to choose from.</p>
<p>But there is also a second problem with reusing tables and figures. They were designed to be part of a paper and often look terrible in a presentation, particularly tables.</p>
<figure>
<img src="https://assets.front-matter.io/ghost/2021/April/10th/plosone-e1285879245843.png" title="plosone" class="kg-image" width="500" height="402" alt="From https://doi.org/10.1371/journal.pone.0006022." /><figcaption aria-hidden="true"><em><em>From</em> https://doi.org/</em><a href="https://web.archive.org/web/20120525130956/http://dx.doi.org/10.1371/journal.pone.0006022"><em><em>10.1371/journal.pone.0006022</em></em></a>.</figcaption>
</figure>
<p>The solution to this problem is to provide the data behind the table or figure, so that the information can be displayed in a way that makes sense in a presentation. Here we usually have to reduce the amount of information, but it could also mean that we remix the content with other sources. The Creative Commons licenses discussed above are <a href="http://pantonprinciples.org/">not appropriate</a> for data. Whenever possible, scientific data should be placed in the public domain.</p>
<p>It is important to distinguish the publication of table and figure data from the publication of the <a href="https://web.archive.org/web/20120525130956/http://blogs.openaccesscentral.com/blogs/bmcblog/entry/bmc_research_notes_wants_your">whole research dataset</a>. The open questions with the latter (e.g. standard data formats, appropriate repositories, archiving) don’t apply to the former. This means that publishers could start providing these data immediately. I’m confident that they would see an increase in paper downloads and citations. But more importantly I hope this would lead to better presentations in seminars and at conferences.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Paper retractions do not induce citation mutations]]></title>
        <id>599ppaw-k0m9yj9-9n1ag54-vh3ge</id>
        <link href="https://blog.front-matter.io/mfenner/paper-retractions-do-not-induce-citation-mutations"/>
        <updated>2010-09-26T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Dear Christian Specht,Thank you very much for your detailed response in <em><em>The Scientist</em></em> to our previous letter regarding citation mutations. You clarified several issues that were raised in your original study,...]]></summary>
        <content type="html"><![CDATA[<p>Dear Christian Specht,</p>
<p>Thank you very much for <a href="https://web.archive.org/web/20120525055229/http://www.the-scientist.com/news/display/57698/">your detailed response</a> in <em><em>The Scientist</em></em> to <a href="https://web.archive.org/web/20120525055229/http://blogs.plos.org/mfenner/2010/09/20/letter-to-the-scientist/">our previous letter</a> regarding citation mutations. You clarified several issues that were raised in <a href="https://web.archive.org/web/20120525055229/http://www.the-scientist.com/news/display/57689/">your original study</a>, particularly that citation mutation rates have dropped significantly in the last 10 years (probably due to the more widespread use of reference management software), and that some citation mutations (e.g. 680→685 in Laemmli 1970) might be introduced not by citing authors, but by the citation database.</p>
<p>You rightfully point out that citation mutations indicate a much bigger problem: <em><em>authors often do not read the publications cited in their work</em></em>. I am not aware of any available direct data, but in an ongoing study Richard Grant is looking at this question (<a href="https://web.archive.org/web/20120525055229/http://blog.the-scientist.com/2010/09/23/mutatis-citandi/">Do you read the papers you cite?</a>). The preliminary data that Richard kindly made available indicate that more than 85% of authors indeed read the papers they cite.</p>
<p>In another study made aware to us by <a href="https://web.archive.org/web/20120525055229/http://twitter.com/noahWG/status/25410352261">Noah Gray</a>, Neale et al. (Neale 2009) used an elegant experimental design to address the same question. They studied lethal acquired mutations – retracted papers that in theory should no longer be cited – as an estimate of how diligent authors were reading the papers they cite.</p>
<p>The authors compared the number of citations of 102 retracted papers (starting 12 months after the retraction) to a control group of papers and found no difference in the average number of citations (26 vs. 27). Content analysis of a subset of citing papers indicated that more than 50% of citations used the retracted paper to support their own findings and less than 5% of citing papers mentioned the retraction.</p>
<p>These findings not only seem to contradict the preliminary findings by Richard Grant, but also indicate that journal publishers and citation databases may not be properly communicating paper retractions.</p>
<h3 id="references">References</h3>
<p><strong><strong>Laemmli UK</strong></strong>. Cleavage of Structural Proteins during the Assembly of the Head of Bacteriophage T4. <em><em>Nature</em></em>. 1970;227:680-685. https://doi.org/<a href="https://web.archive.org/web/20120525055229/http://dx.doi.org/10.1038/227680a0">10.1038/227680a0</a></p>
<p><strong><strong>Neale AVV, Dailey RK, Abrams J</strong></strong>. Analysis of citations to biomedical articles affected by scientific misconduct. <em><em>Science and Engineering Ethics</em></em>. 2010;16(2):251-261. https://doi.org/<a href="https://web.archive.org/web/20120525055229/http://dx.doi.org/10.1007/s11948-009-9151-4">10.1007/s11948-009-9151-4</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Citation Style Language: An Interview with Rintze Zelle and Ian Mulvany]]></title>
        <id>1pev8dh-7d8ahsj-gwdmbsc-g4nw</id>
        <link href="https://blog.front-matter.io/mfenner/citation-style-language-an-interview-with-rintze-zelle-and-ian-mulvany"/>
        <updated>2010-09-24T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Citation styles are one of the greater mysteries for the novice manuscript writer. There are numerous ways that authors, title, journal, etc. can be arranged and formatted (see examples below), and in bibliographies citations can be ordered either...]]></summary>
        <content type="html"><![CDATA[<p>Citation styles are one of the greater mysteries for the novice manuscript writer. There are numerous ways that authors, title, journal, etc. can be arranged and formatted (see examples below), and in bibliographies citations can be ordered either alphabetically or by order of appearance in the text.</p>
<p><strong><strong>Laemmli UK</strong></strong>. Cleavage of Structural Proteins during the Assembly of the Head of Bacteriophage T4. <em><em>Nature</em></em>. 1970;227:680-685.</p>
<p><strong><strong>U. K. Laemmli</strong></strong> (1970). ‘Cleavage of Structural Proteins during the Assembly of the Head of Bacteriophage T4′. <em><em>Nature</em></em> <strong><strong>227</strong></strong>(5259):680-685.</p>
<p><strong><strong>U. K. Laemmli</strong></strong>, <em><em>Nature</em></em> <strong><strong>227</strong></strong>, 680 (1970).</p>
<p>Because of this complexity, it has long become impractical to format citations manually, and formatting of citations and bibliographies is one of the main reasons for using reference management software. I interviewed Rintze Zelle (scientist and open source contributor) and Ian Mulvany (vice president of new product development at Mendeley) to better understand citation styles in general and the open source Citation Style Language (CSL) in particular. CSL co-developers Bruce D’Arcus and Frank Bennett provided important feedback.</p>
<h3 id="1-what-is-the-citation-style-language"><strong><strong>1. What is the Citation Style Language?</strong></strong></h3>
<p>Rintze Zelle: Scientific literature depends heavily on proper referencing. However, when writing a manuscript, manually editing citations and bibliographies is time consuming and error prone. Citation styles also differ between scientific journals, so authors often have to switch citation styles when they submit their manuscript to a different journal than originally anticipated.</p>
<p>The <a href="https://web.archive.org/web/20120525124720/http://citationstyles.org/">Citation Style Language</a> (CSL) is an open XML based language meant to automate the formatting of citations and bibliographies. When provided with the metadata (title, year, authors, etc.) of the cited items (journal articles, books, etc.), and a CSL style, a CSL processor can automatically generate the bibliography and in-text citations. CSL is currently used by <a href="https://web.archive.org/web/20120525124720/http://www.zotero.org/">Zotero</a> and <a href="https://web.archive.org/web/20120525124720/http://www.mendeley.com/">Mendeley</a>, and both programs offer word processor plug-ins for <a href="https://web.archive.org/web/20120525124720/http://office.microsoft.com/en-us/word/">Microsoft Word</a> and <a href="https://web.archive.org/web/20120525124720/http://www.openoffice.org/">OpenOffice.org</a>. Several other projects are working on or exploring CSL support.</p>
<h3 id="2-do-we-really-need-hundreds-of-citation-styles">2. Do we really need hundreds of citation styles?</h3>
<p><strong><strong>Rintze Zelle</strong></strong>: We have asked ourselves this question many times over. Some variability is certainly warranted: numeric, author-date and footnote styles are very different, and each type has its own advantages and disadvantages. However, small variations in citation styles result in a situation where almost every journal or publisher has its unique style. We think that even with the use of automated tools like CSL, reducing the number of citation styles in use could result in significant cost and time savings in scientific publishing. But this is a problem beyond the scope of CSL, so our goal is simply to support all the variability that currently exists in citation styles.</p>
<p><strong><strong>Ian Mulvany</strong></strong>: After working for many years at Springer, and then Nature, I was well aware that most large publishers just push submitted manuscripts out to companies in India where the formatting of the paper happens. The input format and citation formatting really doesn’t matter to most publishers. They just tear the submitted manuscript to pieces and rebuild it in their chosen XML schema.</p>
<p>However, most people using citations are not actually submitting manuscripts for publication, but rather are writing term papers, or theses, or reports. So the weird thing is that citations started off as a required identifier for the literature. Google Scholar and HTTP URIs such as the DOI have almost totally made formatted citations redundant as identifiers, and yet there is still a huge user need to be able to format citations according to a huge variety of styles, and since that need is going to continue for quite a long time, it’s a need that we have to support.</p>
<h3 id="3-what-is-the-difference-between-csl-and-other-citation-style-systems"><strong><strong>3. What is the difference between CSL and other citation style systems?</strong></strong></h3>
<p>Rintze Zelle: Our main “competition” arguably comes from <a href="https://web.archive.org/web/20120525124720/http://www.bibtex.org/">BibTeX</a> and <a href="https://web.archive.org/web/20120525124720/http://www.endnote.com/">EndNote</a>/<a href="https://web.archive.org/web/20120525124720/http://www.refman.com/">Reference Manager</a>. BibTeX is a popular choice for those working with the <a href="https://web.archive.org/web/20120525124720/http://www.latex-project.org/">LaTeX</a> typesetting system, but the user base of LaTeX is relatively small and mostly limited to the sciences. EndNote and Reference Manager are commercial tools offered by Thomson Reuters. While large collections of citation styles are available for each program, the use of these styles is limited to licensed users.</p>
<p>CSL was designed with three main goals:</p>
<ol>
<li>to create an open system that is independent of the operating system, application or document format,</li>
<li>to cover the full range of citation formatting rules in use, extending from the sciences to fields in the humanities as well as law,</li>
<li>to free end users from the complex task of formatting citations.</li>
</ol>
<p>Citation styles should be freely available, up to date, and complete. Switching between styles should be easy, and citation output should automatically localize to the desired language.</p>
<p>We think we’ve come quite far toward reaching these goals with our most recent release, CSL 1.0. The <a href="https://web.archive.org/web/20120525124720/http://citationstyles.org/downloads/specification.html">CSL 1.0 specification</a> covers a wide range of citation rules, and offers advanced features like automatic localization of date formats, terms and punctuation, support for in-field rich text and extensive support for the rendering and disambiguation of names. The first standalone CSL 1.0 processor (the JavaScript <a href="https://web.archive.org/web/20120525124720/http://bitbucket.org/fbennett/citeproc-js/">citeproc-js</a>) is currently being integrated into both Zotero and Mendeley, and is receiving attention for deployment on both the client and the server.</p>
<h3 id="4-can-you-tell-me-how-csl-was-developed">4. Can you tell me how CSL was developed?</h3>
<p><strong><strong>Rintze Zelle</strong></strong>: CSL is the brainchild of Bruce D’Arcus, an associate professor of Geography at Miami University of Ohio. The language was initially implemented for integration into OpenOffice.org, but only became popular in 2006 when Zotero, the first reference manager to use CSL, was released. In these early days major contributions were made to CSL by Zotero developer Simon Kornblith. Subsequently, the Zotero project successfully fostered an active user community, with many users contributing styles to a growing repository of CSL styles.</p>
<p>The year 2008 was a watershed of new developments. Mendeley was released, the second reference manager to use CSL for its citation formatting. Andrea Rossato released the first standalone CSL processor (<a href="https://web.archive.org/web/20120525124720/http://code.haskell.org/citeproc-hs/">citeproc-hs</a>) for use with the <a href="https://web.archive.org/web/20120525124720/http://johnmacfarlane.net/pandoc/">Pandoc</a> text processing system. Also in 2008, two Zotero users who enjoyed the program but felt that CSL could be further improved joined Bruce in CSL development: myself, at that time a PhD researcher in biotechnology at Delft University of Technology in the Netherlands, and Frank Bennett, Jr., an associate law professor at Nagoya University in Japan. Together with Andrea, our different academic and geographic backgrounds proved very useful in CSL development. In preparation for major backward-incompatible changes, CSL 0.8 was released in 2009, and in Spring 2010 CSL 1.0 saw the light of day. The <a href="https://web.archive.org/web/20120525124720/http://citationstyles.org/2010/03/22/citation-style-language-1-0/">1.0 release</a> was accompanied by a move to a new website at <a href="https://web.archive.org/web/20120525124720/http://citationstyles.org/">citationstyles.org</a>, and included improved documentation in the form of a full language specification. CSL development has now calmed down a bit as we await the integration of CSL 1.0 by Zotero and Mendeley.</p>
<p><strong><strong>Ian Mulvany</strong></strong>: We at Mendeley have been using the Citation Style Language for quite a while now. We think it is an amazing project and we are very strongly committed to working with the CSL community in encouraging uptake. We get a lot of feedback from our users and one area that they constantly run into problems with is the need to be able to format a citation in just such a manner. The CSL project is the best way for us to be able to support the needs of our users with these kinds of requests. Our developers have been pushing patches upstream to the <a href="https://web.archive.org/web/20120525124720/http://bitbucket.org/fbennett/citeproc-js/wiki/Home">citeproc-js</a> project, particularly <a href="https://web.archive.org/web/20120525124720/http://www.mendeley.com/profiles/carles-pina/">Carles Pina</a>.</p>
<p>We have added a cut and paste stylebox on our article pages. If you have a look at a <a href="https://web.archive.org/web/20120525124720/http://www.mendeley.com/research/karhunenloeve-eigenvalue-problems-cosmology-we-tackle-large-data-sets/">sample paper</a> you will now see a little citeproc-js driven “Cite this document” box that lets you copy and paste formatted citations in several popular citation styles. We have also been supporting the creation of a <a href="https://web.archive.org/web/20120525124720/http://bitbucket.org/csledit/csl-wysiwyg-editor">WISYWIG citation style editor</a>. The status of the project is that most of the code is complete and we just need to work on getting it integrated into our client, and figuring out the best way to manage the creation of more styles, and how that will work with the CSL community.</p>
<p>One of the things that we have been discussing with Bruce D’Arcus is how to manage the redistribution of new styles, and how to make sure that corrupt styles don’t propagate, and that people get the style that they are looking for. If people want to contribute there is a lot of activity on the <a href="https://web.archive.org/web/20120525124720/https://lists.sourceforge.net/lists/listinfo/xbiblio-devel">mailing list</a> of the CSL project. One thing we think we hope Mendeley can help with is reporting usage statistics on specific style files, so at least people can find the most popular version of a CSL file for a given style.</p>
<h3 id="5-where-can-a-user-find-more-csl-citation-styles-is-it-easy-to-modify-a-csl-style">5. Where can a user find (more) CSL citation styles? Is it easy to modify a CSL style?</h3>
<p><strong><strong>Rintze Zelle</strong></strong>: While anyone is free to write and host their own CSL styles, most CSL styles that are in use are available through the <a href="https://web.archive.org/web/20120525124720/http://www.zotero.org/styles">Zotero Style Repository</a> (many of these styles are licensed under a <a href="https://web.archive.org/web/20120525124720/http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons license</a>). We have to admit that editing CSL styles currently requires some technical skill and knowledge of XML. This hasn’t kept members of the Zotero user community from creating over a thousand CSL styles, but we do recognize that user friendly editing of styles is a very important feature. We therefore applaud Mendeley’s effort to create an online CSL editor.</p>
<h3 id="6-should-publishers-care-about-csl">6. Should publishers care about CSL?</h3>
<p><strong><strong>Ian Mulvany</strong></strong>: As I pointed out the big publishers don’t care about the submission format, but they have not really done a good job of communicating that to their editorial boards. Smaller publishers don’t have the resources to totally reformat submissions, and beyond academic publishing there are a huge number of people who just need to format citations. There is a huge waste of people’s time in reformatting papers for submissions, in fixing styles according to changing requirements from departments, when what should matter is the content. I’d love to get to a point where every publisher accepted the same type of XML input, and our authoring tools all created content conforming to that input format. Citations should be a DOI or other HTTP URI that can be rendered into the appropriate format using CSL and an API.</p>
<p><em><em>Martin Fenner: The Open Access publishers BioMed Central and PLoS plan to add a CSL style download link to their author instruction pages. I hope that more publishers follow this example.</em></em></p>
<h3 id="7-do-you-want-to-talk-about-future-plans-for-csl">7. Do you want to talk about future plans for CSL?</h3>
<p><strong><strong>Rintze Zelle</strong></strong>: We’re very excited about the work Zotero and Mendeley developers are doing to update their programs to support CSL 1.0. The update path should be relatively smooth for users as styles can be automatically updated to the CSL 1.0 format, although styles will often need to be edited to take full advantage of all the new features. Zotero Everywhere <a href="https://web.archive.org/web/20120525124720/http://www.zotero.org/blog/zoteros-next-big-step/">was announced</a> earlier this week and will include a web citation formatting service based on citeproc-js.</p>
<p>There are two things we consider crucial to the further development of CSL: one, we still lack an easy way for users to modify existing styles, although we’re hopeful that <a href="https://web.archive.org/web/20120525124720/http://bitbucket.org/csledit/csl-wysiwyg-editor/">Mendeley’s CSL editor</a> will soon fill this gap. Secondly, we feel there is a need for a more full-featured online style repository which allows users to find their style of choice, to add comments, and to propose style changes.</p>
<p>The goal of CSL is to make citing formatting easier at a general level, across all fields and in all languages. This can only be achieved through a collaborative endeavor, and here we think publishers also share some responsibility. By providing high quality item metadata through robust standards (like <a href="https://web.archive.org/web/20120525124720/http://unapi.info/">unAPI</a> or <a href="https://web.archive.org/web/20120525124720/http://ocoins.info/">COinS</a>), by freely providing clear, correct and complete style guidelines or opting for a standard citation style (like APA, Chicago or <a href="https://web.archive.org/web/20120525124720/http://www.mhra.org.uk/Publications/Books/StyleGuide/download.shtml">MHRA</a>), and perhaps even by creating and hosting their own CSL styles, publishers can make our work, and that of authors, so much easier.</p>
<p>With broad participation and support, we believe that CSL can benefit all fields of scholarship in a similar way Oren Patashnik’s BibTeX helped the sciences, by (further) streamlining the publication process, improving access to metadata of materials of all kinds, and by allowing scholars to spend more of their time on the core of their research.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zotero soon is Everywhere]]></title>
        <id>122wp42-kq68vrr-0xk23tv-s7hfs</id>
        <link href="https://blog.front-matter.io/mfenner/zotero-soon-is-everywhere"/>
        <updated>2010-09-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Whereas most reference managers are either standalone applications and/or web-based, Zotero works as a Firefox plugin. This approach has many advantages, but is not for everybody. So today Zotero announced Zotero Everywhere:A browser pluging for Microsoft Internet Explorer,...]]></summary>
        <content type="html"><![CDATA[<p>Whereas most reference managers are either standalone applications and/or web-based, Zotero works as a Firefox plugin. This approach has many advantages, but is not for everybody. So today Zotero announced <a href="https://web.archive.org/web/20120603054912/http://www.zotero.org/blog/zoteros-next-big-step/">Zotero Everywhere</a>:</p>
<ul>
<li>A browser pluging for Microsoft Internet Explorer, Apple Safari and Google Chrome, and standalone desktop version for Windows, Mac and Linux</li>
<li>An expanded Zotero web API that gives developers full read and write access to all of Zotero’s features.</li>
</ul>
<p>Zotero Everywhere goes beyond allowing users to use Zotero with other browsers. The enhanced API will give developers the chance to build an infrastructure around Zotero, from mobile applications to integration with other services. No dates were given about the availability of Zotero Everywhere. The project is funded by the Andrew W. Mellon Foundation.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Letter to The Scientist]]></title>
        <id>38r4yfa-trn8k68-n3460p8-gdr35</id>
        <link href="https://blog.front-matter.io/mfenner/letter-to-the-scientist"/>
        <updated>2010-09-20T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Dear Scientist,last week you published an interesting article by <strong><strong>Christian Specht</strong></strong> about Mutations of citations. Dr. Specht found more than 600 wrong citations for the paper by Laemmli (Laemmli 1970),...]]></summary>
        <content type="html"><![CDATA[<p>Dear Scientist,</p>
<p>last week you published an interesting article by <strong><strong>Christian Specht</strong></strong> about <a href="https://web.archive.org/web/20120525054056/http://www.the-scientist.com/news/display/57689/">Mutations of citations</a>. Dr. Specht found more than 600 wrong citations for the paper by Laemmli (Laemmli 1970), which has been cited at least 88633 times according to Scopus.</p>
<p><strong><strong>Laemmli UK</strong></strong>. Cleavage of Structural Proteins during the Assembly of the Head of Bacteriophage T4. <em><em>Nature</em></em>. 1970;227:680-685. https://doi.org/<a href="https://web.archive.org/web/20120525054056/http://dx.doi.org/10.1038/227680a0">10.1038/227680a0</a></p>
<p>I was intrigued by the “sequence alignment” in Fig. 1a which clearly demonstrated that point mutations at 2<strong><strong>2</strong></strong>7 and 68<strong><strong>0</strong></strong> are particularly common, and that some mutations are inherited between overlapping groups of scientists. Of particular interest is the “complete nonsense mutation” that attributes the citation to the journal <em><em>Science</em></em>.</p>
<p>However, the author failed to demonstrate that the citation mutations had a paper-not-found phenotype or whether they were simply silent mutations. Missing is also an analysis of whether the mutations</p>
<ul>
<li>originated with the paper authors (who by now should all be using reference managers that automatically import citations),</li>
<li>were introduced by the publisher during manuscript production (many journals use tools such as <a href="https://web.archive.org/web/20120525054056/http://blogs.nature.com/mfenner/2009/05/01/extyles-interview-with-elizabeth-blake-and-bruce-rosenblum">eXtyles</a> to check and fix citations in manuscripts), or</li>
<li>first appeared in the scientific databases that stored the citations (Specht used Web of Science).</li>
</ul>
<p>Of particular interest would be whether there is a decrease in mutation rate over time, as automated tools have increased the fidelity of the citation process, and whether any citation style was particularly prone to mutations (no citation style uses checksums). As a researcher I suggest that the burden of proofreading should rest not with paper authors, and that journal and database publishers invest in appropriate citation repair mechanisms. And please use the DOI, even the paper by Laemmli (Laemmli 1970) has one.</p>
<h2 id="references">References</h2>
<p><strong><strong>Laemmli UK</strong></strong>. Cleavage of Structural Proteins during the Assembly of the Head of Bacteriophage T4. <em><em>Nature</em></em>. 1970;227:680-685. https://doi.org/<a href="https://web.archive.org/web/20120525054056/http://dx.doi.org/10.1038/227680a0">10.1038/227680a0</a></p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Update of Reference Manager Overview Chart]]></title>
        <id>2ndvmeg-86t94r9-zbmzndt-558jn</id>
        <link href="https://blog.front-matter.io/mfenner/update-of-reference-manager-overview-chart"/>
        <updated>2010-09-19T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[In March 2009 I posted an overview chart of how popular reference managers have implemented some important features. I’ve since updated this chart several times, and the chart probably has outgrown the format of a blog post....]]></summary>
        <content type="html"><![CDATA[<p>In <a href="https://web.archive.org/web/20120603140937/http://blogs.nature.com/mfenner/2009/03/15/reference-manager-overview">March 2009</a> I posted an overview chart of how popular reference managers have implemented some important features. I’ve since updated this chart several times, and the chart probably has outgrown the format of a blog post. For that reason I now made the chart available</p>
<ul>
<li>on a dedicated page with an easy to remember URL (<a href="https://web.archive.org/web/20120603140937/http://bit.ly/refman">http://bit.ly/refman</a>),</li>
<li>as PDF file for downloading and printing,</li>
<li>under a <a href="https://web.archive.org/web/20120603140937/http://creativecommons.org/licenses/by/3.0/">Creative Commons Attribution</a> license (like all content here on PLoS Blogs) for redistribution and reuse.</li>
</ul>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[And who are you?]]></title>
        <id>2xjaeap-vt39dea-nb6tz5r-pn7w0</id>
        <link href="https://blog.front-matter.io/mfenner/and-who-are-you"/>
        <updated>2010-09-17T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Dear reader,two weeks ago I moved this blog from Nature Network to <strong><strong>PLoS Blogs</strong></strong>. I hope that most of my old readers have followed me here, and that I might even have a few new readers.<em><em>Flickr image by grewlike.</em></em>Ed...]]></summary>
        <content type="html"><![CDATA[<p>Dear reader,</p>
<p>two weeks ago I moved this blog from <a href="https://web.archive.org/web/20120525131414/http://blogs.nature.com/mfenner">Nature Network</a> to <strong><strong>PLoS Blogs</strong></strong>. I hope that most of my old readers have followed me here, and that I might even have a few new readers.</p>
<figure>
<img src="https://web.archive.org/web/20120525131414im_/http://farm1.static.flickr.com/67/204929063_d90b9a9726.jpg" class="kg-image" />
</figure>
<p><em><em>Flickr image by grewlike.</em></em></p>
<p><a href="https://web.archive.org/web/20120525131414/http://blogs.discovermagazine.com/notrocketscience/">Ed Yong</a> recently <a href="https://web.archive.org/web/20120525131414/http://www.onemanandhisblog.com/archives/2010/09/science_online_bloggers_commenters_and_t.html">talked about how he interacts with his many, many blog readers</a>. One important strategy he uses is to ask his readers for feedback in regular intervals – he calls it delurking. AJ Cann has picked up the idea, and <a href="https://web.archive.org/web/20120525131414/http://www.microbiologybytes.com/blog/2010/09/15/you-like/">on Wednesday asked the readers of his blog MicrobiologyBytes</a> a few questions. I like them and have blatantly reused most of them:</p>
<ul>
<li>Who are you and what’s your background?</li>
<li>How long have you been reading Gobbledygook?</li>
<li>Where do you read Gobbledygook – website or RSS?</li>
<li>What do you like best about Gobbledygook?</li>
<li>How could I make Gobbledygook better for you?</li>
</ul>
<p>It would be kind if you could provide some feedback in the comments section below.</p>
]]></content>
        <author>
            <name>Martin Fenner</name>
            <uri>https://orcid.org/0000-0003-1419-2405</uri>
        </author>
    </entry>
</feed>